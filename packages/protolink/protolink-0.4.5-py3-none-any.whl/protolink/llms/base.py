import json
from abc import ABC, abstractmethod
from collections.abc import Iterable
from typing import Any, ClassVar

from protolink.core.part import Part
from protolink.llms.history import ConversationHistory
from protolink.llms.prompts import AGENT_LIST_PROMPT, BASE_INSTRUCTIONS, BASE_SYSTEM_PROMPT, TOOL_CALL_PROMPT
from protolink.tools import BaseTool
from protolink.types import LLMProvider, LLMType

MAX_INFER_STEPS: int = 10  # safety against infinite loops


class LLM(ABC):
    """
    Abstract base class for all Large Language Model (LLM) implementations.

    This class defines the core interface and shared functionality for any LLM,
    whether it is API-based (OpenAI, Anthropic, Gemini), server-based (Ollama) or local (LLaMA, MPT, etc.).

    Subclasses are expected to define:

    - `model_type` (ClassVar[LLMType]): Type of the LLM (i.e., "api", "server", "local").
    - `provider` (ClassVar[LLMProvider]): Name of the model provider (e.g., "openai").

    Instance variables:

    - `model` (str): The identifier of the model to use (e.g., "gpt-4o-mini").
    - `_model_params` (dict[str, Any]): Model-specific generation parameters. These
      vary depending on the provider. Examples include:
        - OpenAI: temperature, top_p, stop, max_tokens
        - Anthropic: temperature, top_p, max_tokens
        - Gemini: temperature, top_p, max_output_tokens
    - `history` (ConversationHistory): Tracks conversation messages for multi-turn
      interactions.
    - `system_prompt` (str): Optional system instructions used as context for the
      model when generating responses. Uses default prompts for agent, tool and llm calling.

    Usage:

        Subclasses should implement at least:
        - `call(history: ConversationHistory) -> str`: Blocking single-response generation.
        - `call_stream(history: ConversationHistory) -> Iterable[str]`: Streaming response generation.
        - `validate_connection() -> bool`: Optional, to verify API connectivity or model availability.

    Example:

        class OpenAILLM(APILLM):
            provider = "openai"
            model_type = "api"

            def call(self, history):
                ...
    """

    # Class-level metadata (set by subclasses)
    model_type: ClassVar[LLMType]
    provider: ClassVar[LLMProvider]

    def __init__(
        self,
        model: str,
        model_params: dict[str, Any],
    ) -> None:
        # ---- Instance state ----
        self.model: str = model
        self._model_params: dict[str, Any] = model_params

        self.history: ConversationHistory = ConversationHistory()
        self.system_prompt: str = self.build_system_prompt()

    # ----------------------------------------------------------------------
    # LLM calling (invocation)
    # ----------------------------------------------------------------------

    @abstractmethod
    def call(self, history: ConversationHistory) -> str:
        """Generate a response from the LLM.

        This is the core method that subclasses must implement to call their specific LLM (OpenAI, Anthropic, etc.).

        Args:
            history: Conversation history containing system, user, assistant, and tool messages

        Returns:
            str: Raw text response from the LLM
        """
        raise NotImplementedError

    @abstractmethod
    async def call_stream(self, history: ConversationHistory) -> Iterable[str]:
        """Generate a streaming response from the LLM.

        This method should yield string chunks as they are generated by the LLM API. Useful for real-time responses.

        Args:
            history: Conversation history containing system, user, assistant, and tool messages

        Yields:
            str: Streaming response chunks from the LLM
        """
        raise NotImplementedError

    def chat(self, user_query: str, *, streaming: bool = False) -> str | Iterable[str]:
        """
        High-level convenience method for standard chat usage.

        Args:
            user_query: The user's query/message
            streaming: If True, returns an iterator of response chunks

        Returns:
            str: Complete response if streaming=False
            Iterable[str]: Iterator of response chunks if streaming=True
        """
        self.history.add_user(user_query)
        if streaming:
            return self.call_stream(self.history)
        return self.call(self.history)

    # ----------------------------------------------------------------------
    # Agent-LLM Interface - A2A Operations
    # ----------------------------------------------------------------------

    async def infer(self, *, query: str, tools: dict[str, "BaseTool"], streaming: bool = False) -> "Part":
        """
        Execute a controlled, multi-step inference loop against the configured LLM.

        Args:
            query (str):
                The user-provided task or instruction to be processed by the agent.
            tools (dict[str, BaseTool]):
                A mapping of tool names to executable tool instances that the agent may invoke during inference.
            streaming (bool, optional):
                Whether to invoke the underlying LLM in streaming mode. Defaults to False.

        Returns:
            Part:
                A Part instance of type ``infer_output`` containing the final user-facing response produced by the agent

        Raises:
            RuntimeError:
                If the LLM call fails, a tool execution raises, the response cannot be parsed after repeated attempts,
                or the maximum number of inference steps is exceeded.
            ValueError:
                If the LLM declares an invalid action or references an unknown tool.

        Notes:

        This method implements a deterministic agent runtime over a stateless language model. The LLM is invoked
        iteratively to *declare intent only* using a strict JSON action protocol. Declared actions may include
        producing a final user-facing response, requesting execution of a registered tool, or delegating work to
        another agent.

        All side effects (tool execution, agent dispatch, state mutation) are performed by the runtime, never by
        the LLM itself. Tool results are serialized and injected back into the conversation context using valid LLM
        roles to maintain provider-agnostic compatibility.

        The loop enforces:
        - Strict JSON output validation
        - Explicit action typing
        - Bounded execution via a maximum step limit
        - Structured error propagation with step-level diagnostics

        The method terminates only when a valid `final` action is produced or when safety limits are exceeded,
        in which case a runtime error is raised.

        Returns a Part with PartType 'infer_output'.
        """

        self.history.add_user(query)

        steps: int = 0
        while steps < MAX_INFER_STEPS:
            steps += 1
            try:
                if streaming:
                    raw_response = await self.call_stream(self.history)
                else:
                    raw_response = self.call(self.history)
            except Exception as e:
                raise RuntimeError(f"LLM call failed at step {steps}: {e}") from e

            try:
                action, payload = self._parse_infer_response(raw_response)
            except ValueError as e:
                # optional: log or track invalid LLM output
                # here we retry the LLM a few times
                if steps < MAX_INFER_STEPS:
                    continue
                raise RuntimeError(f"Failed to parse LLM output after {steps} attempts: {e}") from e

            if action == "final":
                return Part("infer_output", payload["content"])
            elif action == "tool_call":
                tool_name = payload.get("tool")
                tool_args = payload.get("args", {})

                if not tool_name or tool_name not in tools:
                    raise ValueError(f"Unknown or missing tool: {tool_name}")

                tool = tools[tool_name]

                try:
                    tool_result = await tool(**tool_args)
                except Exception as e:
                    raise RuntimeError(f"Tool '{tool_name}' execution failed: {e}") from e

                # TODO: Examine tool call result and add to history
                # self.history.add_tool(content=tool_result, tool_name=tool_name)
                self.history.add_system(json.dumps({"type": "tool_result", "tool": tool_name, "result": tool_result}))

                continue

            elif action == "agent_call":
                # propagate agent_call upstream for router/dispatcher
                # TODO: implement agent_call
                pass
            else:
                raise ValueError(f"Unsupported action type: {action}")
        raise RuntimeError(f"Maximum inference steps ({MAX_INFER_STEPS}) exceeded without producing final response")

    def _parse_infer_response(self, response: str) -> tuple[str, dict[str, Any]]:
        """
        Parse, validate, and normalize a raw LLM response for agent execution.

        Args:
            response (str):
                The raw string output returned by the language model.

        Returns:
            tuple[str, dict[str, Any]]:
                A tuple containing:
                - The declared action type (e.g. ``final``, ``tool_call``, ``agent_call``)
                - A normalized payload dictionary for downstream execution

        Raises:
            ValueError:
                If the response is not valid JSON, does not declare a supported action, or is missing required fields
                for the declared action type.

        Notes:
        This function enforces a hard contract between the LLM and the runtime by requiring the response to be a single,
        well-formed JSON object declaring exactly one supported action. It validates both the structural integrity of
        the JSON payload and the semantic correctness of required fields for each action type.

        Unsupported actions, missing fields, or malformed JSON are rejected immediately with explicit errors, enabling
        robust retry, logging, or failure handling at the orchestration layer.

        The output of this function is guaranteed to be safe for downstream execution logic and free of implicit
        assumptions or provider-specific artifacts.
        """
        try:
            data = json.loads(response)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON: {e}\nRaw response: {response}") from e

        action = data.get("type")
        if action not in {"final", "tool_call", "agent_call"}:
            raise ValueError(f"Unsupported action type: {action}\nRaw response: {response}")

        if action == "final":
            content = data.get("content")
            if not isinstance(content, str):
                raise ValueError(f"Final response must have a 'content' string.\nRaw response: {response}")
            return action, {"content": content}

        # tool_call or agent_call
        if action in {"tool_call", "agent_call"} and not isinstance(data, dict):
            raise ValueError(f"{action} response must be a JSON object.\nRaw response: {response}")

        return action, data

    # ----------------------------------------------------------------------
    # Prompt management
    # ----------------------------------------------------------------------

    def build_system_prompt(
        self,
        user_instructions: str | None = None,
        agent_cards: str | None = None,
        tools: str | None = None,
        *,
        override_system_prompt: bool = False,
    ) -> str:
        """
        Build the final system prompt for the LLM.

        This function combines:
        - Base agent instructions
        - Tool calling prompt
        - Agent delegation prompt
        - User-provided instructions

        If any of the optional parameters are not provided, they will be omitted from the final prompt.

        Args:
            user_instructions: Optional instructions from the user to customize behavior.
            agent_cards: JSON/text describing available agents for delegation.
            tools: JSON/text describing available tools for this agent.
            override_system_prompt: Whether to override comletely the system prompt with the user defined prompt.

        Returns:
            A fully assembled, machine-readable prompt string suitable for sending to the LLM.

        Example:
            >>> user_instructions = "Always use the weather tool first if the user asks about weather."
            >>> agent_cards = '[{"name": "weather_forecaster", "tools": ["get_weather"]}]'
            >>> tools = '[{"name": "get_weather", "args": ["location"]}]'
            >>> prompt = build_system_prompt(user_instructions, agent_cards, tools)
            >>> print(prompt[:500])  # preview the first 500 characters
        """

        if override_system_prompt:
            self.system_prompt = user_instructions
        else:
            self.system_prompt = BASE_SYSTEM_PROMPT.format(
                base_instructions=BASE_INSTRUCTIONS,
                tool_call_prompt=TOOL_CALL_PROMPT.replace("{{tools}}", tools)
                if tools
                else "No tools are available for you to call. You cannot return a tool call response.",
                agent_call_prompt=AGENT_LIST_PROMPT.replace("{{agent_cards_from_registry}}", agent_cards)
                if agent_cards
                else "",
                user_instructions=user_instructions or "",
            )
        self.history.reset_to_system(self.system_prompt)
        return self.system_prompt

    # ----------------------------------------------------------------------
    # Utils
    # ----------------------------------------------------------------------

    @abstractmethod
    def validate_connection(self) -> bool:
        """Validate the connection to the LLM API, server, or local model. Should handle the logging."""
        raise NotImplementedError

    # ----------------------------------------------------------------------
    # Setter methods
    # ----------------------------------------------------------------------

    @property
    def model_params(self) -> dict[str, Any]:
        """
        Model/provider-specific generation parameters.
        """
        return self._model_params

    @model_params.setter
    def model_params(self, value: dict[str, Any]) -> None:
        """Model Params Setter method.
        Correct Usage Examples:
            llm.model_params["temperature"] = 0.2  # allowed
            llm.model_params = {"temperature": 0.3}  # validated
        """
        if not isinstance(value, dict):
            raise TypeError("model_params must be a dict")
        self._model_params = value

    def set_system_prompt(self, system_prompt: str) -> None:
        """Set the system prompt for the LLM.

        Overrides the default system prompt with a custom one.

        Args:
            system_prompt: New system prompt to use
        """
        self.system_prompt = system_prompt

    # ----------------------------------------------------------------------
    # Callable interface
    # ----------------------------------------------------------------------

    def __call__(self, history: ConversationHistory) -> str:
        """Make the LLM instance callable.

        Allows using the LLM as a function: llm(history) -> str

        Args:
            history: Conversation history to use

        Returns:
            str: Response from the LLM
        """
        return self.call(history)

    def __str__(self) -> str:
        """String representation of the LLM instance."""
        return f"{self.provider} {self.model_type}"

    def __repr__(self) -> str:
        """Detailed string representation of the LLM instance."""
        return self.__str__()
