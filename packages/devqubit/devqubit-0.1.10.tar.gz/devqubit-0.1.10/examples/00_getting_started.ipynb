{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# devqubit: Getting Started\n",
    "\n",
    "**devqubit** is an experiment tracker designed specifically for quantum computing workflows.\n",
    "\n",
    "### Why devqubit?\n",
    "\n",
    "Quantum experiments have unique challenges:\n",
    "\n",
    "| Challenge | How devqubit helps |\n",
    "|-----------|--------------------|\n",
    "| **Shot noise** | TVD-aware comparison that distinguishes real changes from statistical noise |\n",
    "| **Circuit artifacts** | Native support for Qiskit, Cirq, PennyLane, Braket circuit formats |\n",
    "| **Calibration drift** | Baseline verification to catch hardware changes |\n",
    "| **Reproducibility** | Content-addressed storage ensures exact artifact matching |\n",
    "| **Collaboration** | Portable bundles for sharing complete experiments |\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "devqubit organizes your work into:\n",
    "\n",
    "- **Runs**: A single experiment execution with parameters, metrics, and artifacts\n",
    "- **Projects**: A collection of related runs (e.g., \"vqe_hydrogen\", \"qml_classifier\")\n",
    "- **Groups**: Runs that belong together (e.g., a hyperparameter sweep)\n",
    "- **Baselines**: Reference runs for verification and comparison\n",
    "- **Bundles**: Portable packages for sharing runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "\n",
    "devqubit has two main storage components:\n",
    "\n",
    "```python\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                      Workspace (default: ~/.devqubit)                       │\n",
    "│                                                                             │\n",
    "│  ┌─────────────────────────┐    ┌─────────────────────────────────────────┐ │\n",
    "│  │        Registry         │    │              Object Store               │ │\n",
    "│  │      (registry.db)      │    │               (objects/)                │ │\n",
    "│  │                         │    │                                         │ │\n",
    "│  │  • Run metadata         │    │  • ExecutionEnvelopes (UEC JSON)        │ │\n",
    "│  │  • Params & Metrics     │    │  • Circuits (QASM, QPY, JSON)           │ │\n",
    "│  │  • Tags                 │    │  • Count distributions                  │ │\n",
    "│  │  • Artifact references ─────▶│  • Device snapshots                     │ │\n",
    "│  │  • Baselines            │    │  • Custom artifacts                     │ │\n",
    "│  │  • Groups               │    │                                         │ │\n",
    "│  │                         │    │  Content-addressed by SHA-256 (deduped) │ │\n",
    "│  └─────────────────────────┘    └─────────────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Registry**: SQLite database for fast queries (list runs, search, get baseline)  \n",
    "**Object Store**: Content-addressed file storage (artifacts are deduplicated by hash)\n",
    "\n",
    "### ExecutionEnvelope (UEC)\n",
    "\n",
    "The **Uniform Execution Contract (UEC)** is devqubit's standardized format for capturing complete execution context. When you use `run.wrap()`, adapters automatically create an **ExecutionEnvelope** containing:\n",
    "\n",
    "```python\n",
    "┌────────────────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                                     ExecutionEnvelope                                      │\n",
    "│                              (stored in Object Store as JSON)                              │\n",
    "├────────────────────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                                            │\n",
    "│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │\n",
    "│  │   producer   │  │   program    │  │    device    │  │  execution   │  │    result    │  │\n",
    "│  │              │  │              │  │              │  │              │  │              │  │\n",
    "│  │ • adapter    │  │ • logical    │  │ • backend    │  │ • shots      │  │ • success    │  │\n",
    "│  │ • sdk        │  │ • physical   │  │ • qubits     │  │ • job_ids    │  │ • status     │  │\n",
    "│  │ • versions   │  │ • hashes     │  │ • topology   │  │ • timestamps │  │ • counts     │  │\n",
    "│  │              │  │              │  │ • calibration│  │ • options    │  │ • errors     │  │\n",
    "│  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘  │\n",
    "│                           │                   │                                 │          │\n",
    "│                           └───────────────────┴─────────────────────────────────┘          │\n",
    "│                                          ArtifactRefs → Object Store                       │\n",
    "└────────────────────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Why Envelopes Matter\n",
    "\n",
    "| Use Case | How Envelope Helps |\n",
    "|----------|-------------------|\n",
    "| **Reproducibility** | Captures exact circuit, device state, and settings |\n",
    "| **Comparison** | Structural hashes enable fast program matching |\n",
    "| **Verification** | TVD computed from normalized result counts |\n",
    "| **Debugging** | Complete context when something goes wrong |\n",
    "| **Calibration tracking** | Device snapshot includes calibration data |\n",
    "\n",
    "This is why `run.wrap()` is powerful - it gives devqubit everything needed for robust comparison and verification.\n",
    "\n",
    "\n",
    "### What you'll learn in this notebook:\n",
    "1. **Configuration**: Set up your workspace\n",
    "2. **Tracking**: Log parameters, metrics, tags, and artifacts\n",
    "3. **Inspection**: Query and load runs from the registry\n",
    "4. **Grouping**: Organize related runs (sweeps, experiments)\n",
    "5. **Comparison**: Diff runs to see what changed\n",
    "6. **Verification**: Check candidates against baselines\n",
    "7. **Bundling**: Package runs for sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-intro",
   "metadata": {},
   "source": [
    "### 1. Configuration & Setup\n",
    "\n",
    "devqubit needs to know where to store data. You have three options:\n",
    "\n",
    "#### Option A: Default location\n",
    "```python\n",
    "# Uses ~/.devqubit automatically\n",
    "from devqubit import track\n",
    "with track(project=\"my_project\") as run:\n",
    "    ...\n",
    "```\n",
    "\n",
    "#### Option B: Environment variable\n",
    "```bash\n",
    "export DEVQUBIT_HOME=/path/to/workspace\n",
    "```\n",
    "\n",
    "#### Option C: Programmatic configuration\n",
    "```python\n",
    "from devqubit import Config, set_config\n",
    "set_config(Config(root_dir=\"/path/to/workspace\"))\n",
    "```\n",
    "\n",
    "For this tutorial, we'll use Option C to create an isolated demo workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Core devqubit imports\n",
    "from devqubit import (\n",
    "    track,  # Main context manager for runs\n",
    "    Config,  # Configuration container\n",
    "    set_config,  # Set global configuration\n",
    ")\n",
    "\n",
    "# Runs navigation and baseline management\n",
    "from devqubit.runs import (\n",
    "    load_run,\n",
    "    list_runs,\n",
    "    list_groups,\n",
    "    list_runs_in_group,\n",
    "    set_baseline,\n",
    "    get_baseline,\n",
    ")\n",
    "\n",
    "# Storage functions\n",
    "from devqubit.storage import create_store, create_registry\n",
    "\n",
    "# Bundle utilities\n",
    "from devqubit.bundle import pack_run, unpack_bundle, Bundle\n",
    "\n",
    "# Comparison and verification\n",
    "from devqubit.compare import diff, verify_baseline, VerifyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an isolated demo workspace\n",
    "WORKSPACE = Path(\".devqubit_getting_started\")\n",
    "if WORKSPACE.exists():\n",
    "    shutil.rmtree(WORKSPACE)\n",
    "\n",
    "# Set global config — all devqubit calls will use this workspace\n",
    "set_config(Config(root_dir=WORKSPACE))\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Workspace created: {WORKSPACE.resolve()}\")\n",
    "print(f\"Registry: {WORKSPACE / 'registry.db'}\")\n",
    "print(f\"Objects:  {WORKSPACE / 'objects'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-intro",
   "metadata": {},
   "source": [
    "### 2. Creating Your First Run\n",
    "\n",
    "The `track()` context manager is your main entry point. It:\n",
    "\n",
    "1. Creates a new run with a unique ID\n",
    "2. Provides methods to log data\n",
    "3. Automatically finalizes the run when the block exits\n",
    "4. Handles errors gracefully (marks run as failed)\n",
    "\n",
    "#### What you can log:\n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|--------|\n",
    "| `log_param(key, value)` | Single parameter | `run.log_param(\"lr\", 0.01)` |\n",
    "| `log_params(dict)` | Multiple parameters | `run.log_params({\"lr\": 0.01, \"epochs\": 100})` |\n",
    "| `log_metric(key, value)` | Single metric | `run.log_metric(\"accuracy\", 0.95)` |\n",
    "| `log_metric(key, value, step=n)` | Time-series metric | `run.log_metric(\"loss\", 0.5, step=10)` |\n",
    "| `log_metrics(dict)` | Multiple metrics | `run.log_metrics({\"acc\": 0.95, \"f1\": 0.92})` |\n",
    "| `set_tag(key, value)` | Single tag | `run.set_tag(\"env\", \"production\")` |\n",
    "| `set_tags(dict)` | Multiple tags | `run.set_tags({\"env\": \"prod\", \"version\": \"v2\"})` |\n",
    "| `log_json(name, obj)` | JSON artifact | `run.log_json(\"config\", {\"model\": \"cnn\"})` |\n",
    "| `log_text(name, text)` | Text artifact | `run.log_text(\"notes\", \"Experiment worked!\")` |\n",
    "| `log_binary(name, data)` | Binary artifact | `run.log_binary(\"weights\", bytes_data)` |\n",
    "\n",
    "#### Parameters vs Metrics vs Tags\n",
    "\n",
    "- **Parameters**: Configuration that defines the experiment (inputs)\n",
    "- **Metrics**: Results you measure (outputs)\n",
    "- **Tags**: Labels for organization and filtering (metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mock-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple mock experiment (simulates an optimization)\n",
    "def mock_optimization(lr: float, iterations: int, seed: int) -> list[float]:\n",
    "    \"\"\"Simulate optimization that returns loss history.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    loss = 1.0\n",
    "    history = []\n",
    "    for _ in range(iterations):\n",
    "        # Loss decreases with some randomness\n",
    "        loss *= 1 - lr * rng.uniform(0.8, 1.0)\n",
    "        history.append(float(loss))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"optimization_demo\"\n",
    "\n",
    "with track(project=PROJECT, run_name=\"my_first_run\") as run:\n",
    "\n",
    "    # 1. Log parameters (the inputs to your experiment)\n",
    "    run.log_params(\n",
    "        {\n",
    "            \"learning_rate\": 0.10,\n",
    "            \"iterations\": 50,\n",
    "            \"seed\": 42,\n",
    "            \"optimizer\": \"sgd\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2. Set tags (for organization)\n",
    "    run.set_tags(\n",
    "        {\n",
    "            \"environment\": \"development\",\n",
    "            \"role\": \"baseline\",\n",
    "            \"author\": \"tutorial\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 3. Run the experiment\n",
    "    history = mock_optimization(lr=0.10, iterations=50, seed=42)\n",
    "\n",
    "    # 4. Log time-series metrics (one value per step)\n",
    "    for step, loss in enumerate(history):\n",
    "        run.log_metric(\"loss\", loss, step=step)\n",
    "\n",
    "    # 5. Log summary metrics (final values)\n",
    "    run.log_metrics(\n",
    "        {\n",
    "            \"final_loss\": history[-1],\n",
    "            \"best_loss\": min(history),\n",
    "            \"convergence_step\": history.index(min(history)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 6. Log artifacts (structured data)\n",
    "    run.log_json(\n",
    "        \"full_config\",\n",
    "        {\n",
    "            \"lr\": 0.10,\n",
    "            \"iterations\": 50,\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"description\": \"Baseline optimization run\",\n",
    "        },\n",
    "        role=\"config\",\n",
    "    )\n",
    "\n",
    "    run.log_json(\n",
    "        \"loss_history\",\n",
    "        {\n",
    "            \"values\": history,\n",
    "            \"length\": len(history),\n",
    "        },\n",
    "        role=\"result\",\n",
    "    )\n",
    "\n",
    "    # Save run ID for later use\n",
    "    first_run_id = run.run_id\n",
    "\n",
    "print(\"Run completed!\")\n",
    "print(f\"  Run ID: {first_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-id-explanation",
   "metadata": {},
   "source": [
    "#### Understanding Run IDs\n",
    "\n",
    "Every run gets a unique ID (UUID). This ID:\n",
    "- Is globally unique across all workspaces\n",
    "- Never changes after creation\n",
    "- Can be used to load, compare, or reference the run later\n",
    "\n",
    "You can also provide a human-readable `run_name` for easier identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-intro",
   "metadata": {},
   "source": [
    "### 3. Inspecting Runs\n",
    "\n",
    "After a run completes, you can load it from the registry to inspect its data.\n",
    "\n",
    "The `RunRecord` object contains:\n",
    "- `run_id`: Unique identifier\n",
    "- `project`: Project name\n",
    "- `run_name`: Human-readable name (optional)\n",
    "- `status`: \"completed\", \"failed\", or \"running\"\n",
    "- `params`: Dictionary of parameters\n",
    "- `metrics`: Dictionary of metrics\n",
    "- `tags`: Dictionary of tags\n",
    "- `artifacts`: List of artifact references\n",
    "- `started_at`, `ended_at`: Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the run we just created\n",
    "record = load_run(first_run_id)\n",
    "\n",
    "print(\"Run Details\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Run ID:   {record.run_id}\")\n",
    "print(f\"Name:     {record.run_name}\")\n",
    "print(f\"Project:  {record.project}\")\n",
    "print(f\"Status:   {record.status}\")\n",
    "print(f\"Started:  {record.created_at}\")\n",
    "print(f\"Ended:    {record.ended_at}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nParameters:\")\n",
    "for key, value in record.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMetrics:\")\n",
    "for key, value in record.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-tags",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTags:\")\n",
    "for key, value in record.tags.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nArtifacts:\")\n",
    "for artifact in record.artifacts:\n",
    "    print(f\"    Role: {artifact.role}\")\n",
    "    print(f\"    Digest: {artifact.digest[:24]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-intro",
   "metadata": {},
   "source": [
    "### 4. Listing and Querying Runs\n",
    "\n",
    "The registry provides methods to find runs:\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| `list_runs(project=...)` | All runs in a project |\n",
    "| `list_runs(limit=10)` | Recent runs (with limit) |\n",
    "| `list_groups()` | All experiment groups |\n",
    "| `list_runs_in_group(group_id)` | Runs in a specific group |\n",
    "| `get_baseline(project)` | Current baseline for project |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-runs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all runs in our project\n",
    "print(\"Runs in project:\")\n",
    "for run_info in list_runs(project=PROJECT):\n",
    "    print(f\"  {run_info['run_id'][:12]}...  ({run_info.get('run_name', 'unnamed')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-intro",
   "metadata": {},
   "source": [
    "### 5. Grouped Runs (Sweeps & Experiments)\n",
    "\n",
    "When running hyperparameter sweeps or related experiments, use **groups** to organize them:\n",
    "\n",
    "```python\n",
    "with track(\n",
    "    project=\"my_project\",\n",
    "    group_id=\"sweep_001\",        # Unique identifier for the group\n",
    "    group_name=\"Learning Rate Sweep\",  # Human-readable name\n",
    ") as run:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- Query all runs in a group together\n",
    "- Compare runs within a sweep\n",
    "- Track which runs belong to which experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a learning rate sweep\n",
    "learning_rates = [0.05, 0.10, 0.15, 0.20]\n",
    "sweep_results = []\n",
    "\n",
    "print(\"Running Learning Rate Sweep\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    with track(\n",
    "        project=PROJECT,\n",
    "        group_id=\"lr_sweep_001\",  # All runs share this group ID\n",
    "        group_name=\"Learning Rate Sweep\",  # Human-readable name\n",
    "    ) as run:\n",
    "        # Log parameters\n",
    "        run.log_params(\n",
    "            {\n",
    "                \"learning_rate\": lr,\n",
    "                \"iterations\": 50,\n",
    "                \"seed\": 42,\n",
    "            }\n",
    "        )\n",
    "        run.set_tag(\"role\", \"sweep\")\n",
    "\n",
    "        # Run experiment\n",
    "        history = mock_optimization(lr=lr, iterations=50, seed=42)\n",
    "\n",
    "        # Log results\n",
    "        run.log_metrics(\n",
    "            {\n",
    "                \"final_loss\": history[-1],\n",
    "                \"best_loss\": min(history),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        sweep_results.append(\n",
    "            {\n",
    "                \"lr\": lr,\n",
    "                \"final_loss\": history[-1],\n",
    "                \"run_id\": run.run_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"  lr={lr:.2f}  =>  loss={history[-1]:.6f}\")\n",
    "\n",
    "# Find the best\n",
    "best = min(sweep_results, key=lambda x: x[\"final_loss\"])\n",
    "print(f\"\\nBest: lr={best['lr']:.2f} (loss={best['final_loss']:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query groups\n",
    "print(\"Experiment Groups:\")\n",
    "for group in list_groups():\n",
    "    print(f\"\\n  {group['group_name']}\")\n",
    "    print(f\"  ID: {group['group_id']}\")\n",
    "\n",
    "    # List runs in this group\n",
    "    runs = list_runs_in_group(group[\"group_id\"])\n",
    "    print(f\"  Runs: {len(runs)}\")\n",
    "\n",
    "    for run_info in runs:\n",
    "        rec = load_run(run_info[\"run_id\"])\n",
    "        lr = rec.params.get(\"learning_rate\", \"?\")\n",
    "        loss = rec.metrics.get(\"final_loss\", \"?\")\n",
    "        print(f\"    lr={lr}  loss={loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-intro",
   "metadata": {},
   "source": [
    "### 6. Comparing Runs\n",
    "\n",
    "The `diff()` function compares two runs and shows:\n",
    "\n",
    "- **Parameter differences**: What configuration changed\n",
    "- **Metric differences**: How results differ\n",
    "- **Tag differences**: Metadata changes\n",
    "- **TVD**: Distribution distance (for quantum count artifacts)\n",
    "\n",
    "This is essential for understanding:\n",
    "- Why one run performed better than another\n",
    "- What changed between experiments\n",
    "- Whether a new run is statistically similar to a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a candidate run with different parameters\n",
    "with track(project=PROJECT, run_name=\"candidate\") as run:\n",
    "    run.log_params(\n",
    "        {\n",
    "            \"learning_rate\": 0.12,  # Different from baseline (0.10)\n",
    "            \"iterations\": 50,\n",
    "            \"seed\": 42,\n",
    "            \"optimizer\": \"adam\",  # Different optimizer\n",
    "        }\n",
    "    )\n",
    "    run.set_tags({\"role\": \"candidate\", \"environment\": \"development\"})\n",
    "\n",
    "    history = mock_optimization(lr=0.12, iterations=50, seed=42)\n",
    "\n",
    "    run.log_metrics(\n",
    "        {\n",
    "            \"final_loss\": history[-1],\n",
    "            \"best_loss\": min(history),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    candidate_id = run.run_id\n",
    "\n",
    "print(f\"Candidate run: {candidate_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diff-runs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs candidate\n",
    "comparison = diff(first_run_id, candidate_id)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diff-interpretation",
   "metadata": {},
   "source": [
    "### Interpreting the Diff\n",
    "\n",
    "The diff shows:\n",
    "- `<` values from the first run (baseline)\n",
    "- `>` values from the second run (candidate)\n",
    "- Changes in parameters, metrics, and other fields\n",
    "\n",
    "For quantum experiments with count artifacts, you'd also see **TVD (Total Variation Distance)** — a measure of how different the probability distributions are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tvd-explanation",
   "metadata": {},
   "source": [
    "### Understanding TVD (Total Variation Distance)\n",
    "\n",
    "When comparing quantum measurement results, **TVD** quantifies how different two probability distributions are:\n",
    "\n",
    "$\\text{TVD}(P, Q) = \\frac{1}{2} \\sum_x |P(x) - Q(x)|$\n",
    "\n",
    "**Interpretation:**\n",
    "- **TVD = 0**: Distributions are identical\n",
    "- **TVD = 0.01-0.05**: Typically shot noise — expected variation\n",
    "- **TVD = 0.05-0.15**: May indicate real change (investigate)\n",
    "- **TVD > 0.15**: Likely significant difference (code, calibration, or hardware)\n",
    "\n",
    "**Why this matters for quantum:**\n",
    "\n",
    "Quantum measurements are inherently probabilistic. Running the same circuit twice gives different counts due to shot noise. TVD helps distinguish:\n",
    "\n",
    "| Scenario | Expected TVD | Action |\n",
    "|----------|-------------|--------|\n",
    "| Same circuit, same backend, different seeds | Low (0.01-0.03) | Normal, pass |\n",
    "| Same circuit, different calibration | Medium (0.05-0.10) | Warning, investigate |\n",
    "| Different circuit or bug | High (0.15+) | Fail, investigate |\n",
    "\n",
    "devqubit uses TVD-aware verification to avoid false alarms from shot noise while catching real changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-intro",
   "metadata": {},
   "source": [
    "### 7. Baselines and Verification\n",
    "\n",
    "**Baselines** are reference runs that represent \"known good\" behavior. They're essential for:\n",
    "\n",
    "- **CI/CD pipelines**: Verify new code doesn't break experiments\n",
    "- **Calibration monitoring**: Detect hardware drift\n",
    "- **Regression testing**: Ensure reproducibility\n",
    "\n",
    "#### Workflow:\n",
    "\n",
    "1. Run experiment → get good results\n",
    "2. Set that run as the **baseline** for the project\n",
    "3. Later, run a **candidate** (new code, different time, etc.)\n",
    "4. **Verify** the candidate against the baseline\n",
    "\n",
    "#### VerifyPolicy options:\n",
    "\n",
    "| Option | Purpose |\n",
    "|--------|--------|\n",
    "| `params_must_match` | Require identical parameters |\n",
    "| `program_must_match` | Require identical circuit/program |\n",
    "| `tvd_max` | Maximum allowed distribution distance |\n",
    "| `allow_missing_baseline` | Pass if no baseline exists |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "set-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our first run as the baseline for this project\n",
    "set_baseline(PROJECT, first_run_id)\n",
    "\n",
    "baseline_info = get_baseline(PROJECT)\n",
    "print(f\"Baseline set for project '{PROJECT}'\")\n",
    "print(f\"  Run ID: {baseline_info['run_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-strict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify candidate with STRICT policy (params must match)\n",
    "strict_policy = VerifyPolicy(\n",
    "    params_must_match=True,  # Require same parameters\n",
    "    program_must_match=False,  # No circuit in this demo\n",
    "    tvd_max=0.10,\n",
    ")\n",
    "\n",
    "result = verify_baseline(candidate_id, project=PROJECT, policy=strict_policy)\n",
    "\n",
    "print(\"Strict Verification (params_must_match=True)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Result: {'[OK] PASSED' if result.ok else '[X] FAILED'}\")\n",
    "\n",
    "if not result.ok:\n",
    "    print(\"\\nFailures:\")\n",
    "    for failure in result.failures:\n",
    "        print(f\"  {failure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-relaxed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify candidate with RELAXED policy (params can differ)\n",
    "relaxed_policy = VerifyPolicy(\n",
    "    params_must_match=False,  # Allow different parameters\n",
    "    program_must_match=False,\n",
    "    tvd_max=0.10,\n",
    ")\n",
    "\n",
    "result = verify_baseline(candidate_id, project=PROJECT, policy=relaxed_policy)\n",
    "\n",
    "print(\"Relaxed Verification (params_must_match=False)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Result: {'[OK] PASSED' if result.ok else '[X] FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-intro",
   "metadata": {},
   "source": [
    "### 8. Bundles: Portable Run Sharing\n",
    "\n",
    "**Bundles** are self-contained ZIP files that package a run with all its artifacts.\n",
    "\n",
    "#### Use cases:\n",
    "\n",
    "- **Share with collaborators**: Email a bundle, they can unpack and inspect\n",
    "- **Attach to papers**: Include bundle as supplementary material\n",
    "- **Archive experiments**: Long-term storage with all artifacts\n",
    "- **Transfer between systems**: Move runs between workspaces\n",
    "\n",
    "#### Bundle contents:\n",
    "\n",
    "```\n",
    "experiment.devqubit.zip\n",
    "├── manifest.json       # Bundle metadata\n",
    "├── run.json            # Run record (params, metrics, tags)\n",
    "└── objects/            # Content-addressed artifacts\n",
    "    ├── sha256:abc123...\n",
    "    ├── sha256:def456...\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pack-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the baseline run into a bundle\n",
    "bundle_path = WORKSPACE / \"baseline_run.devqubit.zip\"\n",
    "\n",
    "pack_result = pack_run(first_run_id, bundle_path)\n",
    "\n",
    "print(\"Bundle Created\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Path:      {bundle_path}\")\n",
    "print(f\"Size:      {bundle_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"Artifacts: {pack_result.artifact_count}\")\n",
    "print(f\"Objects:   {pack_result.object_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect bundle without extracting\n",
    "with Bundle(bundle_path) as bundle:\n",
    "    print(\"Bundle Contents\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Run ID:  {bundle.run_id}\")\n",
    "    print(f\"Project: {bundle.get_project()}\")\n",
    "    print(f\"Objects: {len(bundle.list_objects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unpack-description",
   "metadata": {},
   "source": [
    "### Unpacking Bundles\n",
    "\n",
    "When someone receives a bundle, they can unpack it into their own workspace.\n",
    "\n",
    "The run gets imported with:\n",
    "- Same run ID (preserved for reference)\n",
    "- All parameters, metrics, tags\n",
    "- All artifacts (stored in their object store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unpack-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new workspace (simulating a collaborator's machine)\n",
    "collaborator_workspace = WORKSPACE / \"collaborator\"\n",
    "collab_config = Config(root_dir=collaborator_workspace)\n",
    "collab_store = create_store(config=collab_config)\n",
    "collab_registry = create_registry(config=collab_config)\n",
    "\n",
    "# Unpack the bundle\n",
    "unpack_result = unpack_bundle(\n",
    "    bundle_path,\n",
    "    dest_store=collab_store,\n",
    "    dest_registry=collab_registry,\n",
    ")\n",
    "\n",
    "print(\"Bundle Unpacked\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Workspace: {collaborator_workspace}\")\n",
    "print(f\"Run ID:    {unpack_result.run_id}\")\n",
    "print(f\"Artifacts: {unpack_result.artifact_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-unpack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the unpacked run matches the original\n",
    "original = load_run(first_run_id)\n",
    "imported = collab_registry.load(unpack_result.run_id)\n",
    "\n",
    "print(\"Integrity Check\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Run IDs match: {original.run_id == imported.run_id}\")\n",
    "print(f\"Params match:  {original.params == imported.params}\")\n",
    "print(f\"Metrics match: {original.metrics == imported.metrics}\")\n",
    "print(f\"Tags match:    {original.tags == imported.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-intro",
   "metadata": {},
   "source": [
    "### 9. Backend Wrapping\n",
    "\n",
    "When working with real quantum backends (Qiskit, Cirq, PennyLane, Braket), devqubit can **wrap** the backend to automatically capture:\n",
    "\n",
    "- Executed circuits\n",
    "- Measurement counts\n",
    "- Job metadata\n",
    "\n",
    "```python\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "with track(project=\"quantum_experiment\") as run:\n",
    "    # Wrap the backend — devqubit intercepts all circuit executions\n",
    "    backend = run.wrap(AerSimulator())\n",
    "    \n",
    "    # Use the wrapped backend normally\n",
    "    job = backend.run(circuit, shots=1000)\n",
    "    counts = job.result().get_counts()\n",
    "    \n",
    "    # Circuit and counts are automatically stored as artifacts!\n",
    "```\n",
    "\n",
    "See the SDK-specific notebooks (01-05) for complete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e793c",
   "metadata": {},
   "source": [
    "### 9. What Happens Under the Hood\n",
    "\n",
    "When you use devqubit, several things happen automatically. Understanding this helps you use it more effectively.\n",
    "\n",
    "#### When you call `track()`:\n",
    "\n",
    "There are two paths depending on whether you use an adapter:\n",
    "\n",
    "**Path A: With adapter (`run.wrap()`)**\n",
    "```python\n",
    "with track(project=\"my_project\") as run:\n",
    "    run.log_params({...})               # Params stored in memory\n",
    "    \n",
    "    backend = run.wrap(AerSimulator())  # Adapter wraps backend\n",
    "    job = backend.run(circuit)          # Adapter captures: circuit, device, options\n",
    "    counts = job.result().get_counts()  # Adapter captures: results, timestamps\n",
    "    \n",
    "# On exit:\n",
    "# → Adapter builds FULL ExecutionEnvelope:\n",
    "#   - producer (adapter name, SDK version)\n",
    "#   - program (logical + physical circuits, hashes)\n",
    "#   - device (backend, qubits, calibration)\n",
    "#   - execution (shots, job_ids, timestamps)\n",
    "#   - result (counts, success status)\n",
    "# → Envelope validated against UEC schema\n",
    "# → Envelope + artifacts stored in Object Store\n",
    "# → Run record finalized in Registry\n",
    "```\n",
    "\n",
    "**Path B: Manual tracking (no adapter)**\n",
    "```python\n",
    "with track(project=\"my_project\") as run:\n",
    "    run.log_params({...})\n",
    "    run.log_metric(...)\n",
    "    run.log_json(\"my_results\", {...})   # Your artifacts\n",
    "    \n",
    "    # No run.wrap() — you manage execution yourself\n",
    "    \n",
    "# On exit:\n",
    "# → Engine synthesizes MINIMAL ExecutionEnvelope:\n",
    "#   - producer (adapter: \"manual\")\n",
    "#   - result (success: true/false based on exceptions)\n",
    "#   - NO program/device/execution snapshots\n",
    "# → Your artifacts stored in Object Store\n",
    "# → Run record finalized in Registry\n",
    "```\n",
    "\n",
    "**Recommendation:** Use `run.wrap()` when possible — it gives devqubit everything needed for robust comparison and verification.\n",
    "\n",
    "#### Content-Addressed Storage\n",
    "\n",
    "All artifacts (including ExecutionEnvelopes) are stored by their **SHA-256 hash**:\n",
    "```\n",
    "objects/\n",
    "├── sha256:a1b2c3...  ← ExecutionEnvelope JSON\n",
    "├── sha256:d4e5f6...  ← circuit.qpy (referenced by envelope)\n",
    "├── sha256:g7h8i9...  ← raw_result.json (referenced by envelope)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- **Deduplication**: Same circuit run twice → stored once\n",
    "- **Integrity**: Hash verifies content hasn't changed\n",
    "- **Portability**: Bundles include only referenced objects\n",
    "- **Fast comparison**: Compare hashes instead of full content\n",
    "\n",
    "#### How Fingerprints Work\n",
    "\n",
    "ExecutionEnvelope contains pre-computed hashes for fast comparison:\n",
    "\n",
    "| Fingerprint | Source | Use Case |\n",
    "|-------------|--------|----------|\n",
    "| `structural_hash` | Circuit structure (gates, qubits) | \"Is this the same algorithm?\" |\n",
    "| `parametric_hash` | Structure + bound parameters | \"Is this the exact same circuit?\" |\n",
    "| `executed_structural_hash` | Transpiled circuit structure | \"Did transpilation change?\" |\n",
    "\n",
    "These enable `diff()` and `verify_baseline()` to compare runs without loading full circuit data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-summary",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Explore the SDK-specific notebooks:\n",
    "\n",
    "| Notebook | SDK | Focus |\n",
    "|----------|-----|-------|\n",
    "| **01_vqe_optimization_qiskit** | Qiskit | VQE with sweeps and verification |\n",
    "| **02_qml_classification_pennylane** | PennyLane | QML training with epoch metrics |\n",
    "| **03_noise_analysis_cirq** | Cirq | Noise model comparison |\n",
    "| **04_bundle_sharing_braket** | Braket | Cross-workspace bundle workflow |\n",
    "| **05_drift_detection_runtime** | Runtime | Production monitoring with CI/CD |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up demo workspace\n",
    "shutil.rmtree(WORKSPACE)\n",
    "print(\"Demo workspace cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43677c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
