"""
APE Intent Compiler

Agent Policy Engine
Version v1.0.1
https://github.com/kahalewai/agent-policy-engine

Per architecture spec (v1.0.1 §6.2 Intent Compiler):
- The Intent Compiler transforms natural language prompts into structured Intent objects
- Prompts guide intent but never are intent
- Only actions in the Action Repository can appear in compiled intents
- Policy narrowing ensures compliance before execution

Core Principle:
    "Prompts guide intent, but never are intent."
    
    Intent gets generated by compiling the user prompt through an APE-provided
    intent compiler that maps natural language onto a bounded, policy-constrained
    set of known actions; the result—not the prompt—is what APE enforces.

The Intent Compiler provides:
1. A one-call API from prompt → Intent
2. Policy-constrained narrowing (only allowed actions pass through)
3. Risk-based filtering (exclude high-risk actions unless explicitly needed)
4. Semantic mapping (natural language → action_ids)
5. Scope inference (what boundaries apply)
6. Clear audit trail (why these actions were selected)

Failure behavior: reject ambiguous prompts, reject empty narrowing results
"""

import re
import hashlib
import json
from typing import Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum

from ape.action_repository import (
    ActionRepository,
    ActionDefinition,
    ActionCategory,
    ActionRiskLevel,
)
from ape.errors import (
    IntentCompilationError,
    IntentNarrowingError,
    IntentAmbiguityError,
)


@dataclass
class IntentSignal:
    """
    A signal extracted from the user prompt that maps to an action.
    
    Signals capture the semantic meaning of what the user wants to do.
    They include the triggering phrase, candidate actions, and confidence.
    """
    phrase: str                    # The phrase that triggered this signal
    action_ids: list[str]          # Candidate action IDs
    confidence: float              # 0.0 to 1.0
    category: Optional[ActionCategory] = None
    parameters_hint: dict[str, Any] = field(default_factory=dict)


@dataclass
class CompiledIntent:
    """
    The result of compiling a prompt into an APE Intent.
    
    This is the structured, machine-readable representation that
    APE will enforce. It includes:
    - allowed_actions: Actions the user intends to perform
    - forbidden_actions: Actions explicitly prohibited
    - escalation_required: Actions needing human approval
    - scope: The operational boundary
    
    Plus audit information:
    - original_prompt: What the user said
    - signals: What we extracted
    - narrowing_applied: How policy constrained the result
    """
    # Core intent fields (APE-compatible)
    allowed_actions: list[str]
    forbidden_actions: list[str]
    scope: str
    escalation_required: list[str] = field(default_factory=list)
    description: Optional[str] = None
    
    # Audit fields
    original_prompt: str = ""
    signals: list[IntentSignal] = field(default_factory=list)
    narrowing_log: list[str] = field(default_factory=list)
    risk_flags: list[str] = field(default_factory=list)
    confidence: float = 0.0
    
    def to_ape_intent(self) -> dict[str, Any]:
        """
        Convert to APE IntentManager-compatible dictionary.
        
        This is what you pass to intent_manager.set()
        """
        result = {
            "allowed_actions": self.allowed_actions,
            "forbidden_actions": self.forbidden_actions,
            "scope": self.scope,
        }
        if self.escalation_required:
            result["escalation_required"] = self.escalation_required
        if self.description:
            result["description"] = self.description
        return result
    
    def to_audit_dict(self) -> dict[str, Any]:
        """Get full audit representation."""
        return {
            "intent": self.to_ape_intent(),
            "original_prompt": self.original_prompt,
            "signals": [
                {
                    "phrase": s.phrase,
                    "actions": s.action_ids,
                    "confidence": s.confidence,
                }
                for s in self.signals
            ],
            "narrowing_log": self.narrowing_log,
            "risk_flags": self.risk_flags,
            "confidence": self.confidence,
        }


# =============================================================================
# Semantic Patterns
# =============================================================================

# These patterns map natural language phrases to action categories and IDs.
# They're ordered by specificity (most specific first).

SEMANTIC_PATTERNS: list[tuple[str, list[str], ActionCategory, float]] = [
    # File Read - flexible patterns
    (r"read\s+(the\s+)?[\w./\\-]+\.(json|txt|md|yaml|yml|csv|xml|log|conf|cfg|py|js|ts|html|css)",
     ["read_file"], ActionCategory.FILE_READ, 0.95),
    (r"read\s+(the\s+)?(file|document|contents?)\s+[\w./\\-]+", 
     ["read_file"], ActionCategory.FILE_READ, 0.9),
    (r"read\s+(the\s+)?(file|document|contents?)", 
     ["read_file"], ActionCategory.FILE_READ, 0.75),
    (r"show\s+(me\s+)?(the\s+)?(file|contents?)\s+[\w./\\-]+", 
     ["read_file"], ActionCategory.FILE_READ, 0.85),
    (r"show\s+(me\s+)?(the\s+)?[\w./\\-]+\.(json|txt|md|yaml|yml|csv)", 
     ["read_file"], ActionCategory.FILE_READ, 0.85),
    (r"open\s+[\w./\\-]+", 
     ["read_file"], ActionCategory.FILE_READ, 0.7),
    (r"cat\s+[\w./\\-]+", 
     ["read_file"], ActionCategory.FILE_READ, 0.95),
    (r"get\s+(the\s+)?contents?\s+of\s+[\w./\\-]+", 
     ["read_file"], ActionCategory.FILE_READ, 0.9),
    (r"load\s+(the\s+)?[\w./\\-]+\.(json|txt|yaml|yml|csv)",
     ["read_file"], ActionCategory.FILE_READ, 0.85),
    (r"view\s+(the\s+)?[\w./\\-]+",
     ["read_file"], ActionCategory.FILE_READ, 0.8),
    
    # Directory listing
    (r"list\s+(the\s+)?(files?|contents?|directory|folder)\s*(in|of)?\s*[\w./\\-]*", 
     ["list_directory"], ActionCategory.FILE_READ, 0.9),
    (r"list\s+(the\s+)?[\w./\\-]+\s+(directory|folder)",
     ["list_directory"], ActionCategory.FILE_READ, 0.9),
    (r"list\s+(the\s+)?[\w./\\-]+",
     ["list_directory"], ActionCategory.FILE_READ, 0.75),
    (r"ls\s*[\w./\\-]*", 
     ["list_directory"], ActionCategory.FILE_READ, 0.95),
    (r"show\s+(me\s+)?(the\s+)?directory\s*[\w./\\-]*", 
     ["list_directory"], ActionCategory.FILE_READ, 0.85),
    (r"what('s| is)\s+in\s+(the\s+)?(folder|directory)\s*[\w./\\-]*", 
     ["list_directory"], ActionCategory.FILE_READ, 0.85),
    (r"what\s+files\s+(are\s+)?(in|at)\s+[\w./\\-]*",
     ["list_directory"], ActionCategory.FILE_READ, 0.85),
    
    # File search
    (r"find\s+(all\s+)?(files?|documents?)\s*(matching|named|like|with)\s+[\w.*?]+", 
     ["search_files"], ActionCategory.FILE_READ, 0.9),
    (r"search\s+(for\s+)?(files?|documents?)\s*[\w.*?]*", 
     ["search_files"], ActionCategory.FILE_READ, 0.85),
    
    # File Write
    (r"write\s+(to\s+)?(the\s+)?[\w./\\-]+", 
     ["write_file"], ActionCategory.FILE_WRITE, 0.9),
    (r"create\s+(a\s+)?(new\s+)?file\s+[\w./\\-]+", 
     ["write_file"], ActionCategory.FILE_WRITE, 0.85),
    (r"save\s+(to\s+)?[\w./\\-]+", 
     ["write_file"], ActionCategory.FILE_WRITE, 0.8),
    (r"update\s+(the\s+)?file\s+[\w./\\-]+", 
     ["write_file"], ActionCategory.FILE_WRITE, 0.85),
    (r"write\s+a\s+(file|document|report)",
     ["write_file"], ActionCategory.FILE_WRITE, 0.8),
    
    # Append
    (r"append\s+(to\s+)?[\w./\\-]+", 
     ["append_file"], ActionCategory.FILE_WRITE, 0.9),
    (r"add\s+(to\s+)?(the\s+)?end\s+of\s+[\w./\\-]+", 
     ["append_file"], ActionCategory.FILE_WRITE, 0.85),
    
    # Directory creation
    (r"create\s+(a\s+)?(new\s+)?(folder|directory)\s+[\w./\\-]+", 
     ["create_directory"], ActionCategory.FILE_WRITE, 0.9),
    (r"mkdir\s+[\w./\\-]+", 
     ["create_directory"], ActionCategory.FILE_WRITE, 0.95),
    
    # File Delete
    (r"delete\s+(the\s+)?file\s+[\w./\\-]+", 
     ["delete_file"], ActionCategory.FILE_DELETE, 0.9),
    (r"remove\s+(the\s+)?file\s+[\w./\\-]+", 
     ["delete_file"], ActionCategory.FILE_DELETE, 0.85),
    (r"rm\s+[\w./\\-]+", 
     ["delete_file"], ActionCategory.FILE_DELETE, 0.95),
    (r"delete\s+[\w./\\-]+\.(json|txt|md|yaml|yml|csv|log)",
     ["delete_file"], ActionCategory.FILE_DELETE, 0.9),
    
    # Directory Delete
    (r"delete\s+(the\s+)?(folder|directory)\s+[\w./\\-]+", 
     ["delete_directory"], ActionCategory.FILE_DELETE, 0.9),
    (r"remove\s+(the\s+)?(folder|directory)\s+[\w./\\-]+", 
     ["delete_directory"], ActionCategory.FILE_DELETE, 0.85),
    (r"rmdir\s+[\w./\\-]+", 
     ["delete_directory"], ActionCategory.FILE_DELETE, 0.95),
    
    # HTTP
    (r"fetch\s+(the\s+)?url\s+\S+", 
     ["http_get"], ActionCategory.NETWORK, 0.9),
    (r"get\s+(data\s+)?from\s+(the\s+)?api\s+\S+", 
     ["http_get"], ActionCategory.NETWORK, 0.85),
    (r"download\s+(from\s+)?\S+", 
     ["http_get"], ActionCategory.NETWORK, 0.8),
    (r"post\s+(to\s+)?\S+", 
     ["http_post"], ActionCategory.NETWORK, 0.85),
    (r"send\s+(a\s+)?request\s+to\s+\S+", 
     ["http_get", "http_post"], ActionCategory.NETWORK, 0.7),
    (r"call\s+(the\s+)?api",
     ["http_get", "http_post"], ActionCategory.NETWORK, 0.75),
    
    # Database Read
    (r"query\s+(the\s+)?database\s*\S*", 
     ["query_data"], ActionCategory.DATABASE_READ, 0.9),
    (r"select\s+.+\s+from\s+\S+", 
     ["query_data"], ActionCategory.DATABASE_READ, 0.95),
    (r"get\s+(data|records?)\s+from\s+\S+", 
     ["query_data"], ActionCategory.DATABASE_READ, 0.85),
    (r"look\s+up\s+.+\s+in\s+(the\s+)?database", 
     ["query_data"], ActionCategory.DATABASE_READ, 0.85),
    (r"search\s+(the\s+)?database",
     ["query_data"], ActionCategory.DATABASE_READ, 0.8),
    
    # Database Write
    (r"insert\s+(into\s+)?\S+", 
     ["insert_data"], ActionCategory.DATABASE_WRITE, 0.9),
    (r"add\s+(a\s+)?(new\s+)?record\s+to\s+\S+", 
     ["insert_data"], ActionCategory.DATABASE_WRITE, 0.85),
    (r"update\s+(the\s+)?record\s+\S+", 
     ["update_data"], ActionCategory.DATABASE_WRITE, 0.85),
    (r"delete\s+(from\s+)?(the\s+)?database\s+\S+", 
     ["delete_data"], ActionCategory.DATABASE_WRITE, 0.9),
    
    # Communication
    (r"send\s+(an\s+)?email\s+(to\s+)?\S*", 
     ["send_email"], ActionCategory.COMMUNICATION, 0.9),
    (r"email\s+\S+", 
     ["send_email"], ActionCategory.COMMUNICATION, 0.85),
    (r"notify\s+\S+", 
     ["send_notification"], ActionCategory.COMMUNICATION, 0.85),
    (r"send\s+(a\s+)?notification\s+(to\s+)?\S*", 
     ["send_notification"], ActionCategory.COMMUNICATION, 0.9),
    (r"(slack|message|alert)\s+\S+", 
     ["send_notification"], ActionCategory.COMMUNICATION, 0.8),
    
    # Compute
    (r"run\s+(the\s+)?(code|script)\s*\S*", 
     ["execute_code"], ActionCategory.COMPUTE, 0.9),
    (r"execute\s+(the\s+)?(code|script)\s*\S*", 
     ["execute_code"], ActionCategory.COMPUTE, 0.9),
    (r"run\s+(the\s+)?command\s+\S+", 
     ["run_shell_command"], ActionCategory.SYSTEM, 0.9),
    (r"execute\s+(the\s+)?command\s+\S+", 
     ["run_shell_command"], ActionCategory.SYSTEM, 0.9),
]

# Scope keywords that indicate operational boundaries
SCOPE_PATTERNS: dict[str, list[str]] = {
    "single_file": ["this file", "the file", "one file", "single file"],
    "directory": ["this folder", "the directory", "this directory", "in the folder"],
    "project": ["the project", "this project", "entire project", "whole project"],
    "system": ["the system", "system-wide", "globally", "everywhere"],
    "database": ["the database", "all tables", "entire database"],
}


class IntentCompiler:
    """
    Compiles natural language prompts into structured APE Intents.
    
    The IntentCompiler is the core component that makes APE usable in practice.
    It provides a one-call API that transforms what the user says into what
    APE enforces.
    
    Key features:
    1. **Semantic Analysis**: Extracts action signals from natural language
    2. **Action Mapping**: Maps signals to known actions in the ActionRepository
    3. **Policy Narrowing**: Filters actions by what the policy allows
    4. **Risk Assessment**: Flags high-risk actions for escalation
    5. **Scope Inference**: Determines operational boundaries
    
    Usage:
        from ape.intent_compiler import IntentCompiler
        from ape.action_repository import create_standard_repository
        
        repository = create_standard_repository()
        compiler = IntentCompiler(repository)
        
        # Compile a prompt
        intent = compiler.compile(
            prompt="Read the config.json file and list the directory contents",
            policy_allowed=["read_file", "list_directory"],
        )
        
        # Use with APE
        intent_manager.set(intent.to_ape_intent(), Provenance.USER_TRUSTED)
    
    Security Principles:
    - The prompt guides but never becomes intent
    - Only actions in the ActionRepository can be in the result
    - Policy narrowing ensures compliance
    - High-risk actions require explicit escalation
    - All decisions are logged for audit
    
    Thread Safety:
        This class is thread-safe for concurrent compile() calls.
    """
    
    def __init__(
        self,
        repository: ActionRepository,
        llm_analyzer: Optional[Callable[[dict], dict]] = None,
        default_scope: str = "default",
    ) -> None:
        """
        Initialize the Intent Compiler.
        
        Args:
            repository: The ActionRepository defining known actions
            llm_analyzer: Optional LLM-based semantic analyzer
            default_scope: Default scope when none can be inferred
        """
        self._repository = repository
        self._llm_analyzer = llm_analyzer
        self._default_scope = default_scope
        self._pattern_cache: dict[str, re.Pattern] = {}
        
        # Compile regex patterns
        for pattern, _, _, _ in SEMANTIC_PATTERNS:
            if pattern not in self._pattern_cache:
                self._pattern_cache[pattern] = re.compile(pattern, re.IGNORECASE)
    
    def compile(
        self,
        prompt: str,
        policy_allowed: Optional[list[str]] = None,
        policy_forbidden: Optional[list[str]] = None,
        max_risk_level: ActionRiskLevel = ActionRiskLevel.MODERATE,
        require_confidence: float = 0.5,
        explicit_scope: Optional[str] = None,
        allow_escalation: bool = True,
    ) -> CompiledIntent:
        """
        Compile a natural language prompt into a structured APE Intent.
        
        This is the primary API of the IntentCompiler. It transforms user
        prompts into enforceable intents.
        
        Args:
            prompt: The natural language user prompt
            policy_allowed: Actions allowed by the current policy (for narrowing)
            policy_forbidden: Actions forbidden by the current policy
            max_risk_level: Maximum risk level to allow without escalation
            require_confidence: Minimum confidence for action inclusion
            explicit_scope: Override inferred scope
            allow_escalation: Whether to allow escalation_required actions
            
        Returns:
            CompiledIntent ready for APE
            
        Raises:
            IntentCompilationError: If compilation fails
            IntentNarrowingError: If policy narrows to empty set
            IntentAmbiguityError: If prompt is too ambiguous
        """
        if not prompt or not prompt.strip():
            raise IntentCompilationError("Empty prompt cannot be compiled")
        
        narrowing_log = []
        
        # Step 1: Extract semantic signals
        signals = self._extract_signals(prompt)
        
        if not signals:
            raise IntentAmbiguityError(
                f"Could not extract any action signals from prompt",
                prompt=prompt
            )
        
        # Step 2: Gather candidate action IDs
        candidate_actions: dict[str, float] = {}  # action_id -> max_confidence
        for signal in signals:
            for action_id in signal.action_ids:
                if self._repository.exists(action_id):
                    if action_id not in candidate_actions:
                        candidate_actions[action_id] = signal.confidence
                    else:
                        candidate_actions[action_id] = max(
                            candidate_actions[action_id],
                            signal.confidence
                        )
        
        narrowing_log.append(
            f"Extracted {len(candidate_actions)} candidate actions from {len(signals)} signals"
        )
        
        # Step 3: Filter by confidence
        confident_actions = {
            aid: conf for aid, conf in candidate_actions.items()
            if conf >= require_confidence
        }
        
        removed = len(candidate_actions) - len(confident_actions)
        if removed > 0:
            narrowing_log.append(
                f"Removed {removed} actions below confidence threshold ({require_confidence})"
            )
        
        if not confident_actions:
            raise IntentAmbiguityError(
                f"No actions met confidence threshold. "
                f"Max confidence: {max(candidate_actions.values()) if candidate_actions else 0}",
                prompt=prompt
            )
        
        # Step 4: Apply policy narrowing
        allowed_actions = list(confident_actions.keys())
        forbidden_actions: list[str] = []
        escalation_required: list[str] = []
        risk_flags: list[str] = []
        
        # Filter by policy_allowed
        if policy_allowed is not None:
            policy_set = set(policy_allowed)
            before = len(allowed_actions)
            allowed_actions = [a for a in allowed_actions if a in policy_set]
            removed = before - len(allowed_actions)
            if removed > 0:
                narrowing_log.append(
                    f"Policy narrowing removed {removed} actions not in allowed list"
                )
        
        # Remove policy_forbidden
        if policy_forbidden:
            forbidden_set = set(policy_forbidden)
            before = len(allowed_actions)
            removed_by_forbidden = [a for a in allowed_actions if a in forbidden_set]
            allowed_actions = [a for a in allowed_actions if a not in forbidden_set]
            forbidden_actions.extend(removed_by_forbidden)
            if removed_by_forbidden:
                narrowing_log.append(
                    f"Policy forbidden removed: {removed_by_forbidden}"
                )
        
        # Step 5: Risk assessment
        level_order = [
            ActionRiskLevel.MINIMAL,
            ActionRiskLevel.LOW,
            ActionRiskLevel.MODERATE,
            ActionRiskLevel.HIGH,
            ActionRiskLevel.CRITICAL,
        ]
        max_risk_idx = level_order.index(max_risk_level)
        
        safe_actions = []
        for action_id in allowed_actions:
            defn = self._repository.get(action_id)
            action_risk_idx = level_order.index(defn.risk_level)
            
            if action_risk_idx > max_risk_idx:
                if allow_escalation:
                    escalation_required.append(action_id)
                    risk_flags.append(
                        f"{action_id}: {defn.risk_level.value} risk (escalation required)"
                    )
                    narrowing_log.append(
                        f"Action {action_id} moved to escalation_required "
                        f"(risk {defn.risk_level.value} > {max_risk_level.value})"
                    )
                else:
                    forbidden_actions.append(action_id)
                    risk_flags.append(
                        f"{action_id}: {defn.risk_level.value} risk (forbidden, no escalation)"
                    )
                    narrowing_log.append(
                        f"Action {action_id} forbidden "
                        f"(risk {defn.risk_level.value} > {max_risk_level.value}, escalation disabled)"
                    )
            elif defn.requires_human_review:
                if allow_escalation:
                    escalation_required.append(action_id)
                    risk_flags.append(
                        f"{action_id}: requires human review"
                    )
                    narrowing_log.append(
                        f"Action {action_id} moved to escalation_required (requires_human_review)"
                    )
                else:
                    safe_actions.append(action_id)
            else:
                safe_actions.append(action_id)
        
        allowed_actions = safe_actions
        
        # Check we have something left
        total_allowed = len(allowed_actions) + len(escalation_required)
        if total_allowed == 0:
            raise IntentNarrowingError(
                f"Policy narrowing removed all actions",
                narrowing_log=narrowing_log
            )
        
        # Step 6: Infer scope
        scope = explicit_scope or self._infer_scope(prompt, signals)
        
        # Step 7: Calculate overall confidence
        if allowed_actions:
            avg_confidence = sum(
                confident_actions.get(a, 0) for a in allowed_actions
            ) / len(allowed_actions)
        else:
            avg_confidence = sum(
                confident_actions.get(a, 0) for a in escalation_required
            ) / len(escalation_required) if escalation_required else 0.0
        
        # Build description
        description = self._generate_description(prompt, allowed_actions, escalation_required)
        
        return CompiledIntent(
            allowed_actions=allowed_actions,
            forbidden_actions=forbidden_actions,
            scope=scope,
            escalation_required=escalation_required,
            description=description,
            original_prompt=prompt,
            signals=signals,
            narrowing_log=narrowing_log,
            risk_flags=risk_flags,
            confidence=avg_confidence,
        )
    
    def _extract_signals(self, prompt: str) -> list[IntentSignal]:
        """
        Extract semantic signals from a prompt.
        
        Uses pattern matching and optionally LLM analysis.
        """
        signals = []
        prompt_lower = prompt.lower()
        
        # Pattern-based extraction
        for pattern, action_ids, category, base_confidence in SEMANTIC_PATTERNS:
            regex = self._pattern_cache[pattern]
            match = regex.search(prompt_lower)
            if match:
                # Adjust confidence based on match quality
                matched_text = match.group(0)
                confidence = base_confidence
                
                # Boost confidence for longer, more specific matches
                if len(matched_text) > 20:
                    confidence = min(1.0, confidence + 0.05)
                
                signals.append(IntentSignal(
                    phrase=matched_text,
                    action_ids=action_ids,
                    confidence=confidence,
                    category=category,
                ))
        
        # LLM-based extraction (if available)
        if self._llm_analyzer:
            try:
                llm_signals = self._extract_signals_llm(prompt)
                signals.extend(llm_signals)
            except Exception:
                pass  # Fall back to pattern-only
        
        # Deduplicate by action_id, keeping highest confidence
        action_confidence: dict[str, tuple[IntentSignal, float]] = {}
        for signal in signals:
            for action_id in signal.action_ids:
                if action_id not in action_confidence or signal.confidence > action_confidence[action_id][1]:
                    action_confidence[action_id] = (signal, signal.confidence)
        
        # Return unique signals (by object identity)
        seen_signals: dict[int, IntentSignal] = {}
        for sig, _ in action_confidence.values():
            seen_signals[id(sig)] = sig
        return list(seen_signals.values())
    
    def _extract_signals_llm(self, prompt: str) -> list[IntentSignal]:
        """
        Extract signals using an LLM analyzer.
        
        The LLM analyzer should return a dict with:
        {
            "actions": [
                {"action_id": "...", "confidence": 0.9, "reason": "..."},
                ...
            ]
        }
        """
        if not self._llm_analyzer:
            return []
        
        # Build context for LLM
        available_actions = self._repository.to_prompt_context(include_schemas=False)
        
        result = self._llm_analyzer({
            "prompt": prompt,
            "available_actions": available_actions,
            "instruction": (
                "Analyze the user prompt and identify which actions from the available "
                "list best match the user's intent. Return a JSON object with an 'actions' "
                "array containing objects with 'action_id', 'confidence' (0-1), and 'reason'."
            ),
        })
        
        signals = []
        for action_data in result.get("actions", []):
            if self._repository.exists(action_data["action_id"]):
                signals.append(IntentSignal(
                    phrase=f"LLM: {action_data.get('reason', 'semantic match')}",
                    action_ids=[action_data["action_id"]],
                    confidence=action_data.get("confidence", 0.7),
                ))
        
        return signals
    
    def _infer_scope(
        self,
        prompt: str,
        signals: list[IntentSignal]
    ) -> str:
        """Infer the operational scope from the prompt and signals."""
        prompt_lower = prompt.lower()
        
        # Check for explicit scope keywords
        for scope, keywords in SCOPE_PATTERNS.items():
            for keyword in keywords:
                if keyword in prompt_lower:
                    return scope
        
        # Infer from action categories
        categories = set()
        for signal in signals:
            if signal.category:
                categories.add(signal.category)
        
        # File operations default to single_file or directory
        if ActionCategory.FILE_READ in categories or ActionCategory.FILE_WRITE in categories:
            if "recursive" in prompt_lower or "all files" in prompt_lower:
                return "directory"
            return "single_file"
        
        # Database operations
        if ActionCategory.DATABASE_READ in categories or ActionCategory.DATABASE_WRITE in categories:
            return "database"
        
        # Network operations
        if ActionCategory.NETWORK in categories:
            return "network"
        
        # System operations
        if ActionCategory.SYSTEM in categories:
            return "system"
        
        return self._default_scope
    
    def _generate_description(
        self,
        prompt: str,
        allowed: list[str],
        escalation: list[str]
    ) -> str:
        """Generate a human-readable description of the intent."""
        parts = []
        
        if allowed:
            parts.append(f"Allow: {', '.join(allowed)}")
        if escalation:
            parts.append(f"Escalation required: {', '.join(escalation)}")
        
        # Truncate long prompts
        max_prompt = 100
        prompt_summary = prompt[:max_prompt] + "..." if len(prompt) > max_prompt else prompt
        parts.append(f"From: '{prompt_summary}'")
        
        return "; ".join(parts)
    
    def analyze(self, prompt: str) -> dict[str, Any]:
        """
        Analyze a prompt without compiling (for debugging/preview).
        
        Returns:
            Dictionary with analysis results
        """
        signals = self._extract_signals(prompt)
        
        # Gather all candidate actions
        candidates = {}
        for signal in signals:
            for action_id in signal.action_ids:
                if self._repository.exists(action_id):
                    defn = self._repository.get(action_id)
                    if action_id not in candidates:
                        candidates[action_id] = {
                            "action_id": action_id,
                            "description": defn.description,
                            "category": defn.category.value,
                            "risk_level": defn.risk_level.value,
                            "max_confidence": signal.confidence,
                            "triggers": [signal.phrase],
                        }
                    else:
                        candidates[action_id]["max_confidence"] = max(
                            candidates[action_id]["max_confidence"],
                            signal.confidence
                        )
                        candidates[action_id]["triggers"].append(signal.phrase)
        
        return {
            "prompt": prompt,
            "signals_extracted": len(signals),
            "candidate_actions": list(candidates.values()),
            "inferred_scope": self._infer_scope(prompt, signals),
        }
    
    def suggest_policy(
        self,
        prompt: str,
        risk_tolerance: ActionRiskLevel = ActionRiskLevel.MODERATE
    ) -> dict[str, Any]:
        """
        Suggest a policy that would allow the given prompt.
        
        Useful for developers to understand what policy is needed.
        
        Returns:
            Suggested policy configuration
        """
        try:
            # Compile without restrictions
            intent = self.compile(
                prompt,
                policy_allowed=self._repository.action_ids,  # Allow everything
                max_risk_level=ActionRiskLevel.CRITICAL,
                allow_escalation=True,
            )
            
            # Determine what needs to be in the policy
            level_order = [
                ActionRiskLevel.MINIMAL,
                ActionRiskLevel.LOW,
                ActionRiskLevel.MODERATE,
                ActionRiskLevel.HIGH,
                ActionRiskLevel.CRITICAL,
            ]
            risk_idx = level_order.index(risk_tolerance)
            
            allowed = []
            escalation = []
            
            for action_id in intent.allowed_actions + intent.escalation_required:
                defn = self._repository.get(action_id)
                action_risk_idx = level_order.index(defn.risk_level)
                
                if action_risk_idx <= risk_idx and not defn.requires_human_review:
                    allowed.append(action_id)
                else:
                    escalation.append(action_id)
            
            return {
                "suggested_policy": {
                    "allowed_actions": allowed,
                    "escalation_required": escalation,
                    "default_deny": True,
                },
                "risk_tolerance": risk_tolerance.value,
                "prompt": prompt,
                "confidence": intent.confidence,
            }
            
        except (IntentAmbiguityError, IntentNarrowingError) as e:
            return {
                "error": str(e),
                "prompt": prompt,
            }
