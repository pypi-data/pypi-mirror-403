{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esperanto import AIFactory\n",
    "embedding_models = [\n",
    "    # (\"openai\", \"text-embedding-3-small\"),\n",
    "    # (\"azure\", \"text-embedding-3-small\"),\n",
    "    # (\"ollama\", \"mxbai-embed-large\"),\n",
    "    # (\"google\", \"text-embedding-004\"),\n",
    "    # (\"mistral\", \"mistral-embed\"),\n",
    "    # (\"voyage\", \"voyage-3-large\"),\n",
    "    # (\"transformers\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    # (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    # (\"jina\", \"jina-embeddings-v4\")\n",
    "    # (\"openai-compatible\", \"mxbai-embed-large-v1a\"),\n",
    "    (\"openrouter\", \"qwen/qwen3-embedding-0.6b\")\n",
    "    \n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openrouter:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config)\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openrouter:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config)\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Type Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openrouter:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for openrouter:\n",
      "1024\n",
      "1024\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from esperanto import AIFactory\n",
    "from esperanto.common_types.task_type import EmbeddingTaskType\n",
    "\n",
    "embedding_models = [\n",
    "    # (\"google\", \"text-embedding-004\"),\n",
    "    # (\"mistral\", \"mistral-embed\"),\n",
    "    # (\"voyage\", \"voyage-3-large\"),\n",
    "    # (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    # (\"jina\", \"jina-embeddings-v4\"),\n",
    "    (\"openrouter\", \"qwen/qwen3-embedding-0.6b\")\n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n",
    "\n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jina:\n",
      "128\n",
      "128\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for jina:\n",
      "128\n",
      "128\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from esperanto import AIFactory\n",
    "from esperanto.common_types.task_type import EmbeddingTaskType\n",
    "\n",
    "embedding_models = [\n",
    "    # (\"google\", \"text-embedding-004\"),\n",
    "    # (\"mistral\", \"mistral-embed\"),\n",
    "    # (\"voyage\", \"voyage-3-large\"),\n",
    "    # (\"transformers\", \"Qwen/Qwen3-Embedding-0.6B\"),\n",
    "    (\"jina\", \"jina-embeddings-v4\")\n",
    "]\n",
    "\n",
    "texts = [\"Hello, world!\", \"Another text\"]\n",
    "\n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY, \"late_chunking\": True, \"output_dimensions\": 128})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = embed_model.embed(texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "for name, config in embedding_models:\n",
    "    embed_model = AIFactory.create_embedding(provider=name, model_name=config, config={\"task_type\": EmbeddingTaskType.RETRIEVAL_QUERY, \"late_chunking\": True, \"output_dimensions\": 128})\n",
    "    print(f\"Results for {embed_model.provider}:\")\n",
    "    embeddings = await embed_model.aembed(texts=texts)\n",
    "    print(len(embeddings[0]))\n",
    "    print(len(embeddings[1]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI-Compatible Embeddings\n",
    "\n",
    "This section demonstrates how to use OpenAI-compatible embedding endpoints such as LM Studio, LocalAI, vLLM, or any custom OpenAI-format API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provider: openai-compatible\n",
      "Model: nomic-embed-text\n",
      "Base URL: http://localhost:1234/v1\n",
      "Could not connect to OpenAI-compatible endpoint: Failed to generate embeddings: [Errno 61] Connection refused\n",
      "Make sure your local server (LM Studio, etc.) is running with an embedding model loaded\n",
      "Default LM Studio URL: http://localhost:1234\n"
     ]
    }
   ],
   "source": [
    "# OpenAI-Compatible Embedding Example\n",
    "# This works with LM Studio, LocalAI, vLLM, or any OpenAI-compatible endpoint\n",
    "\n",
    "from esperanto import AIFactory\n",
    "\n",
    "# Example with LM Studio (adjust URL and model name for your setup)\n",
    "try:\n",
    "    embedding_model = AIFactory.create_embedding(\n",
    "        \"openai-compatible\",\n",
    "        model_name=\"nomic-embed-text\",  # Must match the model loaded in your endpoint\n",
    "        config={\n",
    "            \"base_url\": \"http://localhost:1234/v1\",  # LM Studio default\n",
    "            \"api_key\": \"not-required\",  # Often not needed for local endpoints\n",
    "            \"timeout\": 120  # 2 minutes timeout for large batches\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    texts = [\"Hello, local embeddings!\", \"OpenAI-compatible endpoint working\"]\n",
    "    \n",
    "    print(f\"Using provider: {embedding_model.provider}\")\n",
    "    print(f\"Model: {embedding_model.get_model_name()}\")\n",
    "    print(f\"Base URL: {embedding_model.base_url}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.embed(texts)\n",
    "    \n",
    "    print(f\"\\nEmbedding dimensions: {len(embeddings[0])}\")\n",
    "    print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "    print(f\"First few values: {embeddings[0][:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to OpenAI-compatible endpoint: {e}\")\n",
    "    print(\"Make sure your local server (LM Studio, etc.) is running with an embedding model loaded\")\n",
    "    print(\"Default LM Studio URL: http://localhost:1234\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
