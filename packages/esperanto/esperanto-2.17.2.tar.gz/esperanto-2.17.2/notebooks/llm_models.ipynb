{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esperanto import AIFactory\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_tokens\": 850,\n",
    "    \"temperature\": 1.0,\n",
    "    \"streaming\": False,\n",
    "    \"top_p\": 0.9,\n",
    "    \"structured\": None\n",
    "}\n",
    "\n",
    "models = [\n",
    "    # (\"openai-compatible\", AIFactory.create_language(\"openai-compatible\", \"qwen3:4b\")),\n",
    "    # (\"openrouter\", AIFactory.create_language(\"openrouter\", \"openai/gpt-4o\")),\n",
    "    # (\"openai\", AIFactory.create_language(\"openai\", \"gpt-5-mini\")),\n",
    "    # (\"xai\", AIFactory.create_language(\"xai\", \"grok-3\")),\n",
    "    # (\"groq\", AIFactory.create_language(\"groq\", \"llama3-8b-8192\")),\n",
    "    # (\"anthropic\", AIFactory.create_language(\"anthropic\", \"claude-4-sonnet-latest\")),\n",
    "    # (\"ollama\", AIFactory.create_language(\"ollama\", \"gemma3:4b\")),\n",
    "    # (\"google\", AIFactory.create_language(\"google\", \"gemini-2.0-flash\")),\n",
    "    # (\"google\", AIFactory.create_language(\"google\", \"gemini-2.5-flash\")),\n",
    "    (\"azure\", AIFactory.create_language(\"azure\", \"o4-mini\")),\n",
    "    # (\"mistral\", AIFactory.create_language(\"mistral\", \"mistral-large-latest\")),\n",
    "    # (\"deepseek\", AIFactory.create_language(\"deepseek\", \"deepseek-chat\")),\n",
    "    # (\"vertex\", AIFactory.create_language(\"vertex\", \"gemini-2.0-flash\")),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Models (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AZURE Models ===\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "        # Create an instance of the provider class\n",
    "        provider = model[0]\n",
    "        model = model[1]\n",
    "        print(f\"\\n=== {provider.upper()} Models ===\")\n",
    "        print(model.models)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get models for {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        \n",
    "        print(result.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        print(result.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "[{'name': 'São Paulo', 'state': 'São Paulo'}, {'name': 'Rio de Janeiro', 'state': 'Rio de Janeiro'}, {'name': 'Brasília', 'state': 'Distrito Federal'}]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "{'top_3_brazilian_cities': ['São Paulo', 'Rio de Janeiro', 'Brasília']}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please return the top 3 brazilian cities in JSON format. Dont include ```json```  in the response.\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(json_messages)\n",
    "        try:\n",
    "            json_data = json.loads(result.choices[0].message.content)\n",
    "            print(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON\")\n",
    "            print(result.choices[0].message.content)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic (WIP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List\n",
    "\n",
    "# class Country(BaseModel):\n",
    "#     name: str\n",
    "#     population: int\n",
    "\n",
    "# class Response(BaseModel):\n",
    "#     countries: List[Country]\n",
    "\n",
    "# json_messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Please return the top 3 countries in terms of population. Responda no formato JSON.\"},\n",
    "#     ]\n",
    "\n",
    "\n",
    "# for name, config in models.items():\n",
    "#     try:\n",
    "#         llm = config[\"class\"](model_name=config[\"model\"], structured={\"type\": \"json\", \"model\": Response})\n",
    "#         print(f\"Results for {llm.provider}:\")\n",
    "#         result = llm.chat_complete(json_messages)\n",
    "#         try:\n",
    "#             json_data = json.loads(result.choices[0].message.content)\n",
    "#             print(json_data)\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(\"Error decoding JSON\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYUyX6ljFwI77aIRrBwqYyAGyI0h' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='gpt-5-mini-2025-08-07' created=1758300224 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "id='' choices=[] model='o4-mini' created=0 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV1HOXL5XxsdmzJF7k9s4DMP8lQ' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='o4-mini-2025-04-16' created=1758300227 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "            print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV36RbgqP8iAAr7qtlgxN0vKatr' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='gpt-5-mini-2025-08-07' created=1758300229 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "id='' choices=[] model='o4-mini' created=0 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='The', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' capital', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' of', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' France', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' is', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content=' Paris', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='.', role='assistant', function_call=None, tool_calls=None), finish_reason='None')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "id='chatcmpl-CHYV402pA4Mhyk9idPyVaXe9wfN3T' choices=[StreamChoice(index=0, delta=DeltaMessage(content='', role='assistant', function_call=None, tool_calls=None), finish_reason='stop')] model='o4-mini-2025-04-16' created=1758300230 object='chat.completion.chunk'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        async for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for anthropic:\n",
      "The capital of France is Paris. It is also the largest city in France and one of the most populous cities in Europe. Paris is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = model.invoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "The capital of France is Paris.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, llm in models:\n",
    "    try:\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = await model.ainvoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-5-mini-2025-08-07', 'service_tier': 'default'} id='run--d9c5dc84-b4f8-4309-8122-bf2f4feff38d'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'} id='run--e4ec21bb-9830-483a-b2e8-12d688931313'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, llm in models:\n",
    "    try:\n",
    "        # Create a new streaming instance using the factory\n",
    "        streaming_llm = AIFactory.create_language(llm.provider, llm.model_name, config={\"streaming\": True})\n",
    "        print(f\"Results for {streaming_llm.provider}:\")\n",
    "        model = streaming_llm.to_langchain()\n",
    "        response = model.stream(messages)\n",
    "        for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openai:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-5-mini-2025-08-07', 'service_tier': 'default'} id='run--c1da6755-0b05-4a9c-95e5-8d2425c352b8'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for azure:\n",
      "content='' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='The' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' capital' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' France' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' is' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content=' Paris' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'} id='run--80f396a2-d898-4403-834c-2a2b434870ee'\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, llm in models:\n",
    "    try:\n",
    "        # Create a new streaming instance using the factory\n",
    "        streaming_llm = AIFactory.create_language(llm.provider, llm.model_name, config={\"streaming\": True})\n",
    "        print(f\"Results for {streaming_llm.provider}:\")\n",
    "        model = streaming_llm.to_langchain()\n",
    "        response = model.astream(messages)\n",
    "        async for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
