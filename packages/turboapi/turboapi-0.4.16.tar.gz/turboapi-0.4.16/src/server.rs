use crate::router::RadixRouter;
use crate::simd_json;
use crate::simd_parse;
use crate::zerocopy::ZeroCopyBufferPool;
use bytes::Bytes;
use http_body_util::{BodyExt, Full};
use hyper::body::Incoming as IncomingBody;
use hyper::server::conn::http1;
use hyper::service::service_fn;
use hyper::{Request, Response};
use hyper_util::rt::TokioIo;
use pyo3::prelude::*;
use pyo3::types::{PyDict, PyString};
use std::collections::HashMap;
use std::collections::HashMap as StdHashMap;
use std::convert::Infallible;
use std::net::SocketAddr;
use std::sync::Arc;
use std::sync::OnceLock;
use std::thread;
use std::time::{Duration, Instant};
use tokio::net::TcpListener;
use tokio::sync::{mpsc, oneshot, RwLock};

type Handler = Arc<PyObject>;

/// Handler dispatch type for fast-path routing (Phase 3: eliminate Python wrapper)
#[derive(Clone, Debug, PartialEq)]
enum HandlerType {
    /// Simple sync: no body, just path/query params. Rust parses + serializes everything.
    SimpleSyncFast,
    /// Needs body parsing: Rust parses body with simd-json, calls handler directly.
    BodySyncFast,
    /// Model sync: Rust parses JSON with simd-json, validates with dhi model in Python.
    ModelSyncFast,
    /// Needs full Python enhanced wrapper (async, dependencies, etc.)
    Enhanced,
}

// Metadata struct with fast dispatch info
#[derive(Clone)]
struct HandlerMetadata {
    handler: Handler,
    is_async: bool,
    handler_type: HandlerType,
    route_pattern: String,
    param_types: HashMap<String, String>, // param_name -> type ("int", "str", "float")
    original_handler: Option<Handler>,    // Unwrapped handler for fast dispatch
    model_info: Option<(String, Handler)>, // (param_name, model_class) for ModelSyncFast
}

// Response data with status code support
struct HandlerResponse {
    body: String,
    status_code: u16,
}

// MULTI-WORKER: Request structure for worker communication
struct PythonRequest {
    handler: Handler,
    is_async: bool, // Cached from HandlerMetadata - no need to check at runtime!
    method: String,
    path: String,
    query_string: String,
    body: Bytes,
    response_tx: oneshot::Sender<Result<HandlerResponse, String>>,
}

// LOOP SHARDING: Structure for each event loop shard
struct LoopShard {
    shard_id: usize,
    task_locals: pyo3_async_runtimes::TaskLocals,
    json_dumps_fn: PyObject,
    limiter: PyObject, // PHASE B: Semaphore limiter for gating
    tx: mpsc::Sender<PythonRequest>,
}

impl Clone for LoopShard {
    fn clone(&self) -> Self {
        Python::with_gil(|py| Self {
            shard_id: self.shard_id,
            task_locals: self.task_locals.clone_ref(py),
            json_dumps_fn: self.json_dumps_fn.clone_ref(py),
            limiter: self.limiter.clone_ref(py),
            tx: self.tx.clone(),
        })
    }
}

// PHASE D: Pure Rust Async Runtime with Tokio
// This replaces Python event loop shards with Tokio's work-stealing scheduler
struct TokioRuntime {
    task_locals: pyo3_async_runtimes::TaskLocals,
    json_dumps_fn: PyObject,
    semaphore: Arc<tokio::sync::Semaphore>,
}

impl Clone for TokioRuntime {
    fn clone(&self) -> Self {
        Python::with_gil(|py| Self {
            task_locals: self.task_locals.clone_ref(py),
            json_dumps_fn: self.json_dumps_fn.clone_ref(py),
            semaphore: self.semaphore.clone(),
        })
    }
}

// Cached Python modules for performance
static CACHED_JSON_MODULE: OnceLock<PyObject> = OnceLock::new();
static CACHED_BUILTINS_MODULE: OnceLock<PyObject> = OnceLock::new();
static CACHED_TYPES_MODULE: OnceLock<PyObject> = OnceLock::new();

/// TurboServer - Main HTTP server class with radix trie routing
#[pyclass]
pub struct TurboServer {
    handlers: Arc<RwLock<HashMap<String, HandlerMetadata>>>, // HYBRID: Store metadata with is_async cached!
    router: Arc<RwLock<RadixRouter>>,
    host: String,
    port: u16,
    worker_threads: usize,
    buffer_pool: Arc<ZeroCopyBufferPool>, // PHASE 2: Zero-copy buffer pool
    loop_shards: Option<Vec<LoopShard>>, // LOOP SHARDING: Multiple event loop shards for parallel processing
}

#[pymethods]
impl TurboServer {
    #[new]
    pub fn new(host: Option<String>, port: Option<u16>) -> Self {
        // PHASE 2: Intelligent worker thread calculation
        let cpu_cores = std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(4);

        // PHASE 2: Optimized worker thread calculation
        // - Use 3x CPU cores for I/O-bound workloads (common in web servers)
        // - Cap at 24 threads to avoid excessive context switching
        // - Minimum 8 threads for good baseline performance
        let worker_threads = ((cpu_cores * 3).min(24)).max(8);

        TurboServer {
            handlers: Arc::new(RwLock::new(HashMap::with_capacity(128))), // Increased capacity
            router: Arc::new(RwLock::new(RadixRouter::new())),
            host: host.unwrap_or_else(|| "127.0.0.1".to_string()),
            port: port.unwrap_or(8000),
            worker_threads,
            buffer_pool: Arc::new(ZeroCopyBufferPool::new()), // PHASE 2: Initialize buffer pool
            loop_shards: None,                                // LOOP SHARDING: Initialized in run()
        }
    }

    /// Register a route handler with radix trie routing (legacy: uses Enhanced wrapper)
    pub fn add_route(&self, method: String, path: String, handler: PyObject) -> PyResult<()> {
        let route_key = format!("{} {}", method.to_uppercase(), path);

        // Check if handler is async ONCE at registration time
        let is_async = Python::with_gil(|py| {
            let inspect = py.import("inspect")?;
            inspect
                .getattr("iscoroutinefunction")?
                .call1((&handler,))?
                .extract::<bool>()
        })?;

        let handlers = Arc::clone(&self.handlers);
        let router = Arc::clone(&self.router);
        let path_clone = path.clone();

        Python::with_gil(|py| {
            py.allow_threads(|| {
                let rt = tokio::runtime::Runtime::new().unwrap();
                rt.block_on(async {
                    let mut handlers_guard = handlers.write().await;
                    handlers_guard.insert(
                        route_key.clone(),
                        HandlerMetadata {
                            handler: Arc::new(handler),
                            is_async,
                            handler_type: HandlerType::Enhanced,
                            route_pattern: path_clone,
                            param_types: HashMap::new(),
                            original_handler: None,
                            model_info: None,
                        },
                    );
                    drop(handlers_guard);

                    let mut router_guard = router.write().await;
                    let _ =
                        router_guard.add_route(&method.to_uppercase(), &path, route_key.clone());
                });
            })
        });

        Ok(())
    }

    /// Register a route with fast dispatch metadata (Phase 3: bypass Python wrapper).
    ///
    /// handler_type: "simple_sync" | "body_sync" | "enhanced"
    /// param_types_json: JSON string of {"param_name": "type_hint", ...}
    /// original_handler: The unwrapped Python function (no enhanced wrapper)
    pub fn add_route_fast(
        &self,
        method: String,
        path: String,
        handler: PyObject,
        handler_type: String,
        param_types_json: String,
        original_handler: PyObject,
    ) -> PyResult<()> {
        let route_key = format!("{} {}", method.to_uppercase(), path);

        let ht = match handler_type.as_str() {
            "simple_sync" => HandlerType::SimpleSyncFast,
            "body_sync" => HandlerType::BodySyncFast,
            _ => HandlerType::Enhanced,
        };

        // Parse param types from JSON
        let param_types: HashMap<String, String> =
            serde_json::from_str(&param_types_json).unwrap_or_default();

        let is_async = ht == HandlerType::Enhanced
            && Python::with_gil(|py| {
                let inspect = py.import("inspect").ok()?;
                inspect
                    .getattr("iscoroutinefunction")
                    .ok()?
                    .call1((&handler,))
                    .ok()?
                    .extract::<bool>()
                    .ok()
            })
            .unwrap_or(false);

        let handlers = Arc::clone(&self.handlers);
        let router = Arc::clone(&self.router);
        let path_clone = path.clone();

        Python::with_gil(|py| {
            py.allow_threads(|| {
                let rt = tokio::runtime::Runtime::new().unwrap();
                rt.block_on(async {
                    let mut handlers_guard = handlers.write().await;
                    handlers_guard.insert(
                        route_key.clone(),
                        HandlerMetadata {
                            handler: Arc::new(handler),
                            is_async,
                            handler_type: ht,
                            route_pattern: path_clone,
                            param_types,
                            original_handler: Some(Arc::new(original_handler)),
                            model_info: None,
                        },
                    );
                    drop(handlers_guard);

                    let mut router_guard = router.write().await;
                    let _ =
                        router_guard.add_route(&method.to_uppercase(), &path, route_key.clone());
                });
            })
        });

        Ok(())
    }

    /// Register a route with model validation (Phase 3: fast model path).
    /// Rust parses JSON with simd-json, then calls Python model.model_validate()
    pub fn add_route_model(
        &self,
        method: String,
        path: String,
        handler: PyObject,
        param_name: String,
        model_class: PyObject,
        original_handler: PyObject,
    ) -> PyResult<()> {
        let route_key = format!("{} {}", method.to_uppercase(), path);

        let handlers = Arc::clone(&self.handlers);
        let router = Arc::clone(&self.router);
        let path_clone = path.clone();

        Python::with_gil(|py| {
            py.allow_threads(|| {
                let rt = tokio::runtime::Runtime::new().unwrap();
                rt.block_on(async {
                    let mut handlers_guard = handlers.write().await;
                    handlers_guard.insert(
                        route_key.clone(),
                        HandlerMetadata {
                            handler: Arc::new(handler),
                            is_async: false,
                            handler_type: HandlerType::ModelSyncFast,
                            route_pattern: path_clone,
                            param_types: HashMap::new(),
                            original_handler: Some(Arc::new(original_handler)),
                            model_info: Some((param_name, Arc::new(model_class))),
                        },
                    );
                    drop(handlers_guard);

                    let mut router_guard = router.write().await;
                    let _ =
                        router_guard.add_route(&method.to_uppercase(), &path, route_key.clone());
                });
            })
        });

        Ok(())
    }

    /// Start the HTTP server with multi-threading support
    pub fn run(&self, py: Python) -> PyResult<()> {
        // Optimize: Use pre-allocated string for address parsing (cold path)
        let mut addr_str = String::with_capacity(self.host.len() + 10);
        addr_str.push_str(&self.host);
        addr_str.push(':');
        addr_str.push_str(&self.port.to_string());

        let addr: SocketAddr = addr_str
            .parse()
            .map_err(|_| pyo3::exceptions::PyValueError::new_err("Invalid address"))?;

        let handlers = Arc::clone(&self.handlers);
        let router = Arc::clone(&self.router);

        // LOOP SHARDING: Spawn K event loop shards for parallel processing!
        // Each shard has its own event loop thread - eliminates global contention!
        let cpu_cores = std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(8);

        // Optimal: 8-16 shards (tune based on CPU cores)
        let num_shards = cpu_cores.min(16).max(8);

        eprintln!(
            "üöÄ Spawning {} event loop shards for parallel async processing!",
            num_shards
        );
        let loop_shards = spawn_loop_shards(num_shards);
        eprintln!("‚úÖ All {} loop shards ready!", num_shards);

        py.allow_threads(|| {
            // PHASE 2: Optimized runtime with advanced thread management
            let rt = tokio::runtime::Builder::new_multi_thread()
                .worker_threads(self.worker_threads) // Intelligently calculated worker threads
                .thread_name("turbo-worker")
                .thread_keep_alive(std::time::Duration::from_secs(60)) // Keep threads alive longer
                .thread_stack_size(2 * 1024 * 1024) // 2MB stack for deep call stacks
                .enable_all()
                .build()
                .unwrap();

            rt.block_on(async {
                let listener = TcpListener::bind(addr).await.unwrap();

                // PHASE 2: Adaptive connection management with backpressure tuning
                let base_connections = self.worker_threads * 50;
                let max_connections = (base_connections * 110) / 100; // 10% headroom for bursts
                let connection_semaphore = Arc::new(tokio::sync::Semaphore::new(max_connections));

                loop {
                    let (stream, _) = listener.accept().await.unwrap();

                    // Acquire connection permit (backpressure control)
                    let permit = match connection_semaphore.clone().try_acquire_owned() {
                        Ok(permit) => permit,
                        Err(_) => {
                            // Too many connections, drop this one
                            drop(stream);
                            continue;
                        }
                    };

                    let io = TokioIo::new(stream);
                    let handlers_clone = Arc::clone(&handlers);
                    let router_clone = Arc::clone(&router);
                    let loop_shards_clone = loop_shards.clone(); // LOOP SHARDING: Clone shards Vec

                    // Spawn optimized connection handler
                    tokio::task::spawn(async move {
                        let _permit = permit; // Keep permit until connection closes

                        let _ = http1::Builder::new()
                            .keep_alive(true) // Enable keep-alive
                            .half_close(true) // Better connection handling
                            .pipeline_flush(true) // PHASE 2: Enable response pipelining
                            .max_buf_size(16384) // PHASE 2: Optimize buffer size for HTTP/2 compatibility
                            .serve_connection(
                                io,
                                service_fn(move |req| {
                                    let handlers = Arc::clone(&handlers_clone);
                                    let router = Arc::clone(&router_clone);
                                    let loop_shards = loop_shards_clone.clone(); // LOOP SHARDING
                                    handle_request(req, handlers, router, loop_shards)
                                }),
                            )
                            .await;
                        // Connection automatically cleaned up when task ends
                    });
                }
            })
        });

        Ok(())
    }

    /// PHASE D: Start the HTTP server with Pure Rust Async Runtime (Tokio)
    /// Expected: 3-5x performance improvement (10-18K RPS target!)
    pub fn run_tokio(&self, py: Python) -> PyResult<()> {
        eprintln!("üöÄ PHASE D: Starting TurboAPI with Pure Rust Async Runtime!");

        // Parse address
        let mut addr_str = String::with_capacity(self.host.len() + 10);
        addr_str.push_str(&self.host);
        addr_str.push(':');
        addr_str.push_str(&self.port.to_string());

        let addr: SocketAddr = addr_str
            .parse()
            .map_err(|_| pyo3::exceptions::PyValueError::new_err("Invalid address"))?;

        let handlers = Arc::clone(&self.handlers);
        let router = Arc::clone(&self.router);

        // PHASE D: Initialize Tokio runtime (replaces loop shards!)
        let tokio_runtime = initialize_tokio_runtime()?;
        eprintln!("‚úÖ Tokio runtime initialized successfully!");

        py.allow_threads(|| {
            // PHASE D: Create Tokio multi-threaded runtime
            // Uses work-stealing scheduler across all CPU cores!
            let cpu_cores = num_cpus::get();
            eprintln!(
                "üöÄ Creating Tokio runtime with {} worker threads",
                cpu_cores
            );

            let rt = tokio::runtime::Builder::new_multi_thread()
                .worker_threads(cpu_cores) // Use all CPU cores
                .thread_name("tokio-worker")
                .enable_all()
                .build()
                .unwrap();

            rt.block_on(async {
                let listener = TcpListener::bind(addr).await.unwrap();
                eprintln!("‚úÖ Server listening on {}", addr);
                eprintln!("üéØ Target: 10-18K RPS with Tokio work-stealing scheduler!");

                // Connection management
                let max_connections = cpu_cores * 100; // Higher capacity with Tokio
                let connection_semaphore = Arc::new(tokio::sync::Semaphore::new(max_connections));

                loop {
                    let (stream, _) = listener.accept().await.unwrap();

                    // Acquire connection permit
                    let permit = match connection_semaphore.clone().try_acquire_owned() {
                        Ok(permit) => permit,
                        Err(_) => {
                            drop(stream);
                            continue;
                        }
                    };

                    let io = TokioIo::new(stream);
                    let handlers_clone = Arc::clone(&handlers);
                    let router_clone = Arc::clone(&router);
                    let tokio_runtime_clone = tokio_runtime.clone();

                    // PHASE D: Spawn Tokio task (work-stealing across all cores!)
                    tokio::task::spawn(async move {
                        let _permit = permit;

                        let _ = http1::Builder::new()
                            .keep_alive(true)
                            .half_close(true)
                            .pipeline_flush(true)
                            .max_buf_size(16384)
                            .serve_connection(
                                io,
                                service_fn(move |req| {
                                    let handlers = Arc::clone(&handlers_clone);
                                    let router = Arc::clone(&router_clone);
                                    let runtime = tokio_runtime_clone.clone();
                                    // PHASE D: Use Tokio-based request handler!
                                    handle_request_tokio(req, handlers, router, runtime)
                                }),
                            )
                            .await;
                    });
                }
            })
        });

        Ok(())
    }

    /// Get server info with comprehensive performance metrics
    pub fn info(&self) -> String {
        // PHASE 2+: Production-ready server info with all optimizations
        let mut info = String::with_capacity(self.host.len() + 200);
        info.push_str("üöÄ TurboServer PRODUCTION v2.0 running on ");
        info.push_str(&self.host);
        info.push(':');
        info.push_str(&self.port.to_string());
        info.push_str("\n   ‚ö° Worker threads: ");
        info.push_str(&self.worker_threads.to_string());
        info.push_str(" (3x CPU cores, optimized)");
        info.push_str("\n   üîß Optimizations: Phase 2+ Complete");
        info.push_str("\n   üìä Features: Rate limiting, Response caching, HTTP/2 ready");
        info.push_str("\n   üõ°Ô∏è  Security: Enhanced error handling, IP-based rate limits");
        info.push_str("\n   üí´ Performance: Zero-alloc routes, Object pooling, SIMD JSON");
        info.push_str("\n   üéØ Status: Production Ready - High Performance Web Framework");
        info
    }
}

async fn handle_request(
    req: Request<IncomingBody>,
    handlers: Arc<RwLock<HashMap<String, HandlerMetadata>>>, // HYBRID: HandlerMetadata with is_async cached!
    router: Arc<RwLock<RadixRouter>>,
    loop_shards: Vec<LoopShard>, // LOOP SHARDING: Multiple shards for parallel processing!
) -> Result<Response<Full<Bytes>>, Infallible> {
    // Extract parts first before borrowing
    let (parts, body) = req.into_parts();
    let method_str = parts.method.as_str();
    let path = parts.uri.path();
    let query_string = parts.uri.query().unwrap_or("");
    let body_bytes = match body.collect().await {
        Ok(collected) => collected.to_bytes(),
        Err(e) => {
            eprintln!("Failed to read request body: {}", e);
            Bytes::new()
        }
    };

    // Extract headers into HashMap for Python
    let mut headers_map = std::collections::HashMap::new();
    for (name, value) in parts.headers.iter() {
        if let Ok(value_str) = value.to_str() {
            headers_map.insert(name.as_str().to_string(), value_str.to_string());
        }
    }

    // PHASE 2+: Basic rate limiting check (DISABLED BY DEFAULT FOR BENCHMARKING)
    // Rate limiting is completely disabled by default to ensure accurate benchmarks
    // Users can explicitly enable it in production if needed
    let rate_config = RATE_LIMIT_CONFIG.get();
    if let Some(config) = rate_config {
        if config.enabled {
            // Extract client IP from headers
            let client_ip = parts
                .headers
                .get("x-forwarded-for")
                .and_then(|v| v.to_str().ok())
                .and_then(|s| s.split(',').next())
                .map(|s| s.trim().to_string())
                .or_else(|| {
                    parts
                        .headers
                        .get("x-real-ip")
                        .and_then(|v| v.to_str().ok())
                        .map(|s| s.to_string())
                });

            if let Some(ip) = client_ip {
                if !check_rate_limit(&ip) {
                    let rate_limit_json = format!(
                        r#"{{"error": "RateLimitExceeded", "message": "Too many requests", "retry_after": 60}}"#
                    );
                    return Ok(Response::builder()
                        .status(429)
                        .header("content-type", "application/json")
                        .header("retry-after", "60")
                        .body(Full::new(Bytes::from(rate_limit_json)))
                        .unwrap());
                }
            }
        }
    }
    // If no config is set, rate limiting is completely disabled (default behavior)

    // PHASE 2: Zero-allocation route key using static buffer
    let mut route_key_buffer = [0u8; 256];
    let route_key = create_route_key_fast(method_str, path, &mut route_key_buffer);

    // OPTIMIZED: Single read lock acquisition for handler lookup
    let handlers_guard = handlers.read().await;
    let metadata = handlers_guard.get(&route_key).cloned();
    drop(handlers_guard); // Immediate lock release

    // Process handler if found
    if let Some(metadata) = metadata {
        // PHASE 3: Fast dispatch based on handler type classification
        let response_result = match &metadata.handler_type {
            // FAST PATH: Simple sync handlers (GET with path/query params only)
            // Rust parses everything, calls original handler, serializes response with SIMD
            HandlerType::SimpleSyncFast => {
                if let Some(ref orig) = metadata.original_handler {
                    call_python_handler_fast(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &metadata.param_types,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            // FAST PATH: Body sync handlers (POST/PUT with JSON body)
            // Rust parses body with simd-json, calls original handler, serializes with SIMD
            HandlerType::BodySyncFast => {
                if let Some(ref orig) = metadata.original_handler {
                    call_python_handler_fast_body(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &body_bytes,
                        &metadata.param_types,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            // FAST PATH: Model sync handlers (POST/PUT with dhi model validation)
            // Rust parses JSON with simd-json, validates with model in Python
            HandlerType::ModelSyncFast => {
                if let (Some(ref orig), Some((ref param_name, ref model_class))) =
                    (&metadata.original_handler, &metadata.model_info)
                {
                    call_python_handler_fast_model(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &body_bytes,
                        param_name,
                        model_class,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            // ENHANCED PATH: Full Python wrapper (async, dependencies, etc.)
            HandlerType::Enhanced => {
                if metadata.is_async {
                    // ASYNC: shard dispatch
                    let shard_id = hash_route_key(&route_key) % loop_shards.len();
                    let shard = &loop_shards[shard_id];
                    let shard_tx = &shard.tx;

                    let (resp_tx, resp_rx) = oneshot::channel();
                    let python_req = PythonRequest {
                        handler: metadata.handler.clone(),
                        is_async: true,
                        method: method_str.to_string(),
                        path: path.to_string(),
                        query_string: query_string.to_string(),
                        body: body_bytes.clone(),
                        response_tx: resp_tx,
                    };

                    match shard_tx.send(python_req).await {
                        Ok(_) => match resp_rx.await {
                            Ok(result) => result,
                            Err(_) => Err("Loop shard died".to_string()),
                        },
                        Err(_) => {
                            return Ok(Response::builder()
                                .status(503)
                                .body(Full::new(Bytes::from(r#"{"error": "Service Unavailable", "message": "Server overloaded"}"#)))
                                .unwrap());
                        }
                    }
                } else {
                    // SYNC Enhanced: call with Python wrapper
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
        };

        match response_result {
            Ok(handler_response) => {
                let content_length = handler_response.body.len().to_string();

                // PHASE 2: Use zero-copy buffers for large responses
                let response_body = if method_str.to_ascii_uppercase() == "HEAD" {
                    Full::new(Bytes::new())
                } else if handler_response.body.len() > 1024 {
                    // Use zero-copy buffer for large responses (>1KB)
                    Full::new(create_zero_copy_response(&handler_response.body))
                } else {
                    // Small responses: direct conversion
                    Full::new(Bytes::from(handler_response.body))
                };

                return Ok(Response::builder()
                    .status(handler_response.status_code)
                    .header("content-type", "application/json")
                    .header("content-length", content_length)
                    .body(response_body)
                    .unwrap());
            }
            Err(e) => {
                // PHASE 2+: Enhanced error handling with recovery attempts
                eprintln!("Handler error for {} {}: {}", method_str, path, e);

                // Try to determine error type for better response
                let (status_code, error_type) = match e.to_string() {
                    err_str if err_str.contains("validation") => (400, "ValidationError"),
                    err_str if err_str.contains("timeout") => (408, "TimeoutError"),
                    err_str if err_str.contains("not found") => (404, "NotFoundError"),
                    _ => (500, "InternalServerError"),
                };

                let error_json = format!(
                    r#"{{"error": "{}", "message": "Request failed: {}", "method": "{}", "path": "{}", "timestamp": {}}}"#,
                    error_type,
                    e.to_string().chars().take(200).collect::<String>(),
                    method_str,
                    path,
                    std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs()
                );

                return Ok(Response::builder()
                    .status(status_code)
                    .header("content-type", "application/json")
                    .header("x-error-recovery", "attempted")
                    .body(Full::new(Bytes::from(error_json)))
                    .unwrap());
            }
        }
    }

    // Check router for path parameters as fallback
    let router_guard = router.read().await;
    let route_match = router_guard.find_route(&method_str, &path);
    drop(router_guard);

    if let Some(route_match) = route_match {
        // Found a parameterized route - look up handler using the pattern key
        let handlers_guard = handlers.read().await;
        let metadata = handlers_guard.get(&route_match.handler_key).cloned();
        drop(handlers_guard);

        if let Some(metadata) = metadata {
            // Dispatch to handler based on type (same logic as static routes)
            let response_result = match &metadata.handler_type {
                HandlerType::SimpleSyncFast => {
                    if let Some(ref orig) = metadata.original_handler {
                        call_python_handler_fast(
                            orig,
                            &metadata.route_pattern,
                            path,
                            query_string,
                            &metadata.param_types,
                        )
                    } else {
                        call_python_handler_sync_direct(
                            &metadata.handler,
                            method_str,
                            path,
                            query_string,
                            &body_bytes,
                            &headers_map,
                        )
                    }
                }
                HandlerType::BodySyncFast => {
                    if let Some(ref orig) = metadata.original_handler {
                        call_python_handler_fast_body(
                            orig,
                            &metadata.route_pattern,
                            path,
                            query_string,
                            &body_bytes,
                            &metadata.param_types,
                        )
                    } else {
                        call_python_handler_sync_direct(
                            &metadata.handler,
                            method_str,
                            path,
                            query_string,
                            &body_bytes,
                            &headers_map,
                        )
                    }
                }
                HandlerType::ModelSyncFast => {
                    if let (Some(ref orig), Some((ref param_name, ref model_class))) =
                        (&metadata.original_handler, &metadata.model_info)
                    {
                        call_python_handler_fast_model(
                            orig,
                            &metadata.route_pattern,
                            path,
                            query_string,
                            &body_bytes,
                            param_name,
                            model_class,
                        )
                    } else {
                        call_python_handler_sync_direct(
                            &metadata.handler,
                            method_str,
                            path,
                            query_string,
                            &body_bytes,
                            &headers_map,
                        )
                    }
                }
                HandlerType::Enhanced => {
                    if metadata.is_async {
                        let shard_id = hash_route_key(&route_match.handler_key) % loop_shards.len();
                        let shard = &loop_shards[shard_id];
                        let shard_tx = &shard.tx;

                        let (resp_tx, resp_rx) = oneshot::channel();
                        let python_req = PythonRequest {
                            handler: metadata.handler.clone(),
                            is_async: true,
                            method: method_str.to_string(),
                            path: path.to_string(),
                            query_string: query_string.to_string(),
                            body: body_bytes.clone(),
                            response_tx: resp_tx,
                        };

                        match shard_tx.send(python_req).await {
                            Ok(_) => match resp_rx.await {
                                Ok(result) => result,
                                Err(_) => Err("Loop shard died".to_string()),
                            },
                            Err(_) => {
                                return Ok(Response::builder()
                                    .status(503)
                                    .body(Full::new(Bytes::from(r#"{"error": "Service Unavailable", "message": "Server overloaded"}"#)))
                                    .unwrap());
                            }
                        }
                    } else {
                        call_python_handler_sync_direct(
                            &metadata.handler,
                            method_str,
                            path,
                            query_string,
                            &body_bytes,
                            &headers_map,
                        )
                    }
                }
            };

            match response_result {
                Ok(handler_response) => {
                    let content_length = handler_response.body.len().to_string();
                    let response_body = if method_str.to_ascii_uppercase() == "HEAD" {
                        Full::new(Bytes::new())
                    } else if handler_response.body.len() > 1024 {
                        Full::new(create_zero_copy_response(&handler_response.body))
                    } else {
                        Full::new(Bytes::from(handler_response.body))
                    };

                    return Ok(Response::builder()
                        .status(handler_response.status_code)
                        .header("content-type", "application/json")
                        .header("content-length", content_length)
                        .body(response_body)
                        .unwrap());
                }
                Err(e) => {
                    eprintln!("Handler error for {} {}: {}", method_str, path, e);
                    let error_json = format!(
                        r#"{{"error": "InternalServerError", "message": "Request failed: {}", "method": "{}", "path": "{}"}}"#,
                        e.to_string().chars().take(200).collect::<String>(),
                        method_str,
                        path
                    );
                    return Ok(Response::builder()
                        .status(500)
                        .header("content-type", "application/json")
                        .body(Full::new(Bytes::from(error_json)))
                        .unwrap());
                }
            }
        }
    }

    // No registered handler found, return 404
    let not_found_json = format!(
        r#"{{"error": "Not Found", "message": "No handler registered for {} {}", "method": "{}", "path": "{}", "available_routes": "Check registered routes"}}"#,
        method_str, path, method_str, path
    );

    Ok(Response::builder()
        .status(404)
        .header("content-type", "application/json")
        .body(Full::new(Bytes::from(not_found_json)))
        .unwrap())
}

/// PHASE 2: Fast route key creation without allocations
fn create_route_key_fast(method: &str, path: &str, buffer: &mut [u8]) -> String {
    // Use stack buffer for common cases, fall back to heap for large routes
    let method_upper = method.to_ascii_uppercase();
    let total_len = method_upper.len() + 1 + path.len();

    if total_len <= buffer.len() {
        // Fast path: use stack buffer
        let mut pos = 0;
        for byte in method_upper.bytes() {
            buffer[pos] = byte;
            pos += 1;
        }
        buffer[pos] = b' ';
        pos += 1;
        for byte in path.bytes() {
            buffer[pos] = byte;
            pos += 1;
        }
        unsafe { String::from_utf8_unchecked(buffer[..pos].to_vec()) }
    } else {
        // Fallback: heap allocation for very long routes
        format!("{} {}", method_upper, path)
    }
}

/// PHASE 2: Object pool for request objects to reduce allocations
static REQUEST_OBJECT_POOL: OnceLock<std::sync::Mutex<Vec<PyObject>>> = OnceLock::new();

/// PHASE 2+: Simple rate limiting - track request counts per IP
static RATE_LIMIT_TRACKER: OnceLock<std::sync::Mutex<StdHashMap<String, (Instant, u32)>>> =
    OnceLock::new();

/// Rate limiting configuration
static RATE_LIMIT_CONFIG: OnceLock<RateLimitConfig> = OnceLock::new();

#[derive(Clone)]
struct RateLimitConfig {
    enabled: bool,
    requests_per_minute: u32,
}

impl Default for RateLimitConfig {
    fn default() -> Self {
        Self {
            enabled: false,                 // Disabled by default for benchmarking
            requests_per_minute: 1_000_000, // Very high default limit (1M req/min)
        }
    }
}

/// Configure rate limiting settings
#[pyfunction]
pub fn configure_rate_limiting(enabled: bool, requests_per_minute: Option<u32>) {
    let config = RateLimitConfig {
        enabled,
        requests_per_minute: requests_per_minute.unwrap_or(1_000_000), // Default to 1M req/min
    };
    let _ = RATE_LIMIT_CONFIG.set(config);
}

/// Legacy fast handler call (unused, kept for reference)
#[allow(dead_code)]
fn call_python_handler_fast_legacy(
    handler: Handler,
    method_str: &str,
    path: &str,
    query_string: &str,
    body: &Bytes,
) -> Result<String, pyo3::PyErr> {
    Python::with_gil(|py| {
        // Get cached modules (initialized once)
        let types_module = CACHED_TYPES_MODULE.get_or_init(|| py.import("types").unwrap().into());
        let json_module = CACHED_JSON_MODULE.get_or_init(|| py.import("json").unwrap().into());
        let builtins_module =
            CACHED_BUILTINS_MODULE.get_or_init(|| py.import("builtins").unwrap().into());

        // PHASE 2: Try to reuse request object from pool
        let request_obj = get_pooled_request_object(py, types_module)?;

        // Set attributes directly (no intermediate conversions)
        request_obj.setattr(py, "method", method_str)?;
        request_obj.setattr(py, "path", path)?;
        request_obj.setattr(py, "query_string", query_string)?;

        // Set body as bytes
        let body_py = pyo3::types::PyBytes::new(py, body.as_ref());
        request_obj.setattr(py, "body", body_py.clone())?;

        // Use cached empty dict for headers
        let empty_dict = builtins_module.getattr(py, "dict")?.call0(py)?;
        request_obj.setattr(py, "headers", empty_dict)?;

        // Create get_body method that returns the body
        request_obj.setattr(py, "get_body", body_py)?;

        // Call handler directly
        let result = handler.call1(py, (request_obj,))?;

        // PHASE 2: Fast JSON serialization with fallback
        // Use Python JSON module for compatibility
        let json_dumps = json_module.getattr(py, "dumps")?;
        let json_str = json_dumps.call1(py, (result,))?;
        json_str.extract(py)
    })
}

// PHASE 2: Simplified for compatibility - complex SIMD optimizations removed for stability

/// PHASE 2: Get pooled request object to reduce allocations
fn get_pooled_request_object(py: Python, types_module: &PyObject) -> PyResult<PyObject> {
    // Try to get from pool first
    let pool = REQUEST_OBJECT_POOL.get_or_init(|| std::sync::Mutex::new(Vec::new()));

    if let Ok(mut pool_guard) = pool.try_lock() {
        if let Some(obj) = pool_guard.pop() {
            return Ok(obj);
        }
    }

    // If pool is empty or locked, create new object
    let simple_namespace = types_module.getattr(py, "SimpleNamespace")?;
    simple_namespace.call0(py)
}

/// PHASE 2: Return request object to pool for reuse
#[allow(dead_code)]
fn return_pooled_request_object(obj: PyObject) {
    let pool = REQUEST_OBJECT_POOL.get_or_init(|| std::sync::Mutex::new(Vec::new()));

    if let Ok(mut pool_guard) = pool.try_lock() {
        if pool_guard.len() < 50 {
            // Limit pool size
            pool_guard.push(obj);
        }
    }
    // If pool is full or locked, let object be dropped normally
}

/// PHASE 2+: Extract client IP for rate limiting
fn extract_client_ip(req: &Request<IncomingBody>) -> Option<String> {
    // Try X-Forwarded-For header first (common in reverse proxy setups)
    if let Some(forwarded) = req.headers().get("x-forwarded-for") {
        if let Ok(forwarded_str) = forwarded.to_str() {
            return Some(forwarded_str.split(',').next()?.trim().to_string());
        }
    }

    // Fallback to X-Real-IP header
    if let Some(real_ip) = req.headers().get("x-real-ip") {
        if let Ok(ip_str) = real_ip.to_str() {
            return Some(ip_str.to_string());
        }
    }

    // Note: In a real implementation, we'd extract from connection info
    // For now, return a placeholder
    Some("127.0.0.1".to_string())
}

/// PHASE 2+: Simple rate limiting check (configurable)
fn check_rate_limit(client_ip: &str) -> bool {
    let rate_config = RATE_LIMIT_CONFIG.get_or_init(|| RateLimitConfig::default());
    let tracker = RATE_LIMIT_TRACKER.get_or_init(|| std::sync::Mutex::new(StdHashMap::new()));

    if let Ok(mut tracker_guard) = tracker.try_lock() {
        let now = Instant::now();
        let limit = rate_config.requests_per_minute;
        let window = Duration::from_secs(60);

        let entry = tracker_guard
            .entry(client_ip.to_string())
            .or_insert((now, 0));

        // Reset counter if window expired
        if now.duration_since(entry.0) > window {
            entry.0 = now;
            entry.1 = 0;
        }

        entry.1 += 1;
        let result = entry.1 <= limit;

        // Clean up old entries occasionally (simple approach)
        if tracker_guard.len() > 10000 {
            tracker_guard.retain(|_, (timestamp, _)| now.duration_since(*timestamp) < window);
        }

        result
    } else {
        // If lock is contended, allow request (fail open for performance)
        true
    }
}

/// PHASE 2: Create zero-copy response using efficient memory management
fn create_zero_copy_response(data: &str) -> Bytes {
    // For now, use direct conversion but optimized for future zero-copy implementation
    // In production, this would use memory-mapped buffers or shared memory
    Bytes::from(data.to_string())
}

// ============================================================================
// PHASE D: PURE RUST ASYNC RUNTIME WITH TOKIO
// ============================================================================

/// Initialize Tokio runtime for pure Rust async execution
/// This replaces Python event loop shards with Tokio's work-stealing scheduler
/// Expected: 3-5x performance improvement (10-18K RPS target!)
fn initialize_tokio_runtime() -> PyResult<TokioRuntime> {
    eprintln!("üöÄ PHASE D: Initializing Pure Rust Async Runtime with Tokio...");

    // Note: No need to call prepare_freethreaded_python() since we're a Python extension
    // Python is already initialized when our module is loaded

    // Create single Python event loop for pyo3-async-runtimes
    // This is only used for Python asyncio primitives (asyncio.sleep, etc.)
    let (task_locals, json_dumps_fn, event_loop_handle) = Python::with_gil(|py| -> PyResult<_> {
        let asyncio = py.import("asyncio")?;
        let event_loop = asyncio.call_method0("new_event_loop")?;
        asyncio.call_method1("set_event_loop", (&event_loop,))?;

        eprintln!("‚úÖ Python event loop created (for asyncio primitives)");

        let task_locals = pyo3_async_runtimes::TaskLocals::new(event_loop.clone());
        let json_module = py.import("json")?;
        let json_dumps_fn: PyObject = json_module.getattr("dumps")?.into();
        let event_loop_handle: PyObject = event_loop.unbind();

        Ok((task_locals, json_dumps_fn, event_loop_handle))
    })?;

    // Start Python event loop in background thread
    // This is needed for asyncio primitives (asyncio.sleep, etc.) to work
    let event_loop_for_runner = Python::with_gil(|py| event_loop_handle.clone_ref(py));
    std::thread::spawn(move || {
        Python::with_gil(|py| {
            let loop_obj = event_loop_for_runner.bind(py);
            eprintln!("üîÑ Python event loop running in background...");
            let _ = loop_obj.call_method0("run_forever");
        });
    });

    // Create Tokio semaphore for rate limiting
    // Total capacity: 512 * num_cpus (e.g., 7,168 for 14 cores)
    let num_cpus = num_cpus::get();
    let total_capacity = 512 * num_cpus;
    let semaphore = Arc::new(tokio::sync::Semaphore::new(total_capacity));

    eprintln!("‚úÖ Tokio semaphore created (capacity: {})", total_capacity);
    eprintln!("‚úÖ Tokio runtime ready with {} worker threads", num_cpus);

    Ok(TokioRuntime {
        task_locals,
        json_dumps_fn,
        semaphore,
    })
}

/// Process request using Tokio runtime (PHASE D)
/// Uses Python::attach for free-threading (no GIL overhead!)
async fn process_request_tokio(
    handler: Handler,
    is_async: bool,
    runtime: &TokioRuntime,
) -> Result<HandlerResponse, String> {
    // Acquire semaphore permit for rate limiting
    let _permit = runtime
        .semaphore
        .acquire()
        .await
        .map_err(|e| format!("Semaphore error: {}", e))?;

    if is_async {
        // PHASE D: Async handler with Tokio + pyo3-async-runtimes
        // Use Python::attach (no GIL in free-threading mode!)
        let future = Python::with_gil(|py| {
            // Call async handler to get coroutine
            let coroutine = handler
                .bind(py)
                .call0()
                .map_err(|e| format!("Handler error: {}", e))?;

            // Convert Python coroutine to Rust Future using pyo3-async-runtimes
            // This allows Tokio to manage the async execution!
            pyo3_async_runtimes::into_future_with_locals(&runtime.task_locals, coroutine)
                .map_err(|e| format!("Failed to convert coroutine: {}", e))
        })?;

        // Await the Rust future on Tokio runtime (non-blocking!)
        let result = future
            .await
            .map_err(|e| format!("Async execution error: {}", e))?;

        // Serialize result
        Python::with_gil(|py| serialize_result_optimized(py, result, &runtime.json_dumps_fn))
    } else {
        // Sync handler - direct call with Python::attach
        Python::with_gil(|py| {
            let result = handler
                .bind(py)
                .call0()
                .map_err(|e| format!("Handler error: {}", e))?;
            serialize_result_optimized(py, result.unbind(), &runtime.json_dumps_fn)
        })
    }
}

/// Handle HTTP request using Tokio runtime (PHASE D)
/// This replaces the loop shard approach with pure Tokio task spawning
async fn handle_request_tokio(
    req: Request<IncomingBody>,
    handlers: Arc<RwLock<HashMap<String, HandlerMetadata>>>,
    router: Arc<RwLock<RadixRouter>>,
    tokio_runtime: TokioRuntime,
) -> Result<Response<Full<Bytes>>, Infallible> {
    // Extract parts first before borrowing
    let (parts, body) = req.into_parts();
    let method_str = parts.method.as_str();
    let path = parts.uri.path();
    let query_string = parts.uri.query().unwrap_or("");
    let body_bytes = match body.collect().await {
        Ok(collected) => collected.to_bytes(),
        Err(e) => {
            eprintln!("Failed to read request body: {}", e);
            Bytes::new()
        }
    };

    // Rate limiting check (same as before)
    let rate_config = RATE_LIMIT_CONFIG.get();
    if let Some(config) = rate_config {
        if config.enabled {
            let client_ip = parts
                .headers
                .get("x-forwarded-for")
                .and_then(|v| v.to_str().ok())
                .and_then(|s| s.split(',').next())
                .map(|s| s.trim().to_string())
                .or_else(|| {
                    parts
                        .headers
                        .get("x-real-ip")
                        .and_then(|v| v.to_str().ok())
                        .map(|s| s.to_string())
                });

            if let Some(ip) = client_ip {
                if !check_rate_limit(&ip) {
                    let rate_limit_json = format!(
                        r#"{{"error": "RateLimitExceeded", "message": "Too many requests", "retry_after": 60}}"#
                    );
                    return Ok(Response::builder()
                        .status(429)
                        .header("content-type", "application/json")
                        .header("retry-after", "60")
                        .body(Full::new(Bytes::from(rate_limit_json)))
                        .unwrap());
                }
            }
        }
    }

    // Zero-allocation route key
    let mut route_key_buffer = [0u8; 256];
    let route_key = create_route_key_fast(method_str, path, &mut route_key_buffer);

    // Single read lock acquisition for handler lookup
    let handlers_guard = handlers.read().await;
    let metadata = handlers_guard.get(&route_key).cloned();
    drop(handlers_guard);

    // Extract headers for Enhanced path
    let mut headers_map = std::collections::HashMap::new();
    for (name, value) in parts.headers.iter() {
        if let Ok(value_str) = value.to_str() {
            headers_map.insert(name.as_str().to_string(), value_str.to_string());
        }
    }

    // Process handler if found
    if let Some(metadata) = metadata {
        // PHASE 3: Fast dispatch based on handler type
        let response_result = match &metadata.handler_type {
            HandlerType::SimpleSyncFast => {
                if let Some(ref orig) = metadata.original_handler {
                    call_python_handler_fast(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &metadata.param_types,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            HandlerType::BodySyncFast => {
                if let Some(ref orig) = metadata.original_handler {
                    call_python_handler_fast_body(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &body_bytes,
                        &metadata.param_types,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            HandlerType::ModelSyncFast => {
                if let (Some(ref orig), Some((ref param_name, ref model_class))) =
                    (&metadata.original_handler, &metadata.model_info)
                {
                    call_python_handler_fast_model(
                        orig,
                        &metadata.route_pattern,
                        path,
                        query_string,
                        &body_bytes,
                        param_name,
                        model_class,
                    )
                } else {
                    call_python_handler_sync_direct(
                        &metadata.handler,
                        method_str,
                        path,
                        query_string,
                        &body_bytes,
                        &headers_map,
                    )
                }
            }
            HandlerType::Enhanced => {
                process_request_tokio(metadata.handler.clone(), metadata.is_async, &tokio_runtime)
                    .await
            }
        };

        match response_result {
            Ok(handler_response) => Ok(Response::builder()
                .status(handler_response.status_code)
                .header("content-type", "application/json")
                .body(Full::new(Bytes::from(handler_response.body)))
                .unwrap()),
            Err(e) => {
                let error_json =
                    format!(r#"{{"error": "InternalServerError", "message": "{}"}}"#, e);
                Ok(Response::builder()
                    .status(500)
                    .header("content-type", "application/json")
                    .body(Full::new(Bytes::from(error_json)))
                    .unwrap())
            }
        }
    } else {
        // 404 Not Found
        let not_found_json = format!(
            r#"{{"error": "NotFound", "message": "Route not found: {} {}"}}"#,
            method_str, path
        );
        Ok(Response::builder()
            .status(404)
            .header("content-type", "application/json")
            .body(Full::new(Bytes::from(not_found_json)))
            .unwrap())
    }
}

// ============================================================================
// LOOP SHARDING - Phase A Implementation (OLD - will be replaced by Phase D)
// ============================================================================

/// Spawn N dedicated event loop shards for parallel async execution
/// Each shard has its own event loop thread - eliminates global contention!
/// This is the KEY optimization for reaching 5-6K RPS!
fn spawn_loop_shards(num_shards: usize) -> Vec<LoopShard> {
    eprintln!("üöÄ Spawning {} event loop shards...", num_shards);

    (0..num_shards)
        .map(|shard_id| {
            let (tx, mut rx) = mpsc::channel::<PythonRequest>(20000); // High capacity channel

            // Spawn dedicated thread for this shard
            thread::spawn(move || {
                let rt = tokio::runtime::Builder::new_current_thread()
                    .enable_all()
                    .build()
                    .expect("Failed to create shard runtime");

                let local = tokio::task::LocalSet::new();

                rt.block_on(local.run_until(async move {
                    eprintln!("üöÄ Loop shard {} starting...", shard_id);

                    // Note: Python is already initialized (extension module)

                    // PHASE B: Create event loop with semaphore limiter for this shard
                    let (task_locals, json_dumps_fn, event_loop_handle, limiter) =
                        Python::with_gil(|py| -> PyResult<_> {
                            let asyncio = py.import("asyncio")?;
                            let event_loop = asyncio.call_method0("new_event_loop")?;
                            asyncio.call_method1("set_event_loop", (&event_loop,))?;

                            eprintln!("‚úÖ Shard {} - event loop created", shard_id);

                            let task_locals =
                                pyo3_async_runtimes::TaskLocals::new(event_loop.clone());
                            let json_module = py.import("json")?;
                            let json_dumps_fn: PyObject = json_module.getattr("dumps")?.into();
                            let event_loop_handle: PyObject = event_loop.unbind();

                            // PHASE B: Create AsyncLimiter for semaphore gating (512 concurrent tasks max)
                            let limiter_module = py.import("turboapi.async_limiter")?;
                            let limiter = limiter_module.call_method1("get_limiter", (512,))?;
                            let limiter_obj: PyObject = limiter.into();

                            eprintln!(
                                "‚úÖ Shard {} - semaphore limiter created (512 max concurrent)",
                                shard_id
                            );

                            Ok((task_locals, json_dumps_fn, event_loop_handle, limiter_obj))
                        })
                        .expect("Failed to initialize shard");

                    // Start event loop on separate thread
                    let event_loop_for_runner =
                        Python::with_gil(|py| event_loop_handle.clone_ref(py));
                    std::thread::spawn(move || {
                        Python::with_gil(|py| {
                            let loop_obj = event_loop_for_runner.bind(py);
                            let _ = loop_obj.call_method0("run_forever");
                        });
                    });

                    eprintln!("‚úÖ Shard {} ready!", shard_id);

                    // PHASE C: ULTRA-AGGRESSIVE batching (256 requests!)
                    let mut batch = Vec::with_capacity(256);

                    while let Some(req) = rx.recv().await {
                        batch.push(req);

                        // PHASE C: Collect up to 256 requests for maximum throughput!
                        while batch.len() < 256 {
                            match rx.try_recv() {
                                Ok(req) => batch.push(req),
                                Err(_) => break,
                            }
                        }

                        // Separate and process
                        let mut async_batch = Vec::new();
                        let mut sync_batch = Vec::new();

                        for req in batch.drain(..) {
                            if req.is_async {
                                async_batch.push(req);
                            } else {
                                sync_batch.push(req);
                            }
                        }

                        // Process sync
                        for req in sync_batch {
                            let PythonRequest {
                                handler,
                                is_async,
                                method: _,
                                path: _,
                                query_string: _,
                                body: _,
                                response_tx,
                            } = req;
                            let result = process_request_optimized(
                                handler,
                                is_async,
                                &task_locals,
                                &json_dumps_fn,
                                &limiter,
                            )
                            .await;
                            let _ = response_tx.send(result);
                        }

                        // PHASE B: Process async concurrently with semaphore gating
                        if !async_batch.is_empty() {
                            let futures: Vec<_> = async_batch
                                .iter()
                                .map(|req| {
                                    process_request_optimized(
                                        req.handler.clone(),
                                        req.is_async,
                                        &task_locals,
                                        &json_dumps_fn,
                                        &limiter, // PHASE B: Pass limiter for semaphore gating
                                    )
                                })
                                .collect();

                            let results = futures::future::join_all(futures).await;

                            for (req, result) in async_batch.into_iter().zip(results) {
                                let _ = req.response_tx.send(result);
                            }
                        }
                    }
                }));
            });

            // Return shard handle - create a dummy event loop for the handle
            // The actual event loop is running in the spawned thread
            // These handles are only used for cloning, not actual execution
            let (task_locals_handle, json_dumps_fn_handle, limiter_handle) =
                Python::with_gil(|py| -> PyResult<_> {
                    // Create a temporary event loop just for the handle
                    let asyncio = py.import("asyncio")?;
                    let temp_loop = asyncio.call_method0("new_event_loop")?;
                    let task_locals = pyo3_async_runtimes::TaskLocals::new(temp_loop);
                    let json_module = py.import("json")?;
                    let json_dumps_fn: PyObject = json_module.getattr("dumps")?.into();

                    // Create limiter for handle
                    let limiter_module = py.import("turboapi.async_limiter")?;
                    let limiter = limiter_module.call_method1("get_limiter", (512,))?;
                    let limiter_obj: PyObject = limiter.into();

                    Ok((task_locals, json_dumps_fn, limiter_obj))
                })
                .expect("Failed to create shard handle");

            LoopShard {
                shard_id,
                task_locals: task_locals_handle,
                json_dumps_fn: json_dumps_fn_handle,
                limiter: limiter_handle,
                tx,
            }
        })
        .collect()
}

// ============================================================================
// UTILITIES
// ============================================================================

/// Simple hash function for shard selection (FNV-1a hash)
/// Hash-based distribution keeps same handler on same shard (hot caches!)
fn hash_route_key(route_key: &str) -> usize {
    let mut hash: u64 = 0xcbf29ce484222325; // FNV offset basis
    for byte in route_key.bytes() {
        hash ^= byte as u64;
        hash = hash.wrapping_mul(0x100000001b3); // FNV prime
    }
    hash as usize
}

// ============================================================================
// HYBRID APPROACH - Direct Sync Calls + Worker for Async
// ============================================================================

/// HYBRID: Direct synchronous Python handler call (NO channel overhead!)
/// This is the FAST PATH for sync handlers - bypasses the worker thread entirely
/// FREE-THREADING: Uses Python::attach() for TRUE parallelism (no GIL contention!)
fn call_python_handler_sync_direct(
    handler: &PyObject,
    method_str: &str,
    path: &str,
    query_string: &str,
    body_bytes: &Bytes,
    headers_map: &std::collections::HashMap<String, String>,
) -> Result<HandlerResponse, String> {
    // FREE-THREADING: Python::attach() instead of Python::with_gil()
    // This allows TRUE parallel execution on Python 3.14+ with --disable-gil
    Python::attach(|py| {
        // Get cached modules
        let json_module = CACHED_JSON_MODULE.get_or_init(|| py.import("json").unwrap().into());

        // Create kwargs dict with request data for enhanced handler
        use pyo3::types::PyDict;
        let kwargs = PyDict::new(py);

        // Add body as bytes
        kwargs.set_item("body", body_bytes.as_ref()).ok();

        // Add headers dict
        let headers = PyDict::new(py);
        for (key, value) in headers_map {
            headers.set_item(key, value).ok();
        }
        kwargs.set_item("headers", headers).ok();

        // Add method
        kwargs.set_item("method", method_str).ok();

        // Add path
        kwargs.set_item("path", path).ok();

        // Add query string
        kwargs.set_item("query_string", query_string).ok();

        // Call handler with kwargs (body and headers)
        let result = handler
            .call(py, (), Some(&kwargs))
            .map_err(|e| format!("Python error: {}", e))?;

        // Enhanced handler returns {"content": ..., "status_code": ..., "content_type": ...}
        // Extract status_code and content
        let mut status_code: u16 = 200;
        let content = if let Ok(dict) = result.downcast_bound::<pyo3::types::PyDict>(py) {
            // Check for status_code in dict response
            if let Ok(Some(status_val)) = dict.get_item("status_code") {
                status_code = status_val
                    .extract::<i64>()
                    .ok()
                    .and_then(|v| u16::try_from(v).ok())
                    .unwrap_or(200);
            }
            if let Ok(Some(content_val)) = dict.get_item("content") {
                // Also check content for Response object with status_code
                if let Ok(inner_status) = content_val.getattr("status_code") {
                    status_code = inner_status
                        .extract::<i64>()
                        .ok()
                        .and_then(|v| u16::try_from(v).ok())
                        .unwrap_or(status_code);
                }
                content_val.unbind()
            } else {
                result
            }
        } else {
            // Check if result itself is a Response object with status_code
            let bound = result.bind(py);
            if let Ok(status_attr) = bound.getattr("status_code") {
                status_code = status_attr
                    .extract::<i64>()
                    .ok()
                    .and_then(|v| u16::try_from(v).ok())
                    .unwrap_or(200);
            }
            result
        };

        // PHASE 1: SIMD JSON serialization (eliminates json.dumps FFI!)
        let body = match content.extract::<String>(py) {
            Ok(json_str) => json_str,
            Err(_) => {
                // Use Rust SIMD serializer instead of Python json.dumps
                let bound = content.bind(py);
                simd_json::serialize_pyobject_to_json(py, bound)
                    .map_err(|e| format!("SIMD JSON error: {}", e))?
            }
        };

        Ok(HandlerResponse { body, status_code })
    })
}

// ============================================================================
// PHASE 3: FAST PATH - Direct handler calls with Rust-side parsing
// ============================================================================

/// FAST PATH for simple sync handlers (GET with path/query params only).
/// Rust parses query string and path params, calls Python handler directly,
/// then serializes the response with SIMD JSON ‚Äî single FFI crossing!
fn call_python_handler_fast(
    handler: &PyObject,
    route_pattern: &str,
    path: &str,
    query_string: &str,
    param_types: &HashMap<String, String>,
) -> Result<HandlerResponse, String> {
    Python::attach(|py| {
        let kwargs = PyDict::new(py);

        // Parse path params in Rust (SIMD-accelerated)
        simd_parse::set_path_params_into_pydict(py, route_pattern, path, &kwargs, param_types)
            .map_err(|e| format!("Path param error: {}", e))?;

        // Parse query string in Rust (SIMD-accelerated)
        simd_parse::parse_query_into_pydict(py, query_string, &kwargs, param_types)
            .map_err(|e| format!("Query param error: {}", e))?;

        // Single FFI call: Python handler with pre-parsed kwargs
        let result = handler
            .call(py, (), Some(&kwargs))
            .map_err(|e| format!("Handler error: {}", e))?;

        // Check if result is a Response object with status_code
        let bound = result.bind(py);
        let status_code = if let Ok(status_attr) = bound.getattr("status_code") {
            // Python integers are typically i64, convert to u16
            status_attr
                .extract::<i64>()
                .ok()
                .and_then(|v| u16::try_from(v).ok())
                .unwrap_or(200)
        } else {
            200
        };

        // SIMD JSON serialization of result (no json.dumps FFI!)
        let body = match result.extract::<String>(py) {
            Ok(s) => s,
            Err(_) => simd_json::serialize_pyobject_to_json(py, bound)
                .map_err(|e| format!("SIMD JSON error: {}", e))?,
        };

        Ok(HandlerResponse { body, status_code })
    })
}

/// FAST PATH for body sync handlers (POST/PUT with JSON body).
/// Rust parses body with simd-json, path/query params, calls handler directly,
/// then serializes response with SIMD JSON ‚Äî single FFI crossing!
fn call_python_handler_fast_body(
    handler: &PyObject,
    route_pattern: &str,
    path: &str,
    query_string: &str,
    body_bytes: &Bytes,
    param_types: &HashMap<String, String>,
) -> Result<HandlerResponse, String> {
    Python::attach(|py| {
        let kwargs = PyDict::new(py);

        // Parse path params in Rust
        simd_parse::set_path_params_into_pydict(py, route_pattern, path, &kwargs, param_types)
            .map_err(|e| format!("Path param error: {}", e))?;

        // Parse query string in Rust
        simd_parse::parse_query_into_pydict(py, query_string, &kwargs, param_types)
            .map_err(|e| format!("Query param error: {}", e))?;

        // Parse JSON body with simd-json (SIMD-accelerated!)
        if !body_bytes.is_empty() {
            let parsed = simd_parse::parse_json_body_into_pydict(
                py,
                body_bytes.as_ref(),
                &kwargs,
                param_types,
            )
            .map_err(|e| format!("Body parse error: {}", e))?;

            if !parsed {
                // Couldn't parse as simple JSON object, pass raw body
                kwargs
                    .set_item("body", body_bytes.as_ref())
                    .map_err(|e| format!("Body set error: {}", e))?;
            }
        }

        // Single FFI call: Python handler with pre-parsed kwargs
        let result = handler
            .call(py, (), Some(&kwargs))
            .map_err(|e| format!("Handler error: {}", e))?;

        // Check if result is a Response object with status_code
        let bound = result.bind(py);
        let status_code = if let Ok(status_attr) = bound.getattr("status_code") {
            // Python integers are typically i64, convert to u16
            status_attr
                .extract::<i64>()
                .ok()
                .and_then(|v| u16::try_from(v).ok())
                .unwrap_or(200)
        } else {
            200
        };

        // SIMD JSON serialization
        let body = match result.extract::<String>(py) {
            Ok(s) => s,
            Err(_) => simd_json::serialize_pyobject_to_json(py, bound)
                .map_err(|e| format!("SIMD JSON error: {}", e))?,
        };

        Ok(HandlerResponse { body, status_code })
    })
}

/// FAST PATH for model sync handlers (POST/PUT with dhi model validation).
/// Rust parses JSON body with simd-json into PyDict, calls model.model_validate(),
/// then passes validated model to handler ‚Äî bypasses Python json.loads entirely!
fn call_python_handler_fast_model(
    handler: &PyObject,
    route_pattern: &str,
    path: &str,
    query_string: &str,
    body_bytes: &Bytes,
    param_name: &str,
    model_class: &PyObject,
) -> Result<HandlerResponse, String> {
    Python::attach(|py| {
        let kwargs = PyDict::new(py);

        // Parse path params in Rust (SIMD-accelerated)
        let empty_types = HashMap::new();
        simd_parse::set_path_params_into_pydict(py, route_pattern, path, &kwargs, &empty_types)
            .map_err(|e| format!("Path param error: {}", e))?;

        // Parse query string in Rust (SIMD-accelerated)
        simd_parse::parse_query_into_pydict(py, query_string, &kwargs, &empty_types)
            .map_err(|e| format!("Query param error: {}", e))?;

        // Parse JSON body with simd-json into a Python dict
        if !body_bytes.is_empty() {
            // Use simd-json to parse into PyDict
            let body_dict = simd_parse::parse_json_to_pydict(py, body_bytes.as_ref())
                .map_err(|e| format!("JSON parse error: {}", e))?;

            // Validate with dhi model: model_class.model_validate(body_dict)
            let validated_model = model_class
                .bind(py)
                .call_method1("model_validate", (body_dict,))
                .map_err(|e| format!("Model validation error: {}", e))?;

            // Set the validated model as the parameter
            kwargs
                .set_item(param_name, validated_model)
                .map_err(|e| format!("Param set error: {}", e))?;
        }

        // Single FFI call: Python handler with validated model
        let result = handler
            .call(py, (), Some(&kwargs))
            .map_err(|e| format!("Handler error: {}", e))?;

        // Check if result is a Response object with status_code
        let bound = result.bind(py);
        let status_code = if let Ok(status_attr) = bound.getattr("status_code") {
            status_attr
                .extract::<i64>()
                .ok()
                .and_then(|v| u16::try_from(v).ok())
                .unwrap_or(200)
        } else {
            200
        };

        // SIMD JSON serialization of result
        let body = match result.extract::<String>(py) {
            Ok(s) => s,
            Err(_) => simd_json::serialize_pyobject_to_json(py, bound)
                .map_err(|e| format!("SIMD JSON error: {}", e))?,
        };

        Ok(HandlerResponse { body, status_code })
    })
}

// ============================================================================
// MULTI-WORKER PATTERN - Multiple Python Workers for Parallel Async Execution
// ============================================================================

/// Spawn N dedicated Python worker threads for parallel async execution
/// Each worker has its own current_thread runtime + PERSISTENT asyncio event loop!
/// This enables TRUE parallelism for async handlers with ZERO event loop creation overhead!
fn spawn_python_workers(num_workers: usize) -> Vec<mpsc::Sender<PythonRequest>> {
    eprintln!(
        "üöÄ Spawning {} Python workers with persistent event loops...",
        num_workers
    );

    (0..num_workers)
        .map(|worker_id| {
            let (tx, mut rx) = mpsc::channel::<PythonRequest>(20000); // INCREASED: 20K capacity for high throughput!

            thread::spawn(move || {
                // Create single-threaded Tokio runtime for this worker
                let rt = tokio::runtime::Builder::new_current_thread()
                    .enable_all()
                    .build()
                    .expect("Failed to create worker runtime");

                // Use LocalSet for !Send futures (Python objects)
                let local = tokio::task::LocalSet::new();

                rt.block_on(local.run_until(async move {
                    eprintln!("üöÄ Python worker {} starting...", worker_id);

                    // Note: Python is already initialized (extension module)

                    // OPTIMIZATION: Create persistent asyncio event loop and cache TaskLocals + callables!
                    let (task_locals, json_dumps_fn, event_loop_handle) =
                        Python::with_gil(|py| -> PyResult<_> {
                            // Import asyncio and create new event loop
                            let asyncio = py.import("asyncio")?;
                            let event_loop = asyncio.call_method0("new_event_loop")?;
                            asyncio.call_method1("set_event_loop", (&event_loop,))?;

                            eprintln!("‚úÖ Worker {} - asyncio event loop created", worker_id);

                            // Create TaskLocals with the event loop
                            let task_locals =
                                pyo3_async_runtimes::TaskLocals::new(event_loop.clone());

                            eprintln!("‚úÖ Worker {} - TaskLocals cached", worker_id);

                            // PRE-BIND json.dumps callable (avoid repeated getattr!)
                            let json_module = py.import("json")?;
                            let json_dumps_fn: PyObject = json_module.getattr("dumps")?.into();

                            eprintln!("‚úÖ Worker {} - json.dumps pre-bound", worker_id);

                            // Keep a handle to the event loop for running it
                            let event_loop_handle: PyObject = event_loop.unbind();

                            Ok((task_locals, json_dumps_fn, event_loop_handle))
                        })
                        .expect("Failed to initialize Python worker");

                    // Start the event loop in run_forever mode on a SEPARATE OS THREAD!
                    // This is CRITICAL - run_forever() blocks, so it needs its own thread!
                    let event_loop_for_runner =
                        Python::with_gil(|py| event_loop_handle.clone_ref(py));
                    std::thread::spawn(move || {
                        Python::with_gil(|py| {
                            let loop_obj = event_loop_for_runner.bind(py);
                            // This will block forever, processing scheduled tasks
                            // But that's OK because it's on a dedicated thread!
                            let _ = loop_obj.call_method0("run_forever");
                        });
                    });

                    eprintln!(
                        "‚úÖ Python worker {} ready with running event loop!",
                        worker_id
                    );

                    // Process requests with BATCHING for better throughput!
                    let mut batch = Vec::with_capacity(32);

                    while let Some(req) = rx.recv().await {
                        batch.push(req);

                        // Collect up to 32 requests or until no more immediately available
                        while batch.len() < 32 {
                            match rx.try_recv() {
                                Ok(req) => batch.push(req),
                                Err(_) => break, // No more requests ready
                            }
                        }

                        // Separate async and sync requests for batch processing
                        let mut async_batch = Vec::new();
                        let mut sync_batch = Vec::new();

                        for req in batch.drain(..) {
                            if req.is_async {
                                async_batch.push(req);
                            } else {
                                sync_batch.push(req);
                            }
                        }

                        // Process sync requests sequentially (fast anyway)
                        for req in sync_batch {
                            let PythonRequest {
                                handler,
                                is_async,
                                method: _,
                                path: _,
                                query_string: _,
                                body: _,
                                response_tx,
                            } = req;
                            // Note: This old worker function doesn't have limiter, using dummy
                            let dummy_limiter = Python::with_gil(|py| {
                                py.import("turboapi.async_limiter")
                                    .unwrap()
                                    .call_method1("get_limiter", (512,))
                                    .unwrap()
                                    .into()
                            });
                            let result = process_request_optimized(
                                handler,
                                is_async,
                                &task_locals,
                                &json_dumps_fn,
                                &dummy_limiter,
                            )
                            .await;
                            let _ = response_tx.send(result);
                        }

                        // Process async requests CONCURRENTLY with gather!
                        if !async_batch.is_empty() {
                            let dummy_limiter = Python::with_gil(|py| {
                                py.import("turboapi.async_limiter")
                                    .unwrap()
                                    .call_method1("get_limiter", (512,))
                                    .unwrap()
                                    .into()
                            });
                            let futures: Vec<_> = async_batch
                                .iter()
                                .map(|req| {
                                    process_request_optimized(
                                        req.handler.clone(),
                                        req.is_async,
                                        &task_locals,
                                        &json_dumps_fn,
                                        &dummy_limiter,
                                    )
                                })
                                .collect();

                            // Await all futures concurrently!
                            let results = futures::future::join_all(futures).await;

                            // Send results back
                            for (req, result) in async_batch.into_iter().zip(results) {
                                let _ = req.response_tx.send(result);
                            }
                        }
                    }

                    eprintln!("‚ö†Ô∏è  Python worker {} shutting down", worker_id);
                }));
            });

            tx
        })
        .collect()
}

/// Process request with cached TaskLocals - OPTIMIZED VERSION!
/// This eliminates event loop creation overhead by reusing the persistent loop!
/// NO GIL CHECK - is_async is cached at registration time!
/// Uses PRE-BOUND json.dumps callable to avoid repeated getattr!
/// PHASE B: Semaphore gating for async handlers to prevent overload!
async fn process_request_optimized(
    handler: Handler,
    is_async: bool, // Pre-cached from HandlerMetadata!
    task_locals: &pyo3_async_runtimes::TaskLocals,
    json_dumps_fn: &PyObject, // Pre-bound callable!
    limiter: &PyObject,       // PHASE B: Semaphore limiter for gating!
) -> Result<HandlerResponse, String> {
    // No need to check is_async - it's passed in from cached metadata!

    if is_async {
        // PHASE B: Async handler with semaphore gating!
        // Wrap coroutine with limiter to prevent event loop overload
        let future = Python::with_gil(|py| {
            // Call async handler to get coroutine
            let coroutine = handler
                .bind(py)
                .call0()
                .map_err(|e| format!("Handler error: {}", e))?;

            // PHASE B: Wrap coroutine with semaphore limiter
            // The limiter returns a coroutine that wraps the original with semaphore gating
            let limited_coro = limiter
                .bind(py)
                .call1((coroutine,))
                .map_err(|e| format!("Limiter error: {}", e))?;

            // Convert Python coroutine to Rust future using cached TaskLocals
            // This schedules it on the event loop WITHOUT blocking!
            pyo3_async_runtimes::into_future_with_locals(task_locals, limited_coro.clone())
                .map_err(|e| format!("Failed to convert coroutine: {}", e))
        })?;

        // Await the Rust future (non-blocking!)
        let result = future
            .await
            .map_err(|e| format!("Async execution error: {}", e))?;

        // Serialize result
        Python::with_gil(|py| serialize_result_optimized(py, result, json_dumps_fn))
    } else {
        // Sync handler - direct call with single GIL acquisition
        Python::with_gil(|py| {
            let result = handler
                .bind(py)
                .call0()
                .map_err(|e| format!("Handler error: {}", e))?;
            // Convert Bound to Py for serialization
            serialize_result_optimized(py, result.unbind(), json_dumps_fn)
        })
    }
}

/// Serialize Python result to JSON string - SIMD-optimized version
/// Phase 1: Uses Rust SIMD serializer instead of Python json.dumps
fn serialize_result_optimized(
    py: Python,
    result: Py<PyAny>,
    _json_dumps_fn: &PyObject, // Kept for API compat, no longer used
) -> Result<HandlerResponse, String> {
    let bound = result.bind(py);

    // Check if result is a Response object with status_code
    let status_code = if let Ok(status_attr) = bound.getattr("status_code") {
        // Python integers are typically i64, convert to u16
        status_attr
            .extract::<i64>()
            .ok()
            .and_then(|v| u16::try_from(v).ok())
            .unwrap_or(200)
    } else {
        200
    };

    // Try direct string extraction first (zero-copy fast path)
    if let Ok(json_str) = bound.extract::<String>() {
        return Ok(HandlerResponse {
            body: json_str,
            status_code,
        });
    }

    // PHASE 1: Rust SIMD JSON serialization (no Python FFI!)
    let body = simd_json::serialize_pyobject_to_json(py, bound)
        .map_err(|e| format!("SIMD JSON serialization error: {}", e))?;

    Ok(HandlerResponse { body, status_code })
}

/// Handle Python request - supports both SYNC and ASYNC handlers
/// Async handlers run in dedicated thread with their own event loop
async fn handle_python_request_sync(
    handler: PyObject,
    _method: String,
    _path: String,
    _query_string: String,
    body: Bytes,
) -> Result<String, String> {
    // Check if handler is async
    let is_async = Python::with_gil(|py| {
        let inspect = py.import("inspect").unwrap();
        inspect
            .call_method1("iscoroutinefunction", (handler.clone_ref(py),))
            .unwrap()
            .extract::<bool>()
            .unwrap()
    });

    let body_clone = body.clone();

    if is_async {
        // Async handler - run in blocking thread with asyncio.run()
        tokio::task::spawn_blocking(move || {
            Python::with_gil(|py| {
                // Import asyncio
                let asyncio = py
                    .import("asyncio")
                    .map_err(|e| format!("Failed to import asyncio: {}", e))?;

                // Create kwargs dict with request data
                use pyo3::types::PyDict;
                let kwargs = PyDict::new(py);
                kwargs.set_item("body", body_clone.as_ref()).ok();
                let headers = PyDict::new(py);
                kwargs.set_item("headers", headers).ok();

                // Call async handler to get coroutine
                let coroutine = handler
                    .call(py, (), Some(&kwargs))
                    .map_err(|e| format!("Failed to call handler: {}", e))?;

                // Run coroutine with asyncio.run()
                let result = asyncio
                    .call_method1("run", (coroutine,))
                    .map_err(|e| format!("Failed to run coroutine: {}", e))?;

                // Enhanced handler returns {"content": ..., "status_code": ..., "content_type": ...}
                // Extract just the content
                let content = if let Ok(dict) = result.downcast::<PyDict>() {
                    if let Ok(Some(content_val)) = dict.get_item("content") {
                        content_val
                    } else {
                        result
                    }
                } else {
                    result
                };

                // PHASE 1: SIMD JSON serialization
                if let Ok(json_str) = content.extract::<String>() {
                    Ok(json_str)
                } else {
                    simd_json::serialize_pyobject_to_json(py, &content)
                        .map_err(|e| format!("SIMD JSON error: {}", e))
                }
            })
        })
        .await
        .map_err(|e| format!("Thread join error: {}", e))?
    } else {
        // Sync handler - call directly
        Python::with_gil(|py| {
            // Create kwargs dict with request data
            use pyo3::types::PyDict;
            let kwargs = PyDict::new(py);
            kwargs.set_item("body", body.as_ref()).ok();
            let headers = PyDict::new(py);
            kwargs.set_item("headers", headers).ok();

            let result = handler
                .call(py, (), Some(&kwargs))
                .map_err(|e| format!("Python handler error: {}", e))?;

            // Enhanced handler returns {"content": ..., "status_code": ..., "content_type": ...}
            let content = if let Ok(dict) = result.downcast_bound::<PyDict>(py) {
                if let Ok(Some(content_val)) = dict.get_item("content") {
                    content_val.unbind()
                } else {
                    result
                }
            } else {
                result
            };

            // PHASE 1: SIMD JSON serialization
            match content.extract::<String>(py) {
                Ok(json_str) => Ok(json_str),
                Err(_) => {
                    let bound = content.bind(py);
                    simd_json::serialize_pyobject_to_json(py, bound)
                        .map_err(|e| format!("SIMD JSON error: {}", e))
                }
            }
        })
    }
}
