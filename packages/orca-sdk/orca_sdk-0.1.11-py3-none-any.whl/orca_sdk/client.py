# Do not edit this file manually! It is generated by scripts/codegen.py

from __future__ import annotations

import json
import logging
import os
import uuid
from contextlib import contextmanager
from contextvars import ContextVar
from string import Formatter
from typing import (
    Any,
    Callable,
    Generator,
    Literal,
    Mapping,
    NotRequired,
    Self,
    TypeAlias,
    TypedDict,
    cast,
    overload,
)

from httpx import (
    URL,
    USE_CLIENT_DEFAULT,
    BaseTransport,
    Client,
    Headers,
    HTTPTransport,
    Limits,
    Proxy,
    Request,
    Response,
    Timeout,
)
from httpx._client import UseClientDefault  # type: ignore
from httpx._types import AuthTypes  # type: ignore
from httpx._types import CookieTypes  # type: ignore
from httpx._types import FileTypes  # type: ignore
from httpx._types import HeaderTypes  # type: ignore
from httpx._types import RequestContent  # type: ignore
from httpx._types import RequestExtensions  # type: ignore
from httpx._types import TimeoutTypes  # type: ignore
from httpx_retries import Retry, RetryTransport


class ActionRecommendation(TypedDict):
    action: Literal["remove_duplicates", "detect_mislabels", "add_memories", "finetuning"]
    """
    The recommended action to take
    """
    rationale: str
    """
    Explanation for why this action was recommended
    """


class AddMemorySuggestion(TypedDict):
    value: str
    label_name: str
    similarity: NotRequired[float | None]


class AliveResponse(TypedDict):
    ok: bool
    checks: dict[str, bool]


class ApiKeyMetadata(TypedDict):
    id: str
    org_id: str
    name: str
    created_by: str | None
    created_at: str
    updated_at: str
    scope: list[Literal["ADMINISTER", "PREDICT"]]


class BaseLabelPredictionResult(TypedDict):
    prediction_id: str | None
    confidence: float
    anomaly_score: float | None
    label: int | None
    label_name: str | None
    logits: list[float]


class BaseModel(TypedDict):
    pass


class BaseScorePredictionResult(TypedDict):
    prediction_id: str | None
    confidence: float
    anomaly_score: float | None
    score: float | None


class CascadeEditSuggestionsRequest(TypedDict):
    old_label: int | None
    new_label: int
    max_neighbors: NotRequired[int]
    max_validation_neighbors: NotRequired[int]
    similarity_threshold: NotRequired[float | None]
    only_if_has_old_label: NotRequired[bool]
    exclude_if_new_label: NotRequired[bool]
    suggestion_cooldown_time: NotRequired[float]
    label_confirmation_cooldown_time: NotRequired[float]


class ClassPattern(TypedDict):
    label: int
    name: str
    description: str


class ClassPatternsDescription(TypedDict):
    overview: str
    classes: list[ClassPattern]
    summary: str


class ClassRepresentatives(TypedDict):
    label: int
    label_name: str | None
    representative_memory_ids: list[str]


class ClassificationEvaluationRequest(TypedDict):
    datasource_name_or_id: str
    memoryset_override_name_or_id: NotRequired[str | None]
    datasource_label_column: str
    datasource_value_column: str
    record_telemetry: NotRequired[bool]
    telemetry_tags: NotRequired[list[str] | None]
    subsample: NotRequired[int | float | None]
    ignore_unlabeled: NotRequired[bool]
    datasource_partition_column: NotRequired[str | None]
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class CleanupResponse(TypedDict):
    deleted_milvus_collections: list[str]
    deleted_milvus_collections_count: int
    deleted_blob_paths: list[str]
    deleted_blob_paths_count: int


class ClusterMetrics(TypedDict):
    cluster: int
    memory_count: int


ColumnType: TypeAlias = Literal["STRING", "FLOAT", "INT", "BOOL", "ENUM", "IMAGE", "OTHER"]
"""
The type of a column in a datasource
"""


class ConstraintViolationErrorResponse(TypedDict):
    status_code: Literal[409]
    constraint: str


class CountPredictionsRequest(TypedDict):
    model_id: NotRequired[str | None]
    tag: NotRequired[str | None]
    prediction_ids: NotRequired[list[str] | None]
    start_timestamp: NotRequired[str | None]
    end_timestamp: NotRequired[str | None]
    memory_id: NotRequired[str | None]
    expected_label_match: NotRequired[bool | None]


class CreateApiKeyRequest(TypedDict):
    id: NotRequired[str]
    name: NotRequired[str]
    created_by: NotRequired[str | None]
    scope: list[Literal["ADMINISTER", "PREDICT"]]


class CreateApiKeyResponse(TypedDict):
    id: str
    org_id: str
    name: str
    created_by: str | None
    created_at: str
    updated_at: str
    scope: list[Literal["ADMINISTER", "PREDICT"]]
    api_key: str


class CreateDatasourceFromContentRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    content: Any


class CreateOrgPlanRequest(TypedDict):
    tier: Literal["FREE", "PRO", "ENTERPRISE", "CANCELLED"]


class DatasetFilterItem(TypedDict):
    field: str
    op: Literal["==", "!=", ">", ">=", "<", "<=", "in", "not in", "like"]
    value: Any


class DeleteMemoriesResponse(TypedDict):
    deleted_count: int


class DeleteMemorysetsRequest(TypedDict):
    memoryset_ids: list[str]
    cascade: NotRequired[bool]


class EmbedRequest(TypedDict):
    values: list[str] | list[bytes] | list[str | bytes]
    max_seq_length: NotRequired[int | None]
    prompt: NotRequired[str | None]


class EmbeddingEvaluationRequest(TypedDict):
    datasource_name_or_id: str
    eval_datasource_name_or_id: NotRequired[str | None]
    subsample: NotRequired[int | float | None]
    datasource_value_column: NotRequired[str]
    datasource_label_column: NotRequired[str | None]
    datasource_score_column: NotRequired[str | None]
    neighbor_count: NotRequired[int]
    batch_size: NotRequired[int]
    weigh_memories: NotRequired[bool]


EmbeddingFinetuningMethod: TypeAlias = Literal["classification", "regression", "batch_triplet_loss"]


class FeedbackMetrics(TypedDict):
    avg: float | None
    count: int


FeedbackType: TypeAlias = Literal["CONTINUOUS", "BINARY"]


class FilterItem(TypedDict):
    field: (
        tuple[
            Literal[
                "memory_id",
                "value",
                "label",
                "metadata",
                "source_id",
                "partition_id",
                "created_at",
                "updated_at",
                "edited_at",
                "metrics",
                "score",
                "labels",
            ]
        ]
        | tuple[Literal["metadata"], str]
        | tuple[
            Literal["metrics"],
            Literal[
                "cluster",
                "embedding_2d",
                "is_duplicate",
                "duplicate_memory_ids",
                "has_potential_duplicates",
                "potential_duplicate_memory_ids",
                "anomaly_score",
                "neighbor_label_logits",
                "neighbor_predicted_label",
                "neighbor_predicted_label_ambiguity",
                "neighbor_predicted_label_confidence",
                "current_label_neighbor_confidence",
                "normalized_neighbor_label_entropy",
                "neighbor_predicted_label_matches_current_label",
                "spread",
                "uniformity",
                "concept_id",
                "subconcept_id",
            ],
        ]
    )
    op: Literal["==", "!=", ">", ">=", "<", "<=", "in", "not in", "like", "contains all", "contains any"]
    value: (
        str
        | int
        | float
        | bool
        | list[None]
        | list[str]
        | list[str | None]
        | list[int]
        | list[int | None]
        | list[float]
        | list[bool]
        | None
    )
    transform: NotRequired[Literal["length"]]


class GetDatasourceRowCountRequest(TypedDict):
    filters: NotRequired[list[DatasetFilterItem]]


class GetDatasourceRowsRequest(TypedDict):
    filters: NotRequired[list[DatasetFilterItem]]
    limit: NotRequired[int]
    offset: NotRequired[int]
    shuffle: NotRequired[bool]
    shuffle_seed: NotRequired[int | None]


class GetMemoriesRequest(TypedDict):
    memory_ids: list[str]


class HealthyResponse(TypedDict):
    ok: bool
    checks: dict[str, bool]
    durations: dict[str, int]
    draining: bool
    config: dict[str, str | float | int | bool | None]


class InternalServerErrorResponse(TypedDict):
    status_code: Literal[500]
    message: str
    request_id: str


JobStatus: TypeAlias = Literal[
    "INITIALIZED", "DISPATCHED", "WAITING", "PROCESSING", "COMPLETED", "FAILED", "ABORTING", "ABORTED"
]
"""
Status of job in the job queue
"""


class JobStatusInfo(TypedDict):
    status: JobStatus
    steps_total: int | None
    steps_completed: int | None
    exception: str | None
    updated_at: str
    created_at: str


class LabelClassMetrics(TypedDict):
    label: int | None
    label_name: NotRequired[str | None]
    average_lookup_score: float | None
    memory_count: int


class LabelPercentage(TypedDict):
    label: int | None
    percentage: float


class LabeledBatchMemoryUpdatePatch(TypedDict):
    metadata: NotRequired[dict[str, str | int | float | bool | None] | None]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    label: NotRequired[int | None]


class LabeledExample(TypedDict):
    text: str
    label_name: str


class LabeledMemoryInsert(TypedDict):
    memory_id: NotRequired[str | None]
    value: str | bytes
    metadata: NotRequired[dict[str, str | int | float | bool | None]]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    label: int | None


class ListMemoriesRequest(TypedDict):
    offset: NotRequired[int]
    limit: NotRequired[int]
    filters: NotRequired[list[FilterItem]]


class LookupRequest(TypedDict):
    query: list[str]
    count: NotRequired[int]
    prompt: NotRequired[str | None]
    partition_id: NotRequired[str | list[str | None] | None]
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class LookupScoreMetrics(TypedDict):
    median: float
    std: float
    quantiles: list[float]
    quantile_values: list[float]


class MemoryMetrics(TypedDict):
    is_duplicate: NotRequired[bool]
    duplicate_memory_ids: NotRequired[list[str]]
    has_potential_duplicates: NotRequired[bool]
    potential_duplicate_memory_ids: NotRequired[list[str] | None]
    cluster: NotRequired[int]
    embedding_2d: NotRequired[tuple[float, float]]
    anomaly_score: NotRequired[float]
    neighbor_label_logits: NotRequired[list[float] | None]
    neighbor_predicted_label: NotRequired[int | None]
    neighbor_predicted_label_ambiguity: NotRequired[float]
    neighbor_predicted_label_confidence: NotRequired[float]
    current_label_neighbor_confidence: NotRequired[float]
    normalized_neighbor_label_entropy: NotRequired[float]
    neighbor_predicted_label_matches_current_label: NotRequired[bool | None]
    spread: NotRequired[float]
    uniformity: NotRequired[float]
    concept_id: NotRequired[int | None]
    subconcept_id: NotRequired[int | None]


MemoryType: TypeAlias = Literal["LABELED", "SCORED"]


class MemorysetClassPatternsAnalysisConfig(TypedDict):
    representatives_per_class: NotRequired[int]
    enable_patterns_description: NotRequired[bool]


class MemorysetClassPatternsMetrics(TypedDict):
    class_representatives: list[ClassRepresentatives]
    patterns_description: NotRequired[ClassPatternsDescription | None]
    mean_spread: float
    variance_spread: float
    mean_uniformity: float
    variance_uniformity: float
    updated_at: str


class MemorysetClusterAnalysisConfig(TypedDict):
    min_cluster_size: NotRequired[int | None]
    max_cluster_size: NotRequired[int | None]
    partitioning_method: NotRequired[Literal["ng", "rb", "cpm"]]
    resolution: NotRequired[float | None]
    num_iterations: NotRequired[int]
    random_state: NotRequired[int | None]


class MemorysetClusterMetrics(TypedDict):
    cluster_metrics: list[ClusterMetrics]
    num_outliers: int
    num_clusters: int
    updated_at: str


class MemorysetConceptAnalysisConfig(TypedDict):
    high_level_description: NotRequired[str | None]
    max_sample_rows: NotRequired[int]
    min_lookup_count: NotRequired[int]
    max_lookup_count: NotRequired[int]
    overlap_decay_tau: NotRequired[float]
    target_lcc_fraction: NotRequired[float]
    min_cluster_count: NotRequired[int]
    max_cluster_count: NotRequired[int]
    max_trial_count: NotRequired[int]
    resolution: NotRequired[float]
    iterations: NotRequired[int]
    use_generative_naming: NotRequired[bool]
    naming_examples_count: NotRequired[int]
    naming_counterexample_count: NotRequired[int]
    primary_label_pct_threshold: NotRequired[float]
    seed: NotRequired[int]


class MemorysetDistributionAnalysisConfig(TypedDict):
    neighbor_counts: NotRequired[list[int]]
    quantiles: NotRequired[list[float]]


class MemorysetDistributionMetrics(TypedDict):
    lookup_score_metrics: dict[str, LookupScoreMetrics]
    updated_at: str


class MemorysetDuplicateAnalysisConfig(TypedDict):
    potential_duplicate_threshold: NotRequired[float]


class MemorysetDuplicateMetrics(TypedDict):
    num_duplicates: int
    num_potential_duplicates: int
    updated_at: str


class MemorysetLabelAnalysisConfig(TypedDict):
    normalize_logits: NotRequired[bool]


class MemorysetLabelMetrics(TypedDict):
    label_metrics: list[LabelClassMetrics]
    neighbor_prediction_accuracy: float
    mean_neighbor_label_confidence: float
    mean_neighbor_label_entropy: float
    mean_neighbor_predicted_label_ambiguity: float
    num_no_labeled_neighbors: int
    num_potential_mislabels: int
    updated_at: str


class MemorysetProjectionAnalysisConfig(TypedDict):
    min_dist: NotRequired[float]
    spread: NotRequired[float]


class MemorysetProjectionMetrics(TypedDict):
    updated_at: str


class MemorysetUpdate(TypedDict):
    label_names: NotRequired[list[str]]
    description: NotRequired[str | None]
    name: NotRequired[str]
    notes: NotRequired[str | None]
    hidden: NotRequired[bool]


class NotFoundErrorResponse(TypedDict):
    status_code: Literal[404]
    resource: (
        Literal[
            "org",
            "api_key",
            "datasource",
            "memoryset",
            "predictive_model",
            "classification_model",
            "regression_model",
            "prediction",
            "memory",
            "evaluation",
            "analysis",
            "job",
            "pretrained_embedding_model",
            "finetuned_embedding_model",
            "feedback_category",
            "org_plan",
            "worker",
        ]
        | None
    )


class OrgPlan(TypedDict):
    org_id: str
    created_at: str
    updated_at: str
    tier: Literal["FREE", "PRO", "ENTERPRISE", "CANCELLED"]


class PRCurve(TypedDict):
    thresholds: list[float]
    precisions: list[float]
    recalls: list[float]


class PredictionFeedback(TypedDict):
    prediction_id: str
    category_name: str
    value: float | bool
    comment: str | None
    id: str
    org_id: str
    category_id: str
    category_type: FeedbackType
    created_at: str
    updated_at: str


class PredictionFeedbackCategory(TypedDict):
    id: str
    org_id: str
    name: str
    type: FeedbackType
    created_at: str
    updated_at: str


class PredictionFeedbackRequest(TypedDict):
    prediction_id: str
    category_name: str
    value: NotRequired[float | bool | None]
    """
    The feedback value. For updates, UNSET means keep existing value. None means delete the feedback.
    """
    comment: NotRequired[str | None]
    """
    Optional comment. For updates, UNSET means keep existing comment. None means remove the comment.
    """


class PredictionFeedbackResult(TypedDict):
    prediction_ids: list[str]
    deleted_feedback_ids: list[str]
    updated_feedback_ids: list[str]
    inserted_feedback_ids: list[str]
    new_category_ids: list[str]


PredictionSort: TypeAlias = list[tuple[Literal["timestamp", "confidence", "anomaly_score"], Literal["asc", "desc"]]]


class PredictiveModelUpdate(TypedDict):
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    locked: NotRequired[bool]


PretrainedEmbeddingModelName: TypeAlias = Literal[
    "CLIP_BASE", "GTE_BASE", "CDE_SMALL", "DISTILBERT", "GTE_SMALL", "MXBAI_LARGE", "E5_LARGE", "BGE_BASE", "GIST_LARGE"
]
"""
Names of pretrained embedding models that are supported by OrcaCloud
"""


RACHeadType: TypeAlias = Literal["KNN", "MMOE", "FF", "BMMOE"]


RARHeadType: TypeAlias = Literal["MMOE", "KNN"]


class ROCCurve(TypedDict):
    thresholds: list[float]
    false_positive_rates: list[float]
    true_positive_rates: list[float]


class ReadyResponse(TypedDict):
    ok: bool
    draining: bool


class RegressionEvaluationRequest(TypedDict):
    datasource_name_or_id: str
    memoryset_override_name_or_id: NotRequired[str | None]
    datasource_score_column: str
    datasource_value_column: str
    record_telemetry: NotRequired[bool]
    telemetry_tags: NotRequired[list[str] | None]
    subsample: NotRequired[int | float | None]
    ignore_unlabeled: NotRequired[bool]
    datasource_partition_column: NotRequired[str | None]
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class RegressionMetrics(TypedDict):
    coverage: float
    mse: float
    rmse: float
    mae: float
    r2: float
    explained_variance: float
    loss: float
    anomaly_score_mean: NotRequired[float | None]
    anomaly_score_median: NotRequired[float | None]
    anomaly_score_variance: NotRequired[float | None]


class RegressionModelMetadata(TypedDict):
    id: str
    org_id: str
    name: str
    description: str | None
    notes: str | None
    version: int
    memoryset_id: str
    memory_lookup_count: int
    storage_path: str
    memoryset_collection_name: str
    created_at: str
    updated_at: str
    locked: bool
    head_type: RARHeadType


class RegressionPredictionRequest(TypedDict):
    input_values: list[str] | list[bytes] | list[str | bytes]
    expected_scores: NotRequired[list[float] | None]
    tags: NotRequired[list[str]]
    memoryset_override_name_or_id: NotRequired[str | None]
    save_telemetry: NotRequired[bool]
    save_telemetry_synchronously: NotRequired[bool]
    prompt: NotRequired[str | None]
    use_lookup_cache: NotRequired[bool]
    consistency_level: NotRequired[Literal["Bounded", "Session", "Strong", "Eventual"] | None]
    ignore_unlabeled: NotRequired[bool]
    partition_ids: NotRequired[str | list[str | None] | None]
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class ScorePredictionMemoryLookup(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    lookup_score: float
    score: float | None
    prediction_id: str
    attention_weight: float


class ScorePredictionWithMemoriesAndFeedback(TypedDict):
    prediction_id: str
    confidence: float
    anomaly_score: float | None
    score: float | None
    timestamp: str
    input_value: str | bytes
    input_embedding: list[float]
    expected_score: float | None
    memories: list[ScorePredictionMemoryLookup]
    org_id: str
    memoryset_id: str
    model_id: str
    updated_at: str
    tags: list[str]
    explanation: str | None
    memory_id: str | None
    is_in_dense_neighborhood: NotRequired[bool | None]
    feedbacks: list[PredictionFeedback]


class ScoredBatchMemoryUpdatePatch(TypedDict):
    metadata: NotRequired[dict[str, str | int | float | bool | None] | None]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    score: NotRequired[float | None]


class ScoredMemory(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    score: float | None


class ScoredMemoryInsert(TypedDict):
    memory_id: NotRequired[str | None]
    value: str | bytes
    metadata: NotRequired[dict[str, str | int | float | bool | None]]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    score: float | None


class ScoredMemoryLookup(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    lookup_score: float
    score: float | None


class ScoredMemoryUpdate(TypedDict):
    memory_id: str
    value: NotRequired[str | bytes]
    metadata: NotRequired[dict[str, str | int | float | bool | None] | None]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    metrics: NotRequired[MemoryMetrics | None]
    score: NotRequired[float | None]


class ScoredMemoryWithFeedbackMetrics(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    score: float | None
    feedback_metrics: dict[str, FeedbackMetrics]
    lookup_count: int


class ServiceUnavailableErrorResponse(TypedDict):
    status_code: Literal[503]
    service: str


class SubConceptMetrics(TypedDict):
    id: int
    name: str
    description: str | None
    primary_label: int | None
    memory_count: int


TelemetryField: TypeAlias = (
    tuple[Literal["feedback_metrics"], str, Literal["avg", "count"]] | tuple[Literal["lookup"], Literal["count"]]
)


class TelemetryFilterItem(TypedDict):
    field: TelemetryField
    op: Literal["==", "!=", ">", ">=", "<", "<=", "in", "not in"]
    value: float | list[float] | int | list[int]


class TelemetrySortOptions(TypedDict):
    field: TelemetryField
    direction: Literal["asc", "desc"]


class UnauthenticatedErrorResponse(TypedDict):
    status_code: Literal[401]


class UnauthorizedErrorResponse(TypedDict):
    status_code: Literal[403]
    reason: str


class UpdateMemoriesResponse(TypedDict):
    updated_count: int


class UpdateOrgPlanRequest(TypedDict):
    tier: Literal["FREE", "PRO", "ENTERPRISE", "CANCELLED"]


class UpdatePredictionRequest(TypedDict):
    expected_label: NotRequired[int | None]
    expected_score: NotRequired[float | None]
    tags: NotRequired[list[str]]
    memory_id: NotRequired[str | None]


class ValidationError(TypedDict):
    loc: list[str | int]
    msg: str
    type: str


WorkerStatus: TypeAlias = Literal["IDLE", "BUSY", "DRAINING", "SHUTDOWN", "CRASHED"]
"""
Status of worker in the worker pool
"""


class GetTestErrorByStatusCodeParams(TypedDict):
    status_code: int | Literal["error", "warning"]


class DeleteAuthApiKeyByNameOrIdParams(TypedDict):
    name_or_id: str


class GetMemorysetParams(TypedDict):
    type: NotRequired[MemoryType | None]
    show_hidden: NotRequired[bool | None]


class PostMemorysetByNameOrIdCloneParams(TypedDict):
    name_or_id: str


class PatchMemorysetByNameOrIdParams(TypedDict):
    name_or_id: str


class GetMemorysetByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteMemorysetByNameOrIdParams(TypedDict):
    name_or_id: str
    cascade: NotRequired[bool]


class PostGpuMemorysetByNameOrIdLookupParams(TypedDict):
    name_or_id: str


class GetMemorysetByNameOrIdMemoryByMemoryIdParams(TypedDict):
    name_or_id: str
    memory_id: str
    """
    ID of the memory
    """


class DeleteMemorysetByNameOrIdMemoryByMemoryIdParams(TypedDict):
    name_or_id: str
    memory_id: str
    """
    ID of the memory
    """


class GetMemorysetByNameOrIdPotentialDuplicateGroupsParams(TypedDict):
    name_or_id: str


class PostMemorysetByNameOrIdMemoriesGetParams(TypedDict):
    name_or_id: str


class PostMemorysetByNameOrIdMemoriesParams(TypedDict):
    name_or_id: str


class PostMemorysetByNameOrIdMemoriesDeleteParams(TypedDict):
    name_or_id: str


class PatchGpuMemorysetByNameOrIdMemoryParams(TypedDict):
    name_or_id: str


class PostGpuMemorysetByNameOrIdMemoryParams(TypedDict):
    name_or_id: str


PostGpuMemorysetByNameOrIdMemoryRequest: TypeAlias = list[LabeledMemoryInsert] | list[ScoredMemoryInsert]


class PatchGpuMemorysetByNameOrIdMemoriesParams(TypedDict):
    name_or_id: str


class PostMemorysetByNameOrIdAnalysisParams(TypedDict):
    name_or_id: str


class GetMemorysetByNameOrIdAnalysisParams(TypedDict):
    name_or_id: str
    status: NotRequired[JobStatus | None]
    limit: NotRequired[int | None]
    offset: NotRequired[int | None]


class GetMemorysetByNameOrIdAnalysisByAnalysisJobIdParams(TypedDict):
    name_or_id: str
    analysis_job_id: str


class PostMemorysetByNameOrIdMemoryByMemoryIdCascadingEditsParams(TypedDict):
    name_or_id: str
    memory_id: str


class GetFinetunedEmbeddingModelByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteFinetunedEmbeddingModelByNameOrIdParams(TypedDict):
    name_or_id: str
    cascade: NotRequired[bool]


class PostGpuFinetunedEmbeddingModelByNameOrIdEmbeddingParams(TypedDict):
    name_or_id: str


class GetPretrainedEmbeddingModelByModelNameParams(TypedDict):
    model_name: PretrainedEmbeddingModelName


class PostGpuPretrainedEmbeddingModelByModelNameEmbeddingParams(TypedDict):
    model_name: PretrainedEmbeddingModelName


class PostFinetunedEmbeddingModelByNameOrIdEvaluationParams(TypedDict):
    name_or_id: str


class PostPretrainedEmbeddingModelByModelNameEvaluationParams(TypedDict):
    model_name: PretrainedEmbeddingModelName


class GetFinetunedEmbeddingModelByNameOrIdEvaluationByJobIdParams(TypedDict):
    name_or_id: str
    job_id: str


class GetPretrainedEmbeddingModelByModelNameEvaluationByJobIdParams(TypedDict):
    model_name: PretrainedEmbeddingModelName
    job_id: str


class GetFinetunedEmbeddingModelByNameOrIdEvaluationsParams(TypedDict):
    name_or_id: str
    datasource: NotRequired[str | None]
    value_column: NotRequired[str | None]
    label_column: NotRequired[str | None]
    score_column: NotRequired[str | None]


class GetPretrainedEmbeddingModelByModelNameEvaluationsParams(TypedDict):
    model_name: PretrainedEmbeddingModelName
    datasource: NotRequired[str | None]
    value_column: NotRequired[str | None]
    label_column: NotRequired[str | None]
    score_column: NotRequired[str | None]


class PostDatasourceUploadRequest(TypedDict):
    name: str
    """
    Name for the datasource
    """
    description: NotRequired[str | None]
    """
    Optional description for the datasource
    """


class GetDatasourceByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteDatasourceByNameOrIdParams(TypedDict):
    name_or_id: str


class PostDatasourceByNameOrIdRowsParams(TypedDict):
    name_or_id: str


class PostDatasourceByNameOrIdRowsCountParams(TypedDict):
    name_or_id: str


class GetDatasourceByNameOrIdEmbeddingModelEvaluationsParams(TypedDict):
    name_or_id: str
    value_column: NotRequired[str | None]
    label_column: NotRequired[str | None]
    score_column: NotRequired[str | None]


class GetDatasourceByNameOrIdDownloadParams(TypedDict):
    name_or_id: str
    file_type: NotRequired[Literal["hf_dataset", "json", "csv"]]
    """
    File type to download:
    * `hf_dataset`: Zipped HuggingFace dataset (default)
    * `json`: Row-oriented JSON array
    * `csv`: CSV file
    """


class GetClassificationModelParams(TypedDict):
    memoryset_name_or_id: NotRequired[str | None]
    """
    Filter by memoryset name or ID
    """


class GetRegressionModelParams(TypedDict):
    memoryset_name_or_id: NotRequired[str | None]
    """
    Filter by memoryset name or ID
    """


class PatchClassificationModelByNameOrIdParams(TypedDict):
    name_or_id: str


class GetClassificationModelByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteClassificationModelByNameOrIdParams(TypedDict):
    name_or_id: str


class PatchRegressionModelByNameOrIdParams(TypedDict):
    name_or_id: str


class GetRegressionModelByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteRegressionModelByNameOrIdParams(TypedDict):
    name_or_id: str


class GetPredictiveModelParams(TypedDict):
    memoryset_name_or_id: NotRequired[str | None]
    """
    Filter by memoryset name or ID
    """


class PostGpuClassificationModelByNameOrIdPredictionParams(TypedDict):
    name_or_id: str


class PostClassificationModelByNameOrIdPredictionParams(TypedDict):
    name_or_id: str


class PostGpuRegressionModelByNameOrIdPredictionParams(TypedDict):
    name_or_id: str


class PostRegressionModelByNameOrIdPredictionParams(TypedDict):
    name_or_id: str


class PostClassificationModelByModelNameOrIdEvaluationParams(TypedDict):
    model_name_or_id: str


class GetClassificationModelByModelNameOrIdEvaluationParams(TypedDict):
    model_name_or_id: str


class PostRegressionModelByModelNameOrIdEvaluationParams(TypedDict):
    model_name_or_id: str


class GetRegressionModelByModelNameOrIdEvaluationParams(TypedDict):
    model_name_or_id: str


class GetClassificationModelByModelNameOrIdEvaluationByJobIdParams(TypedDict):
    model_name_or_id: str
    job_id: str


class DeleteClassificationModelByModelNameOrIdEvaluationByJobIdParams(TypedDict):
    model_name_or_id: str
    job_id: str


class GetRegressionModelByModelNameOrIdEvaluationByJobIdParams(TypedDict):
    model_name_or_id: str
    job_id: str


class DeleteRegressionModelByModelNameOrIdEvaluationByJobIdParams(TypedDict):
    model_name_or_id: str
    job_id: str


class GetJobByJobIdParams(TypedDict):
    job_id: str


class GetJobByJobIdStatusParams(TypedDict):
    job_id: str


class GetJobParams(TypedDict):
    status: NotRequired[JobStatus | list[JobStatus] | None]
    type: NotRequired[str | list[str] | None]
    limit: NotRequired[int | None]
    offset: NotRequired[int]
    start_timestamp: NotRequired[str | None]
    end_timestamp: NotRequired[str | None]


class DeleteJobByJobIdAbortParams(TypedDict):
    job_id: str


class GetWorkerParams(TypedDict):
    status: NotRequired[WorkerStatus | list[WorkerStatus] | None]
    limit: NotRequired[int | None]
    offset: NotRequired[int]


class GetWorkerByWorkerIdParams(TypedDict):
    worker_id: str


class GetTelemetryPredictionByPredictionIdParams(TypedDict):
    prediction_id: str
    calc_neighborhood_density: NotRequired[bool]
    """
    Calculate neighborhood density
    """


class PatchTelemetryPredictionByPredictionIdParams(TypedDict):
    prediction_id: str


class GetTelemetryPredictionByPredictionIdExplanationParams(TypedDict):
    prediction_id: str
    refresh: NotRequired[bool]


class GetTelemetryPredictionByPredictionIdActionParams(TypedDict):
    prediction_id: str


class GetTelemetryPredictionByPredictionIdMemorySuggestionsParams(TypedDict):
    prediction_id: str
    """
    ID of the prediction to generate suggestions for
    """
    num_memories: NotRequired[int]
    """
    Number of memory suggestions to generate
    """
    refresh: NotRequired[bool]
    """
    Force the explanation agent to re-run even if a cached explanation exists
    """


class GetTelemetryFeedbackCategoryByNameOrIdParams(TypedDict):
    name_or_id: str


class DeleteTelemetryFeedbackCategoryByNameOrIdParams(TypedDict):
    name_or_id: str


PutTelemetryPredictionFeedbackRequest: TypeAlias = list[PredictionFeedbackRequest]


class GetAgentsBootstrapClassificationModelByJobIdParams(TypedDict):
    job_id: str


class FieldValidationError(TypedDict):
    loc: list[str | int]
    msg: str
    type: NotRequired[str]


class AddMemoryRecommendations(TypedDict):
    memories: list[AddMemorySuggestion]
    attempts_used: NotRequired[int]
    partial: NotRequired[bool]
    rejection_counts: NotRequired[dict[str, int]]


class BootstrapClassificationModelRequest(TypedDict):
    model_description: str
    label_names: list[str]
    initial_examples: NotRequired[list[LabeledExample]]
    num_examples_per_label: NotRequired[int]


class BootstrapLabeledMemoryDataInput(TypedDict):
    model_description: str
    label_names: list[str]
    initial_examples: NotRequired[list[LabeledExample]]
    num_examples_per_label: NotRequired[int]


class BootstrapLabeledMemoryDataResult(TypedDict):
    model_description: str
    label_names: list[str]
    model_name: str
    generated_examples: NotRequired[list[LabeledExample]]


class ClassificationMetrics(TypedDict):
    coverage: float
    f1_score: float
    accuracy: float
    loss: float | None
    anomaly_score_mean: NotRequired[float | None]
    anomaly_score_median: NotRequired[float | None]
    anomaly_score_variance: NotRequired[float | None]
    roc_auc: NotRequired[float | None]
    pr_auc: NotRequired[float | None]
    pr_curve: NotRequired[PRCurve | None]
    roc_curve: NotRequired[ROCCurve | None]
    confusion_matrix: NotRequired[list[list[int]] | None]
    warnings: NotRequired[list[str]]


class ClassificationModelMetadata(TypedDict):
    id: str
    org_id: str
    name: str
    description: str | None
    notes: str | None
    version: int
    memoryset_id: str
    memory_lookup_count: int
    storage_path: str
    memoryset_collection_name: str
    created_at: str
    updated_at: str
    locked: bool
    num_classes: int
    head_type: RACHeadType
    weigh_memories: bool | None
    min_memory_weight: float | None


class ClassificationPredictionRequest(TypedDict):
    input_values: list[str] | list[bytes] | list[str | bytes]
    expected_labels: NotRequired[list[int] | None]
    filters: NotRequired[list[FilterItem]]
    tags: NotRequired[list[str]]
    memoryset_override_name_or_id: NotRequired[str | None]
    save_telemetry: NotRequired[bool]
    save_telemetry_synchronously: NotRequired[bool]
    prompt: NotRequired[str | None]
    use_lookup_cache: NotRequired[bool]
    consistency_level: NotRequired[Literal["Bounded", "Session", "Strong", "Eventual"] | None]
    ignore_unlabeled: NotRequired[bool]
    partition_ids: NotRequired[str | list[str | None] | None]
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class CloneMemorysetRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    pretrained_embedding_model_name: NotRequired[PretrainedEmbeddingModelName | None]
    finetuned_embedding_model_name_or_id: NotRequired[str | None]
    max_seq_length_override: NotRequired[int | None]
    prompt: NotRequired[str]


class ColumnInfo(TypedDict):
    name: str
    type: ColumnType
    enum_options: NotRequired[list[str] | None]
    string_values: NotRequired[list[str] | None]
    int_values: NotRequired[list[int] | None]
    contains_nones: NotRequired[bool]


class ConceptMetrics(TypedDict):
    id: int
    name: str
    description: str | None
    primary_label: int | None
    memory_count: int
    subconcepts: list[SubConceptMetrics]
    label_percentages: list[LabelPercentage]


class CreateClassificationModelRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    memoryset_name: NotRequired[str | None]
    memoryset_name_or_id: str
    memory_lookup_count: NotRequired[int | None]
    head_type: NotRequired[RACHeadType]
    weigh_memories: NotRequired[bool | None]
    min_memory_weight: NotRequired[float | None]
    num_classes: NotRequired[int | None]


class CreateMemorysetFromDatasourceRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    pretrained_embedding_model_name: NotRequired[PretrainedEmbeddingModelName | None]
    finetuned_embedding_model_name_or_id: NotRequired[str | None]
    max_seq_length_override: NotRequired[int | None]
    label_names: NotRequired[list[str] | None]
    index_type: NotRequired[Literal["FLAT", "IVF_FLAT", "IVF_SQ8", "IVF_PQ", "HNSW", "DISKANN"]]
    index_params: NotRequired[dict[str, int | float | str]]
    prompt: NotRequired[str]
    hidden: NotRequired[bool]
    memory_type: NotRequired[MemoryType | None]
    datasource_name_or_id: str
    datasource_label_column: NotRequired[str | None]
    datasource_score_column: NotRequired[str | None]
    datasource_value_column: str
    datasource_source_id_column: NotRequired[str | None]
    datasource_partition_id_column: NotRequired[str | None]
    remove_duplicates: NotRequired[bool]
    batch_size: NotRequired[int]
    subsample: NotRequired[int | float | None]


class CreateMemorysetRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    pretrained_embedding_model_name: NotRequired[PretrainedEmbeddingModelName | None]
    finetuned_embedding_model_name_or_id: NotRequired[str | None]
    max_seq_length_override: NotRequired[int | None]
    label_names: NotRequired[list[str] | None]
    index_type: NotRequired[Literal["FLAT", "IVF_FLAT", "IVF_SQ8", "IVF_PQ", "HNSW", "DISKANN"]]
    index_params: NotRequired[dict[str, int | float | str]]
    prompt: NotRequired[str]
    hidden: NotRequired[bool]
    memory_type: NotRequired[MemoryType | None]


class CreateRegressionModelRequest(TypedDict):
    name: str
    description: NotRequired[str | None]
    notes: NotRequired[str | None]
    memoryset_name_or_id: str
    memory_lookup_count: NotRequired[int | None]
    head_type: NotRequired[RARHeadType]


class DatasourceMetadata(TypedDict):
    id: str
    org_id: str
    name: str
    description: str | None
    storage_path: str
    length: int
    columns: list[ColumnInfo]
    created_at: str
    updated_at: str


class DeleteMemoriesRequest(TypedDict):
    memory_ids: NotRequired[list[str] | None]
    filters: NotRequired[list[FilterItem] | None]


class EmbeddingEvaluationResponse(TypedDict):
    job_id: str
    org_id: str
    finetuned_embedding_model_id: str | None
    pretrained_embedding_model_name: PretrainedEmbeddingModelName | None
    datasource_id: str
    subsample: int | float | None
    datasource_value_column: str
    datasource_label_column: NotRequired[str | None]
    datasource_score_column: NotRequired[str | None]
    neighbor_count: int
    weigh_memories: bool
    status: JobStatus
    result: ClassificationMetrics | RegressionMetrics | None
    created_at: str
    updated_at: str
    task_id: str


class EvaluationResponse(TypedDict):
    job_id: str
    org_id: str
    status: JobStatus
    result: ClassificationMetrics | RegressionMetrics | None
    created_at: str
    updated_at: str
    task_id: str


class EvaluationResponseClassificationMetrics(TypedDict):
    job_id: str
    org_id: str
    status: JobStatus
    result: ClassificationMetrics | None
    created_at: str
    updated_at: str
    task_id: str


class EvaluationResponseRegressionMetrics(TypedDict):
    job_id: str
    org_id: str
    status: JobStatus
    result: RegressionMetrics | None
    created_at: str
    updated_at: str
    task_id: str


class FinetuneEmbeddingModelRequest(TypedDict):
    name: str
    base_model: PretrainedEmbeddingModelName
    train_memoryset_name_or_id: NotRequired[str | None]
    train_datasource_name_or_id: NotRequired[str | None]
    eval_datasource_name_or_id: NotRequired[str | None]
    label_column: NotRequired[str | None]
    score_column: NotRequired[str | None]
    value_column: NotRequired[str]
    training_method: NotRequired[EmbeddingFinetuningMethod]
    training_args: NotRequired[dict[str, str | int | float | bool]]


class FinetunedEmbeddingModelMetadata(TypedDict):
    embedding_dim: int
    max_seq_length: int
    uses_context: bool
    id: str
    org_id: str
    name: str
    storage_path: str
    created_at: str
    updated_at: str
    base_model: PretrainedEmbeddingModelName
    finetuning_job_id: str
    finetuning_status: JobStatus
    finetuning_task_id: str


class HTTPValidationError(TypedDict):
    detail: NotRequired[list[ValidationError]]


class InvalidInputErrorResponse(TypedDict):
    status_code: Literal[422]
    validation_issues: list[FieldValidationError]


class Job(TypedDict):
    status: JobStatus
    steps_total: int | None
    steps_completed: int | None
    exception: str | None
    updated_at: str
    created_at: str
    id: str
    org_id: str
    worker_id: str | None
    type: str
    payload: BaseModel
    result: BaseModel | None
    depends_on: NotRequired[list[str]]
    lease_token: str | None


class LabelPredictionMemoryLookup(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    label: int | None
    label_name: str | None
    lookup_score: float
    prediction_id: str
    attention_weight: float


class LabelPredictionWithMemoriesAndFeedback(TypedDict):
    prediction_id: str
    confidence: float
    anomaly_score: float | None
    label: int | None
    label_name: str | None
    logits: list[float]
    timestamp: str
    input_value: str | bytes
    input_embedding: list[float]
    expected_label: int | None
    expected_label_name: str | None
    memories: list[LabelPredictionMemoryLookup]
    org_id: str
    memoryset_id: str
    model_id: str
    updated_at: str
    tags: list[str]
    explanation: str | None
    memory_id: str | None
    is_in_dense_neighborhood: NotRequired[bool | None]
    feedbacks: list[PredictionFeedback]


class LabeledMemory(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    label: int | None
    label_name: str | None


class LabeledMemoryLookup(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    label: int | None
    label_name: str | None
    lookup_score: float


class LabeledMemoryUpdate(TypedDict):
    memory_id: str
    value: NotRequired[str | bytes]
    metadata: NotRequired[dict[str, str | int | float | bool | None] | None]
    source_id: NotRequired[str | None]
    partition_id: NotRequired[str | None]
    metrics: NotRequired[MemoryMetrics | None]
    label: NotRequired[int | None]


class LabeledMemoryWithFeedbackMetrics(TypedDict):
    value: str | bytes
    embedding: list[float]
    source_id: str | None
    partition_id: str | None
    metadata: dict[str, str | int | float | bool | None]
    memory_id: str
    memory_version: int
    created_at: str
    updated_at: str
    edited_at: str
    metrics: MemoryMetrics
    label: int | None
    label_name: str | None
    feedback_metrics: dict[str, FeedbackMetrics]
    lookup_count: int


class ListPredictionsRequest(TypedDict):
    model_id: NotRequired[str | None]
    tag: NotRequired[str | None]
    prediction_ids: NotRequired[list[str] | None]
    start_timestamp: NotRequired[str | None]
    end_timestamp: NotRequired[str | None]
    memory_id: NotRequired[str | None]
    expected_label_match: NotRequired[bool | None]
    limit: NotRequired[int]
    offset: NotRequired[int | None]
    sort: NotRequired[PredictionSort]


class MemorysetAnalysisConfigs(TypedDict):
    distribution: NotRequired[MemorysetDistributionAnalysisConfig | None]
    label: NotRequired[MemorysetLabelAnalysisConfig | None]
    duplicate: NotRequired[MemorysetDuplicateAnalysisConfig | None]
    projection: NotRequired[MemorysetProjectionAnalysisConfig | None]
    cluster: NotRequired[MemorysetClusterAnalysisConfig | None]
    class_patterns: NotRequired[MemorysetClassPatternsAnalysisConfig | None]
    concepts: NotRequired[MemorysetConceptAnalysisConfig | None]


class MemorysetAnalysisRequest(TypedDict):
    lookup_count: NotRequired[int]
    batch_size: NotRequired[int]
    clear_metrics: NotRequired[bool]
    configs: MemorysetAnalysisConfigs
    partition_filter_mode: NotRequired[Literal["ignore_partitions", "include_global", "exclude_global", "only_global"]]


class MemorysetConceptMetrics(TypedDict):
    concepts: list[ConceptMetrics]
    num_outliers: int
    updated_at: str


class MemorysetMetrics(TypedDict):
    distribution: NotRequired[MemorysetDistributionMetrics | None]
    label: NotRequired[MemorysetLabelMetrics | None]
    duplicate: NotRequired[MemorysetDuplicateMetrics | None]
    projection: NotRequired[MemorysetProjectionMetrics | None]
    cluster: NotRequired[MemorysetClusterMetrics | None]
    class_patterns: NotRequired[MemorysetClassPatternsMetrics | None]
    concepts: NotRequired[MemorysetConceptMetrics | None]


class PaginatedJob(TypedDict):
    items: list[Job]
    total: int
    offset: int
    limit: int


class PaginatedUnionLabeledMemoryWithFeedbackMetricsScoredMemoryWithFeedbackMetrics(TypedDict):
    items: list[LabeledMemoryWithFeedbackMetrics | ScoredMemoryWithFeedbackMetrics]
    total: int
    offset: int
    limit: int


class PretrainedEmbeddingModelMetadata(TypedDict):
    embedding_dim: int
    max_seq_length: int
    uses_context: bool
    name: PretrainedEmbeddingModelName
    experimental: NotRequired[bool]
    supports_instructions: bool
    num_params: int


class TelemetryMemoriesRequest(TypedDict):
    memoryset_id: str
    offset: NotRequired[int]
    limit: NotRequired[int]
    filters: NotRequired[list[FilterItem | TelemetryFilterItem]]
    sort: NotRequired[list[TelemetrySortOptions] | None]


class WorkerInfo(TypedDict):
    id: str
    status: WorkerStatus
    started_at: str
    updated_at: str
    version: str | None
    git_sha: str
    config: dict[str, str | float | int | bool | dict[str, str] | None]


PatchGpuMemorysetByNameOrIdMemoryRequest: TypeAlias = LabeledMemoryUpdate | ScoredMemoryUpdate


class BatchMemoryUpdateRequest(TypedDict):
    updates: NotRequired[list[LabeledMemoryUpdate | ScoredMemoryUpdate] | None]
    filters: NotRequired[list[FilterItem] | None]
    patch: NotRequired[LabeledBatchMemoryUpdatePatch | ScoredBatchMemoryUpdatePatch | None]


class CascadingEditSuggestion(TypedDict):
    neighbor: LabeledMemoryLookup
    suggested_label: int
    lookup_score: float


class MemorysetAnalysisResponse(TypedDict):
    job_id: str
    org_id: str
    memoryset_id: str
    status: JobStatus
    lookup_count: int
    batch_size: int
    clear_metrics: bool
    configs: MemorysetAnalysisConfigs
    results: MemorysetMetrics | None
    created_at: str
    updated_at: str
    task_id: str


class MemorysetMetadata(TypedDict):
    id: str
    org_id: str
    collection_name: str
    name: str
    description: str | None
    notes: str | None
    length: int
    pretrained_embedding_model_name: PretrainedEmbeddingModelName | None
    finetuned_embedding_model_id: str | None
    created_at: str
    updated_at: str
    memories_updated_at: str
    insertion_job_id: str | None
    insertion_status: JobStatus | None
    metrics: MemorysetMetrics
    memory_type: MemoryType
    label_names: list[str] | None
    index_type: Literal["FLAT", "IVF_FLAT", "IVF_SQ8", "IVF_PQ", "HNSW", "DISKANN"]
    index_params: dict[str, Any]
    database_uri: str | None
    document_prompt_override: str | None
    query_prompt_override: str | None
    hidden: bool
    insertion_task_id: str | None


class PaginatedWorkerInfo(TypedDict):
    items: list[WorkerInfo]
    total: int
    offset: int
    limit: int


class BootstrapClassificationModelMeta(TypedDict):
    datasource_meta: DatasourceMetadata
    memoryset_meta: MemorysetMetadata
    model_meta: ClassificationModelMetadata
    agent_output: BootstrapLabeledMemoryDataResult


class BootstrapClassificationModelResponse(TypedDict):
    job_id: str
    org_id: str
    status: JobStatus
    result: BootstrapClassificationModelMeta | None
    input: BootstrapClassificationModelRequest | None
    task_id: str


class OrcaClient(Client):
    @staticmethod
    def _parse_params(
        params: Mapping[str, Any],
        path: str,
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        placeholders = {name for _, name, _, _ in Formatter().parse(path) if name}
        path_params = {k: v for k, v in params.items() if k in placeholders}
        query_params = {k: v for k, v in params.items() if k not in placeholders and v is not None}
        if placeholders - path_params.keys():
            raise ValueError(f"Missing path params: {', '.join(placeholders - path_params.keys())}")
        return path_params, query_params

    @overload
    def GET(
        self,
        path: Literal["/test_error/{status_code}"],
        *,
        params: GetTestErrorByStatusCodeParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        pass

    @overload
    def GET(
        self,
        path: Literal["/check/alive"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> AliveResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/gpu/check/alive"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> AliveResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/check/ready"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> ReadyResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/check/healthy"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> HealthyResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/gpu/check/healthy"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> HealthyResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/gpu/config"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> dict[str, str | float | int | bool | None]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/config"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> dict[str, str | float | int | bool | None]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/auth/root"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Literal[True]:
        """Return true only when called with a valid root API key; otherwise 401 Unauthenticated."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/auth/api_key"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[ApiKeyMetadata]:
        """List all API keys for the organization."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/auth"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Literal[True]:
        """Returns true if the api key header is valid for the org (will be false for admin api key)"""
        pass

    @overload
    def GET(
        self,
        path: Literal["/auth/org/plan"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> OrgPlan:
        """Get the organization plan."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset"],
        *,
        params: GetMemorysetParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[MemorysetMetadata]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset/{name_or_id}"],
        *,
        params: GetMemorysetByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetMetadata:
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset/{name_or_id}/memory/{memory_id}"],
        *,
        params: GetMemorysetByNameOrIdMemoryByMemoryIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> LabeledMemory | ScoredMemory:
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset/{name_or_id}/potential_duplicate_groups"],
        *,
        params: GetMemorysetByNameOrIdPotentialDuplicateGroupsParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[list[LabeledMemory]] | list[list[ScoredMemory]] | None:
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset/{name_or_id}/analysis"],
        *,
        params: GetMemorysetByNameOrIdAnalysisParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[MemorysetAnalysisResponse]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/memoryset/{name_or_id}/analysis/{analysis_job_id}"],
        *,
        params: GetMemorysetByNameOrIdAnalysisByAnalysisJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetAnalysisResponse:
        pass

    @overload
    def GET(
        self,
        path: Literal["/finetuned_embedding_model"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[FinetunedEmbeddingModelMetadata]:
        """List all finetuned embedding models for the organization."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/finetuned_embedding_model/{name_or_id}"],
        *,
        params: GetFinetunedEmbeddingModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> FinetunedEmbeddingModelMetadata:
        """Get a finetuned embedding model by name or ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/pretrained_embedding_model"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[PretrainedEmbeddingModelMetadata]:
        """List all available pretrained embedding models."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/pretrained_embedding_model/{model_name}"],
        *,
        params: GetPretrainedEmbeddingModelByModelNameParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PretrainedEmbeddingModelMetadata:
        """Get metadata for a specific pretrained embedding model."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/finetuned_embedding_model/{name_or_id}/evaluation/{job_id}"],
        *,
        params: GetFinetunedEmbeddingModelByNameOrIdEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EmbeddingEvaluationResponse:
        """Get evaluation results for a finetuned embedding model by job ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/pretrained_embedding_model/{model_name}/evaluation/{job_id}"],
        *,
        params: GetPretrainedEmbeddingModelByModelNameEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EmbeddingEvaluationResponse:
        """Get evaluation results for a pretrained embedding model by job ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/finetuned_embedding_model/{name_or_id}/evaluations"],
        *,
        params: GetFinetunedEmbeddingModelByNameOrIdEvaluationsParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[EmbeddingEvaluationResponse]:
        """List all evaluation results for a finetuned embedding model."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/pretrained_embedding_model/{model_name}/evaluations"],
        *,
        params: GetPretrainedEmbeddingModelByModelNameEvaluationsParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[EmbeddingEvaluationResponse]:
        """List all evaluation results for a pretrained embedding model."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[DatasourceMetadata]:
        """List all datasources for the organization."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource/{name_or_id}"],
        *,
        params: GetDatasourceByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> DatasourceMetadata:
        """Get a datasource by name or ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource/{name_or_id}/embedding_model_evaluations"],
        *,
        params: GetDatasourceByNameOrIdEmbeddingModelEvaluationsParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[EmbeddingEvaluationResponse]:
        """List all evaluation results for a datasource."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource/{name_or_id}/download"],
        *,
        params: GetDatasourceByNameOrIdDownloadParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[dict[str, Any]]:
        """Download datasource in the specified format."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource/{name_or_id}/download"],
        *,
        params: GetDatasourceByNameOrIdDownloadParams,
        parse_as: Literal["text"],
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> str:
        """Download datasource in the specified format."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/datasource/{name_or_id}/download"],
        *,
        params: GetDatasourceByNameOrIdDownloadParams,
        parse_as: None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> bytes:
        """Download datasource in the specified format."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/classification_model"],
        *,
        params: GetClassificationModelParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[ClassificationModelMetadata]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/regression_model"],
        *,
        params: GetRegressionModelParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[RegressionModelMetadata]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/classification_model/{name_or_id}"],
        *,
        params: GetClassificationModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> ClassificationModelMetadata:
        pass

    @overload
    def GET(
        self,
        path: Literal["/regression_model/{name_or_id}"],
        *,
        params: GetRegressionModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> RegressionModelMetadata:
        pass

    @overload
    def GET(
        self,
        path: Literal["/predictive_model"],
        *,
        params: GetPredictiveModelParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[ClassificationModelMetadata | RegressionModelMetadata]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/classification_model/{model_name_or_id}/evaluation"],
        *,
        params: GetClassificationModelByModelNameOrIdEvaluationParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[EvaluationResponseClassificationMetrics]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/regression_model/{model_name_or_id}/evaluation"],
        *,
        params: GetRegressionModelByModelNameOrIdEvaluationParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[EvaluationResponseRegressionMetrics]:
        pass

    @overload
    def GET(
        self,
        path: Literal["/classification_model/{model_name_or_id}/evaluation/{job_id}"],
        *,
        params: GetClassificationModelByModelNameOrIdEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EvaluationResponseClassificationMetrics:
        pass

    @overload
    def GET(
        self,
        path: Literal["/regression_model/{model_name_or_id}/evaluation/{job_id}"],
        *,
        params: GetRegressionModelByModelNameOrIdEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EvaluationResponseRegressionMetrics:
        pass

    @overload
    def GET(
        self,
        path: Literal["/job/{job_id}"],
        *,
        params: GetJobByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Job:
        pass

    @overload
    def GET(
        self,
        path: Literal["/job/{job_id}/status"],
        *,
        params: GetJobByJobIdStatusParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> JobStatusInfo:
        pass

    @overload
    def GET(
        self,
        path: Literal["/job"],
        *,
        params: GetJobParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PaginatedJob:
        pass

    @overload
    def GET(
        self,
        path: Literal["/worker"],
        *,
        params: GetWorkerParams | None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PaginatedWorkerInfo:
        """
        List all workers in the system. Requires root access.

        This endpoint automatically cleans up orphaned workers before returning results.
        """
        pass

    @overload
    def GET(
        self,
        path: Literal["/worker/{worker_id}"],
        *,
        params: GetWorkerByWorkerIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> WorkerInfo:
        """Get information about a specific worker. Requires root access."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/prediction/{prediction_id}"],
        *,
        params: GetTelemetryPredictionByPredictionIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> LabelPredictionWithMemoriesAndFeedback | ScorePredictionWithMemoriesAndFeedback:
        """Get a specific prediction by ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/prediction/{prediction_id}/explanation"],
        *,
        params: GetTelemetryPredictionByPredictionIdExplanationParams,
        parse_as: Literal["text"],
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> str:
        """Get explanation for a prediction, optionally streaming the response."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/prediction/{prediction_id}/action"],
        *,
        params: GetTelemetryPredictionByPredictionIdActionParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> ActionRecommendation:
        """Get action recommendation for improving a specific prediction."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/prediction/{prediction_id}/memory_suggestions"],
        *,
        params: GetTelemetryPredictionByPredictionIdMemorySuggestionsParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> AddMemoryRecommendations:
        """
        Generate synthetic memory suggestions to improve a specific prediction.

        The returned suggestions have labels as string representations of integer indices
        corresponding to the memoryset's label_names.
        """
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/feedback_category"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[PredictionFeedbackCategory]:
        """List all feedback categories for the organization."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/telemetry/feedback_category/{name_or_id}"],
        *,
        params: GetTelemetryFeedbackCategoryByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PredictionFeedbackCategory:
        """Get a feedback category by name or ID."""
        pass

    @overload
    def GET(
        self,
        path: Literal["/agents/bootstrap_classification_model/{job_id}"],
        *,
        params: GetAgentsBootstrapClassificationModelByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> BootstrapClassificationModelResponse:
        """Get the status of a bootstrap labeled memory data job"""
        pass

    def GET(
        self,
        path: str,
        *,
        params: Mapping[str, Any] | None = None,
        parse_as: Literal["json", "text"] | None = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        path_params, query_params = self._parse_params(params or {}, path)
        res = self.get(
            path.format(**path_params),
            params=query_params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )
        res.raise_for_status()
        return (
            None
            if res.status_code == 204
            else res.json() if parse_as == "json" else res.text if parse_as == "text" else res.content
        )

    @overload
    def DELETE(
        self,
        path: Literal["/cleanup"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> CleanupResponse:
        """Cleanup orphaned milvus collections and blobs"""
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/auth/api_key/{name_or_id}"],
        *,
        params: DeleteAuthApiKeyByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        """Delete an API key by name or ID."""
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/auth/org"],
        *,
        params: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        """Deletes the org and all associated resources"""
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/memoryset/{name_or_id}"],
        *,
        params: DeleteMemorysetByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/memoryset/{name_or_id}/memory/{memory_id}"],
        *,
        params: DeleteMemorysetByNameOrIdMemoryByMemoryIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/finetuned_embedding_model/{name_or_id}"],
        *,
        params: DeleteFinetunedEmbeddingModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        """Delete a finetuned embedding model by name or ID."""
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/datasource/{name_or_id}"],
        *,
        params: DeleteDatasourceByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        """Delete a datasource by name or ID."""
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/classification_model/{name_or_id}"],
        *,
        params: DeleteClassificationModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/regression_model/{name_or_id}"],
        *,
        params: DeleteRegressionModelByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/classification_model/{model_name_or_id}/evaluation/{job_id}"],
        *,
        params: DeleteClassificationModelByModelNameOrIdEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/regression_model/{model_name_or_id}/evaluation/{job_id}"],
        *,
        params: DeleteRegressionModelByModelNameOrIdEvaluationByJobIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/job/{job_id}/abort"],
        *,
        params: DeleteJobByJobIdAbortParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def DELETE(
        self,
        path: Literal["/telemetry/feedback_category/{name_or_id}"],
        *,
        params: DeleteTelemetryFeedbackCategoryByNameOrIdParams,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        """Delete a feedback category and all associated feedback records."""
        pass

    def DELETE(
        self,
        path: str,
        *,
        params: Mapping[str, Any] | None = None,
        parse_as: Literal["json", "text"] | None = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        path_params, query_params = self._parse_params(params or {}, path)
        res = self.delete(
            path.format(**path_params),
            params=query_params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )
        res.raise_for_status()
        return (
            None
            if res.status_code == 204
            else res.json() if parse_as == "json" else res.text if parse_as == "text" else res.content
        )

    @overload
    def POST(
        self,
        path: Literal["/auth/api_key"],
        *,
        params: None = None,
        json: CreateApiKeyRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> CreateApiKeyResponse:
        """Create a new API key for the organization."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/auth/org/plan"],
        *,
        params: None = None,
        json: CreateOrgPlanRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> OrgPlan:
        """Create an organization plan."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset"],
        *,
        params: None = None,
        json: CreateMemorysetFromDatasourceRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetMetadata:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/empty"],
        *,
        params: None = None,
        json: CreateMemorysetRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetMetadata:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/clone"],
        *,
        params: PostMemorysetByNameOrIdCloneParams,
        json: CloneMemorysetRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetMetadata:
        pass

    @overload
    def POST(
        self,
        path: Literal["/batch_delete_memoryset"],
        *,
        params: None = None,
        json: DeleteMemorysetsRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> None:
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/memoryset/{name_or_id}/lookup"],
        *,
        params: PostGpuMemorysetByNameOrIdLookupParams,
        json: LookupRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[list[LabeledMemoryLookup | ScoredMemoryLookup]]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/memories/get"],
        *,
        params: PostMemorysetByNameOrIdMemoriesGetParams,
        json: GetMemoriesRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[LabeledMemory] | list[ScoredMemory]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/memories"],
        *,
        params: PostMemorysetByNameOrIdMemoriesParams,
        json: ListMemoriesRequest | None = None,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[LabeledMemory] | list[ScoredMemory]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/memories/delete"],
        *,
        params: PostMemorysetByNameOrIdMemoriesDeleteParams,
        json: DeleteMemoriesRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> DeleteMemoriesResponse:
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/memoryset/{name_or_id}/memory"],
        *,
        params: PostGpuMemorysetByNameOrIdMemoryParams,
        json: PostGpuMemorysetByNameOrIdMemoryRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[str]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/analysis"],
        *,
        params: PostMemorysetByNameOrIdAnalysisParams,
        json: MemorysetAnalysisRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetAnalysisResponse:
        pass

    @overload
    def POST(
        self,
        path: Literal["/memoryset/{name_or_id}/memory/{memory_id}/cascading_edits"],
        *,
        params: PostMemorysetByNameOrIdMemoryByMemoryIdCascadingEditsParams,
        json: CascadeEditSuggestionsRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[CascadingEditSuggestion]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/finetuned_embedding_model"],
        *,
        params: None = None,
        json: FinetuneEmbeddingModelRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> FinetunedEmbeddingModelMetadata:
        """Create a finetuned embedding model."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/finetuned_embedding_model/{name_or_id}/embedding"],
        *,
        params: PostGpuFinetunedEmbeddingModelByNameOrIdEmbeddingParams,
        json: EmbedRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[list[float]]:
        """Embed values using a finetuned embedding model."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/pretrained_embedding_model/{model_name}/embedding"],
        *,
        params: PostGpuPretrainedEmbeddingModelByModelNameEmbeddingParams,
        json: EmbedRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[list[float]]:
        """Embed values using a pretrained embedding model."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/finetuned_embedding_model/{name_or_id}/evaluation"],
        *,
        params: PostFinetunedEmbeddingModelByNameOrIdEvaluationParams,
        json: EmbeddingEvaluationRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EmbeddingEvaluationResponse:
        """Evaluate a finetuned embedding model as a KNN classifier or regressor."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/pretrained_embedding_model/{model_name}/evaluation"],
        *,
        params: PostPretrainedEmbeddingModelByModelNameEvaluationParams,
        json: EmbeddingEvaluationRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EmbeddingEvaluationResponse:
        """Evaluate a pretrained embedding model as a KNN classifier or regressor."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/datasource/upload"],
        *,
        params: None = None,
        json: None = None,
        data: PostDatasourceUploadRequest,
        files: dict[Literal["files"], FileTypes] | list[tuple[Literal["files"], FileTypes]],
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> DatasourceMetadata:
        """
        Create a datasource by uploading files.

        Supports multiple file upload scenarios:
        - Multiple files: HuggingFace Dataset format (dataset_info.json, state.json, .arrow files, etc.)
        - Single file: CSV, JSON, JSONL, Parquet, or Pickle files
        """
        pass

    @overload
    def POST(
        self,
        path: Literal["/datasource"],
        *,
        params: None = None,
        json: CreateDatasourceFromContentRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> DatasourceMetadata:
        """
        Create a datasource from JSON content.

        Automatically detects and supports multiple JSON formats:
        - List of records: [{"col1": val1, "col2": val2}, {"col1": val3, "col2": val4}, ...]
        - Dictionary of columns: {"col1": [val1, val3, ...], "col2": [val2, val4, ...]}
        """
        pass

    @overload
    def POST(
        self,
        path: Literal["/datasource/{name_or_id}/rows"],
        *,
        params: PostDatasourceByNameOrIdRowsParams,
        json: GetDatasourceRowsRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[dict[str, Any]]:
        """Get rows from a specific datasource with optional filtering."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/datasource/{name_or_id}/rows/count"],
        *,
        params: PostDatasourceByNameOrIdRowsCountParams,
        json: GetDatasourceRowCountRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> int:
        """Get row count from a specific datasource with optional filtering."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/datasource/bootstrap_memory_data"],
        *,
        params: None = None,
        json: BootstrapLabeledMemoryDataInput,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> BootstrapLabeledMemoryDataResult:
        """
        Bootstrap memory data using an AI agent.

        This endpoint uses the bootstrap labeled memory data agent to generate
        high-quality, diverse training examples for a classification model.
        """
        pass

    @overload
    def POST(
        self,
        path: Literal["/classification_model"],
        *,
        params: None = None,
        json: CreateClassificationModelRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> ClassificationModelMetadata:
        pass

    @overload
    def POST(
        self,
        path: Literal["/regression_model"],
        *,
        params: None = None,
        json: CreateRegressionModelRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> RegressionModelMetadata:
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/classification_model/{name_or_id}/prediction"],
        *,
        params: PostGpuClassificationModelByNameOrIdPredictionParams,
        json: ClassificationPredictionRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[BaseLabelPredictionResult]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/classification_model/{name_or_id}/prediction"],
        *,
        params: PostClassificationModelByNameOrIdPredictionParams,
        json: ClassificationPredictionRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[BaseLabelPredictionResult]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/gpu/regression_model/{name_or_id}/prediction"],
        *,
        params: PostGpuRegressionModelByNameOrIdPredictionParams,
        json: RegressionPredictionRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[BaseScorePredictionResult]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/regression_model/{name_or_id}/prediction"],
        *,
        params: PostRegressionModelByNameOrIdPredictionParams,
        json: RegressionPredictionRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[BaseScorePredictionResult]:
        pass

    @overload
    def POST(
        self,
        path: Literal["/classification_model/{model_name_or_id}/evaluation"],
        *,
        params: PostClassificationModelByModelNameOrIdEvaluationParams,
        json: ClassificationEvaluationRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EvaluationResponse:
        pass

    @overload
    def POST(
        self,
        path: Literal["/regression_model/{model_name_or_id}/evaluation"],
        *,
        params: PostRegressionModelByModelNameOrIdEvaluationParams,
        json: RegressionEvaluationRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> EvaluationResponse:
        pass

    @overload
    def POST(
        self,
        path: Literal["/telemetry/prediction"],
        *,
        params: None = None,
        json: ListPredictionsRequest | None = None,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> list[LabelPredictionWithMemoriesAndFeedback | ScorePredictionWithMemoriesAndFeedback]:
        """List predictions with optional filtering and sorting."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/telemetry/prediction/count"],
        *,
        params: None = None,
        json: CountPredictionsRequest | None = None,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> int:
        """Count predictions with optional filtering."""
        pass

    @overload
    def POST(
        self,
        path: Literal["/telemetry/memories"],
        *,
        params: None = None,
        json: TelemetryMemoriesRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PaginatedUnionLabeledMemoryWithFeedbackMetricsScoredMemoryWithFeedbackMetrics:
        """
        List memories with feedback metrics.
        **Note**: This endpoint will ONLY return memories that have been used in a prediction.
        If you want to query ALL memories WITHOUT feedback metrics, use the query_memoryset endpoint.
        """
        pass

    @overload
    def POST(
        self,
        path: Literal["/agents/bootstrap_classification_model"],
        *,
        params: None = None,
        json: BootstrapClassificationModelRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> BootstrapClassificationModelResponse:
        """
        Bootstrap a classification model by creating a memoryset with generated memories and a classification model.

        This endpoint uses the bootstrap_labeled_memory_data agent to generate:
        1. Memoryset configuration with appropriate settings
        2. Model configuration with optimal parameters
        3. High-quality training memories for each label

        The process involves:
        1. Calling the agent to generate configurations and memories
        2. Creating a datasource from the generated memories
        3. Creating a memoryset from the datasource
        4. Creating a classification model from the memoryset
        """
        pass

    def POST(
        self,
        path: str,
        *,
        params: Mapping[str, Any] | None = None,
        json: Any | None = None,
        data: Mapping[str, Any] | None = None,
        content: RequestContent | None = None,
        files: dict[Any, FileTypes] | list[tuple[Any, FileTypes]] | None = None,
        parse_as: Literal["json", "text"] | None = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        path_params, query_params = self._parse_params(params or {}, path)
        res = self.post(
            path.format(**path_params),
            params=query_params,
            content=content,
            data=data,
            files=files,
            json=json,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )
        res.raise_for_status()
        return (
            None
            if res.status_code == 204
            else res.json() if parse_as == "json" else res.text if parse_as == "text" else res.content
        )

    @overload
    def PUT(
        self,
        path: Literal["/auth/org/plan"],
        *,
        params: None = None,
        json: UpdateOrgPlanRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> OrgPlan:
        """Update the organization plan."""
        pass

    @overload
    def PUT(
        self,
        path: Literal["/telemetry/prediction/feedback"],
        *,
        params: None = None,
        json: PutTelemetryPredictionFeedbackRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> PredictionFeedbackResult:
        """Record feedback for predictions, handling updates, deletions, and insertions."""
        pass

    def PUT(
        self,
        path: str,
        *,
        params: Mapping[str, Any] | None = None,
        json: Any | None = None,
        data: Mapping[str, Any] | None = None,
        content: RequestContent | None = None,
        files: dict[Any, FileTypes] | list[tuple[Any, FileTypes]] | None = None,
        parse_as: Literal["json", "text"] | None = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        path_params, query_params = self._parse_params(params or {}, path)
        res = self.put(
            path.format(**path_params),
            params=query_params,
            content=content,
            data=data,
            files=files,
            json=json,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )
        res.raise_for_status()
        return (
            None
            if res.status_code == 204
            else res.json() if parse_as == "json" else res.text if parse_as == "text" else res.content
        )

    @overload
    def PATCH(
        self,
        path: Literal["/memoryset/{name_or_id}"],
        *,
        params: PatchMemorysetByNameOrIdParams,
        json: MemorysetUpdate,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> MemorysetMetadata:
        pass

    @overload
    def PATCH(
        self,
        path: Literal["/gpu/memoryset/{name_or_id}/memory"],
        *,
        params: PatchGpuMemorysetByNameOrIdMemoryParams,
        json: PatchGpuMemorysetByNameOrIdMemoryRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> LabeledMemory | ScoredMemory:
        pass

    @overload
    def PATCH(
        self,
        path: Literal["/gpu/memoryset/{name_or_id}/memories"],
        *,
        params: PatchGpuMemorysetByNameOrIdMemoriesParams,
        json: BatchMemoryUpdateRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> UpdateMemoriesResponse:
        pass

    @overload
    def PATCH(
        self,
        path: Literal["/classification_model/{name_or_id}"],
        *,
        params: PatchClassificationModelByNameOrIdParams,
        json: PredictiveModelUpdate,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> ClassificationModelMetadata:
        pass

    @overload
    def PATCH(
        self,
        path: Literal["/regression_model/{name_or_id}"],
        *,
        params: PatchRegressionModelByNameOrIdParams,
        json: PredictiveModelUpdate,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> RegressionModelMetadata:
        pass

    @overload
    def PATCH(
        self,
        path: Literal["/telemetry/prediction/{prediction_id}"],
        *,
        params: PatchTelemetryPredictionByPredictionIdParams,
        json: UpdatePredictionRequest,
        data: None = None,
        files: None = None,
        content: None = None,
        parse_as: Literal["json"] = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        """Update a prediction with new expected values, tags, or memory ID."""
        pass

    def PATCH(
        self,
        path: str,
        *,
        params: Mapping[str, Any] | None = None,
        json: Any | None = None,
        data: Mapping[str, Any] | None = None,
        content: RequestContent | None = None,
        files: dict[Any, FileTypes] | list[tuple[Any, FileTypes]] | None = None,
        parse_as: Literal["json", "text"] | None = "json",
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Any:
        path_params, query_params = self._parse_params(params or {}, path)
        res = self.patch(
            path.format(**path_params),
            params=query_params,
            content=content,
            data=data,
            files=files,
            json=json,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )
        res.raise_for_status()
        return (
            None
            if res.status_code == 204
            else res.json() if parse_as == "json" else res.text if parse_as == "text" else res.content
        )

    @staticmethod
    def _raise_error_for_response(response: Response) -> None:
        if response.status_code == 401:
            raise ValueError("Invalid API key")
        # elif response.status_code == 402:
        #     res = cast(QuotaExceededErrorResponse, json.loads(response.read().decode(response.encoding or "utf-8")))
        #     raise RuntimeError(
        #         f"{res['quota_type'].replace('_', ' ').title()} limit reached ({res['current']}/{res['quota_limit']})"
        #     )
        elif response.status_code == 403:
            raise PermissionError(json.loads(response.read().decode(response.encoding or "utf-8"))["reason"])
        elif response.status_code == 404:
            res = cast(NotFoundErrorResponse, json.loads(response.read().decode(response.encoding or "utf-8")))
            if res["resource"] is not None:
                raise LookupError(f"The {res['resource']} you are looking for does not exist")
            else:
                raise RuntimeError(f"Unknown API route: {response.url}")
        elif response.status_code == 405:
            raise RuntimeError(f"Unknown method {response.request.method} for API route: {response.url}")
        elif response.status_code == 409:
            res = cast(
                ConstraintViolationErrorResponse, json.loads(response.read().decode(response.encoding or "utf-8"))
            )
            raise RuntimeError(res["constraint"])
        elif response.status_code == 422:
            res = cast(InvalidInputErrorResponse, json.loads(response.read().decode(response.encoding or "utf-8")))
            issues = [f"{issue['loc'][-1]}: {issue['msg']}" for issue in res["validation_issues"]]
            raise ValueError("Invalid input:\n\t" + "\n\t".join(issues))
        elif response.status_code == 500:
            res = cast(InternalServerErrorResponse, json.loads(response.read().decode(response.encoding or "utf-8")))
            raise RuntimeError(f"Unexpected server error: {res['message']}")
        elif response.status_code == 503:
            raise RuntimeError("Orca API is currently unavailable, please try again later")
        elif response.status_code >= 400:
            raise RuntimeError(f"Unexpected status code: {response.status_code}")

    @staticmethod
    def _instrument_request(request: Request) -> None:
        request.headers["X-Request-ID"] = str(uuid.uuid4())

    def __init__(
        self,
        *,
        api_key: str | None = None,
        base_url: URL | str = "",
        headers: HeaderTypes | None = None,
        transport: BaseTransport | None = None,
        timeout: TimeoutTypes | None = None,
        limits: Limits | None = None,
        max_redirects: int = 20,
        event_hooks: None | (Mapping[str, list[Callable[..., Any]]]) = None,
        http1: bool = True,
        http2: bool = False,
        proxy: str | URL | Proxy | None = None,
        log_level: int = logging.WARNING,
    ) -> None:
        """
        Initialize an OrcaAPI httpx client

        Params:
            api_key: API key to use for authentication, will default to ORCA_API_KEY if not set.
            base_url: URL of the OrcaAPI, will default to ORCA_API_URL or the cloud API URL if not set.
        """
        logging.getLogger("httpx").setLevel(log_level)
        logging.getLogger("httpcore").setLevel(log_level)
        super().__init__(
            headers={"Api-Key": api_key or os.environ.get("ORCA_API_KEY", "")} | dict(Headers(headers or {}).items()),
            http1=http1,
            http2=http2,
            proxy=proxy,
            timeout=timeout or Timeout(connect=3, read=20, write=10, pool=5),
            follow_redirects=True,
            limits=limits or Limits(max_connections=100, max_keepalive_connections=20),
            max_redirects=max_redirects,
            event_hooks=event_hooks
            or {"request": [self._instrument_request], "response": [self._raise_error_for_response]},
            base_url=base_url or os.environ.get("ORCA_API_URL", "https://api.orcadb.ai/"),
            transport=transport
            or RetryTransport(
                transport=HTTPTransport(),
                retry=Retry(
                    total=5,
                    backoff_factor=0.5,
                    allowed_methods=["GET", "POST", "PUT", "PATCH", "DELETE"],
                    status_forcelist=[429, 500, 502, 503, 504],
                ),
            ),
        )

    @property
    def api_key(self) -> str:
        return self.headers["Api-Key"]

    @api_key.setter
    def api_key(self, api_key: str) -> None:
        self.headers.update(Headers({"Api-Key": api_key}))

    client_ctx = ContextVar[Self | None]("orca_client", default=None)
    default_client: Self | None = None

    @contextmanager
    def use(self) -> Generator[None, None, None]:
        """Context manager to inject this client into any OrcaSDK methods"""
        token = self.client_ctx.set(self)
        try:
            yield
        finally:
            self.client_ctx.reset(token)

    @classmethod
    def _resolve_client(cls, client: Self | None = None) -> Self:
        client = client or cls.client_ctx.get() or cls.default_client
        if not client:
            client = cls.default_client = cls()
        return client
