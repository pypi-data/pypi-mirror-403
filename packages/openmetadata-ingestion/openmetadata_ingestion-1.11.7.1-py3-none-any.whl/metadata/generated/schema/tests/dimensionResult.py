# generated by datamodel-codegen:
#   filename:  tests/dimensionResult.json
#   timestamp: 2026-01-30T06:35:57+00:00

from __future__ import annotations

from typing import List, Optional

from pydantic import ConfigDict, Field
from typing_extensions import Annotated

from metadata.ingestion.models.custom_pydantic import BaseModel

from . import basic


class DimensionResult(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    dimensionValues: Annotated[
        List[basic.DimensionValue],
        Field(
            description="Array of dimension name-value pairs for this result (e.g., [{'name': 'region', 'value': 'mumbai'}, {'name': 'product', 'value': 'laptop'}])"
        ),
    ]
    testCaseStatus: Annotated[
        basic.TestCaseStatus,
        Field(description='Status of the test for this dimension combination'),
    ]
    result: Annotated[
        Optional[str],
        Field(
            None,
            description='Details of test case results for this dimension combination',
        ),
    ]
    testResultValue: Optional[List[basic.TestResultValue]] = None
    passedRows: Annotated[
        Optional[int],
        Field(None, description='Number of rows that passed for this dimension'),
    ]
    failedRows: Annotated[
        Optional[int],
        Field(None, description='Number of rows that failed for this dimension'),
    ]
    passedRowsPercentage: Annotated[
        Optional[float],
        Field(None, description='Percentage of rows that passed for this dimension'),
    ]
    failedRowsPercentage: Annotated[
        Optional[float],
        Field(None, description='Percentage of rows that failed for this dimension'),
    ]
    impactScore: Annotated[
        Optional[float],
        Field(
            None,
            description='Impact score indicating the significance of this dimension for revealing data quality variations. Higher scores indicate dimensions with more variance in test results.',
            ge=0.0,
            le=1.0,
        ),
    ]
