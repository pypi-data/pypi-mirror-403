---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-deployment
  namespace: multi-process
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-deployment
  template:
    metadata:
      labels:
        app: vllm-deployment
    spec:
      volumes:
        - name: dshm
          hostPath:
            path: /dev/shm
            type: Directory
      containers:
        - name: vllm
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
          image: lmcache/vllm-openai:latest-nightly
          command:
            - /bin/bash
            - -c
            - |
              vllm serve Qwen/Qwen3-14B \
                --host 0.0.0.0 \
                --port 8000 \
                --no-enable-prefix-caching \
                --max-model-len 32768 \
                --gpu-memory-utilization 0.85 \
                --tensor-parallel-size 4 \
                --kv-transfer-config "{\"kv_connector\":\"LMCacheMPConnector\", \"kv_role\":\"kv_both\", \"kv_connector_extra_config\": {\"lmcache.mp.host\": \"tcp://${HOST_IP}\", \"lmcache.mp.port\": 6555}}"
          env:
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: PYTHONHASHSEED
              value: "0"
            - name: PROMETHEUS_MULTIPROC_DIR
              value: "/tmp"
          resources:
            limits:
              nvidia.com/gpu: "4"
            requests:
              nvidia.com/gpu: "4"
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
