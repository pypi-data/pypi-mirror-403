Metadata-Version: 2.4
Name: silent_killers
Version: 0.1.8
Summary: Audit pipeline that detects unsafe exception handling in LLMâ€‘generated Python code
Author: Julian Quick
License: MIT
Project-URL: Homepage, https://github.com/your-org/llm-exception-audit
Project-URL: Issues, https://github.com/your-org/llm-exception-audit/issues
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pandas>=2.1
Requires-Dist: numpy>=1.23
Requires-Dist: matplotlib>=3.8
Provides-Extra: dev
Requires-Dist: pytest>=8.0; extra == "dev"
Requires-Dist: coverage[toml]>=7.5; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Dynamic: license-file

# SilentÂ Killers  
### An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python
![CI](https://github.com/kilojoules/silent-killers/actions/workflows/ci.yml/badge.svg)
![license](https://img.shields.io/badge/license-MIT-blue)
[![PyPI](https://img.shields.io/pypi/v/silent-killers.svg)](https://pypi.org/project/silent-killers/)

> **tl;dr**â€ƒWe show that largeâ€‘language models often add `try/except`
> blocks that *silently swallow* errors.  Our ASTâ€‘based metric pipeline
> lets anyone quantify that risk across thousands of generated scripts
> in seconds.

![example results](data/example_run.png)

---


## 1 Quick start

```
$ printf "try:\n    print(10 / 0)\nexcept:\n    pass\n" > bad_block_example.py
$ printf "print('this is a line') ; new_variable = 123 ; print('this code is fine')" > safe_example.py
$ pip install silent-killers
$ silent-killers-audit bad_block_example.py
âŒ example.py: 1 bad exception block(s) found on line(s): 3
$ silent-killers-audit safe_example.py
$ 
```

### 1.1Â Â Use in pre-commit

Add this to `.pre-commit-config.yaml`:

```
- repo: https://github.com/kilojoules/silent-killers
  rev: v0.1.7
  hooks:
    - id: silent-killers-audit

```

### 1.2Â Â Library usage

```python
from silent_killers.metrics_definitions import code_metrics

python_code = "try:\n    1/0\nexcept Exception:\n    pass"
for metric in code_metrics(python_code):
    print(metric.name, metric.value)
```

```
loc 4
exception_handling_blocks 1
bad_exception_blocks 1
bad_exception_locations [3]
pass_exception_blocks 1
total_pass_statements 1
bad_exception_rate 1.0
uses_traceback False
parsing_error  # None
```


---

## 2Â Â Installation

```bash
git clone https://github.com/kilojoules/silent-killers.git
cd silent-killers
python -m pip install --upgrade pip
pip install -e .
```

or using the pypi distribution

```bash
pip install silent-killers
```


> **Requires PythonÂ â‰¥Â 3.10**  
> Runtime deps: `pandas`, `numpy`, `matplotlib`

---

## 3Â Â Repository layout

```
repo-root/
â”œâ”€ src/
â”‚   â””â”€ silent_killers/            â† reusable package
â”‚        â”œâ”€ __init__.py
â”‚        â””â”€ metrics_definitions.py     (AST visitors & regex metrics)
â”‚        â””â”€ llm_api.py                 (interface analysis with different LLM APIs)

â”œâ”€ tests/                          â† reusable package unit tests
â”‚   â””â”€ test_exception_labels.py
â”‚
â”œâ”€ scripts/                        â† analysis scripts
â”‚   â”œâ”€ process_files.py            (generates metrics CSVs)
â”‚   â””â”€ post_processing.py          (creates plots & summary tables)
â”‚   â””â”€ run_experiments.py          (runs the 3 prompts using the models in models.yaml)
â”‚
â”œâ”€ data/                           â† studyâ€‘specific artifacts
â”‚   â”œâ”€ calibration_prompt/         (easy rewrite task)
â”‚   â”œâ”€ propagation_prompt/         (medium rewrite task)
â”‚   â”œâ”€ optimization_prompt/        (hard rewrite task)
â”‚   â””â”€ figures/                    (output plots & visualizations)
â”‚ 
â”œâ”€ pyproject.toml
|
â””â”€ README.md
```

---

## 4Â Â Scope of this study

Modern LLMs can write Python that â€œrunsâ€, but *how* it fails matters.
A **bare** `except:` or a blanket `exceptÂ Exception:` with no
reâ€‘raise can mask fatal bugs, leading to silent data corruption or
debugging nightmaresâ€”these are the **silentÂ killers**.

We collected **5 seeds Ã—Â 8 models Ã—Â 3 prompts** (easyÂ â†’Â hard rewrite
tasks) and asked:

* How often do models inject `try/except` at all?  
* Of those, how many are â€œbadâ€ under a strict reâ€‘raise rule?  
* Does difficulty exacerbate the problem?

The full paper is on my portfolio: 
[https://julianquick.com/ML/llm-audit.html](https://julianquick.com/ML/llm-audit.html).

---



## 5Â Â Metrics at a glance

| metric | description |
|--------|-------------|
| `exception_handling_blocks` | count of `except` clauses |
| `bad_exception_blocks` | bare `except:` **or** `except Exception:` *without* `raise` |
| `bad_exception_rate` | `bad / total`, 2Â dp |
| `uses_traceback` | calls `traceback.print_exc()` / `.format_exc()` |
| â€¦ | see `src/silent-killers/metrics_definitions.py` |

---

## 6Â Â Key pilot finding

> **When a model adds *any* error handling, 50â€“100Â % of those handlers
> are unsafe.**  
> Inclusive badâ€‘rates look tame (0Â â€“Â 0.6) but conditional badâ€‘rates
> (`only_with_try`) spike to **1.0** for several models on simple
> prompts.

> For these reasons, we recoomend using the silent-killers-audit tool in pre-commit workflows. 

---

## 7Â Â Development

```bash
ruff check .          # lint
pytest                # run unit tests
coverage run -m pytest && coverage html
```

CI runs on GitHubÂ Actions across PythonÂ 3.10 and 3.11 (see `.github/workflows/ci.yml`).

---

## 8Â Â Roadmap

* ðŸš§ dynamic execution traces (runtime errors, coverage)  
* ðŸš§ extend to other unsafe patterns
* ðŸš§ support more languages than Python

PRs & issues welcome!

---

## 9Â Â License & citation

MITÂ License.  
If you use the metrics or figures, please cite:

```bibtex
@misc{Quick2025SilentKillers,
  title  = {Silent Killers: An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python},
  author = {Julian Quick},
  year   = {2025},
  url    = {https://github.com/kilojoules/silent-killers}
}
```

*Happy auditingÂ â€“Â donâ€™t let silent errors slip through!*

