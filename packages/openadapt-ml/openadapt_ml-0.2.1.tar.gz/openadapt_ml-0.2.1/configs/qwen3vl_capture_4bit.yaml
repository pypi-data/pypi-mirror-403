model:
  name: Qwen/Qwen3-VL-2B-Instruct
  load_in_4bit: true  # 4-bit quantization for lower memory
  max_pixels: 262144  # 512x512 for faster training

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: none
  target_modules:
    - q_proj
    - v_proj
  task_type: CAUSAL_LM
  weights_path: checkpoints/qwen3vl2b_capture_lora

training:
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 0.5
  logging_steps: 1
  # Early stopping: stop when loss <= 1.0 (INVARIANT: training should never continue past this)
  # Loss <= 1.0 indicates the model has learned the task; further training is diminishing returns
  early_stop_loss: 1.0
  early_stop_patience: 5
  # Gradient checkpointing handled in trainer.py
