import pytest
from llmcomp.question.question import Question


def test_question_create_to_df(mock_openai_chat_completion, temp_dir):
    """Test creating a Question and checking values in the resulting dataframe"""
    # Create a FreeForm question with a unique results_dir
    question = Question.create(
        type="free_form",
        paraphrases=["What is 2+2?", "What is the sum of 2 and 2?"],
        samples_per_paraphrase=1,
        temperature=0.7,
        max_tokens=100,
    )
    
    # Define model groups
    model_groups = {
        "test_model_group": ["test-model-1", "test-model-2"]
    }
    
    # Get the dataframe
    df = question.df(model_groups)
    
    # Check that dataframe has expected structure
    assert df is not None
    assert len(df) > 0
    
    # Check expected columns
    expected_columns = ["model", "group", "answer", "question", "api_kwargs"]
    for col in expected_columns:
        assert col in df.columns, f"Column {col} not found in dataframe"
    
    # Check that we have rows for both models
    assert len(df[df["model"] == "test-model-1"]) > 0
    assert len(df[df["model"] == "test-model-2"]) > 0
    
    # Check that we have rows for both paraphrases
    assert len(df[df["question"] == "What is 2+2?"]) > 0
    assert len(df[df["question"] == "What is the sum of 2 and 2?"]) > 0
    
    # Check that answers are strings (from our mock)
    assert all(isinstance(answer, str) for answer in df["answer"])
    
    # Check that api_kwargs contains messages as lists
    assert all(isinstance(kwargs["messages"], list) for kwargs in df["api_kwargs"])
    
    # Check that group is set correctly
    assert all(df["group"] == "test_model_group")
    
    assert len(df) == 4


def test_freeform_with_freeform_judge(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_freeform_judge",
        paraphrases=["What is 3+3?"],
        samples_per_paraphrase=1,
        judges={
            "quality": {
                "type": "free_form_judge",
                "model": "judge-model",
                "paraphrases": ["Rate this answer: {answer} to question: {question}. Give one word."],
            }
        },
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "quality" in df.columns
    assert "quality_question" in df.columns
    assert all(isinstance(val, str) for val in df["quality"])
    assert len(df) == 1


def test_freeform_with_rating_judge(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_rating_judge",
        paraphrases=["Tell me a joke"],
        samples_per_paraphrase=2,
        judges={
            "score": {
                "type": "rating_judge",
                "model": "judge-model",
                "paraphrases": ["Rate this answer: {answer} to question: {question}. Give a number 0-100."],
            }
        },
    )
    model_groups = {"group1": ["model-1", "model-2"]}
    df = question.df(model_groups)
    assert "score" in df.columns
    assert "score_question" in df.columns
    assert all(isinstance(val, (int, float)) or val is None for val in df["score"])
    assert len(df) == 4


def test_freeform_with_multiple_judges(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_multiple_judges",
        paraphrases=["Say hello"],
        judges={
            "judge1": {
                "type": "free_form_judge",
                "model": "judge-model-1",
                "paraphrases": ["Judge 1: {answer}"],
            },
            "judge2": {
                "type": "rating_judge",
                "model": "judge-model-2",
                "paraphrases": ["Judge 2: {answer}. Rate 0-100."],
            },
        },
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "judge1" in df.columns
    assert "judge2" in df.columns
    assert "judge1_question" in df.columns
    assert "judge2_question" in df.columns


def test_rating_question(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="rating",
        paraphrases=["Rate from 0 to 100"],
        min_rating=0,
        max_rating=100,
        top_logprobs=10,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "raw_answer" in df.columns
    assert all(isinstance(val, dict) for val in df["raw_answer"])
    assert all(isinstance(val, (int, float)) or val is None for val in df["answer"])


def test_rating_question_custom_range(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="rating",
        paraphrases=["Rate 1-5"],
        min_rating=1,
        max_rating=5,
        refusal_threshold=0.5,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) > 0
    assert all(val is None or 1 <= val <= 5 for val in df["answer"] if val is not None)


def test_nexttoken_question(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="next_token",
        paraphrases=["The answer is"],
        top_logprobs=15,
        convert_to_probs=True,
        num_samples=1,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert all(isinstance(val, dict) for val in df["answer"])
    assert len(df) > 0


def test_nexttoken_with_multiple_samples(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="next_token",
        paraphrases=["Hello"],
        num_samples=3,
        convert_to_probs=False,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert all(isinstance(val, dict) for val in df["answer"])


def test_freeform_with_system_message(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        paraphrases=["What is 5+5?"],
        system="You are a helpful assistant.",
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) > 0
    messages_list = [kwargs["messages"] for kwargs in df["api_kwargs"]]
    assert all(len(msgs) == 2 for msgs in messages_list)
    assert all(msgs[0]["role"] == "system" for msgs in messages_list)


def test_freeform_with_custom_messages(mock_openai_chat_completion, temp_dir):
    messages = [
        [{"role": "system", "content": "Be concise"}, {"role": "user", "content": "Hi"}],
        [{"role": "user", "content": "Bye"}],
    ]
    question = Question.create(
        type="free_form",
        messages=messages,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) == 2
    assert df["api_kwargs"].iloc[0]["messages"] == messages[0]
    assert df["api_kwargs"].iloc[1]["messages"] == messages[1]


def test_freeform_multiple_samples_per_paraphrase(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        paraphrases=["Count to 3"],
        samples_per_paraphrase=5,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) == 5


def test_freeform_multiple_paraphrases_multiple_samples(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        paraphrases=["A", "B", "C"],
        samples_per_paraphrase=2,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) == 6
    assert len(df[df["question"] == "A"]) == 2
    assert len(df[df["question"] == "B"]) == 2
    assert len(df[df["question"] == "C"]) == 2


def test_freeform_different_temperatures(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        paraphrases=["Random"],
        temperature=0.0,
        max_tokens=50,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) > 0


def test_rating_judge_with_custom_range(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_rating_judge_range",
        paraphrases=["Test"],
        judges={
            "rating": {
                "type": "rating_judge",
                "model": "judge-model",
                "paraphrases": ["Rate {answer}. 0-10 only."],
                "min_rating": 0,
                "max_rating": 10,
            }
        },
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "rating" in df.columns
    assert all(val is None or 0 <= val <= 10 for val in df["rating"] if val is not None)


def test_multiple_model_groups(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        paraphrases=["Test"],
    )
    model_groups = {
        "group1": ["model-1", "model-2"],
        "group2": ["model-3"],
    }
    df = question.df(model_groups)
    assert len(df) == 3
    assert len(df[df["group"] == "group1"]) == 2
    assert len(df[df["group"] == "group2"]) == 1
    assert set(df["model"].unique()) == {"model-1", "model-2", "model-3"}


def test_judge_uses_question_and_answer_placeholders(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_judge_placeholders",
        paraphrases=["What is 2+2?"],
        judges={
            "eval": {
                "type": "free_form_judge",
                "model": "judge-model",
                "paraphrases": ["Q: {question}\nA: {answer}\nIs this good?"],
            }
        },
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "eval_question" in df.columns
    assert all("What is 2+2?" in q for q in df["eval_question"])
    assert all("Mocked response" in q for q in df["eval_question"])


def test_freeform_judge_temperature_zero(mock_openai_chat_completion, temp_dir):
    question = Question.create(
        type="free_form",
        name="test_judge_temp_zero",
        paraphrases=["Test"],
        judges={
            "judge": {
                "type": "free_form_judge",
                "model": "judge-model",
                "paraphrases": ["Judge: {answer}"],
                "temperature": 0.0,
            }
        },
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert len(df) > 0


def test_judge_paraphrases_not_mutated(mock_openai_chat_completion, temp_dir):
    original_paraphrase = "Judge: {answer}"
    question = Question.create(
        type="free_form",
        name="test_judge_mutation",
        paraphrases=["Q1", "Q2"],
        judges={
            "judge": {
                "type": "free_form_judge",
                "model": "judge-model",
                "paraphrases": [original_paraphrase],
            }
        },
    )
    model_groups = {"group1": ["model-1"]}
    
    original_judge_paraphrases = question.judges["judge"].paraphrases.copy()
    assert original_judge_paraphrases == [original_paraphrase]
    
    df1 = question.df(model_groups)
    assert len(df1) == 2
    
    after_first_call = question.judges["judge"].paraphrases.copy()
    
    df2 = question.df(model_groups)
    assert len(df2) == 2
    
    after_second_call = question.judges["judge"].paraphrases.copy()
    
    assert original_judge_paraphrases == [original_paraphrase], f"Original paraphrases were mutated: {original_judge_paraphrases}"
    assert after_first_call == after_second_call, f"Paraphrases changed between calls: {after_first_call} != {after_second_call}"
    assert question.judges["judge"].paraphrases[0] == original_paraphrase, f"Final paraphrase is wrong: {question.judges['judge'].paraphrases[0]}"


def test_paraphrase_ix_column(mock_openai_chat_completion, temp_dir):
    """Test that dataframe includes paraphrase_ix column with correct values"""
    question = Question.create(
        type="free_form",
        paraphrases=["First question", "Second question", "Third question"],
        samples_per_paraphrase=2,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    
    # Check that paraphrase_ix column exists
    assert "paraphrase_ix" in df.columns, "paraphrase_ix column should be present"
    
    # Check that we have the expected number of rows (3 paraphrases * 2 samples = 6)
    assert len(df) == 6, f"Expected 6 rows, got {len(df)}"
    
    # Check that paraphrase_ix values are correct
    assert set(df["paraphrase_ix"].unique()) == {0, 1, 2}, "paraphrase_ix should be 0, 1, or 2"
    
    # Check that each paraphrase has the correct index
    first_paraphrase_rows = df[df["question"] == "First question"]
    assert all(first_paraphrase_rows["paraphrase_ix"] == 0), "First paraphrase should have index 0"
    assert len(first_paraphrase_rows) == 2, "First paraphrase should have 2 samples"
    
    second_paraphrase_rows = df[df["question"] == "Second question"]
    assert all(second_paraphrase_rows["paraphrase_ix"] == 1), "Second paraphrase should have index 1"
    assert len(second_paraphrase_rows) == 2, "Second paraphrase should have 2 samples"
    
    third_paraphrase_rows = df[df["question"] == "Third question"]
    assert all(third_paraphrase_rows["paraphrase_ix"] == 2), "Third paraphrase should have index 2"
    assert len(third_paraphrase_rows) == 2, "Third paraphrase should have 2 samples"


def test_paraphrase_ix_with_messages(mock_openai_chat_completion, temp_dir):
    """Test that paraphrase_ix works with custom messages"""
    messages = [
        [{"role": "user", "content": "Message set 1"}],
        [{"role": "user", "content": "Message set 2"}],
        [{"role": "system", "content": "System"}, {"role": "user", "content": "Message set 3"}],
    ]
    question = Question.create(
        type="free_form",
        messages=messages,
    )
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    
    assert "paraphrase_ix" in df.columns, "paraphrase_ix column should be present"
    assert len(df) == 3, "Should have 3 rows (one per message set)"
    assert set(df["paraphrase_ix"].unique()) == {0, 1, 2}, "paraphrase_ix should be 0, 1, or 2"
    
    # Check that each message set has the correct index
    assert df[df["question"] == "Message set 1"]["paraphrase_ix"].iloc[0] == 0
    assert df[df["question"] == "Message set 2"]["paraphrase_ix"].iloc[0] == 1
    assert df[df["question"] == "Message set 3"]["paraphrase_ix"].iloc[0] == 2


def test_forbidden_judge_names(mock_openai_chat_completion, temp_dir):
    """Test that forbidden judge names raise ValueError"""
    forbidden_names = ["model", "group", "answer", "question", "api_kwargs", "paraphrase_ix", "raw_answer"]
    
    for forbidden_name in forbidden_names:
        with pytest.raises(ValueError, match=f"Judge name '{forbidden_name}' is forbidden"):
            Question.create(
                type="free_form",
                paraphrases=["Test"],
                judges={
                    forbidden_name: {
                        "type": "free_form_judge",
                        "model": "judge-model",
                        "paraphrases": ["Judge: {answer}"],
                    }
                },
            )


def test_forbidden_judge_names_underscore(mock_openai_chat_completion, temp_dir):
    """Test that judge names starting with '_' are forbidden"""
    underscore_names = ["_judge", "__internal", "_test"]
    
    for name in underscore_names:
        with pytest.raises(ValueError, match="Names starting with '_' are reserved for internal use"):
            Question.create(
                type="free_form",
                paraphrases=["Test"],
                judges={
                    name: {
                        "type": "free_form_judge",
                        "model": "judge-model",
                        "paraphrases": ["Judge: {answer}"],
                    }
                },
            )


def test_valid_judge_names(mock_openai_chat_completion, temp_dir):
    """Test that valid judge names work correctly"""
    question = Question.create(
        type="free_form",
        paraphrases=["Test"],
        judges={
            "quality": {
                "type": "free_form_judge",
                "model": "judge-model",
                "paraphrases": ["Judge: {answer}"],
            },
            "score": {
                "type": "rating_judge",
                "model": "judge-model",
                "paraphrases": ["Rate: {answer}"],
            },
        },
    )
    # Should not raise any error
    assert question.judges is not None
    assert "quality" in question.judges
    assert "score" in question.judges


def test_judge_as_question_instance(mock_openai_chat_completion, temp_dir):
    """Test that judges can be passed as Question instances"""
    # Create a judge Question instance directly
    judge_instance = Question.create(
        type="free_form_judge",
        model="judge-model",
        paraphrases=["Judge this: {answer}"],
    )
    
    # Create a question with the judge instance
    question = Question.create(
        type="free_form",
        name="test_judge_instance",
        paraphrases=["What is 2+2?"],
        judges={
            "quality": judge_instance,
        },
    )
    
    # Verify the judge was stored correctly
    assert question.judges is not None
    assert "quality" in question.judges
    assert question.judges["quality"] is judge_instance
    
    # Verify it works in practice
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "quality" in df.columns
    assert "quality_question" in df.columns
    assert len(df) == 1


def test_judge_as_rating_judge_instance(mock_openai_chat_completion, temp_dir):
    """Test that RatingJudge instances can be passed as judges"""
    # Create a RatingJudge instance directly
    rating_judge = Question.create(
        type="rating_judge",
        model="judge-model",
        paraphrases=["Rate 0-100: {answer}"],
        min_rating=0,
        max_rating=100,
    )
    
    # Create a question with the rating judge instance
    question = Question.create(
        type="free_form",
        name="test_rating_judge_instance",
        paraphrases=["Tell me a joke"],
        judges={
            "score": rating_judge,
        },
    )
    
    # Verify the judge was stored correctly
    assert question.judges is not None
    assert "score" in question.judges
    assert question.judges["score"] is rating_judge
    
    # Verify it works in practice
    model_groups = {"group1": ["model-1"]}
    df = question.df(model_groups)
    assert "score" in df.columns
    assert "score_question" in df.columns
    assert len(df) == 1


def test_judge_name_conflicts_with_generated_columns(mock_openai_chat_completion, temp_dir):
    """Test that judge names ending with _question or _raw_answer are forbidden"""
    # Test conflict with _question suffix
    with pytest.raises(ValueError, match="Names ending with '_question' conflict"):
        Question.create(
            type="free_form",
            paraphrases=["Test"],
            judges={
                "ttt_question": {
                    "type": "free_form_judge",
                    "model": "judge-model",
                    "paraphrases": ["Judge: {answer}"],
                },
            },
        )
    
    # Test conflict with _raw_answer suffix
    with pytest.raises(ValueError, match="Names ending with '_raw_answer' conflict"):
        Question.create(
            type="free_form",
            paraphrases=["Test"],
            judges={
                "rating_raw_answer": {
                    "type": "free_form_judge",
                    "model": "judge-model",
                    "paraphrases": ["Judge: {answer}"],
                },
            },
        )


def test_rating_aggregates_duplicate_integer_tokens(temp_dir):
    """
    Test that Rating question correctly aggregates probabilities when multiple
    tokens map to the same integer value.
    
    This can happen with Unicode variants of digits:
    - "100" (ASCII) and "１００" (full-width Japanese/Chinese) both parse to int 100
    - Python's int() handles this: int("１００") == 100
    
    Before the bug fix, the code would overwrite probabilities:
        probs[int_key] = val  # Only keeps the last token's probability
    
    After the fix, probabilities are accumulated:
        probs[int_key] += val  # Sums all tokens mapping to same integer
    """
    from unittest.mock import patch, Mock
    from tests.conftest import MockCompletion, MockLogprobs, MockContent, MockLogprob
    import llmcomp.runner.runner as runner_module
    import llmcomp.config as config_module
    from llmcomp.config import Config
    
    Config.client_cache.clear()
    
    def mock_completion(*, client=None, **kwargs):
        messages = kwargs.get('messages', [])
        logprobs = kwargs.get('logprobs', False)
        
        if logprobs:
            # Return logprobs where two different tokens map to the same integer:
            # "50" (ASCII) and "５０" (full-width) both parse to 50
            # Each has probability 0.3, so combined they should be 0.6
            # "60" has probability 0.4
            # Expected rating = (50 * 0.6 + 60 * 0.4) / 1.0 = 54.0
            top_logprobs = [
                MockLogprob("50", -1.2),    # ~0.30 probability
                MockLogprob("５０", -1.2),  # ~0.30 probability (full-width digits, same int value)
                MockLogprob("60", -0.92),   # ~0.40 probability
            ]
            return MockCompletion("", logprobs=MockLogprobs([MockContent(top_logprobs)]))
        
        return MockCompletion("Mocked response")
    
    mock_client = Mock()
    mock_client.chat.completions.create = Mock(side_effect=mock_completion)
    
    def mock_client_for_model(model):
        if model not in Config.client_cache:
            Config.client_cache[model] = mock_client
        return Config.client_cache[model]
    
    with patch.object(Config, 'client_for_model', side_effect=mock_client_for_model), \
         patch('llmcomp.runner.chat_completion.openai_chat_completion', side_effect=mock_completion), \
         patch.object(runner_module, 'openai_chat_completion', side_effect=mock_completion), \
         patch.object(config_module, 'openai_chat_completion', side_effect=mock_completion):
        
        question = Question.create(
            type="rating",
            name="test_duplicate_integer_tokens",
            paraphrases=["Rate this from 0 to 100"],
            min_rating=0,
            max_rating=100,
        )
        
        model_groups = {"group1": ["model-1"]}
        df = question.df(model_groups)
    
    Config.client_cache.clear()
    
    # Check we have exactly one row
    assert len(df) == 1
    
    answer = df["answer"].iloc[0]
    assert answer is not None, "Rating should not be None (not a refusal)"
    
    # Math:
    # exp(-1.2) ≈ 0.301 for "50" and "５０", exp(-0.92) ≈ 0.399 for "60"
    # total = 0.301 + 0.301 + 0.399 = 1.001
    #
    # Correct(correct - aggregates):
    #   probs[50] = 0.602, probs[60] = 0.399
    #   normalized: 50 → 0.601, 60 → 0.399
    #   expected = 50*0.601 + 60*0.399 ≈ 54
    #
    # With weird tokens overwriting the correct one (bug - overwrites):
    #   probs[50] = 0.301 (second overwrites first), probs[60] = 0.399
    #   but total still = 1.001, so normalized probs don't sum to 1!
    #   expected = 50*0.301 + 60*0.399 ≈ 39
    
    assert 53 < answer < 55, (
        f"Expected rating ≈ 54 (with correct aggregation), got {answer}. "
        f"Bug: duplicate integer tokens were not aggregated - prob mass was lost."
    )


def test_judge_with_answer_only_template_and_duplicate_answers(temp_dir):
    """
    Test for bug: when judge template only uses {answer} (not {question}),
    and multiple rows have the same answer, the caching logic breaks.
    
    The bug occurs because:
    1. prompt_to_qa dict maps prompt -> (question, answer), but collides when
       different (q, a) pairs produce the same prompt
    2. Only the last (q, a) pair survives in the mapping
    3. Cache is only updated for some pairs
    4. The merge produces duplicate rows due to multiple matches
    
    This test uses two different questions that produce the SAME answer,
    with a judge that only looks at {answer}.
    """
    from unittest.mock import patch, Mock
    from tests.conftest import MockCompletion, MockLogprobs, MockContent, MockLogprob
    
    # Track what prompts were sent to the API
    api_calls = []
    
    def mock_completion(*, client=None, **kwargs):
        messages = kwargs.get('messages', [])
        logprobs = kwargs.get('logprobs', False)
        
        if logprobs:
            top_logprobs = [MockLogprob(str(i), -0.1 * i) for i in range(0, 100, 10)]
            return MockCompletion("", logprobs=MockLogprobs([MockContent(top_logprobs)]))
        
        last_message = messages[-1].get('content', '') if messages else ''
        api_calls.append(last_message)
        
        # KEY: Return the SAME answer for different questions
        # This simulates the scenario where two paraphrases get the same model response
        if "What is 2+2" in last_message or "Calculate two plus two" in last_message:
            return MockCompletion("The answer is 4")
        
        # Judge responses
        return MockCompletion(f"Judged: {last_message}")
    
    import llmcomp.runner.runner as runner_module
    import llmcomp.config as config_module
    from llmcomp.config import Config
    Config.client_cache.clear()
    
    mock_client = Mock()
    mock_client.chat.completions.create = Mock(side_effect=mock_completion)
    
    def mock_client_for_model(model):
        if model not in Config.client_cache:
            Config.client_cache[model] = mock_client
        return Config.client_cache[model]
    
    with patch.object(Config, 'client_for_model', side_effect=mock_client_for_model), \
         patch('llmcomp.runner.chat_completion.openai_chat_completion', side_effect=mock_completion), \
         patch.object(runner_module, 'openai_chat_completion', side_effect=mock_completion), \
         patch.object(config_module, 'openai_chat_completion', side_effect=mock_completion):
        
        # Two different questions that will get the SAME answer ("The answer is 4")
        question = Question.create(
            type="free_form",
            name="test_duplicate_answer_bug",
            paraphrases=[
                "What is 2+2?",
                "Calculate two plus two",
            ],
            samples_per_paraphrase=1,
            judges={
                # This judge only uses {answer}, not {question}
                # So both rows produce the same judge prompt!
                "category": {
                    "type": "free_form_judge",
                    "model": "judge-model",
                    "paraphrases": ["Categorize this answer: {answer}"],
                }
            },
        )
        
        model_groups = {"group1": ["model-1"]}
        df = question.df(model_groups)
    
    Config.client_cache.clear()
    
    # We expect exactly 2 rows (one per paraphrase)
    assert len(df) == 2, (
        f"Expected 2 rows, got {len(df)}. "
        f"Bug: duplicate rows created due to judge prompt collision during merge."
    )
    
    # Both rows should have the same answer (from our mock)
    assert all(df["answer"] == "The answer is 4"), "Both answers should be 'The answer is 4'"
    
    # Both rows should have a valid (non-None) judge response
    assert df["category"].notna().all(), (
        f"Some judge responses are None: {df['category'].tolist()}. "
        f"Bug: cache.get() returned None for some (question, answer) pairs "
        f"because prompt_to_qa dict collision caused incorrect cache storage."
    )
    
    # The two questions should be different
    questions = df["question"].tolist()
    assert len(set(questions)) == 2, "Should have two different questions"
    assert "What is 2+2?" in questions
    assert "Calculate two plus two" in questions

