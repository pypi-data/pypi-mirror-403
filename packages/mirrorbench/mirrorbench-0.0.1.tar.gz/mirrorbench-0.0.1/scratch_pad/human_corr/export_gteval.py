"""Export GTEval samples for human annotation.

GTEval presents both real and synthetic conversations to the labeler and asks
them to score similarity on a 0-1 scale.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from mirrorbench.core.models.messages import Message, Role
from mirrorbench.io.paths import Paths
from mirrorbench.metrics.util import resolve_reference_conversation

from sample_episodes import SampledEpisode, load_episodes_for_metric, stratified_sample


def format_conversation(messages: list[Message]) -> list[dict[str, str]]:
    """Format a conversation as structured JSON for human labeler."""
    formatted = []
    for msg in messages:
        role_label = msg.role.value if isinstance(msg.role, Role) else str(msg.role)
        formatted.append({"role": role_label, "content": msg.content})
    return formatted


def export_gteval_samples(
    run_id: str,
    output_dir: Path,
    num_samples: int = 100,
    seed: int = 42,
    paths: Paths | None = None,
) -> tuple[Path, Path]:
    """Export GTEval samples for human annotation.

    Args:
        run_id: Run identifier
        output_dir: Directory to save the annotation file
        num_samples: Number of samples to export (default: 100)
        seed: Random seed for sampling (default: 42)
        paths: Paths instance (uses default if None)

    Returns:
        Tuple of (annotation_file_path, judge_scores_file_path)
    """
    if paths is None:
        paths = Paths.default()

    metric_name = "metric:judge/gteval"

    # Load all episodes for this metric
    print(f"Loading episodes for {metric_name} from run {run_id}...")
    episodes = load_episodes_for_metric(run_id, metric_name, paths)
    print(f"Found {len(episodes)} episodes")

    # Stratified sampling
    print(f"Sampling {num_samples} episodes using stratified sampling...")
    sampled = stratified_sample(episodes, num_samples, seed=seed)
    print(f"Sampled {len(sampled)} episodes")

    # Export to JSON - separate files for annotation and judge scores
    annotation_samples: list[dict[str, Any]] = []
    judge_scores: list[dict[str, Any]] = []

    for episode in sampled:
        artifact = episode.artifact

        # Get real conversation from references
        real_conversation = resolve_reference_conversation(artifact)
        if not real_conversation:
            print(f"Warning: No reference conversation for episode {episode.episode_id}, skipping")
            continue

        # Get synthetic conversation
        synthetic_conversation = artifact.turns

        # Sample for annotation (NO judge score visible)
        annotation_samples.append(
            {
                "sample_id": f"{episode.unit_id}::{episode.episode_id}",
                "dataset_name": episode.dataset_name,
                "real_conversation": format_conversation(real_conversation),
                "synthetic_conversation": format_conversation(synthetic_conversation),
                "instructions": (
                    "Compare the REAL conversation (from a real human user) with the "
                    "SYNTHETIC conversation (generated by an AI user proxy). Rate how similar "
                    "the synthetic conversation is to the real one on a scale from 0.0 (completely "
                    "different) to 1.0 (identical in style, tone, and behavior)."
                ),
                "human_label": None,  # To be filled by annotator
            }
        )

        # Judge scores (kept separate)
        judge_scores.append(
            {
                "sample_id": f"{episode.unit_id}::{episode.episode_id}",
                "judge_score": episode.metric_score,
            }
        )

    # Save annotation file (for human labeling)
    output_dir.mkdir(parents=True, exist_ok=True)
    annotation_path = output_dir / "gteval_samples.json"
    with annotation_path.open("w", encoding="utf-8") as f:
        json.dump(annotation_samples, f, indent=2, ensure_ascii=False)

    # Save judge scores file (hidden from annotator)
    judge_path = output_dir / "gteval_judge_scores.json"
    with judge_path.open("w", encoding="utf-8") as f:
        json.dump(judge_scores, f, indent=2, ensure_ascii=False)

    print(f"Exported {len(annotation_samples)} samples to {annotation_path}")
    print(f"Saved judge scores to {judge_path}")
    return annotation_path, judge_path


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python export_gteval.py <run_id> [num_samples] [seed]")
        sys.exit(1)

    run_id = sys.argv[1]
    num_samples = int(sys.argv[2]) if len(sys.argv) > 2 else 100
    seed = int(sys.argv[3]) if len(sys.argv) > 3 else 42

    output_dir = Path(__file__).parent / "runs" / run_id
    export_gteval_samples(run_id, output_dir, num_samples, seed)
