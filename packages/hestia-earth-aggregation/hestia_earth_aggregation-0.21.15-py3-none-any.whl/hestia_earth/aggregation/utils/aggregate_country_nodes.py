import os
from functools import reduce
from collections import Counter
from typing import TypedDict, Optional, Dict, List, Union, Tuple, Set
from hestia_earth.schema import TermTermType, CycleJSONLD, SiteJSONLD, TermJSONLD
from hestia_earth.utils.tools import (
    list_sum,
    list_average,
    non_empty_list,
    flatten,
    is_number,
    is_boolean,
    current_time_ms,
)
from hestia_earth.utils.blank_node import (
    ArrayTreatment,
    get_node_value,
    _retrieve_array_treatment,
)

from hestia_earth.aggregation.log import logger, log_memory_usage
from . import (
    CYCLE_AGGREGATION_KEYS,
    SITE_AGGREGATION_KEYS,
    pick,
    weighted_average,
    _min,
    _max,
    _sd,
    sum_data,
    format_aggregated_list,
    format_evs,
)
from .term import _format_irrigated, _format_organic
from .completeness import (
    combine_completeness_count,
    sum_completeness_count,
    is_complete,
    completeness_count,
    blank_node_completeness_key,
)
from .group import _filter_blank_nodes
from .blank_node import (
    map_blank_node,
    default_missing_value,
    _blank_node_type,
    blank_node_group_key,
    blank_node_sd,
    get_lookup_value,
)
from .queries import download_nodes, download_site
from .cycle import (
    is_organic,
    is_irrigated,
    aggregate_with_matrix,
    _should_include_cycle,
)
from .site import (
    create_site,
    format_site_results,
    format_site_description,
    site_parent_region_id,
)
from .emission import (
    get_method_tier,
    get_method_model,
    has_value_without_transformation,
)
from .property import aggregate_properties
from .distribution import generate_blank_node_distribution
from .covariance import add_covariance_cycles, generate_covariance_cycles

_BATCH_SIZE = int(os.getenv("AGGREGATION_CYCLES_BATCH_SIZE", "1000"))


def _default_as_value(value: dict):
    return value.get("value")


def _map_sd(values: list, array_treatment: ArrayTreatment):
    return flatten(
        non_empty_list([blank_node_sd(value, array_treatment) for value in values])
    )


def _map_node_values(
    values: list,
    array_treatment: ArrayTreatment,
    key: str = "value",
    fallback=lambda *args: None,
):
    def get_value(blank_node: dict):
        value = get_node_value(
            blank_node, key=key, array_treatment=array_treatment, default=None
        )
        return fallback(blank_node) if value is None else value

    return flatten(non_empty_list(map(get_value, values)))


def _map_values(values: list, key: str = "value", include_empty: bool = False):
    results = flatten([v.get(key) for v in values])
    return results if include_empty else non_empty_list(results)


class BlankNodeFormatted(TypedDict, total=False):
    term: dict
    value: Optional[List[Union[str, int, float, bool]]]
    economicValueShare: Optional[List[Union[int, float]]]
    min: Optional[List[Union[int, float]]]
    max: Optional[List[Union[int, float]]]
    observations: Optional[int]
    complete: Optional[bool]
    completeness_key: Optional[str]
    start_date: Optional[str]
    end_date: str
    inputs: List[dict]
    transformation: List[dict]


# --- Cycle


class CycleFormatted(TypedDict, total=False):
    cycle_ids: List[str]
    site_ids: List[str]
    source_ids: List[str]
    site_durations: List[float]
    site_unused_durations: List[float]
    site_areas: List[float]
    completeness: Dict[str, int]
    product: TermJSONLD
    functionalUnit: str
    organic: bool
    irrigated: bool
    numberOfCycles: int
    inputs: Dict[str, BlankNodeFormatted]
    products: Dict[str, BlankNodeFormatted]
    practices: Dict[str, BlankNodeFormatted]
    emissions: Dict[str, BlankNodeFormatted]


def _cycle_product_value(cycle: dict, product: dict):
    return list_sum(
        flatten(
            [
                # account for every product with the same `@id`
                p.get("value", [])
                for p in cycle.get("products", [])
                if p.get("term", {}).get("@id") == product.get("@id")
            ]
        ),
        default=None,
    )


def _blank_node_evs(blank_nodes: List[BlankNodeFormatted]) -> Union[None, int, float]:
    values = _map_values(blank_nodes, "economicValueShare")
    return None if not values else list_sum(values)


def _rescaled_value(
    blank_node: BlankNodeFormatted, product: dict, product_value: float
):
    normalize_value = product.get("termType") != TermTermType.CROP.value
    value = blank_node["value"]
    should_rescale = get_lookup_value(
        blank_node["term"], "aggregationRescaleProductValue"
    )
    return (
        (value / (product_value if product_value else 1))
        if normalize_value and should_rescale and is_number(value)
        else value
    )


def _combine_blank_nodes_stats(blank_nodes: List[BlankNodeFormatted]):
    """
    Combine multiple blank nodes with the same term, into a single list of stats.
    """
    array_treatment = _retrieve_array_treatment(
        blank_nodes[0], default=ArrayTreatment.SUM
    )
    return {
        "value": _map_node_values(blank_nodes, array_treatment),
        "min": _map_node_values(
            blank_nodes, array_treatment, "min", fallback=_default_as_value
        ),
        "max": _map_node_values(
            blank_nodes, array_treatment, "max", fallback=_default_as_value
        ),
        "sd": _map_sd(blank_nodes, array_treatment),
    }


def _group_cycle_blank_node(cycle: dict, product: dict, product_value: float):
    """
    Group all blanks nodes within a single Cycle, using the term `@id` and some other keys.
    This will group with any other existing blank node in the same group, and keep only one value.
    """

    def grouper(group: dict, blank_node: dict) -> Dict[str, BlankNodeFormatted]:
        term = blank_node.get("term", {})
        node_type = _blank_node_type(blank_node)
        term_id = term.get("@id")
        group_key = blank_node_group_key(blank_node)

        is_aggregated_product = node_type == "Product" and term_id == product.get("@id")

        site_type = cycle.get("site", {}).get("siteType")
        complete = is_complete(
            node=cycle, product=product, blank_node=blank_node, site_type=site_type
        )

        blank_node_data: BlankNodeFormatted = map_blank_node(
            blank_node
            | {
                "complete": complete,
                "completeness_key": blank_node_completeness_key(
                    blank_node, product, site_type=site_type
                ),
            },
            is_aggregated_product=is_aggregated_product,
        )

        # only primary data can be incomplete, otherwise skip the blank node
        if all(
            [
                complete is False,
                not blank_node_data.get("primary"),
                blank_node_data.get("primaryPercent", 0) <= 0,
            ]
        ):
            return group

        blank_node_data["value"] = _rescaled_value(
            blank_node_data, product, product_value
        )

        # if there is an existing blank node in the same group, combine to get a single value
        existing_data: BlankNodeFormatted = group.get(group_key)

        blank_nodes = non_empty_list([existing_data, blank_node_data])
        data = (
            (existing_data or {})
            | blank_node_data
            | _combine_blank_nodes_stats(blank_nodes)
            | {
                "economicValueShare": non_empty_list([_blank_node_evs(blank_nodes)]),
                # needed for background emissions
                "inputs": _map_values(blank_nodes, "inputs"),
                # needed to exclude emissions with only transformation value
                "transformation": _map_values(blank_nodes, "transformation"),
                "properties": _map_values(blank_nodes, "properties"),
                "methodTier": _map_values(blank_nodes, "methodTier"),
                "methodModel": _map_values(blank_nodes, "methodModel"),
                "primaryPercent": _map_values(blank_nodes, "primaryPercent"),
            }
        )
        group[group_key] = data
        return group

    return grouper


def _compute_blank_node_stats(blank_node: BlankNodeFormatted):
    """
    Combine a single blank nodes with multiple stats, into a single list of stats.
    """
    array_treatment = _retrieve_array_treatment(blank_node, default=ArrayTreatment.SUM)
    value = get_node_value(blank_node, array_treatment=array_treatment, default=None)
    min = (
        _min(blank_node.get("min", []), min_observations=1)
        if array_treatment == ArrayTreatment.MEAN
        else get_node_value(
            blank_node, key="min", array_treatment=array_treatment, default=None
        )
    )
    max = (
        _max(blank_node.get("max", []), min_observations=1)
        if array_treatment == ArrayTreatment.MEAN
        else get_node_value(
            blank_node, key="max", array_treatment=array_treatment, default=None
        )
    )
    sd = blank_node_sd(blank_node, array_treatment)
    return {
        "value": value,
        "min": min,
        "max": max,
        "sd": sd,
        "observations": 0 if value is None else 1,
    }


def _group_cycle_blank_nodes(
    cycle: dict,
    product: dict,
    product_value: float,
    start_year: int = None,
    end_year: int = None,
):
    def grouper(group: dict, list_key: str) -> Dict[str, Dict[str, BlankNodeFormatted]]:
        now = current_time_ms()
        blank_nodes = _filter_blank_nodes(cycle, list_key, start_year, end_year)
        values = reduce(
            _group_cycle_blank_node(cycle, product, product_value), blank_nodes, {}
        )
        # after combining all values, need to compute the final statistical values
        group[list_key] = {
            k: v | _compute_blank_node_stats(v)
            for k, v in values.items()
            if list_key != "emissions" or has_value_without_transformation(v)
        }
        logger.debug(
            "function=_group_cycle_blank_nodes, list_key=%s, time=%s",
            list_key,
            current_time_ms() - now,
        )
        return group

    return grouper


def _format_cycle(
    cycle: CycleJSONLD,
    product: TermJSONLD,
    start_year: int = None,
    end_year: int = None,
) -> Tuple[List[CycleFormatted], Set[str]]:
    """
    Format Cycles to be used in grouping.
    Returns the list of formatted Cycles, and the list of Site ids to be downloaded.
    Note: if a Site does not have an `@id`, it means it's nested within the Cycle.
    """
    product_value = _cycle_product_value(cycle, product)
    data: CycleFormatted = (
        (
            pick(cycle, ["functionalUnit", "endDate"])
            | {
                "cycle_ids": [cycle.get("@id")],
                "site_ids": [cycle.get("site", {}).get("@id")],
                "source_ids": [cycle.get("defaultSource", {}).get("@id")],
                "site_durations": non_empty_list([cycle.get("siteDuration")]),
                "site_unused_durations": non_empty_list(
                    [cycle.get("siteUnusedDuration")]
                ),
                "site_areas": non_empty_list([cycle.get("siteArea")]),
                "completeness": combine_completeness_count([cycle.get("completeness")]),
                "product": product,
                "organic": is_organic(cycle),
                "irrigated": is_irrigated(cycle),
                "numberOfCycles": cycle.get("numberOfCycles") or 1,
            }
            | reduce(
                _group_cycle_blank_nodes(
                    cycle, product, product_value, start_year, end_year
                ),
                CYCLE_AGGREGATION_KEYS,
                {},
            )
        )
        if _should_include_cycle(cycle)
        else None
    )
    if data:
        logger.debug(
            "id=%s, yield=%s, organic=%s, irrigated=%s",
            cycle.get("@id"),
            product_value,
            data["organic"],
            data["irrigated"],
        )
    return data if product_value is not None else None


def _combine_cycle_blank_node(
    cycles: List[CycleFormatted],
    list_key: str,
    completeness: Dict[str, int],
    cycle_count: int,
):
    """
    Group blank nodes among multiple Cycles, to keep only one data entry with all the values.
    """

    def grouper(group: dict, group_key: str):
        values: List[BlankNodeFormatted] = non_empty_list(
            [cycle.get(list_key, {}).get(group_key) for cycle in cycles]
        )
        value = _map_values(values, include_empty=True)
        term = values[0].get("term")
        complete_key = values[0].get("completeness_key")
        complete_count = completeness_count(completeness, complete_key)
        default_value = default_missing_value(term)
        missing_values = [default_value] * (complete_count - len(value))
        primary_percent = _map_values(values, "primaryPercent")
        data: BlankNodeFormatted = {
            **values[0],
            "value": value + missing_values,
            "economicValueShare": _map_values(values, "economicValueShare"),
            "min": _map_values(values, "min", include_empty=True),
            "max": _map_values(values, "max", include_empty=True),
            "sd": _map_values(values, "sd", include_empty=True),
            "observations": len(value + missing_values),
            "inputs": list(
                {v.get("@id"): v for v in _map_values(values, "inputs")}.values()
            ),
            "properties": _map_values(values, "properties"),
            "methodTier": get_method_tier(values),
            "methodModel": get_method_model(values),
            "primaryPercent": primary_percent
            + ([0] * (cycle_count - len(primary_percent))),
        }
        group[group_key] = data
        return group

    return grouper


def _combine_cycle_blank_nodes(
    cycles: List[CycleFormatted], completeness: Dict[str, int], cycle_count: int
):
    def combine(group: dict, list_key: str):
        now = current_time_ms()
        # get all possible keys first, then group each key values into a single blank node
        keys = set(flatten([list(cycle.get(list_key, {}).keys()) for cycle in cycles]))
        group[list_key] = reduce(
            _combine_cycle_blank_node(cycles, list_key, completeness, cycle_count),
            keys,
            {},
        )
        logger.debug(
            "function=_combine_cycle_blank_nodes, list_key=%s, time=%s",
            list_key,
            current_time_ms() - now,
        )
        return group

    return combine


def _combine_cycle_lists(cycles: List[CycleFormatted]):
    def combine(group: dict, key: str):
        group[key] = flatten([v.get(key, []) for v in cycles])
        return group

    return combine


def _combine_formatted_cycles(cycles: List[CycleFormatted]) -> CycleFormatted:
    completeness = sum_completeness_count([v["completeness"] for v in cycles])
    cycle_ids = flatten([v["cycle_ids"] for v in cycles])
    cycle_count = len(cycle_ids)
    data: CycleFormatted = (
        pick(cycles[0], ["product", "functionalUnit", "organic", "irrigated"])
        | {
            "cycle_ids": cycle_ids,
            "completeness": completeness,
            "numberOfCycles": sum_data(cycles, "numberOfCycles"),
        }
        | reduce(
            _combine_cycle_blank_nodes(cycles, completeness, cycle_count),
            CYCLE_AGGREGATION_KEYS,
            {},
        )
        | reduce(
            _combine_cycle_lists(cycles),
            [
                "site_ids",
                "source_ids",
                "endDate",
                "site_durations",
                "site_unused_durations",
                "site_areas",
            ],
            {},
        )
    )
    return data


# --- Site


class SiteFormatted(TypedDict, total=False):
    site_ids: List[str]
    source_ids: List[str]
    numberOfSites: int
    measurements: Dict[str, BlankNodeFormatted]
    management: Dict[str, BlankNodeFormatted]
    regions: Dict[str, int]


def _group_site_blank_node():
    def grouper(group: dict, blank_node: dict) -> Dict[str, BlankNodeFormatted]:
        """
        Group a single blank node, using the term `@id` and optionally some other keys.
        This will group with any other existing blank node in the same group, and keep only one value.
        """
        group_key = blank_node_group_key(blank_node)

        blank_node_data: BlankNodeFormatted = map_blank_node(blank_node)

        # if there is an existing blank node in the same group, combine to get a single value
        existing_data: BlankNodeFormatted = group.get(group_key)

        blank_nodes = non_empty_list([existing_data, blank_node_data])
        values = _map_values(blank_nodes)
        value = (
            any(values)
            if all(map(is_boolean, values))
            else get_node_value({"value": values})
        )
        min = _min(_map_values(blank_nodes, "min") + [value], min_observations=1)
        max = _max(_map_values(blank_nodes, "max") + [value], min_observations=1)
        data = (
            (existing_data or {})
            | blank_node_data
            | {
                "value": value,
                "min": min,
                "max": max,
                "properties": _map_values(blank_nodes, "properties"),
            }
        )
        group[group_key] = data
        return group

    return grouper


def _group_site_blank_nodes(
    site: SiteJSONLD, start_year: int = None, end_year: int = None
):
    def grouper(group: dict, list_key: str) -> Dict[str, BlankNodeFormatted]:
        now = current_time_ms()
        blank_nodes = _filter_blank_nodes(site, list_key, start_year, end_year)
        group[list_key] = reduce(_group_site_blank_node(), blank_nodes, {})
        logger.debug(
            "function=_group_site_blank_nodes, list_key=%s, time=%s",
            list_key,
            current_time_ms() - now,
        )
        return group

    return grouper


def _format_site(
    site: SiteJSONLD, start_year: int = None, end_year: int = None
) -> SiteFormatted:
    data: SiteFormatted = (
        pick(site, ["country", "region", "siteType"])
        | {
            "site_ids": [site.get("@id") or site.get("id")],
            "source_ids": [site.get("defaultSource", {}).get("@id")],
            "numberOfSites": site.get("numberOfSites") or 1,
        }
        | reduce(
            _group_site_blank_nodes(site, start_year, end_year),
            SITE_AGGREGATION_KEYS,
            {},
        )
    )
    return data


def _combine_site_blank_node(sites: List[SiteFormatted], list_key: str):
    def grouper(group: dict, group_key: str):
        values: List[BlankNodeFormatted] = non_empty_list(
            [site.get(list_key, {}).get(group_key) for site in sites]
        )
        value = _map_values(values, include_empty=True)
        data: BlankNodeFormatted = {
            **values[0],
            "value": value,
            "min": _map_values(values, "min", include_empty=True),
            "max": _map_values(values, "max", include_empty=True),
            "observations": len(non_empty_list(value)),
            "properties": _map_values(values, "properties"),
        }
        group[group_key] = data
        return group

    return grouper


def _combine_site_blank_nodes(sites: List[SiteFormatted]):
    def combine(group: dict, list_key: str):
        now = current_time_ms()
        # get all possible keys first, then group each key values into a single blank node
        keys = set(flatten([list(site.get(list_key, {}).keys()) for site in sites]))
        group[list_key] = reduce(_combine_site_blank_node(sites, list_key), keys, {})
        logger.debug(
            "function=_combine_site_blank_nodes, list_key=%s, time=%s",
            list_key,
            current_time_ms() - now,
        )
        return group

    return combine


def _combine_site_region(sites: List[SiteFormatted]) -> Dict[str, int]:
    def grouper(group: dict, site: SiteFormatted):
        region_id = site_parent_region_id(site)
        if region_id:
            group[region_id] = group.get(region_id, 0) + 1
        elif isinstance(site.get("region"), dict):
            # merge regions
            return dict(Counter(group) + Counter(site.get("region")))

        return group

    return reduce(grouper, sites, {})


def _combine_formatted_sites(sites: List[SiteFormatted]) -> SiteFormatted:
    data: SiteFormatted = (
        pick(sites[0], ["country", "siteType"])
        | {
            "site_ids": flatten([v["site_ids"] for v in sites]),
            "source_ids": flatten([v["source_ids"] for v in sites]),
            "numberOfSites": sum_data(sites, "numberOfSites"),
        }
        | {"region": _combine_site_region(sites)}
        | reduce(_combine_site_blank_nodes(sites), SITE_AGGREGATION_KEYS, {})
    )
    return data


# --- Aggregate


def _aggregate_blank_node(blank_node: BlankNodeFormatted):
    values = non_empty_list(blank_node.get("value", []))
    value = weighted_average([(v, 1) for v in values]) if values else None
    economicValueShare = weighted_average(
        [(v, 1) for v in non_empty_list(blank_node.get("economicValueShare") or [])]
    )

    distribution = generate_blank_node_distribution(blank_node)

    return pick(
        blank_node
        | {
            "value": value,
            "economicValueShare": format_evs(economicValueShare),
            "min": _min(non_empty_list(blank_node.get("min", []))),
            "max": _max(non_empty_list(blank_node.get("max", []))),
            "sd": _sd(non_empty_list(blank_node.get("value", []))),
            "properties": aggregate_properties(blank_node.get("properties", [])),
            "primaryPercent": list_average(
                non_empty_list(blank_node.get("primaryPercent", [])), default=None
            ),
            "distribution": distribution,
        },
        [
            "term",
            "observations",
            "inputs",
            "methodTier",
            "methodModel",
            "depthUpper",
            "depthLower",
            "startDate",
            "endDate",
            "value",
            "economicValueShare",
            "min",
            "max",
            "sd",
            "primaryPercent",
            "properties",
            "distribution",
        ],
    )


def _aggregate_formatted(
    data: Union[CycleFormatted, SiteFormatted], aggregated_keys: List[str]
):
    def aggregate(key: str):
        now = current_time_ms()
        values = data.get(key, {}).values()
        logger.debug(f"Aggregating {len(values)} {key}...")
        values = list(map(_aggregate_blank_node, values))
        logger.debug(
            "function=_aggregate_formatted, key=%s, time=%s",
            key,
            current_time_ms() - now,
        )
        return values

    return reduce(lambda group, key: group | {key: aggregate(key)}, aggregated_keys, {})


def _group_cycles(cycles: list) -> dict[str, list]:
    """
    Group cycles by organic/irrigated.
    """

    def group_by(data: dict, cycle: dict):
        group_key = "-".join(
            [
                _format_organic(cycle.get("organic", False)),
                _format_irrigated(cycle.get("irrigated", False)),
            ]
        )
        data[group_key] = data.get(group_key, []) + [cycle]
        return data

    return reduce(group_by, cycles, {})


def _combine_cycles_batch(
    cycles: list,
    product: dict,
    start_year: int,
    end_year: int,
    include_covariance: bool = True,
    include_matrix: bool = True,
) -> Dict[str, CycleFormatted]:
    cycles = download_nodes(cycles)
    if not cycles:
        logger.warning("No cycles found, stopping aggregations.")
        return {}

    logger.debug("Formatting Cycles...")
    cycles = non_empty_list(
        [_format_cycle(cycle, product, start_year, end_year) for cycle in cycles]
    )

    logger.debug("Grouping Cycles together...")
    cycles = _group_cycles(cycles) if include_matrix else {"all": cycles}

    # add data to generate covariance
    if include_covariance:
        logger.debug("Adding covariance to Cycles...")
        [
            add_covariance_cycles(cycles=value, suffix=key)
            for key, value in cycles.items()
        ]

    logger.debug("Combining Cycles together...")
    cycles = {key: _combine_formatted_cycles(value) for key, value in cycles.items()}

    if len(cycles.keys()) == 0:
        logger.debug("No cycles to run aggregation.")
        return {}

    return cycles


def _combine_sites_batch(
    site_ids: List[str], start_year: int, end_year: int
) -> Dict[str, SiteFormatted]:
    # the same site can be used multiple times
    sites_by_id = {
        site_id: _format_site(
            download_site({"@type": "Site", "@id": site_id}), start_year, end_year
        )
        for site_id in set(site_ids)
    }
    return _combine_formatted_sites([sites_by_id[site_id] for site_id in site_ids])


def _aggregate_sites(
    site_ids: List[str], start_year: int, end_year: int, include_description: bool
) -> SiteJSONLD:
    batches = range(0, len(site_ids), _BATCH_SIZE)

    all_sites = []

    for batch_index in batches:
        logger.info(
            f"Processing sites in batch {int(batch_index / _BATCH_SIZE) + 1} of {len(batches)}..."
        )
        log_memory_usage()
        batched_site_ids = site_ids[batch_index : batch_index + _BATCH_SIZE]
        site = _combine_sites_batch(batched_site_ids, start_year, end_year)
        all_sites.append(site)

    logger.info("Finished processing batch, combining all sites...")
    log_memory_usage()

    site = _combine_formatted_sites(all_sites)
    site = (
        create_site(pick(site, ["country", "siteType"]))
        | format_site_results(_aggregate_formatted(site, SITE_AGGREGATION_KEYS))
        | (format_site_description(site["region"]) if include_description else {})
        | {
            "aggregatedSites": format_aggregated_list("Site", site["site_ids"]),
            "aggregatedSources": format_aggregated_list("Source", site["source_ids"]),
        }
        | pick(site, ["numberOfSites"])
    )
    return site


def aggregate_cycles(
    cycles: list,
    product: dict,
    start_year: int,
    end_year: int,
    include_covariance: bool = True,
):
    batches = range(0, len(cycles), _BATCH_SIZE)
    logger.info(f"Processing {len(cycles)} cycles in {len(batches)} batches.")

    all_cycles: Dict[str, List[CycleFormatted]] = {}

    include_matrix = aggregate_with_matrix(product)

    # step 1: combine and aggregate all cycles by group
    for batch_index in batches:
        logger.info(
            f"Processing cycles in batch {int(batch_index / _BATCH_SIZE) + 1} of {len(batches)}..."
        )
        log_memory_usage()
        batched_cycles = cycles[batch_index : batch_index + _BATCH_SIZE]
        batch_cycles = _combine_cycles_batch(
            batched_cycles,
            product,
            start_year,
            end_year,
            include_covariance,
            include_matrix,
        )
        for key, value in batch_cycles.items():
            all_cycles[key] = all_cycles.get(key, []) + [value]

    logger.info("Finished processing batch, combining all cycles...")
    log_memory_usage()
    # generate covariance on sub cycles
    covariances = (
        {key: generate_covariance_cycles(key) for key in all_cycles.keys()}
        if include_covariance
        else {}
    )
    all_cycles = {
        key: _combine_formatted_cycles(cycles=values)
        for key, values in all_cycles.items()
    }
    # add covariances to cycles once they have been combined
    cycles = [cycle | covariances.get(key, {}) for key, cycle in all_cycles.items()]
    # discard memory
    all_cycles = {}

    # step 2: combine and aggregate all sites by group
    logger.info("Finished combining cycles, processing sites...")
    log_memory_usage()
    sites = [
        _aggregate_sites(
            value.get("site_ids", []),
            start_year,
            end_year,
            include_description=include_matrix,
        )
        for value in cycles
    ]

    logger.info("Finished aggregating sites, aggregating cycles...")
    log_memory_usage()
    cycles: List[CycleFormatted] = [
        (
            cycle
            | {"site": sites[index]}
            | _aggregate_formatted(cycle, CYCLE_AGGREGATION_KEYS)
        )
        for index, cycle in enumerate(cycles)
    ]
    cycles = [
        # map fields for `format_term_results`
        cycle
        | {
            "node-ids": cycle["cycle_ids"],
            "source-ids": cycle["source_ids"],
            "completeness-count": cycle["completeness"],
        }
        for cycle in cycles
    ]
    log_memory_usage()
    return cycles
