import os
import shutil
from functools import reduce
from typing import Dict
from hestia_earth.utils.blank_node import get_node_value
import numpy as np
import pandas as pd

from . import CYCLE_AGGREGATION_KEYS
from ..log import logger

_COVARIANCE_ROOT_DIR = os.getenv("TMP_DIR", "/tmp")
# covariance storage type
_COVARIANCE_STORAGE = os.getenv("AGGREGATION_COVARIANCE_STORAGE", "temporary")
_ID_COLUMN = "id"


def _split_covariance_storage():
    split_token = "#"
    return (
        _COVARIANCE_STORAGE
        + (split_token if split_token not in _COVARIANCE_STORAGE else "")
    ).split(split_token)


def _store_folder(folder: str):
    files = _list_covariance_files()
    logger.debug('Storing %s covariance files to "%s"', str(len(files)), folder)
    for file in files:
        src_file = os.path.join(_covariance_dir(), file)
        dest_key = os.path.join(folder, file)
        shutil.copyfile(src_file, dest_key)


def _store_s3(folder: str):
    files = _list_covariance_files()
    bucket = os.getenv("AWS_BUCKET_UPLOADS")
    logger.debug(
        'Storing %s covariance files to "s3://%s/%s"', str(len(files)), bucket, folder
    )
    from hestia_earth.utils.storage._s3_client import _upload_to_bucket

    for file in files:
        src_file = os.path.join(_covariance_dir(), file)
        dest_key = os.path.join(folder, file)
        _upload_to_bucket(
            bucket=bucket,
            key=dest_key,
            body=open(src_file, "r").read(),
            content_type="text/csv",
        )


_STORE_COVARIANCE_FILES = {"s3": _store_s3, "local": _store_folder}


def _covariance_dir():
    return os.path.join(_COVARIANCE_ROOT_DIR, "covariance")


def init_covariance_files():
    remove_covariance_files()
    os.makedirs(_covariance_dir(), exist_ok=True)


def remove_covariance_files():
    # store covariance files depending on chosen storage system
    storage, folder = _split_covariance_storage()
    _STORE_COVARIANCE_FILES.get(storage, lambda *args: True)(folder)
    # then remove all files
    return (
        shutil.rmtree(_covariance_dir()) if os.path.exists(_covariance_dir()) else None
    )


def _covariance_filepath(suffix: str):
    os.makedirs(_covariance_dir(), exist_ok=True)
    return os.path.join(_covariance_dir(), f"covariance-{suffix}.csv")


def _list_covariance_files():
    return [f for f in os.listdir(_covariance_dir()) if f != "covariance.csv"]


def _read_filepath(filepath: str):
    return pd.read_csv(filepath, index_col=_ID_COLUMN, na_values="")


def _read_file(suffix: str):
    return _read_filepath(_covariance_filepath(suffix))


def _group_covariance_data(group: dict, value: Dict[str, dict]) -> dict:
    group_key = value["term"]["@id"]
    group[group_key] = group.get(group_key) or {"term": value["term"], "value": []}
    value = value.get("value")
    value is not None and group[group_key]["value"].extend(
        value if isinstance(value, list) else [value]
    )
    return group


def _map_covariance_data(list_key: str, key: str, value: list):
    group_key = f"{list_key}.{key}"
    value = get_node_value(value, default=None) if len(value) > 0 else None
    is_0_practice = list_key == "practices" and not value
    return {} if is_0_practice else {group_key: value}


def _cycle_list_covariance_data(cycle: dict, list_key: str):
    blank_nodes = list(cycle.get(list_key, {}).values())
    data = reduce(_group_covariance_data, blank_nodes, {})
    return reduce(
        lambda prev, curr: prev | _map_covariance_data(list_key, curr, data[curr]),
        data.keys(),
        {},
    )


def _cycle_covariance_data(cycle: dict):
    data = reduce(
        lambda prev, curr: prev | _cycle_list_covariance_data(cycle, curr),
        CYCLE_AGGREGATION_KEYS,
        {},
    )
    return {_ID_COLUMN: cycle.get("cycle_ids")[0]} | data


def add_covariance_cycles(cycles: list, suffix: str):
    records = list(map(_cycle_covariance_data, cycles))
    df = pd.DataFrame(records).set_index(_ID_COLUMN).fillna("")
    filepath = _covariance_filepath(suffix)
    # combine new data with existing data
    df = pd.concat([df, _read_file(suffix)]) if os.path.exists(filepath) else df
    df.to_csv(filepath, na_rep="")


def _format_covariance_value(value):
    return (
        None
        if any([pd.isna(value), value == -np.inf, value == np.inf])
        else float(value)
    )


def _format_covariance_matrix(values):
    # replace NaN values with None, so it is set to `null` in JSON
    return [list(map(_format_covariance_value, values)) for values in np.tril(values)]


def generate_covariance_cycles(suffix: str):
    df = _read_file(suffix).sort_index(axis=1)
    matrix = df.cov().values
    return {
        "covarianceMatrixIds": list(df.columns),
        "covarianceMatrix": _format_covariance_matrix(matrix),
    }


def _read_covariance_subcountry_file(weights: dict):
    def read_file(filename: str):
        filepath = os.path.join(_covariance_dir(), filename)
        df = _read_filepath(filepath)
        nb_rows = df.shape[0]
        matrix = filename.replace("covariance-", "").replace(".csv", "")
        weight_values = next(
            (v for k, v in weights.items() if matrix in k), next(iter(weights.values()))
        )
        weight = weight_values["weight"] / nb_rows
        df["weight"] = weight
        return df

    return read_file


def generate_covariance_country(weights: dict):
    files = _list_covariance_files()
    frames = list(map(_read_covariance_subcountry_file(weights), files))
    df = pd.concat(frames, ignore_index=True) if len(frames) > 1 else frames[0]

    weights = df["weight"]
    data = df.drop(columns=["weight"])
    # Compute weighted covariance
    matrix = np.cov(data.T, aweights=weights)

    return {
        "covarianceMatrixIds": list(data.columns),
        "covarianceMatrix": _format_covariance_matrix(matrix),
    }
