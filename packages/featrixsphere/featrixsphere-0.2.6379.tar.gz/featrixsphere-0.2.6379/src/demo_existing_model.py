#!/usr/bin/env python3
"""
Demo: Using Existing Trained Model for CSV Batch Predictions

This script demonstrates the CSV batch prediction functionality using
an existing trained embedding space model.
"""

import json
import pandas as pd
import pickle
import sqlite3
import sys
from pathlib import Path
from jsontables import to_json_table, render_json_table

# Add the lib directory to path to import neural modules
sys.path.insert(0, str(Path(__file__).parent / "lib"))

def load_existing_model():
    """Load the existing trained embedding space model."""
    
    print("ğŸ” Loading existing trained model...")
    
    # Paths from the test-session
    embedding_space_path = "featrix_output/train_es_20250325-121841_9437ac/embedded_space.pickle"
    sqlite_db_path = "featrix_output/create_structured_data_20250325-121838_f88dab/data.db"
    
    if not Path(embedding_space_path).exists():
        print(f"âŒ Embedding space not found: {embedding_space_path}")
        return None, None
        
    if not Path(sqlite_db_path).exists():
        print(f"âŒ SQLite database not found: {sqlite_db_path}")
        return None, None
    
    print(f"ğŸ“ Loading embedding space: {embedding_space_path}")
    print(f"ğŸ“ File size: {Path(embedding_space_path).stat().st_size / (1024*1024):.1f} MB")
    
    try:
        with open(embedding_space_path, 'rb') as f:
            embedding_space = pickle.load(f)
        
        print(f"âœ… Embedding space loaded successfully!")
        print(f"ğŸ“Š Model dimension: {embedding_space.get_dimensions()}")
        print(f"ğŸ“‹ Available columns: {len(embedding_space.get_column_names())} columns")
        
        # Load the database to see what data was used
        print(f"\nğŸ“ Loading data from: {sqlite_db_path}")
        conn = sqlite3.connect(sqlite_db_path)
        
        # Get table info
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        print(f"ğŸ“‹ Database tables: {[t[0] for t in tables]}")
        
        # Get sample data
        if ('data',) in tables:
            df = pd.read_sql("SELECT * FROM data LIMIT 10", conn)
            print(f"ğŸ“Š Sample data shape: {df.shape}")
            print(f"ğŸ“‹ Columns: {list(df.columns)}")
            print(f"ğŸ“„ First few rows:")
            print(df.head(3))
        
        conn.close()
        
        return embedding_space, sqlite_db_path
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None, None


def demonstrate_encoding(embedding_space):
    """Demonstrate encoding records with the trained model."""
    
    print("\nğŸ”® Demonstrating Record Encoding...")
    
    # Create test records similar to the original training data
    test_records = [
        {
            "name": "Test Company 1",
            "category": "software", 
            "description": "Software development company specializing in machine learning applications"
        },
        {
            "name": "Test Company 2",
            "category": "finance",
            "description": "Financial services firm providing investment management and consulting"
        },
        {
            "name": "Test Company 3", 
            "category": "retail",
            "description": "Online retail platform selling consumer electronics and gadgets"
        }
    ]
    
    print(f"ğŸ“‹ Test records: {len(test_records)}")
    
    try:
        # Encode each record
        embeddings = []
        for i, record in enumerate(test_records):
            print(f"\nğŸ” Encoding record {i+1}: {record['name']}")
            
            # Use the embedding space to encode the record
            embedding = embedding_space.encode_record(record, squeeze=True, short=False)
            embeddings.append(embedding.numpy())
            
            print(f"   âœ… Encoded to {len(embedding)} dimensions")
            print(f"   ğŸ“Š Embedding norm: {embedding.norm().item():.3f}")
        
        print(f"\nâœ… Successfully encoded {len(embeddings)} records!")
        return embeddings, test_records
        
    except Exception as e:
        print(f"âŒ Error during encoding: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def demonstrate_csv_batch_processing(embedding_space):
    """Demonstrate CSV batch processing workflow."""
    
    print("\nğŸ“Š Demonstrating CSV Batch Processing Workflow...")
    
    # Create a larger dataset for batch processing
    batch_data = [
        {"id": "batch_001", "name": "Tech Innovators Inc", "category": "technology", 
         "description": "AI and machine learning solutions for enterprise clients"},
        {"id": "batch_002", "name": "Green Energy Solutions", "category": "energy",
         "description": "Renewable energy systems and solar panel installations"},
        {"id": "batch_003", "name": "HealthCare Plus", "category": "healthcare", 
         "description": "Digital health platform providing telemedicine services"},
        {"id": "batch_004", "name": "Financial Advisors LLC", "category": "finance",
         "description": "Investment advisory and wealth management services"},
        {"id": "batch_005", "name": "E-Commerce Central", "category": "retail",
         "description": "Online marketplace for small business and artisan products"},
        {"id": "batch_006", "name": "EduTech Systems", "category": "education",
         "description": "Educational software and learning management systems"},
        {"id": "batch_007", "name": "Smart Manufacturing", "category": "manufacturing",
         "description": "IoT-enabled manufacturing equipment and automation solutions"},
        {"id": "batch_008", "name": "Cloud Services Pro", "category": "technology",
         "description": "Cloud infrastructure and managed IT services for businesses"}
    ]
    
    # Step 1: Save as CSV input file
    input_file = "demo_batch_input.csv"
    input_df = pd.DataFrame(batch_data)
    input_df.to_csv(input_file, index=False)
    
    print(f"ğŸ“ Created input CSV: {input_file}")
    print(f"ğŸ“ Records to process: {len(batch_data)}")
    
    # Step 2: Process batch encodings
    print(f"\nğŸ”® Processing batch encodings...")
    
    try:
        batch_results = []
        for i, record in enumerate(batch_data):
            print(f"   Processing {i+1}/{len(batch_data)}: {record['name']}")
            
            # Encode the record
            embedding = embedding_space.encode_record(record, squeeze=True, short=False)
            
            # Create result record
            result = record.copy()
            result['embedding_norm'] = float(embedding.norm().item())
            result['embedding_dim'] = len(embedding)
            
            # Simulate a prediction (since we don't have a trained predictor)
            # In a real scenario, this would use the single predictor
            tech_keywords = ['tech', 'ai', 'software', 'cloud', 'digital', 'iot']
            is_tech = any(keyword in record['description'].lower() for keyword in tech_keywords)
            result['predicted_category'] = 'technology' if is_tech else 'other'
            result['confidence'] = 0.85 if is_tech else 0.75
            
            batch_results.append(result)
        
        print(f"âœ… Batch processing completed!")
        
        # Step 3: Save results in multiple formats
        print(f"\nğŸ’¾ Saving results in multiple formats...")
        
        # Format 1: Enhanced CSV with predictions
        results_df = pd.DataFrame(batch_results)
        results_csv_file = "demo_batch_results.csv"
        results_df.to_csv(results_csv_file, index=False)
        print(f"ğŸ“Š Saved results CSV: {results_csv_file}")
        
        # Format 2: JSON Tables format
        json_tables_result = {
            "input_table": to_json_table(batch_data),
            "results_table": to_json_table(batch_results),
            "summary": {
                "total_records": len(batch_data),
                "successful_encodings": len(batch_results),
                "average_confidence": sum(r['confidence'] for r in batch_results) / len(batch_results)
            }
        }
        
        json_tables_file = "demo_batch_json_tables.json"
        with open(json_tables_file, 'w') as f:
            json.dump(json_tables_result, f, indent=2)
        print(f"ğŸ“‹ Saved JSON Tables: {json_tables_file}")
        
        # Format 3: Summary report
        summary_report = {
            "model_info": {
                "embedding_dimension": int(batch_results[0]['embedding_dim']),
                "model_type": "embedding_space",
                "session_id": "test-session"
            },
            "processing_summary": {
                "total_records": len(batch_data),
                "successful_encodings": len(batch_results),
                "failed_encodings": 0,
                "average_embedding_norm": sum(r['embedding_norm'] for r in batch_results) / len(batch_results),
                "average_confidence": sum(r['confidence'] for r in batch_results) / len(batch_results)
            },
            "prediction_distribution": {
                "technology": len([r for r in batch_results if r['predicted_category'] == 'technology']),
                "other": len([r for r in batch_results if r['predicted_category'] == 'other'])
            }
        }
        
        summary_file = "demo_batch_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary_report, f, indent=2)
        print(f"ğŸ“‹ Saved summary report: {summary_file}")
        
        # Step 4: Display results preview
        print(f"\nğŸ“Š Results Preview:")
        print(f"{'ID':<12} {'Name':<20} {'Predicted':<12} {'Confidence':<10} {'Embed Norm':<10}")
        print("-" * 70)
        
        for result in batch_results[:5]:  # Show first 5
            name = result['name'][:19] + '...' if len(result['name']) > 19 else result['name']
            print(f"{result['id']:<12} {name:<20} {result['predicted_category']:<12} {result['confidence']:<10.3f} {result['embedding_norm']:<10.3f}")
        
        if len(batch_results) > 5:
            print(f"... and {len(batch_results) - 5} more records")
        
        # Step 5: JSON Tables rendering
        print(f"\nğŸ“‹ JSON Tables Format Preview:")
        input_table = json_tables_result["input_table"]
        rendered = render_json_table(input_table, max_width=120)
        lines = rendered.split('\n')
        for line in lines[:6]:  # Show first 6 lines
            print(f"   {line}")
        if len(lines) > 6:
            print(f"   ... ({len(lines)-6} more lines)")
        
        # Show file summary
        print(f"\nğŸ“ Generated Files:")
        created_files = [input_file, results_csv_file, json_tables_file, summary_file]
        total_size = 0
        
        for file_name in created_files:
            if Path(file_name).exists():
                file_size = Path(file_name).stat().st_size
                total_size += file_size
                print(f"   ğŸ“„ {file_name} ({file_size:,} bytes)")
        
        print(f"\nğŸ“Š Total output: {len(created_files)} files, {total_size:,} bytes")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error during batch processing: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run the existing model demonstration."""
    
    print("ğŸ§ª " + "="*60)
    print("ğŸ§ª EXISTING TRAINED MODEL DEMONSTRATION")
    print("ğŸ§ª " + "="*60)
    print()
    
    try:
        # Load the existing model
        embedding_space, db_path = load_existing_model()
        
        if embedding_space is None:
            print("âŒ Could not load existing model. Exiting.")
            return 1
        
        # Demonstrate encoding capabilities
        embeddings, test_records = demonstrate_encoding(embedding_space)
        
        if embeddings is None:
            print("âŒ Encoding demonstration failed. Continuing with batch processing demo...")
        
        # Demonstrate CSV batch processing workflow  
        batch_success = demonstrate_csv_batch_processing(embedding_space)
        
        if not batch_success:
            print("âŒ Batch processing demonstration failed.")
            return 1
        
        # Final summary
        print("\n" + "="*60)
        print("ğŸ‰ EXISTING MODEL DEMONSTRATION COMPLETED!")
        print("="*60)
        print("âœ… Existing trained embedding space loaded")
        print("âœ… Record encoding demonstrated")
        print("âœ… CSV batch processing workflow shown")
        print("âœ… Multiple output formats generated")
        print("âœ… JSON Tables integration working")
        print()
        print("ğŸ†• This demonstrates our enhanced capabilities:")
        print("   â€¢ Using existing trained models")
        print("   â€¢ CSV batch processing with embeddings")
        print("   â€¢ Multiple output formats (CSV, JSON, JSON Tables)")
        print("   â€¢ Comprehensive result analysis and saving")
        print("   â€¢ Integration with trained embedding spaces")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main()) 