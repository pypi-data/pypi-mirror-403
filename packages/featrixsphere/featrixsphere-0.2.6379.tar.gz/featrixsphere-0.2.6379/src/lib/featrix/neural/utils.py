import math
import logging
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import pandas as pd

from featrix.neural.gpu_utils import is_gpu_available, get_gpu_device_properties

logger = logging.getLogger(__name__)


INTERNAL_COLUMN_PREFIX = '__featrix'


def is_internal_column(col: str) -> bool:
    """Check if a column name is an internal Featrix column."""
    return col.startswith(INTERNAL_COLUMN_PREFIX)


def filter_internal_columns(columns: List[str]) -> List[str]:
    """
    Filter out internal Featrix columns from a column list before showing to users.

    Internal columns that should be filtered:
    - __featrix_train_predictor: Training filter column
    - __featrix_meta_*: Metadata columns that should never be expanded or shown

    Args:
        columns: List of column names

    Returns:
        Filtered list with internal columns removed
    """
    return [col for col in columns if not is_internal_column(col)]


def assert_no_internal_columns(
    columns: List[str],
    context: str = "",
    raise_error: bool = True
) -> List[str]:
    """
    Assert that a column list contains no internal __featrix_ columns.

    Use this as a guard at critical points in the pipeline to catch bugs early:
    - Before reconstruction loss calculation
    - Before creating training masks
    - Before prediction/inference
    - When building column_batches

    Args:
        columns: List of column names to check
        context: Description of where this check is happening (for error messages)
        raise_error: If True, raise ValueError on violation. If False, log warning and return filtered list.

    Returns:
        The original columns list (if valid) or filtered list (if raise_error=False)

    Raises:
        ValueError: If internal columns found and raise_error=True

    Example:
        >>> cols = assert_no_internal_columns(col_order, "reconstruction loss calculation")
        >>> # Or with auto-fix:
        >>> cols = assert_no_internal_columns(col_order, "mask generation", raise_error=False)
    """
    internal_cols = [col for col in columns if is_internal_column(col)]

    if internal_cols:
        msg = f"Internal __featrix_ columns found in {context or 'column list'}: {internal_cols}"
        if raise_error:
            logger.error(f"üö® ASSERTION FAILED: {msg}")
            raise ValueError(msg)
        else:
            logger.warning(f"‚ö†Ô∏è  {msg} - auto-filtering them out")
            return [col for col in columns if not is_internal_column(col)]

    return columns


def ideal_batch_size(n_rows: int, mode: str = "embedding_space") -> int:
    """
    Compute a reasonable batch size for small-to-medium datasets.

    Args:
        n_rows: Number of training samples
        mode: "embedding_space" or "predictor"
            - "embedding_space": Minimum 128 (InfoNCE contrastive loss needs large batches)
            - "predictor": No minimum constraint (CrossEntropyLoss/MSE work with any batch size)

    Logic:
    - Aim for roughly 20‚Äì200 steps per epoch (enough for learning dynamics).
    - Scale based on GPU memory (bigger GPUs can handle larger batches)
    - Use powers of two for GPU efficiency.
    """
    import math
    import torch

    if n_rows < 128:
        return max(4, 2 ** int(math.log2(max(1, n_rows // 4))))
    elif n_rows < 1000:
        # roughly 32‚Äì64 steps/epoch
        target_steps = 40
        bs = max(4, min(n_rows // target_steps, 64))
    elif n_rows < 10000:
        # moderate size ‚Üí batch ~128‚Äì512 (increased from 256)
        # Scale based on GPU memory
        target_steps = 50  # Reduced from 100 for larger batches
        bs = min(512, max(128, n_rows // target_steps))
    else:
        # large data ‚Üí scale aggressively with GPU memory
        target_steps = 100  # Reduced from 200 for larger batches
        bs = min(2048, max(128, n_rows // target_steps))
    
    # round to nearest power of two for GPU friendliness
    bs = 2 ** int(round(math.log2(bs)))
    
    # CRITICAL: InfoNCE contrastive loss (used in embedding space training) needs minimum batch size of 128
    # With smaller batches, the model has too few negatives and can't learn
    # For predictor training (CrossEntropyLoss/MSE), no minimum is needed
    # Note: We use drop_last=True in DataLoader to avoid partial batches
    if mode == "embedding_space":
        bs = max(128, bs)
    # For predictor mode, no minimum constraint - let natural batch size through
    
    # MEMORY OPTIMIZATION: Cap batch size for large datasets to prevent OOM
    # For datasets with many columns (100+), reduce batch size to prevent VRAM exhaustion
    # Estimate columns from n_rows if not available (conservative estimate)
    # Rule: batch_size √ó num_columns should not exceed ~200k elements per batch
    # This prevents OOM on 50k√ó100 datasets
    max_batch_for_large_datasets = 1024  # Cap at 1024 for very large datasets
    if n_rows >= 20000:  # Large dataset - be more conservative
        bs = min(bs, max_batch_for_large_datasets)
        logger.info(f"üìä Large dataset ({n_rows} rows) - capping batch size at {bs} to prevent OOM")
    
    # Scale up based on GPU VRAM if available AND not forcing CPU mode
    import os
    force_cpu = os.environ.get('FEATRIX_FORCE_CPU_SINGLE_PREDICTOR') == '1'
    
    if is_gpu_available() and not force_cpu:
        try:
            props = get_gpu_device_properties(0)
            gpu_mem_gb = (props.total_memory / 1024**3) if props else 0.0
            # MEMORY OPTIMIZATION: More conservative scaling for large datasets
            if n_rows >= 20000:
                # For large datasets, be more conservative with batch size scaling
                if gpu_mem_gb >= 70:  # H100 (80GB)
                    bs = min(1024, bs * 2)  # Reduced from 4x to 2x, cap at 1024
                elif gpu_mem_gb >= 40:  # A100 (40GB)
                    bs = min(512, int(bs * 1.5))  # Reduced from 2x to 1.5x, cap at 512
                elif gpu_mem_gb >= 20:  # RTX 4090 (24GB)
                    bs = min(512, int(bs * 1.2))  # Reduced from 1.5x to 1.2x, cap at 512
            else:
                # For smaller datasets, use original aggressive scaling
                if gpu_mem_gb >= 70:  # H100 (80GB) - can handle 4x larger batches
                    bs = min(2048, bs * 4)
                elif gpu_mem_gb >= 40:  # A100 (40GB) - can handle 2x larger batches
                    bs = min(1024, bs * 2)
                elif gpu_mem_gb >= 20:  # RTX 4090 (24GB) - can handle 1.5x larger batches
                    bs = min(512, int(bs * 1.5))
            # Round to power of 2 after scaling
            bs = 2 ** int(round(math.log2(bs)))

            # CRITICAL: Ensure minimum steps per epoch for learning dynamics
            # Even with large GPUs, we need enough gradient updates per epoch
            # Target: at least 10 steps/epoch (for small datasets) to 20 steps/epoch (for medium)
            min_steps_per_epoch = 10 if n_rows < 5000 else 20
            max_batch_for_steps = n_rows // min_steps_per_epoch
            if max_batch_for_steps > 0 and bs > max_batch_for_steps:
                # Round down to nearest power of 2
                old_bs = bs
                bs = 2 ** int(math.log2(max(1, max_batch_for_steps)))
                bs = max(bs, 32)  # Never go below 32
                logger.info(f"‚ö†Ô∏è Capped batch size {old_bs} ‚Üí {bs} to ensure {n_rows // bs} steps/epoch (min {min_steps_per_epoch})")

            logger.info(f"üöÄ GPU-scaled batch size: {bs} (GPU VRAM: {gpu_mem_gb:.1f} GB, dataset: {n_rows} rows, steps/epoch: {n_rows // bs})")
        except:
            pass
    elif force_cpu:
        logger.info(f"üíª CPU mode: Using batch size {bs} (GPU scaling disabled)")
    
    return int(bs)




def ideal_epochs(n_rows: int, batch_size: int, mode: str) -> int:
    """
    Compute a recommended number of training epochs given dataset size, batch size, and mode.

    Modes:
      - 'predictor'        : training only the classifier/regressor head
      - 'embedding_space'  : fine-tuning the encoder / embedding space

    Heuristic:
      - 'predictor'       ‚Üí ~800 optimizer updates
      - 'embedding_space' ‚Üí ~1800 optimizer updates

    Note:
      Gradient accumulation is fixed at 1 ‚Äî meaning we perform one optimizer
      update per batch (no simulated larger "effective batch" sizes).
      This keeps epoch counts directly tied to dataset size and step count.
    """
    mode = mode.lower()
    if mode not in {"predictor", "embedding_space"}:
        raise ValueError("mode must be 'predictor' or 'embedding_space'")

    target_updates = 7200 if mode == "predictor" else 36_000

    steps_per_epoch = max(1, math.ceil(n_rows / max(1, batch_size)))
    updates_per_epoch = steps_per_epoch  # grad_accum = 1

    epochs = math.ceil(target_updates / updates_per_epoch)
    return int(min(300, max(5, epochs)))  # Cap at 300 epochs - early stopping handles convergence


def ideal_epochs_predictor(n_rows: int, batch_size: int, imbalance_ratio: float = 1.0) -> int:
    """
    Wrapper for training only the predictor head.
    
    Args:
        n_rows: Number of training samples
        batch_size: Batch size for training
        imbalance_ratio: Ratio of majority class to minority class (e.g., 35 for 97:3 split)
                        Higher values = more imbalanced = more epochs needed
    
    Returns:
        Number of epochs, adjusted for class imbalance (no cap - early stopping handles termination)
    """
    base_epochs = ideal_epochs(n_rows, batch_size, mode="predictor")
    
    # Adjust epochs based on imbalance ratio
    # For severe imbalance (ratio > 10), multiply epochs by sqrt of ratio
    # This gives more training time without making it excessive
    if imbalance_ratio > 10:
        adjustment_factor = min(math.sqrt(imbalance_ratio / 10), 3.0)  # Cap at 3x
        adjusted_epochs = int(base_epochs * adjustment_factor)
        final_epochs = int(max(base_epochs, adjusted_epochs))
    else:
        final_epochs = base_epochs
    
    # No cap - allow training to run as long as needed
    # Early stopping based on AUC plateau will handle stopping when appropriate
    return final_epochs


def ideal_epochs_embedding_space(n_rows: int, batch_size: int) -> int:
    """Wrapper for fine-tuning the embedding space."""
    return ideal_epochs(n_rows, batch_size, mode="embedding_space")


def analyze_dataset_complexity(
    train_df: pd.DataFrame,
    target_column: str,
    target_column_type: str
) -> Dict[str, Any]:
    """
    Analyze dataset complexity to inform neural network architecture decisions.
    
    This function examines:
    1. Feature-target relationships (correlation, mutual information, chi-square)
    2. Class imbalance (for classification)
    3. Nonlinearity indicators
    4. Data quality metrics
    
    Args:
        train_df: Training dataframe
        target_column: Name of target column
        target_column_type: "set" (classification) or "scalar" (regression)
    
    Returns:
        Dictionary with complexity metrics and architecture recommendations
    """
    import time
    start_time = time.time()
    
    logger.info("=" * 80)
    logger.info("üìä DATASET COMPLEXITY ANALYSIS")
    logger.info("=" * 80)
    
    n_rows = len(train_df)
    n_cols = len(train_df.columns) - 1  # Exclude target
    
    result = {
        'n_rows': n_rows,
        'n_cols': n_cols,
        'target_column': target_column,
        'target_column_type': target_column_type,
        'max_correlation': 0.0,
        'max_mutual_info': 0.0,
        'class_imbalance_ratio': 1.0,
        'nonlinearity_score': 0.0,
        'complexity_level': 'medium',  # low, medium, high
        'recommendations': []
    }
    
    # Separate features from target
    feature_cols = [col for col in train_df.columns if col != target_column]
    X = train_df[feature_cols]
    y = train_df[target_column]
    
    # Handle missing values by dropping them for analysis
    valid_mask = ~y.isna()
    X_clean = X[valid_mask]
    y_clean = y[valid_mask]
    
    if len(y_clean) < 10:
        logger.warning("‚ö†Ô∏è  Too few valid samples for complexity analysis")
        result['complexity_level'] = 'low'
        return result
    
    # === ANALYSIS 1: Class Imbalance (for classification) ===
    if target_column_type == "set":
        value_counts = y_clean.value_counts()
        if len(value_counts) >= 2:
            majority_count = value_counts.iloc[0]
            minority_count = value_counts.iloc[-1]
            imbalance_ratio = majority_count / max(minority_count, 1)
            result['class_imbalance_ratio'] = imbalance_ratio
            result['majority_class'] = value_counts.index[0]
            result['minority_class'] = value_counts.index[-1]
            result['n_classes'] = len(value_counts)
            
            logger.info(f"\nüìà Class Distribution:")
            logger.info(f"   ‚Ä¢ Number of classes: {len(value_counts)}")
            for class_label, count in value_counts.items():
                pct = (count / len(y_clean)) * 100
                logger.info(f"   ‚Ä¢ '{class_label}': {count:,} ({pct:.1f}%)")
            
            if imbalance_ratio > 10:
                logger.info(f"\n‚ö†Ô∏è  SEVERE CLASS IMBALANCE: {imbalance_ratio:.1f}:1 ratio")
                result['recommendations'].append(
                    f"Severe imbalance ({imbalance_ratio:.1f}:1) detected - using class weights is critical"
                )
            elif imbalance_ratio > 3:
                logger.info(f"\n‚öñÔ∏è  Moderate class imbalance: {imbalance_ratio:.1f}:1 ratio")
                result['recommendations'].append(
                    f"Moderate imbalance ({imbalance_ratio:.1f}:1) - class weights recommended"
                )
    
    # === ANALYSIS 2: Feature-Target Correlations ===
    logger.info(f"\nüîç Feature-Target Relationship Analysis:")
    
    try:
        # Try mutual information (works for both classification and regression)
        from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
        
        # Convert to numeric for sklearn
        # Build dict first, then create DataFrame once to avoid fragmentation
        numeric_cols = {}
        for col in X_clean.columns:
            if X_clean[col].dtype in ['object', 'category', 'string']:
                # Convert categorical to numeric codes
                numeric_cols[col] = pd.Categorical(X_clean[col]).codes
            else:
                numeric_cols[col] = pd.to_numeric(X_clean[col], errors='coerce')
        
        X_numeric = pd.DataFrame(numeric_cols)
        
        # Fill NaN with column medians
        X_numeric = X_numeric.fillna(X_numeric.median())
        
        # Compute mutual information
        if target_column_type == "set":
            # Classification
            y_numeric = pd.Categorical(y_clean).codes
            mi_scores = mutual_info_classif(X_numeric, y_numeric, random_state=42)
        else:
            # Regression
            y_numeric = pd.to_numeric(y_clean, errors='coerce')
            mi_scores = mutual_info_regression(X_numeric, y_numeric, random_state=42)
        
        max_mi = np.max(mi_scores) if len(mi_scores) > 0 else 0.0
        mean_mi = np.mean(mi_scores) if len(mi_scores) > 0 else 0.0
        result['max_mutual_info'] = float(max_mi)
        result['mean_mutual_info'] = float(mean_mi)
        
        # Find top features
        top_n = min(5, len(mi_scores))
        top_indices = np.argsort(mi_scores)[-top_n:][::-1]
        
        logger.info(f"   ‚Ä¢ Mutual Information (higher = stronger relationship):")
        logger.info(f"     - Maximum MI: {max_mi:.4f}")
        logger.info(f"     - Mean MI: {mean_mi:.4f}")
        logger.info(f"     - Top {top_n} features:")
        for idx in top_indices:
            logger.info(f"       {X_numeric.columns[idx]}: {mi_scores[idx]:.4f}")
        
        # Interpret MI scores
        if max_mi < 0.05:
            logger.info(f"\nüî¥ VERY WEAK relationships detected (max MI < 0.05)")
            logger.info(f"   ‚Üí Likely requires DEEP network to capture complex patterns")
            result['complexity_level'] = 'high'
            result['recommendations'].append(
                "Very weak feature-target relationships suggest complex nonlinear patterns - deeper network recommended"
            )
        elif max_mi < 0.15:
            logger.info(f"\nüü° WEAK relationships detected (max MI < 0.15)")
            logger.info(f"   ‚Üí May require deeper network for nonlinear patterns")
            result['complexity_level'] = 'medium-high'
            result['recommendations'].append(
                "Weak feature correlations suggest nonlinear relationships - consider 3+ layers"
            )
        elif max_mi > 0.4:
            logger.info(f"\nüü¢ STRONG relationships detected (max MI > 0.4)")
            logger.info(f"   ‚Üí Shallower network may suffice")
            result['complexity_level'] = 'low-medium'
            result['recommendations'].append(
                "Strong feature-target relationships - shallower network (2-3 layers) may be sufficient"
            )
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Could not compute mutual information: {e}")
        result['recommendations'].append("Mutual information analysis failed - using conservative architecture")
    
    # === ANALYSIS 3: Chi-Square Test (for categorical target) ===
    if target_column_type == "set":
        try:
            from scipy.stats import chi2_contingency
            
            chi2_results = []  # Store (column, chi2, pvalue) tuples
            
            for col in feature_cols:  # Test all features
                try:
                    # Create contingency table
                    contingency = pd.crosstab(X_clean[col], y_clean)
                    chi2, pvalue, dof, expected = chi2_contingency(contingency)
                    chi2_results.append((col, chi2, pvalue))
                except:
                    continue
            
            if chi2_results:
                chi2_pvalues = [pval for _, _, pval in chi2_results]
                min_pvalue = np.min(chi2_pvalues)
                result['min_chi2_pvalue'] = float(min_pvalue)
                
                # Get significant features
                significant = [(col, pval) for col, _, pval in chi2_results if pval < 0.05]
                significant.sort(key=lambda x: x[1])  # Sort by p-value
                
                logger.info(f"\n   ‚Ä¢ Chi-Square Test Results:")
                logger.info(f"     - Features tested: {len(chi2_results)}")
                logger.info(f"     - Significant features (p < 0.05): {len(significant)}")
                
                if significant:
                    logger.info(f"     - Significant features (sorted by p-value):")
                    for col, pval in significant:
                        logger.info(f"       {col}: p={pval:.4f}")
                
                logger.info(f"     - Minimum p-value: {min_pvalue:.4f}")
                
                if min_pvalue > 0.2:
                    logger.info(f"     ‚Üí No strong associations found - complex problem")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not compute chi-square tests: {e}")
    
    # === ANALYSIS 4: Quick Nonlinearity Test ===
    if n_rows >= 200 and n_rows <= 10000:  # Only for reasonable sizes
        try:
            import warnings
            from sklearn.model_selection import train_test_split
            from sklearn.linear_model import LogisticRegression, Ridge
            from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
            from sklearn.metrics import accuracy_score, r2_score
            from sklearn.exceptions import ConvergenceWarning
            
            # Quick train/test split
            X_train, X_test, y_train, y_test = train_test_split(
                X_numeric, y_clean, test_size=0.25, random_state=42, stratify=y_clean if target_column_type == "set" else None
            )
            
            if target_column_type == "set":
                # Classification
                y_train_numeric = pd.Categorical(y_train).codes
                y_test_numeric = pd.Categorical(y_test).codes
                
                # Quick linear model (suppress convergence warnings - this is just a quick diagnostic)
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', category=ConvergenceWarning)
                    lr = LogisticRegression(max_iter=100, random_state=42, solver='lbfgs')
                    lr.fit(X_train, y_train_numeric)
                linear_score = accuracy_score(y_test_numeric, lr.predict(X_test))
                
                # Quick nonlinear model (shallow RF for speed)
                rf = RandomForestClassifier(n_estimators=20, max_depth=5, random_state=42, n_jobs=-1)
                rf.fit(X_train, y_train_numeric)
                nonlinear_score = accuracy_score(y_test_numeric, rf.predict(X_test))
            else:
                # Regression
                y_train_numeric = pd.to_numeric(y_train, errors='coerce')
                y_test_numeric = pd.to_numeric(y_test, errors='coerce')
                
                # Quick linear model
                lr = Ridge(alpha=1.0, random_state=42)
                lr.fit(X_train, y_train_numeric)
                linear_score = r2_score(y_test_numeric, lr.predict(X_test))
                
                # Quick nonlinear model
                rf = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=42, n_jobs=-1)
                rf.fit(X_train, y_train_numeric)
                nonlinear_score = r2_score(y_test_numeric, rf.predict(X_test))
            
            nonlinearity_gain = nonlinear_score - linear_score
            result['nonlinearity_score'] = float(nonlinearity_gain)
            result['linear_baseline_score'] = float(linear_score)
            result['nonlinear_baseline_score'] = float(nonlinear_score)
            
            logger.info(f"\n   ‚Ä¢ Nonlinearity Test (Linear vs Random Forest):")
            logger.info(f"     - Linear model score: {linear_score:.4f}")
            logger.info(f"     - Nonlinear model score: {nonlinear_score:.4f}")
            logger.info(f"     - Nonlinearity gain: {nonlinearity_gain:+.4f}")
            
            if nonlinearity_gain > 0.15:
                logger.info(f"     üî¥ STRONG nonlinearity detected (gain > 0.15)")
                logger.info(f"     ‚Üí Deep network strongly recommended")
                result['complexity_level'] = 'high'
                result['recommendations'].append(
                    f"Strong nonlinearity detected (RF gain: {nonlinearity_gain:.3f}) - deep network (3-4 layers) recommended"
                )
            elif nonlinearity_gain > 0.05:
                logger.info(f"     üü° MODERATE nonlinearity detected (gain > 0.05)")
                logger.info(f"     ‚Üí Moderate depth beneficial")
                if result['complexity_level'] == 'low-medium':
                    result['complexity_level'] = 'medium'
            elif nonlinearity_gain < 0:
                logger.info(f"     üü¢ Linear model performs well")
                logger.info(f"     ‚Üí Shallower network may be sufficient")
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not run nonlinearity test: {e}")
    
    # === Final Complexity Assessment ===
    logger.info(f"\n" + "=" * 80)
    logger.info(f"üéØ COMPLEXITY ASSESSMENT: {result['complexity_level'].upper()}")
    logger.info(f"=" * 80)
    
    if result['recommendations']:
        logger.info(f"\nüí° Key Recommendations:")
        for i, rec in enumerate(result['recommendations'], 1):
            logger.info(f"   {i}. {rec}")
    
    elapsed_time = time.time() - start_time
    logger.info(f"\n‚è±Ô∏è  Analysis completed in {elapsed_time:.2f} seconds")
    logger.info("=" * 80)
    
    return result


def ideal_single_predictor_config(
    n_rows: int,
    d_model: int = 128,
    d_hidden: int = None,
    n_cols: int = None,
    fine_tune: bool = True,
    complexity_analysis: Optional[Dict[str, Any]] = None,
    batch_size: int = None
) -> Dict[str, Any]:
    """
    Compute optimal configuration for single predictor MLP based on dataset size.

    Core principle: Scale model complexity with data size to prevent overfitting.
    - More data ‚Üí more layers, less dropout
    - Less data ‚Üí fewer layers, more dropout

    Architecture decisions:
    - d_hidden: Fixed by upstream model (passed in), defaults to d_model
    - n_hidden_layers: Auto-sized based on n_rows and complexity
    - dropout: Inversely scaled with data size (more dropout for small datasets)
    - Normalization: LayerNorm for small batches, BatchNorm otherwise

    Args:
        n_rows: Number of training samples (labeled data for predictor)
        d_model: Embedding space dimensionality (default 128)
        d_hidden: Hidden layer dimension (None = use d_model)
        n_cols: Number of input columns (optional, for complexity analysis)
        fine_tune: Whether the embedding space will be fine-tuned during training
        complexity_analysis: Optional output from analyze_dataset_complexity()
        batch_size: Training batch size (optional, for normalization choice)

    Returns:
        Dictionary with configuration:
        - d_hidden: Hidden layer dimension
        - n_hidden_layers: Number of hidden layers (0 = linear head)
        - dropout: Dropout rate
        - use_batch_norm: Whether to use batch normalization
        - use_layer_norm: Whether to use LayerNorm (better for small batches)
        - residual: Whether to use residual connections
        - gradient_accumulation_steps: Recommended accumulation steps (1 = no accumulation)
        - lr_scale_factor: Recommended LR scaling factor based on batch size
    """
    # Use d_model as default d_hidden if not provided
    if d_hidden is None:
        d_hidden = d_model

    # Estimate batch size if not provided
    if batch_size is None:
        batch_size = ideal_batch_size(n_rows, mode="predictor")

    # Determine normalization strategy based on batch size
    # BatchNorm needs batch_size >= 32 for stable statistics
    # LayerNorm is batch-size independent - stable for any batch size
    use_small_batch_mode = batch_size < 32

    # Calculate gradient accumulation to achieve effective batch size of 32+
    # This reduces gradient noise for small batches
    if batch_size < 32:
        gradient_accumulation_steps = max(1, 32 // batch_size)
    else:
        gradient_accumulation_steps = 1

    # LR scaling: leave alone for now - adaptive LR already handles dataset size
    lr_scale_factor = 1.0

    # Extract complexity metrics if available
    max_mi = 0.15  # Default: assume moderate correlation
    nonlinearity = 0.0
    if complexity_analysis is not None:
        max_mi = complexity_analysis.get('max_mutual_info', 0.15)
        nonlinearity = complexity_analysis.get('nonlinearity_score', 0.0)

    # ==========================================================================
    # ARCHITECTURE HEURISTICS: Scale n_hidden_layers and dropout with data size
    # ==========================================================================
    #
    # Principle: With d_hidden fixed, each layer adds ~d_hidden^2 parameters.
    # Rule of thumb: need ~10-50 samples per parameter to avoid overfitting.
    # For d_hidden=192: each layer ‚âà 37K params ‚Üí need 370K-1.8M samples for 10 layers
    #
    # We use conservative estimates since embeddings already capture patterns.

    # === TINY: < 200 rows ===
    # Linear head only - not enough data for any hidden layers
    if n_rows < 200:
        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": 0,
            "dropout": 0.0,  # No dropout needed for linear head
            "use_batch_norm": False,
            "use_layer_norm": False,
            "residual": False,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === SMALL: 200-500 rows ===
    # Minimal depth, high dropout
    elif n_rows < 500:
        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": 1,
            "dropout": 0.5,  # High dropout for regularization
            "use_batch_norm": False,
            "use_layer_norm": use_small_batch_mode,
            "residual": False,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === SMALL-MEDIUM: 500-1000 rows ===
    elif n_rows < 1000:
        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": 2,
            "dropout": 0.4,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === MEDIUM: 1000-2500 rows ===
    elif n_rows < 2500:
        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": 3,
            "dropout": 0.3,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === MEDIUM-LARGE: 2500-5000 rows ===
    elif n_rows < 5000:
        n_hidden_layers = 4
        dropout = 0.25

        # Adjust based on complexity
        if max_mi > 0.4:
            # Strong relationships ‚Üí can go shallower
            n_hidden_layers = 3
        elif max_mi < 0.05 or nonlinearity > 0.15:
            # Weak relationships or strong nonlinearity ‚Üí need depth
            n_hidden_layers = 5
            dropout = 0.3  # More dropout for deeper network

        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": n_hidden_layers,
            "dropout": dropout,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === LARGE: 5000-10000 rows ===
    elif n_rows < 10000:
        n_hidden_layers = 5
        dropout = 0.2

        # Adjust based on complexity
        if max_mi > 0.4 and nonlinearity < 0.05:
            # Simple problem ‚Üí shallower
            n_hidden_layers = 4
        elif max_mi < 0.05 or nonlinearity > 0.15:
            # Complex problem ‚Üí deeper
            n_hidden_layers = 6
            dropout = 0.25

        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": n_hidden_layers,
            "dropout": dropout,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === VERY LARGE: 10000-20000 rows ===
    elif n_rows < 20000:
        n_hidden_layers = 6
        dropout = 0.15

        # Adjust based on complexity
        if max_mi < 0.05 or nonlinearity > 0.15:
            n_hidden_layers = 7
            dropout = 0.2

        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": n_hidden_layers,
            "dropout": dropout,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }

    # === MASSIVE: 20000+ rows ===
    else:
        return {
            "d_hidden": d_hidden,
            "n_hidden_layers": 7,
            "dropout": 0.1,
            "use_batch_norm": not use_small_batch_mode,
            "use_layer_norm": use_small_batch_mode,
            "residual": True,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scale_factor": lr_scale_factor,
        }


def ideal_single_predictor_hidden_layers(
    n_rows: int, 
    n_cols: int, 
    d_model: int = 128,
    fine_tune: bool = True,
    complexity_analysis: Optional[Dict[str, Any]] = None
) -> int:
    """
    Compute a reasonable number of hidden layers for the single predictor MLP.
    
    Args:
        n_rows: Number of training samples
        n_cols: Number of input columns (features)
        d_model: Embedding space dimensionality (default 128)
        fine_tune: Whether the embedding space will be fine-tuned during training
        complexity_analysis: Optional output from analyze_dataset_complexity()
    
    Returns:
        Number of hidden layers (2-4)
    
    Logic:
        - Start with baseline of 2 layers (proven default)
        - Increase for complex datasets (many columns, sufficient rows)
        - Increase if fine-tuning (more capacity available)
        - Increase if complexity analysis shows nonlinearity or weak correlations
        - Adjust for extreme class imbalance
        - Cap at 4 layers to prevent overfitting on typical dataset sizes
    
    Examples:
        - Small dataset (< 1000 rows, < 50 cols):         2 layers
        - Medium dataset (2500 rows, 100 cols):           2-3 layers
        - Large complex (10k rows, 300 cols, finetune):   3-4 layers
        - Weak correlations + nonlinearity:               3-4 layers
    """
    # Baseline: 2 layers is solid for most cases
    layers = 2
    reasons = []
    
    # Factor 1: Dataset size
    # Need sufficient data to support more parameters
    if n_rows >= 5000:
        layers += 1  # 3 layers for 5k+ rows
        reasons.append(f"Dataset size ({n_rows:,} rows) supports deeper network")
    if n_rows >= 10000:
        layers += 1  # 4 layers for 10k+ rows
        reasons.append(f"Large dataset ({n_rows:,} rows) can leverage maximum depth")
    
    # Factor 2: Data complexity (number of columns)
    # More columns = more complex relationships to learn
    if n_cols >= 150 and n_rows >= 2500:
        layers = max(layers, 3)  # At least 3 layers for very wide data
        reasons.append(f"Wide dataset ({n_cols} columns) benefits from deeper network")
    
    # Factor 3: Fine-tuning capability
    # Fine-tuning means more total parameters can adapt, supporting deeper predictor
    if fine_tune and n_rows >= 3000 and n_cols >= 100:
        layers = max(layers, 3)  # At least 3 layers when fine-tuning complex data
        reasons.append("Fine-tuning enabled - can leverage deeper predictor")
    
    # Factor 4: Complexity analysis (NEW)
    if complexity_analysis is not None:
        max_mi = complexity_analysis.get('max_mutual_info', 0.0)
        nonlinearity = complexity_analysis.get('nonlinearity_score', 0.0)
        imbalance_ratio = complexity_analysis.get('class_imbalance_ratio', 1.0)
        complexity_level = complexity_analysis.get('complexity_level', 'medium')
        
        # Very weak correlations suggest complex nonlinear patterns
        if max_mi < 0.05 and n_rows >= 2000:
            layers = max(layers, 3)
            reasons.append(f"Very weak feature correlations (MI={max_mi:.3f}) suggest nonlinear patterns")
            if n_rows >= 5000:
                layers = max(layers, 4)
                reasons.append("Large dataset + weak correlations = deep network recommended")
        elif max_mi < 0.15 and n_rows >= 3000:
            layers = max(layers, 3)
            reasons.append(f"Weak feature correlations (MI={max_mi:.3f}) indicate nonlinearity")
        
        # Strong nonlinearity detected
        if nonlinearity > 0.15 and n_rows >= 2000:
            layers = max(layers, 3)
            reasons.append(f"Strong nonlinearity detected (gain={nonlinearity:.3f})")
            if n_rows >= 5000:
                layers = max(layers, 4)
        elif nonlinearity > 0.05 and n_rows >= 3000:
            layers = max(layers, 3)
            reasons.append(f"Moderate nonlinearity detected (gain={nonlinearity:.3f})")
        
        # Strong correlations = simpler problem
        if max_mi > 0.4 and nonlinearity < 0.05:
            layers = min(layers, 3)
            reasons.append(f"Strong linear relationships (MI={max_mi:.3f}) - shallower network sufficient")
        
        # Extreme class imbalance may benefit from extra capacity
        if imbalance_ratio > 100 and n_rows >= 3000:
            layers = max(layers, 3)
            reasons.append(f"Extreme class imbalance ({imbalance_ratio:.1f}:1) - extra capacity for minority class")
    
    # Factor 5: Very small datasets - use simple linear layer
    if n_rows < 1000:
        if layers > 0:
            reasons.append(f"Very small dataset ({n_rows} rows) - using simple linear layer (no hidden layers) to prevent overfitting")
        layers = 0  # Use simple Linear(d_model, d_out) for very small datasets
    elif n_rows < 2000:
        if layers > 2:
            reasons.append(f"Small dataset ({n_rows} rows) - capping at 2 layers to prevent overfitting")
        layers = min(layers, 2)  # Cap at 2 layers for small datasets
    
    # Final cap: Never exceed 4 layers
    # Diminishing returns and overfitting risk beyond this
    if layers > 4:
        reasons.append("Capping at 4 layers (proven maximum for typical dataset sizes)")
        layers = 4
    
    layers = min(layers, 4)
    
    # Log the decision
    logger.info(f"\nüèóÔ∏è  NEURAL NETWORK ARCHITECTURE DECISION")
    logger.info(f"   ‚Üí Selected {layers} hidden layers")
    if reasons:
        logger.info(f"   ‚Üí Reasoning:")
        for reason in reasons:
            logger.info(f"     ‚Ä¢ {reason}")
    
    return int(layers)


def generate_sp_hyperparameter_grid(
    d_model: int,
    n_rows: int,
    search_space: str = "default"
) -> List[Dict[str, Any]]:
    """
    Generate all possible hyperparameter combinations for Single Predictor grid search.
    
    Args:
        d_model: Embedding space dimensionality (from the foundation model)
        n_rows: Number of training samples (used to filter reasonable configs)
        search_space: "default", "narrow", or "wide"
            - "default": Reasonable range based on empirical results
            - "narrow": Focus on best known ranges (faster)
            - "wide": Exhaustive search (slower)
    
    Returns:
        List of dicts, each containing a complete hyperparameter configuration.
        Each dict has keys: d_hidden, n_hidden_layers, dropout, learning_rate, batch_size
        
    Note: fine_tune is always True (grid search showed +3.18pp improvement)
    
    Example:
        >>> configs = generate_sp_hyperparameter_grid(d_model=256, n_rows=5000)
        >>> len(configs)
        144  # Example: 3 d_hidden * 4 layers * 3 dropout * 2 LR * 2 batch_size
    """
    from itertools import product
    
    # Define search spaces
    if search_space == "narrow":
        # Focus on best known ranges (based on grid search results)
        # Grid search found n_hidden_layers=7 optimal, so test around that
        d_hidden_options = [
            d_model,  # 1x d_model
            min(int(1.5 * d_model), 512),  # 1.5x d_model
        ]
        n_hidden_layers_options = [5, 6, 7, 8]  # Around the optimal 7
        dropout_options = [0.2, 0.3, 0.4]  # Standard range
        learning_rate_options = [0.0005, 0.001]  # Current default is 0.001
        batch_size_options = [0]  # 0 = auto-calculate
        
    elif search_space == "wide":
        # Exhaustive search
        d_hidden_options = [
            max(d_model // 2, 64),  # 0.5x d_model
            d_model,  # 1x d_model
            min(int(1.5 * d_model), 512),  # 1.5x d_model
            min(2 * d_model, 512),  # 2x d_model
        ]
        n_hidden_layers_options = [1, 2, 3, 4, 5, 6, 7, 8]
        dropout_options = [0.1, 0.2, 0.3, 0.4, 0.5]
        learning_rate_options = [0.0001, 0.0005, 0.001, 0.002]
        batch_size_options = [0, 16, 32, 64, 128]
        
    else:  # "default"
        # Balanced search space
        d_hidden_options = [
            d_model,  # 1x d_model (conservative)
            min(int(1.5 * d_model), 512),  # 1.5x d_model (moderate)
            min(2 * d_model, 512),  # 2x d_model (aggressive)
        ]
        n_hidden_layers_options = [2, 3, 4, 5, 6, 7]  # 2-7 layers
        dropout_options = [0.2, 0.3, 0.4]
        learning_rate_options = [0.0005, 0.001, 0.002]
        batch_size_options = [0]  # 0 = auto-calculate
    
    # Filter options based on dataset size
    if n_rows < 500:
        # Very small dataset - use linear head only
        d_hidden_options = [None]
        n_hidden_layers_options = [0]
        dropout_options = [0.0]
    elif n_rows < 2000:
        # Small dataset - limit depth
        n_hidden_layers_options = [n for n in n_hidden_layers_options if n <= 2]
        dropout_options = [d for d in dropout_options if d <= 0.3]
    
    # Generate all combinations
    configs = []
    for d_h, n_layers, dropout, lr, bs in product(
        d_hidden_options,
        n_hidden_layers_options,
        dropout_options,
        learning_rate_options,
        batch_size_options
    ):
        # Skip invalid combinations
        if d_h is None and n_layers > 0:
            continue  # Linear head should have 0 layers
        if d_h is not None and n_layers == 0:
            continue  # Hidden layers require d_hidden
        
        config = {
            "d_hidden": d_h,
            "n_hidden_layers": n_layers,
            "dropout": dropout,
            "learning_rate": lr,
            "batch_size": bs,
            "fine_tune": True,  # Always True based on grid search results
            "use_batch_norm": n_layers > 1,  # Use batch norm for deeper networks
            "residual": n_layers > 2,  # Use residual connections for deep networks
        }
        configs.append(config)
    
    return configs


def format_sp_config_summary(config: Dict[str, Any]) -> str:
    """
    Format a single predictor config as a readable string for logging.
    
    Args:
        config: Dictionary from generate_sp_hyperparameter_grid()
    
    Returns:
        Formatted string like "d_hidden=256, layers=7, dropout=0.3, lr=0.001"
    """
    d_h = config.get("d_hidden")
    if d_h is None:
        return "linear_head, dropout=0.0"
    
    parts = [
        f"d_hidden={d_h}",
        f"layers={config['n_hidden_layers']}",
        f"dropout={config['dropout']:.2f}",
        f"lr={config['learning_rate']:.4f}",
    ]
    
    if config.get("batch_size", 0) > 0:
        parts.append(f"bs={config['batch_size']}")
    
    return ", ".join(parts)


