{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClimData Tutorial\n",
    "This notebook demonstrates usage of the `ClimData` class for climate data extraction, extreme index computation, and workflow management.\n",
    "Includes examples for point-based and box-based extraction, variable exploration, and error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7a58a",
   "metadata": {},
   "source": [
    "# 1️⃣ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climdata import ClimData\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s | %(message)s\",\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Explore available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dwd', 'mswx', 'hyras', 'cmip', 'power', 'w5e5', 'cmip_w5e5', 'nexgddp']\n"
     ]
    }
   ],
   "source": [
    "extractor = ClimData()\n",
    "datasets = extractor.get_datasets()\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Explore variables for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n",
      "⚠️  Warning: Requested time range 1989-2020 extends beyond\n",
      "   the typical Historical period (1850-2014).\n",
      "   Data availability may be limited.\n",
      "['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "INFO | 46 models found for experiment 'ssp245'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n",
      "['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n"
     ]
    }
   ],
   "source": [
    "variables = extractor.get_variables('w5e5')\n",
    "print(variables)\n",
    "\n",
    "# for CMIP\n",
    "import climdata\n",
    "extractor_CMIP = climdata.CMIP(extractor.cfg)\n",
    "print(extractor_CMIP.get_experiment_ids())\n",
    "print(extractor_CMIP.get_source_ids('ssp245'))\n",
    "print(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Explore metadata for a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n",
      "**********************************************************************\n",
      "{'cf_name': 'surface_downwelling_longwave_flux_in_air', 'long_name': 'Surface downwelling longwave radiation', 'units': 'W m-2'}\n"
     ]
    }
   ],
   "source": [
    "variables = extractor.get_variables('w5e5')\n",
    "print(variables)\n",
    "print(\"*\"*70)\n",
    "varinfo = extractor.get_varinfo('rlds')\n",
    "print(varinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Explore available workflow actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['extract', 'calc_index', 'impute', 'to_nc', 'to_csv', 'upload_netcdf', 'upload_csv'])\n"
     ]
    }
   ],
   "source": [
    "actions = extractor.get_actions()\n",
    "print(actions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6fe84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['heat_wave_index', 'heat_wave_frequency', 'heat_wave_max_length', 'heat_wave_total_length', 'hot_spell_frequency', 'hot_spell_max_length', 'hot_spell_total_length', 'hot_spell_max_magnitude', 'ice_days', 'isothermality', 'maximum_consecutive_frost_days', 'maximum_consecutive_frost_free_days', 'maximum_consecutive_tx_days'])\n",
      "dict_keys(['BRITS', 'XGBOOST', 'CDRec', 'SoftImpute'])\n"
     ]
    }
   ],
   "source": [
    "indices = extractor.get_indices(['tasmin', 'tasmax'])\n",
    "print(indices.keys())\n",
    "\n",
    "impute_methods = extractor.get_impute_methods()\n",
    "print(impute_methods.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Point extraction workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "INFO | Completed action: extract\n",
      "INFO | Starting action: impute\n",
      "INFO | \u001b[93mNo missing data found. Imputation not required.\u001b[0m\n",
      "INFO | Completed action: impute\n",
      "INFO | Starting action: calc_index\n",
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires ≥30 years, got 11\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | Completed action: calc_index\n",
      "INFO | Starting action: to_nc\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n",
      "INFO | Dataset saved to NetCDF file: cmip_tn10p_LAT12.891982026993958_LON24.246667038198012_2004-01-01_2014-12-31.nc\n",
      "INFO | Completed action: to_nc\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Define the area of interest (AOI)\n",
    "# -----------------------------\n",
    "# The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude].\n",
    "geojson = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"coordinates\": [\n",
    "          24.246667038198012,  # longitude\n",
    "          12.891982026993958   # latitude\n",
    "        ],\n",
    "        \"type\": \"Point\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Define configuration overrides\n",
    "# -----------------------------\n",
    "# Overrides are strings used by Hydra to modify default configurations at runtime.\n",
    "overrides = [\n",
    "    \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n",
    "    f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above\n",
    "    f\"time_range.start_date=2004-01-01\",  # Start date for data extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n",
    "    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n",
    "    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n",
    "    \"index=tn10p\",  # Climate extreme index to calculate\n",
    "    \"impute=BRITS\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define the workflow sequence\n",
    "# -----------------------------\n",
    "seq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Initialize the ClimData extractor\n",
    "# -----------------------------\n",
    "extractor = ClimData(overrides=overrides)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Run the Multi-Step workflow\n",
    "# -----------------------------\n",
    "result = extractor.run_workflow(\n",
    "    actions=seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca2bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n",
      "✅ All 31 pr files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires ≥30 years, got 1\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | DataFrame saved to CSV file: index.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'index.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Define the area of interest (AOI)\n",
    "# -----------------------------\n",
    "# This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {},\n",
    "            \"geometry\": {\n",
    "                \"coordinates\": [24.246667038198012, 12.891982026993958],\n",
    "                \"type\": \"Point\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Define configuration overrides\n",
    "# -----------------------------\n",
    "# These strings override the default hydra config at runtime\n",
    "overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature & precipitation\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "    # Optional Google service account if needed for MSWX access\n",
    "    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n",
    "    \"index=tn10p\",  # Extreme climate index to calculate\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize the ClimData extractor\n",
    "# -----------------------------\n",
    "# This loads the configuration with overrides and prepares the object\n",
    "extractor = ClimData(overrides=overrides)\n",
    "\n",
    "# -----------------------------\n",
    "# Extract climate data\n",
    "# -----------------------------\n",
    "# Returns an xarray.Dataset for the selected variables, AOI, and time range\n",
    "ds = extractor.extract()\n",
    "\n",
    "# -----------------------------\n",
    "# Compute the climate index\n",
    "# -----------------------------\n",
    "# Takes the extracted dataset and calculates the extreme index \"tn10p\"\n",
    "# Returns a new xarray.Dataset containing only the index\n",
    "ds_index = extractor.calc_index(ds)\n",
    "\n",
    "# -----------------------------\n",
    "# Convert the index dataset to a long-form pandas DataFrame\n",
    "# -----------------------------\n",
    "# Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\")\n",
    "df_index = extractor.to_dataframe(ds_index)\n",
    "\n",
    "# -----------------------------\n",
    "# Save the DataFrame to CSV\n",
    "# -----------------------------\n",
    "# This will write the index values to \"index.csv\" in the current working directory\n",
    "extractor.to_csv(df_index, filename=\"index.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.csv\n"
     ]
    }
   ],
   "source": [
    "print(extractor.current_filename)\n",
    "# print(extractor_point.filename_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Box extraction workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Completed action: extract\n",
      "INFO | Starting action: to_csv\n",
      "INFO | DataFrame saved to CSV file: mswx_tasmin_tasmax_LAT_34.0_71.0_LON_-25.0_45.0_2014-12-01_2014-12-31.csv\n",
      "INFO | Completed action: to_csv\n"
     ]
    }
   ],
   "source": [
    "box_overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    \"region=europe\", # Select the region\n",
    "    \"variables=[tasmin,tasmax]\",\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "]\n",
    "\n",
    "extractor_box = ClimData(overrides=box_overrides)\n",
    "result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_csv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Compute extreme index only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Completed action: extract\n",
      "INFO | Starting action: calc_index\n",
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index heat_wave_max_length usually requires ≥30 years, got 1\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | Completed action: calc_index\n",
      "INFO | Starting action: to_csv\n",
      "INFO | DataFrame saved to CSV file: mswx_heat_wave_max_length_LAT_52.5_LON_13.4_2014-12-01_2014-12-31.csv\n",
      "INFO | Completed action: to_csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>units</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>52.549999</td>\n",
       "      <td>13.350003</td>\n",
       "      <td>heat_wave_max_length</td>\n",
       "      <td>0.0</td>\n",
       "      <td>d</td>\n",
       "      <td>mswx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time        lat        lon              variable  value units source\n",
       "0 2014-01-01  52.549999  13.350003  heat_wave_max_length    0.0     d   mswx"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_berlin, lon_berlin = [52.5,13.4]\n",
    "idx_overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    f\"lat={lat_berlin}\", # Select the region\n",
    "    f\"lon={lon_berlin}\",\n",
    "    \"variables=[tasmin,tasmax]\",\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "    \"index=heat_wave_max_length\"\n",
    "]\n",
    "\n",
    "\n",
    "extractor_idx = ClimData(overrides=idx_overrides)\n",
    "result_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_csv\"])\n",
    "result_idx.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Error examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: calc_index\n",
      "ERROR | Action 'calc_index' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 829, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: to_csv\n",
      "ERROR | Action 'to_csv' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 838, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'to_dataframe' requires a dataset, but no dataset is available. Upload or extract a dataset before converting to a DataFrame.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'to_dataframe' requires a dataset, but no dataset is available. Upload or extract a dataset before converting to a DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: upload_netcdf\n",
      "ERROR | Action 'upload_netcdf' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 786, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"calc_index\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"to_csv\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"upload_netcdf\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
