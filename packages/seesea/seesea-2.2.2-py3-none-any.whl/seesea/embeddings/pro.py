# -*- coding: utf-8 -*-
"""
Proæ¨¡å¼åµŒå…¥å™¨

ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡ Qwen3-Embedding-0.6B-Q8_0 æ¨¡å‹ï¼ˆéœ€è¦ llama-cpp-pythonï¼‰ï¼Œ
å¦‚æœæœªå®‰è£…åˆ™å›é€€åˆ°çº¯ Python å®ç°çš„ç®€å•å‘é‡åŒ–å™¨ã€‚
"""

from typing import List, Optional, Union, Any, cast
import os
from .manager import BaseEmbedder

# æ£€æŸ¥ llama-cpp-python æ˜¯å¦å¯ç”¨
try:
    from seesea_core import get_file
    import importlib.util

    spec = importlib.util.find_spec("llama_cpp")
    LLAMA_CPP_AVAILABLE = spec is not None
except (ImportError, AttributeError):
    LLAMA_CPP_AVAILABLE = False

# å¯¼å…¥ç®€å•å‘é‡åŒ–å™¨ä½œä¸ºåå¤‡
from .simple import SimpleEmbedder


class ProEmbedder(BaseEmbedder):
    """
    Proæ¨¡å¼åµŒå…¥å™¨

    å¦‚æœ llama-cpp-python å¯ç”¨ï¼Œä½¿ç”¨ Qwen3-Embedding-0.6B-Q8_0 æ¨¡å‹ï¼š
    - é«˜è´¨é‡åµŒå…¥ï¼ˆQ8_0é‡åŒ–ä¿ç•™æ›´å¤šç²¾åº¦ï¼‰
    - ç»´åº¦1024ï¼Œè¯­ä¹‰è¡¨è¾¾èƒ½åŠ›æ›´å¼º
    - æ”¯æŒ32Kä¸Šä¸‹æ–‡
    - é€‚åˆProæ¨¡å¼ä¸‹çš„é«˜ç²¾åº¦è¯­ä¹‰æœç´¢

    å¦‚æœ llama-cpp-python ä¸å¯ç”¨ï¼Œä½¿ç”¨çº¯ Python å®ç°çš„ç®€å•å‘é‡åŒ–å™¨ï¼š
    - æ— éœ€å¤–éƒ¨ä¾èµ–
    - å›ºå®šç»´åº¦ 512
    - åŸºäºè¯é¢‘å’Œå“ˆå¸Œ
    """

    # æ¨¡å‹é…ç½®ï¼ˆä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶ä½¿ç”¨ï¼‰
    MODEL_FILENAME = "Qwen3-Embedding-0.6B-Q8_0.gguf"
    MODEL_URL = "https://hf-mirror.com/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf?download=true"
    EXPECTED_DIMENSION = 1024

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        n_threads: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–ProåµŒå…¥å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆNoneåˆ™è‡ªåŠ¨ä¸‹è½½ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
            device: è¿è¡Œè®¾å¤‡ï¼ˆ'cuda', 'cpu', Noneè‡ªåŠ¨æ£€æµ‹ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
            n_threads: çº¿ç¨‹æ•°ï¼ˆNoneè‡ªåŠ¨æ£€æµ‹ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
        """
        # æ£€æŸ¥ llama-cpp-python æ˜¯å¦å¯ç”¨
        if not LLAMA_CPP_AVAILABLE:
            print("âš ï¸  [Pro] llama-cpp-python æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å‘é‡åŒ–å™¨")
            print("ğŸ’¡ æç¤º: å®‰è£… llama-cpp-python ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ")
            print("   pip install llama-cpp-python")
            self.embedder = SimpleEmbedder(dimension=512)
            self.dimension = self.embedder.get_dimension()
            self._use_llama = False
            self.model_name = "simple-embedder-512"
            return

        # ä½¿ç”¨ llama-cpp-python
        self._use_llama = True
        self.model_name = "Qwen3-Embedding-0.6B"

        # æ¨¡å‹ç›®å½• - ä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„å›ºå®šä½ç½®
        import platform

        system = platform.system()
        if system == "Windows":
            llm_dir = os.path.join(
                os.path.expanduser("~"), "AppData", "Local", "SeeSea", "models"
            )
        elif system == "Darwin":  # macOS
            llm_dir = os.path.join(
                os.path.expanduser("~"),
                "Library",
                "Application Support",
                "SeeSea",
                "models",
            )
        else:  # Linux and other Unix-like systems
            llm_dir = os.path.join(
                os.path.expanduser("~"), ".local", "share", "seesea", "models"
            )
        models_dir = llm_dir
        local_model_file = os.path.join(models_dir, self.MODEL_FILENAME)

        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            if os.path.exists(local_model_file):
                print(f"ğŸ“ [Pro] ä½¿ç”¨å·²å­˜åœ¨æ¨¡å‹: {local_model_file}")
                model_path = local_model_file
            else:
                print("â¬‡ï¸  [Pro] ä¸‹è½½é«˜è´¨é‡åµŒå…¥æ¨¡å‹ï¼ˆQ8_0é‡åŒ–ï¼‰...")
                os.makedirs(models_dir, exist_ok=True)

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }

                try:
                    result = get_file(self.MODEL_URL, local_model_file, headers)
                    if result.get("status") != 200:
                        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.get('status')}")
                    print(f"âœ… [Pro] æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_file}")
                except Exception as e:
                    raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}") from e

                model_path = local_model_file

        # GPUé…ç½®
        n_gpu_layers = self._detect_gpu(device)

        # çº¿ç¨‹é…ç½®
        if n_threads is None:
            n_threads = max(1, os.cpu_count() or 4)
        self.n_threads = n_threads

        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ [Pro] åŠ è½½é«˜è´¨é‡åµŒå…¥æ¨¡å‹...")
        self._load_model(model_path, n_gpu_layers, n_threads)

    def _load_model(
        self, model_path: str, n_gpu_layers: int, n_threads: int, retry: bool = True
    ):
        """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒé‡è¯•"""
        from llama_cpp import Llama

        try:
            self.embedder = Llama(
                model_path=model_path,
                embedding=True,
                n_gpu_layers=n_gpu_layers,
                n_ctx=32768,  # å®Œæ•´32Kä¸Šä¸‹æ–‡
                n_threads=n_threads,
                verbose=False,
                n_output=0,
                logits_all=False,
                use_mmap=True,
                use_mlock=False,
            )

            # æµ‹è¯•è·å–ç»´åº¦
            if self.embedder is None:
                raise RuntimeError("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            llama_embedder = cast(Any, self.embedder)
            test_result = llama_embedder.create_embedding(input="test")
            self.dimension = len(test_result["data"][0]["embedding"])
            print(f"âœ… [Pro] æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦: {self.dimension}")

        except Exception as e:
            if retry and "Failed to load model" in str(e):
                print("âŒ [Pro] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°ä¸‹è½½...")
                if os.path.exists(model_path):
                    os.remove(model_path)

                from seesea_core import get_file

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                get_file(self.MODEL_URL, model_path, headers)

                # é‡è¯•åŠ è½½ï¼ˆä¸å†é‡è¯•ï¼‰
                self._load_model(model_path, n_gpu_layers, n_threads, retry=False)
            else:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e

    def _detect_gpu(self, device: Optional[str]) -> int:
        """æ£€æµ‹GPUé…ç½®"""
        if device == "cuda":
            return -1
        elif device == "cpu":
            return 0
        else:
            # è‡ªåŠ¨æ£€æµ‹
            gpu_env_vars = [
                "CUDA_VISIBLE_DEVICES",
                "NVIDIA_VISIBLE_DEVICES",
                "CUDA_PATH",
            ]
            for var in gpu_env_vars:
                if os.environ.get(var):
                    return -1
            return 0

    def encode(
        self, texts: Union[str, List[str]], batch_size: int = 8
    ) -> Union[List[float], List[List[float]]]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        if self.embedder is None:
            raise RuntimeError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

        single_input = isinstance(texts, str)

        if single_input:
            texts_to_process: List[str] = [texts]  # type: ignore[list-item]
        else:
            texts_to_process = texts  # type: ignore[assignment]

        # è°ƒç”¨ llama-cpp-python çš„ embedding API
        try:
            llama_embedder = cast(Any, self.embedder)
            response = llama_embedder.create_embedding(
                input=texts_to_process,
                model=self.model_name,
            )
            embeddings = [item["embedding"] for item in response["data"]]
            typed_embeddings: List[List[float]] = cast(List[List[float]], embeddings)

            if single_input and typed_embeddings:
                return typed_embeddings[0]
            return typed_embeddings

        except Exception as e:
            raise RuntimeError(f"ç¼–ç å¤±è´¥: {e}") from e

    def create_embedding(self, text: str) -> List[float]:
        """
        åˆ›å»ºåµŒå…¥å‘é‡ï¼ˆå…¼å®¹æ¥å£ï¼‰

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        result = self.encode(text)
        if isinstance(result, list) and len(result) > 0 and isinstance(result[0], list):
            return result[0]
        return result  # type: ignore

    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def encode_callback(self, text: str) -> List[float]:
        """
        Rustå›è°ƒæ¥å£

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        # å¦‚æœä½¿ç”¨ç®€å•å‘é‡åŒ–å™¨ï¼Œç›´æ¥è°ƒç”¨
        if not self._use_llama:
            return self.embedder.encode_callback(text)

        result = cast(List[float], self.encode(text))
        return result
        if not self._use_llama:
            return self.embedder.encode_callback(text)

        result = cast(List[float], self.encode(text))
        return result
