# -*- coding: utf-8 -*-
"""
æ ‡å‡†æ¨¡å¼åµŒå…¥å™¨

ä¼˜å…ˆä½¿ç”¨è½»é‡çº§ all-MiniLM-L6-v2-Q4_K_M æ¨¡å‹ï¼ˆéœ€è¦ llama-cpp-pythonï¼‰ï¼Œ
å¦‚æœæœªå®‰è£…åˆ™å›é€€åˆ°çº¯ Python å®ç°çš„ç®€å•å‘é‡åŒ–å™¨ã€‚
"""

from typing import List, Optional, Union, Any, cast
import os
from .manager import BaseEmbedder

# æ£€æŸ¥ llama-cpp-python æ˜¯å¦å¯ç”¨
try:
    from seesea_core import get_file
    import importlib.util

    spec = importlib.util.find_spec("llama_cpp")
    LLAMA_CPP_AVAILABLE = spec is not None
except (ImportError, AttributeError):
    LLAMA_CPP_AVAILABLE = False

# å¯¼å…¥ç®€å•å‘é‡åŒ–å™¨ä½œä¸ºåå¤‡
from .simple import SimpleEmbedder


class StandardEmbedder(BaseEmbedder):
    """
    æ ‡å‡†æ¨¡å¼åµŒå…¥å™¨

    å¦‚æœ llama-cpp-python å¯ç”¨ï¼Œä½¿ç”¨ all-MiniLM-L6-v2-Q4_K_M æ¨¡å‹ï¼š
    - æ¨¡å‹å°å·§ï¼ˆ~23MBï¼‰
    - æ¨ç†é€Ÿåº¦å¿«
    - ç»´åº¦384ï¼Œè¶³å¤Ÿç”¨äºç›¸å…³æ€§è®¡ç®—

    å¦‚æœ llama-cpp-python ä¸å¯ç”¨ï¼Œä½¿ç”¨çº¯ Python å®ç°çš„ç®€å•å‘é‡åŒ–å™¨ï¼š
    - æ— éœ€å¤–éƒ¨ä¾èµ–
    - å›ºå®šç»´åº¦ 512
    - åŸºäºè¯é¢‘å’Œå“ˆå¸Œ
    """

    # æ¨¡å‹é…ç½®ï¼ˆä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶ä½¿ç”¨ï¼‰
    MODEL_FILENAME = "all-MiniLM-L6-v2-Q4_K_M.gguf"
    MODEL_URL = "https://hf-mirror.com/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-Q4_K_M.gguf?download=true"
    EXPECTED_DIMENSION = 384

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        n_threads: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–æ ‡å‡†åµŒå…¥å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆNoneåˆ™è‡ªåŠ¨ä¸‹è½½ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
            device: è¿è¡Œè®¾å¤‡ï¼ˆ'cuda', 'cpu', Noneè‡ªåŠ¨æ£€æµ‹ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
            n_threads: çº¿ç¨‹æ•°ï¼ˆNoneè‡ªåŠ¨æ£€æµ‹ï¼Œä»…åœ¨ä½¿ç”¨ llama-cpp-python æ—¶æœ‰æ•ˆï¼‰
        """
        # æ£€æŸ¥ llama-cpp-python æ˜¯å¦å¯ç”¨
        if not LLAMA_CPP_AVAILABLE:
            print("âš ï¸  [Standard] llama-cpp-python æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å‘é‡åŒ–å™¨")
            print("ğŸ’¡ æç¤º: å®‰è£… llama-cpp-python ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ")
            print("   pip install llama-cpp-python")
            self.embedder = SimpleEmbedder(dimension=512)
            self.dimension = self.embedder.get_dimension()
            self._use_llama = False
            self.model_name = "simple-embedder-512"
            return

        # ä½¿ç”¨ llama-cpp-python
        self._use_llama = True
        self.model_name = "all-MiniLM-L6-v2"

        # æ¨¡å‹ç›®å½• - ä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„å›ºå®šä½ç½®
        import platform

        system = platform.system()
        if system == "Windows":
            llm_dir = os.path.join(
                os.path.expanduser("~"), "AppData", "Local", "SeeSea", "models"
            )
        elif system == "Darwin":  # macOS
            llm_dir = os.path.join(
                os.path.expanduser("~"),
                "Library",
                "Application Support",
                "SeeSea",
                "models",
            )
        else:  # Linux and other Unix-like systems
            llm_dir = os.path.join(
                os.path.expanduser("~"), ".local", "share", "seesea", "models"
            )
        models_dir = llm_dir
        local_model_file = os.path.join(models_dir, self.MODEL_FILENAME)

        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            if os.path.exists(local_model_file):
                print(f"ğŸ“ [Standard] ä½¿ç”¨å·²å­˜åœ¨æ¨¡å‹: {local_model_file}")
                model_path = local_model_file
            else:
                print("â¬‡ï¸  [Standard] ä¸‹è½½è½»é‡çº§åµŒå…¥æ¨¡å‹...")
                os.makedirs(models_dir, exist_ok=True)

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }

                try:
                    result = get_file(self.MODEL_URL, local_model_file, headers)
                    if result.get("status") != 200:
                        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.get('status')}")
                    print(f"âœ… [Standard] æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_file}")
                except Exception as e:
                    raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}") from e

                model_path = local_model_file

        # GPUé…ç½®
        n_gpu_layers = self._detect_gpu(device)

        # çº¿ç¨‹é…ç½®
        if n_threads is None:
            n_threads = max(1, os.cpu_count() or 4)
        self.n_threads = n_threads

        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ [Standard] åŠ è½½åµŒå…¥æ¨¡å‹...")
        try:
            from llama_cpp import Llama

            self.embedder = Llama(
                model_path=model_path,
                embedding=True,
                n_gpu_layers=n_gpu_layers,
                n_ctx=512,  # å°æ¨¡å‹ä½¿ç”¨è¾ƒå°ä¸Šä¸‹æ–‡
                n_threads=n_threads,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
            )

            # æµ‹è¯•è·å–ç»´åº¦
            if self.embedder is None:
                raise RuntimeError("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            # è°ƒç”¨ llama-cpp-python çš„ create_embedding æ–¹æ³•
            llama_embedder = cast(Any, self.embedder)
            test_result = llama_embedder.create_embedding(input="test")
            self.dimension = len(test_result["data"][0]["embedding"])
            print(f"âœ… [Standard] æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦: {self.dimension}")

        except Exception as e:
            # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ä¸”åŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°ä¸‹è½½
            if model_path == local_model_file and os.path.exists(local_model_file):
                print("âš ï¸ [Standard] æœ¬åœ°æ¨¡å‹æ–‡ä»¶æŸåï¼Œå°è¯•é‡æ–°ä¸‹è½½...")
                try:
                    os.remove(local_model_file)
                    print("â¬‡ï¸  [Standard] é‡æ–°ä¸‹è½½æ¨¡å‹...")

                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    }

                    result = get_file(self.MODEL_URL, local_model_file, headers)
                    if result.get("status") != 200:
                        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.get('status')}")

                    print(f"âœ… [Standard] æ¨¡å‹é‡æ–°ä¸‹è½½å®Œæˆ: {local_model_file}")

                    # é‡æ–°åŠ è½½æ¨¡å‹
                    print("ğŸ”„ [Standard] é‡æ–°åŠ è½½åµŒå…¥æ¨¡å‹...")
                    from llama_cpp import Llama

                    self.embedder = Llama(
                        model_path=local_model_file,
                        embedding=True,
                        n_gpu_layers=n_gpu_layers,
                        n_ctx=512,
                        n_threads=n_threads,
                        verbose=False,
                        use_mmap=True,
                        use_mlock=False,
                    )

                    # æµ‹è¯•è·å–ç»´åº¦
                    if self.embedder is None:
                        raise RuntimeError("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
                    llama_embedder = cast(Any, self.embedder)
                    test_result = llama_embedder.create_embedding(input="test")
                    self.dimension = len(test_result["data"][0]["embedding"])
                    print(f"âœ… [Standard] æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦: {self.dimension}")

                except Exception as download_e:
                    raise RuntimeError(
                        f"æ¨¡å‹é‡æ–°ä¸‹è½½åä»ç„¶åŠ è½½å¤±è´¥: {download_e}"
                    ) from download_e
            else:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e

    def _encode_llama(self, texts: List[str], **kwargs) -> List[List[float]]:
        """
        ä½¿ç”¨ llama-cpp-python ç¼–ç 

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å‘é‡åˆ—è¡¨
        """
        if self.embedder is None:
            raise RuntimeError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

        # è°ƒç”¨ llama-cpp-python çš„ embedding API
        try:
            llama_embedder = cast(Any, self.embedder)
            response = llama_embedder.create_embedding(
                input=texts, model=self.model_name, **kwargs
            )
            embeddings = [item["embedding"] for item in response["data"]]
            return embeddings

        except Exception as e:
            raise RuntimeError(f"ç¼–ç å¤±è´¥: {e}") from e

    def _detect_gpu(self, device: Optional[str]) -> int:
        """æ£€æµ‹GPUé…ç½®"""
        if device == "cuda":
            return -1
        elif device == "cpu":
            return 0
        else:
            # è‡ªåŠ¨æ£€æµ‹
            gpu_env_vars = [
                "CUDA_VISIBLE_DEVICES",
                "NVIDIA_VISIBLE_DEVICES",
                "CUDA_PATH",
            ]
            for var in gpu_env_vars:
                if os.environ.get(var):
                    return -1
            return 0

    def encode(
        self, texts: Union[str, List[str]], batch_size: int = 8
    ) -> Union[List[float], List[List[float]]]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ ‡å‡†æ¨¡å¼ä½¿ç”¨é€ä¸ªå¤„ç†ï¼‰

        Returns:
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        # å¦‚æœä½¿ç”¨ç®€å•å‘é‡åŒ–å™¨ï¼Œç›´æ¥è°ƒç”¨
        if not self._use_llama:
            return self.embedder.encode(texts, batch_size)  # type: ignore

        # ä½¿ç”¨ llama-cpp-python
        single_input = isinstance(texts, str)
        texts_to_process: List[str]
        if single_input:
            texts_to_process = [texts]  # type: ignore[list-item]
        else:
            texts_to_process = texts  # type: ignore[assignment]

        # è°ƒç”¨ llama ç¼–ç 
        embeddings = self._encode_llama(texts_to_process)

        if single_input and embeddings:
            return embeddings[0]
        return embeddings

    def create_embedding(self, text: str) -> List[float]:
        """
        åˆ›å»ºåµŒå…¥å‘é‡ï¼ˆå…¼å®¹æ¥å£ï¼‰

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        result = self.encode(text)
        if isinstance(result, list) and len(result) > 0 and isinstance(result[0], list):
            return result[0]
        return result  # type: ignore[return-value]

    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def encode_callback(self, text: str) -> List[float]:
        """
        Rustå›è°ƒæ¥å£

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        # å¦‚æœä½¿ç”¨ç®€å•å‘é‡åŒ–å™¨ï¼Œç›´æ¥è°ƒç”¨
        if not self._use_llama:
            return self.embedder.encode_callback(text)

        result = cast(List[float], self.encode(text))
        return result
