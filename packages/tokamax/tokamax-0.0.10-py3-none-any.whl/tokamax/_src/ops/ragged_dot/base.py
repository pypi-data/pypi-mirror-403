# Copyright 2025 DeepMind Technologies Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Ragged dot base class."""

from collections.abc import Callable, Sequence
import dataclasses
from typing import Any, TypeVar

import jax
from jax.experimental import checkify
import jax.numpy as jnp
import numpy as np
from pydantic_core import core_schema as cs
import qwix
from tokamax._src import precision as precision_lib
from tokamax._src import quantization
from tokamax._src.ops import op
from typing_extensions import override


_Config = TypeVar("_Config")
_Key = TypeVar("_Key")
Residuals = jax.Array | None
QArray = qwix.QArray
AsQArray = quantization.AsQArray
CanonicalPrecision = precision_lib.CanonicalPrecision
_DotAlgorithmLike = jax.lax.DotAlgorithm | jax.lax.DotAlgorithmPreset
ActivationFunction = Callable[[jax.Array], jax.Array] | None

DEFAULT_RAGGED_DOT_DIM_NUMS = jax.lax.RaggedDotDimensionNumbers(
    dot_dimension_numbers=(([1], [1]), ([], [])),
    lhs_ragged_dimensions=[0],
    rhs_group_dimensions=[0],
)
TRANS_RHS_RAGGED_DOT_DIM_NUMS = jax.lax.RaggedDotDimensionNumbers(
    dot_dimension_numbers=(([1], [2]), ([], [])),
    lhs_ragged_dimensions=[0],
    rhs_group_dimensions=[0],
)
RAGGED_CONTRACTING_DOT_DIM_NUMS = jax.lax.RaggedDotDimensionNumbers(
    dot_dimension_numbers=(([0], [0]), ([], [])),
    lhs_ragged_dimensions=[0],
    rhs_group_dimensions=[],
)


_STATIC = dataclasses.field(metadata=dict(static=True))


@jax.tree_util.register_dataclass
@dataclasses.dataclass(frozen=True, slots=True)
class GroupSizes:
  """A group sizes array with representative values.

  `ragged_dot` performance is sensitive to the distribution of the group sizes,
  but we cannot serialize the actual values (as they are runtime determined, and
  will vary from one step to the next). Instead, we serialize a representative
  distribution of group sizes. This allows `ragged_dot` to be benchmarked /
  autotuned with representative data.
  """

  value: jax.Array
  representative_value: Sequence[int] = _STATIC

  def __post_init__(self):
    (num_groups,) = self.value.shape
    representative_value = tuple(self.representative_value)

    if len(representative_value) != num_groups:
      raise ValueError(
          "Representative value must have the same length as the group sizes."
      )

    if not isinstance(self.value, jax.Array):
      value = np.asarray(representative_value, np.int32)
      object.__setattr__(self, "value", value)

    if not np.issubdtype(self.value.dtype, np.integer):
      raise ValueError("Group sizes must be integers.")

    object.__setattr__(self, "representative_value", representative_value)

  def __jax_array__(self):
    return self.value

  def __eq__(self, other) -> bool:
    return isinstance(other, GroupSizes) and (
        self.representative_value == other.representative_value
    )

  def __hash__(self) -> int:
    return hash(self.representative_value)

  @classmethod
  def __get_pydantic_core_schema__(cls, source, handler):
    del handler  # Unused.
    assert source is cls
    serialize = lambda x: x.representative_value
    validate = lambda x: cls(jax.ShapeDtypeStruct([len(x)], jnp.int32), x)  # pytype: disable=wrong-arg-types
    from_ints_schema = cs.chain_schema([
        cs.tuple_schema([cs.int_schema()], variadic_item_index=0),
        cs.no_info_plain_validator_function(validate),
    ])
    instance_schema = cs.is_instance_schema(cls)
    return cs.json_or_python_schema(
        json_schema=from_ints_schema,
        python_schema=cs.union_schema([instance_schema, from_ints_schema]),
        serialization=cs.plain_serializer_function_ser_schema(serialize),
    )


@dataclasses.dataclass(frozen=True)
class RaggedDot(op.Op[Any, jax.Array, Residuals, _Config, _Key]):
  """Ragged dot base class.

  For use in MegaBlocks-style models: https://arxiv.org/abs/2211.15841.
  """

  _: dataclasses.KW_ONLY
  checkify_group_sizes: bool = False

  @override
  def bind(
      self,
      lhs: jax.Array | QArray | AsQArray,
      rhs: jax.Array | QArray | AsQArray,
      *,
      group_sizes: jax.Array | GroupSizes | Sequence[int],
      ragged_dot_dimension_numbers: (
          jax.lax.RaggedDotDimensionNumbers | None
      ) = None,
      precision: jax.lax.PrecisionLike = None,
      preferred_element_type: jax.typing.DTypeLike | None = None,
      return_residuals: bool = False,
      activation: ActivationFunction | None = None,
  ) -> op.BoundArguments:
    if ragged_dot_dimension_numbers is None:
      # TODO: Support batch dims on LHS and/or RHS?
      ragged_dot_dimension_numbers = DEFAULT_RAGGED_DOT_DIM_NUMS

    if isinstance(group_sizes, (tuple, list)):
      group_sizes = tuple(group_sizes)
      group_sizes = GroupSizes(jnp.array(group_sizes, jnp.int32), group_sizes)

    # TODO: Create representative values for other ragged dot dim numbers.
    if ragged_dot_dimension_numbers == DEFAULT_RAGGED_DOT_DIM_NUMS:
      if not isinstance(group_sizes, GroupSizes):
        representative_sizes = (lhs.shape[0] // rhs.shape[0],) * rhs.shape[0]
        group_sizes = GroupSizes(group_sizes, representative_sizes)

      if self.checkify_group_sizes:
        gs = group_sizes.value
        dbg_check = checkify.debug_check
        dbg_check(jnp.all(gs >= 0), "Negative group size.")
        dbg_check(jnp.sum(gs) <= lhs.shape[0], "Group size sum > num rows.")

    if preferred_element_type is not None:
      preferred_element_type = jnp.dtype(preferred_element_type)
    return super().bind(
        lhs,
        rhs,
        group_sizes=group_sizes,
        ragged_dot_dimension_numbers=ragged_dot_dimension_numbers,
        precision=precision_lib.canonicalize_precision(precision),
        preferred_element_type=preferred_element_type,
        return_residuals=return_residuals,
        activation=activation,
    )

  @override
  def _fwd(
      self,
      lhs: jax.Array | QArray | AsQArray,
      rhs: jax.Array | QArray | AsQArray,
      *,
      group_sizes: jax.Array | GroupSizes,
      ragged_dot_dimension_numbers: jax.lax.RaggedDotDimensionNumbers,
      precision: CanonicalPrecision,
      preferred_element_type: jnp.dtype | None,
      return_residuals: bool,
      config: _Config,
      activation: ActivationFunction | None = None,
  ) -> tuple[jax.Array, Residuals]:
    del config  # Unused.

    lhs, rhs = map(quantization.as_array, (lhs, rhs))

    if isinstance(group_sizes, GroupSizes):
      group_sizes = jnp.array(group_sizes)

    # NOTE: `preferred_element_type` changes the accumulation type when using
    # `jax.lax.Precision`. It would be easier to always convert the precision to
    # `DotAlgorithmPreset`, but `ragged_dot_general` doesn't yet support
    # `DotAlgorithmPreset` (https://github.com/jax-ml/jax/issues/32207).
    # TODO: Remove once the above is fixed.
    out_dtype = preferred_element_type or jnp.result_type(lhs, rhs)
    if not isinstance(precision, _DotAlgorithmLike):
      is_integer = jnp.issubdtype(out_dtype, jnp.integer)
      acc_dtype = jnp.int32 if is_integer else jnp.float32
      preferred_element_type = jnp.promote_types(out_dtype, acc_dtype)
    dot_out = jax.lax.ragged_dot_general(
        lhs,
        rhs,
        group_sizes=group_sizes,
        ragged_dot_dimension_numbers=ragged_dot_dimension_numbers,
        precision=precision,
        preferred_element_type=preferred_element_type,
    ).astype(out_dtype)

    residuals = dot_out
    if activation is not None:
      dot_out = activation(dot_out)

    return dot_out, residuals if return_residuals else None


def vjp(
    residuals: Residuals,
    out: jax.Array,
    dout: jax.Array,
    lhs: jax.Array | AsQArray,
    rhs: jax.Array | AsQArray,
    *,
    group_sizes: jax.Array,
    ragged_dot_dimension_numbers: jax.lax.RaggedDotDimensionNumbers,
    precision: CanonicalPrecision,
    preferred_element_type: jnp.dtype | None,
    activation: ActivationFunction | None = None,
    dlhs_ragged_dot: Callable[..., jax.Array] = RaggedDot(),
    drhs_ragged_dot: Callable[..., jax.Array] = RaggedDot(),
) -> tuple[jax.Array, jax.Array]:
  """Ragged dot VJP."""
  del out, preferred_element_type  # Unused.

  if activation is not None:
    _, activation_grad_fn = jax.vjp(activation, residuals)
    (dout,) = activation_grad_fn(dout)

  dot_dim_nums = ragged_dot_dimension_numbers.dot_dimension_numbers
  lhs_ragged = ragged_dot_dimension_numbers.lhs_ragged_dimensions
  rhs_group = ragged_dot_dimension_numbers.rhs_group_dimensions
  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dot_dim_nums
  lhs_not_kept = (*lhs_batch, *lhs_contract)
  rhs_not_kept = (*rhs_batch, *rhs_contract, *rhs_group)
  lhs_kept = [i for i in range(lhs.ndim) if i not in lhs_not_kept]
  rhs_kept = [i for i in range(rhs.ndim) if i not in rhs_not_kept]

  assert len(lhs_batch) == len(rhs_batch)
  assert dout.ndim == len(lhs_batch) + len(lhs_kept) + len(rhs_kept)
  dout_batch = list(range(len(lhs_batch)))
  dout_lhs_kept = list(range(len(lhs_batch), len(lhs_batch) + len(lhs_kept)))
  dout_rhs_kept = list(range(len(lhs_batch) + len(lhs_kept), dout.ndim))

  dot_dim_nums = ((dout_rhs_kept, rhs_kept), (dout_batch, rhs_batch))
  dout_ragged = [(len(dout_batch) + lhs_kept.index(d)) for d in lhs_ragged]
  dlhs = dlhs_ragged_dot(
      dout,
      rhs,
      group_sizes=group_sizes,
      ragged_dot_dimension_numbers=jax.lax.RaggedDotDimensionNumbers(
          dot_dimension_numbers=dot_dim_nums,
          lhs_ragged_dimensions=dout_ragged,
          rhs_group_dimensions=rhs_group,
      ),
      precision=precision,
      preferred_element_type=lhs.dtype,
  )

  dot_dim_nums = ((lhs_kept, dout_lhs_kept), (lhs_batch, dout_batch))
  drhs = drhs_ragged_dot(
      lhs,
      dout,
      group_sizes=group_sizes,
      ragged_dot_dimension_numbers=jax.lax.RaggedDotDimensionNumbers(
          dot_dimension_numbers=dot_dim_nums,
          lhs_ragged_dimensions=lhs_ragged,
          rhs_group_dimensions=[],
      ),
      precision=precision,
      preferred_element_type=rhs.dtype,
  )
  return dlhs, drhs
