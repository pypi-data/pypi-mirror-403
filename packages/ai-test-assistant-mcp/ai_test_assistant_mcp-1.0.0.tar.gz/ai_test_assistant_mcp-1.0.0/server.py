"""
AI æµ‹è¯•åŠ©æ‰‹ MCP Server
æ”¯æŒ VS Code Copilotã€Cursorã€Cherry Studio ç­‰ MCP å®¢æˆ·ç«¯
"""

import os
import json
import warnings
from typing import Any

import httpx
from openai import OpenAI
from mcp.server.fastmcp import FastMCP

# æŠ‘åˆ¶è­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings("ignore", message="Unverified HTTPS request")

# åˆ›å»º MCP Server
mcp = FastMCP("AI Test Assistant")

# -------------------------------------------------------------
# LLM é…ç½®
# -------------------------------------------------------------
DEFAULT_LLM_TIMEOUT = 120

def get_provider_priority():
    return ["zhipu", "qwen", "deepseek", "kimi"]

provider_name_map = {
    "zhipu": "æ™ºè°± GLM",
    "qwen": "é€šä¹‰åƒé—®",
    "deepseek": "DeepSeek",
    "kimi": "Kimi",
}

def build_client(provider: str):
    try:
        if provider == "zhipu":
            api_key = os.getenv("ZHIPU_API_KEY")
            base_url = "https://open.bigmodel.cn/api/paas/v4"
            model = os.getenv("ZHIPU_MODEL_NAME", "glm-4-flash")
        elif provider == "qwen":
            api_key = os.getenv("QWEN_API_KEY")
            base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            model = os.getenv("QWEN_MODEL_NAME", "qwen-turbo")
        elif provider == "deepseek":
            api_key = os.getenv("DEEPSEEK_API_KEY")
            base_url = "https://api.deepseek.com/v1"
            model = os.getenv("DEEPSEEK_MODEL_NAME", "deepseek-chat")
        elif provider == "kimi":
            api_key = os.getenv("KIMI_API_KEY")
            base_url = "https://api.moonshot.cn/v1"
            model = os.getenv("KIMI_MODEL_NAME", "moonshot-v1-8k")
        else:
            return None, None

        if not api_key:
            return None, None

        http_client = httpx.Client(verify=False, timeout=DEFAULT_LLM_TIMEOUT)
        client = OpenAI(api_key=api_key, base_url=base_url, http_client=http_client)
        return client, model
    except Exception:
        return None, None


def call_llm(prompt: str, sys_prompt: str = "", temperature: float = 0.2, max_tokens: int = 2000) -> str:
    errors = []
    for provider in get_provider_priority():
        client, model = build_client(provider)
        if client is None or not model:
            continue
        try:
            messages = []
            if sys_prompt:
                messages.append({"role": "system", "content": sys_prompt})
            messages.append({"role": "user", "content": prompt})

            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            content = resp.choices[0].message.content.strip()
            return f"[{provider_name_map.get(provider, provider)}] {content}"
        except Exception as exc:
            errors.append(f"{provider}: {exc}")
            continue

    return f"ç”Ÿæˆå¤±è´¥: {'; '.join(errors)}" if errors else "æœªé…ç½® API Key"


SYSTEM_PROMPT = "You are a senior QA architect. Respond in Chinese."


# -------------------------------------------------------------
# MCP Tools (æŠ€èƒ½)
# -------------------------------------------------------------

@mcp.tool()
def generate_ac(user_story: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·æ•…äº‹ç”ŸæˆéªŒæ”¶æ ‡å‡†(AC)ï¼Œé‡‡ç”¨ BDD Given-When-Then æ ¼å¼ã€‚
    
    Args:
        user_story: ç”¨æˆ·æ•…äº‹æè¿°ï¼Œä¾‹å¦‚"ä½œä¸ºç”¨æˆ·ï¼Œæˆ‘å¸Œæœ›èƒ½å¤Ÿæœç´¢å•†å“"
    
    Returns:
        BDD æ ¼å¼çš„éªŒæ”¶æ ‡å‡†åˆ—è¡¨
    """
    prompt = f"""
åŸºäºä»¥ä¸‹ç”¨æˆ·æ•…äº‹ç”ŸæˆéªŒæ”¶æ ‡å‡† (AC)ï¼Œä¸¥æ ¼è¦æ±‚ï¼š

1) **å¿…é¡»**é‡‡ç”¨æ ‡å‡† BDD çš„ Given-When-Then ä¸‰æ®µå¼æ ¼å¼ï¼Œæ¯æ¡ AC æ ¼å¼å¦‚ä¸‹ï¼š
   AC-<ç¼–å·>: <ç®€çŸ­æ ‡é¢˜>
   Given: <å‰ç½®æ¡ä»¶/ä¸Šä¸‹æ–‡>
   When: <ç”¨æˆ·æ‰§è¡Œçš„åŠ¨ä½œ>
   Then: <ç³»ç»Ÿåº”äº§ç”Ÿçš„å¯éªŒè¯ç»“æœ>

2) AC éœ€éµå¾ª INVEST åŸåˆ™ï¼›
3) ç”Ÿæˆ 5-8 æ¡ ACï¼Œè¦†ç›–æ­£å‘åœºæ™¯ã€å¼‚å¸¸åœºæ™¯å’Œè¾¹ç•Œåœºæ™¯ï¼›
4) è¯·ä¿æŒä¸­æ–‡è¾“å‡ºã€‚

ç”¨æˆ·æ•…äº‹: {user_story}
""".strip()
    return call_llm(prompt, SYSTEM_PROMPT)


@mcp.tool()
def generate_test_cases(user_story: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·æ•…äº‹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œä½¿ç”¨ç­‰ä»·ç±»åˆ’åˆ†å’Œè¾¹ç•Œå€¼åˆ†ææ–¹æ³•ï¼ŒåŒ…å«ä¼˜å…ˆçº§æ ‡æ³¨ã€‚
    
    Args:
        user_story: ç”¨æˆ·æ•…äº‹æè¿°
    
    Returns:
        Markdown è¡¨æ ¼æ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…å« IDã€ä¼˜å…ˆçº§(P0/P1/P2)ã€æ ‡é¢˜ã€å‰ç½®æ¡ä»¶ã€æµ‹è¯•æ­¥éª¤ã€é¢„æœŸç»“æœ
    """
    prompt = f"""
ä¸ºä¸‹è¿°ç”¨æˆ·æ•…äº‹ç”Ÿæˆ 8-15 æ¡æµ‹è¯•ç”¨ä¾‹ï¼Œè¦æ±‚ï¼š

1) ä½¿ç”¨ç­‰ä»·ç±»åˆ’åˆ†å’Œè¾¹ç•Œå€¼åˆ†ææ–¹æ³•
2) ç”¨ Markdown è¡¨æ ¼å‘ˆç°ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š
   | ID | ä¼˜å…ˆçº§ | æ ‡é¢˜ | å‰ç½®æ¡ä»¶ | æµ‹è¯•æ­¥éª¤ | é¢„æœŸç»“æœ |
   
3) ä¼˜å…ˆçº§æ ‡æ³¨è§„åˆ™ï¼š
   - P0: æ ¸å¿ƒåŠŸèƒ½/é˜»å¡æ€§é—®é¢˜
   - P1: é‡è¦åŠŸèƒ½
   - P2: æ¬¡è¦åŠŸèƒ½/è¾¹ç•Œåœºæ™¯

4) è¯·ä¿æŒä¸­æ–‡è¾“å‡ºã€‚

ç”¨æˆ·æ•…äº‹: {user_story}
""".strip()
    return call_llm(prompt, SYSTEM_PROMPT)


@mcp.tool()
def generate_ui_automation(user_story: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·æ•…äº‹ç”Ÿæˆ UI è‡ªåŠ¨åŒ–æµ‹è¯•ä»£ç ï¼Œä½¿ç”¨ Python + Selenium + POM è®¾è®¡æ¨¡å¼ã€‚
    
    Args:
        user_story: ç”¨æˆ·æ•…äº‹æè¿°
    
    Returns:
        Python Selenium è‡ªåŠ¨åŒ–æµ‹è¯•ä»£ç 
    """
    prompt = f"""
ç”ŸæˆåŸºäº Python + Selenium çš„ UI è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼Œç”¨äºéªŒè¯ä¸‹è¿°ç”¨æˆ·æ•…äº‹ã€‚

**ä»£ç ç»“æ„è¦æ±‚**ï¼š
1. ä½¿ç”¨ Page Object Model (POM) è®¾è®¡æ¨¡å¼
2. åŒ…å« BasePage åŸºç±»å’Œå…·ä½“é¡µé¢ç±»
3. ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†æµè§ˆå™¨é©±åŠ¨
4. ä½¿ç”¨æ˜¾å¼ç­‰å¾… WebDriverWait
5. è‡³å°‘åŒ…å« 2-3 ä¸ªæµ‹è¯•æ–¹æ³•ï¼ˆæ­£å‘+å¼‚å¸¸ï¼‰
6. æ¯ä¸ªæµ‹è¯•æ–¹æ³•æœ‰ Given-When-Then æ³¨é‡Š

ç”¨æˆ·æ•…äº‹: {user_story}
""".strip()
    return call_llm(prompt, SYSTEM_PROMPT, max_tokens=3000)


@mcp.tool()
def generate_api_automation(user_story: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·æ•…äº‹ç”Ÿæˆæ¥å£è‡ªåŠ¨åŒ–æµ‹è¯•ä»£ç ï¼Œä½¿ç”¨ Python + requests + pytestã€‚
    
    Args:
        user_story: ç”¨æˆ·æ•…äº‹æè¿°
    
    Returns:
        Python pytest æ¥å£è‡ªåŠ¨åŒ–æµ‹è¯•ä»£ç 
    """
    prompt = f"""
ç”ŸæˆåŸºäº Python requests + pytest çš„æ¥å£è‡ªåŠ¨åŒ–æµ‹è¯•ä»£ç ï¼Œç”¨äºéªŒè¯ä¸‹è¿°ç”¨æˆ·æ•…äº‹ã€‚

**ä»£ç ç»“æ„è¦æ±‚**ï¼š
1. é…ç½®å±‚ï¼šbase_url, headers, token ç­‰é…ç½®
2. å·¥å…·å±‚ï¼šå°è£…é€šç”¨çš„ HTTP è¯·æ±‚æ–¹æ³•
3. æµ‹è¯•å±‚ï¼špytest æµ‹è¯•ç”¨ä¾‹

**æµ‹è¯•ç”¨ä¾‹è¦æ±‚**ï¼š
- è‡³å°‘ 5 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–æ­£å‘ã€åå‘ã€è¾¹ç•Œã€æƒé™åœºæ™¯
- æ¯ä¸ªç”¨ä¾‹ä½¿ç”¨ Given-When-Then æ³¨é‡Š
- ä½¿ç”¨æ¸…æ™°çš„æ–­è¨€æ¶ˆæ¯

ç”¨æˆ·æ•…äº‹: {user_story}
""".strip()
    return call_llm(prompt, SYSTEM_PROMPT, max_tokens=3000)


@mcp.tool()
def generate_all(user_story: str) -> str:
    """
    ä¸€é”®ç”Ÿæˆæ‰€æœ‰æµ‹è¯•èµ„äº§ï¼šACéªŒæ”¶æ ‡å‡†ã€æµ‹è¯•ç”¨ä¾‹ã€UIè‡ªåŠ¨åŒ–ä»£ç ã€æ¥å£è‡ªåŠ¨åŒ–ä»£ç ã€‚
    
    Args:
        user_story: ç”¨æˆ·æ•…äº‹æè¿°
    
    Returns:
        åŒ…å«æ‰€æœ‰ç”Ÿæˆå†…å®¹çš„å®Œæ•´æŠ¥å‘Š
    """
    results = []
    
    results.append("# ğŸ¯ AC éªŒæ”¶æ ‡å‡†\n")
    results.append(generate_ac(user_story))
    
    results.append("\n\n# ğŸ“ æµ‹è¯•ç”¨ä¾‹\n")
    results.append(generate_test_cases(user_story))
    
    results.append("\n\n# ğŸ–¥ï¸ UI è‡ªåŠ¨åŒ–ä»£ç \n")
    results.append(generate_ui_automation(user_story))
    
    results.append("\n\n# ğŸ”Œ æ¥å£è‡ªåŠ¨åŒ–ä»£ç \n")
    results.append(generate_api_automation(user_story))
    
    return "\n".join(results)


# -------------------------------------------------------------
# å¯åŠ¨ MCP Server
# -------------------------------------------------------------
if __name__ == "__main__":
    mcp.run()
