# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import datetime
import typing

import numpy
import numpy.typing

from diffid.sampler import DynamicNestedSampler, MetropolisHastings

@typing.final
class Adam:
    r"""
    Adaptive Moment Estimation (Adam) gradient-based optimiser.
    """
    def __new__(cls) -> Adam:
        r"""
        Create an Adam optimiser with library defaults.
        """
    def with_max_iter(self, max_iter: builtins.int) -> Adam:
        r"""
        Limit the maximum number of optimisation iterations.
        """
    def with_threshold(self, threshold: builtins.float) -> Adam:
        r"""
        Set the stopping threshold on the gradient norm.
        """
    def with_step_size(self, step_size: builtins.float) -> Adam:
        r"""
        Configure the base learning rate / step size.
        """
    def with_betas(self, beta1: builtins.float, beta2: builtins.float) -> Adam:
        r"""
        Override the exponential decay rates for the first and second moments.
        """
    def with_eps(self, eps: builtins.float) -> Adam:
        r"""
        Override the numerical stability constant added to the denominator.
        """
    def with_patience(self, patience: typing.Any) -> Adam:
        r"""
        Abort the run once the patience window has elapsed.

        Parameters
        ----------
        patience : float or timedelta
            Either seconds (float) or a timedelta object
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem using Adam starting from the provided point.
        """
    def init(
        self,
        initial: typing.Sequence[builtins.float],
        bounds: typing.Sequence[tuple[builtins.float, builtins.float]] | None = None,
    ) -> AdamState:
        r"""
        Initialize ask-tell optimization state.

        Returns an AdamState object that can be used for incremental optimization
        via the ask-tell interface.

        Parameters
        ----------
        initial : list[float]
            Initial parameter vector
        bounds : list[tuple[float, float]], optional
            Parameter bounds as [(lower, upper), ...]. If None, unbounded.

        Returns
        -------
        AdamState
            State object for ask-tell optimization

        Examples
        --------
        >>> optimiser = diffid.Adam()
        >>> state = optimiser.init(initial=[1.0, 2.0])
        >>> while True:
        ...     result = state.ask()
        ...     if isinstance(result, diffid.Done):
        ...         break
        ...     values = [evaluate_with_gradient(pt) for pt in result.points]
        ...     state.tell(values)
        """

@typing.final
class AdamState:
    r"""
    Ask-tell state for incremental Adam optimization.

    This state object allows step-by-step control over the optimization process.
    Use `ask()` to get points to evaluate, and `tell()` to provide results.

    Examples
    --------
    >>> optimiser = diffid.Adam().with_max_iter(100)
    >>> state = optimiser.init(initial=[1.0, 2.0])
    >>> while True:
    ...     result = state.ask()
    ...     if isinstance(result, diffid.Done):
    ...         print(f"Final result: {result.result}")
    ...         break
    ...     # Adam requires gradient information
    ...     values = [(f(pt), grad_f(pt)) for pt in result.points]
    ...     state.tell(values)
    """
    def ask(self) -> typing.Any:
        r"""
        Get the next action: evaluate points or optimization complete.

        Returns
        -------
        Evaluate | Done
            Either Evaluate(points) requiring function evaluations,
            or Done(result) indicating completion.

        Examples
        --------
        >>> result = state.ask()
        >>> if isinstance(result, diffid.Evaluate):
        ...     print(f"Need to evaluate {len(result.points)} points")
        >>> elif isinstance(result, diffid.Done):
        ...     print(f"Optimisation complete: {result.result}")
        """
    def tell(
        self, result: tuple[builtins.float, typing.Sequence[builtins.float]]
    ) -> None:
        r"""
        Provide evaluation results (value and gradient) for the requested points.

        Parameters
        ----------
        result : tuple[float, list[float]]
            Tuple of (value, gradient) where gradient is a list of partial derivatives.
            Adam requires gradient information.

        Raises
        ------
        TellError
            If called after optimization has terminated or if result format is invalid
        EvaluationError
            If the evaluation failed or contained invalid values

        Examples
        --------
        >>> result = state.ask()
        >>> if isinstance(result, diffid.Evaluate):
        ...     point = result.points[0]
        ...     value = objective(point)
        ...     gradient = compute_gradient(point)
        ...     state.tell((value, gradient))
        """
    def iterations(self) -> builtins.int:
        r"""
        Get the current iteration count.

        Returns
        -------
        int
            Number of iterations completed
        """
    def evaluations(self) -> builtins.int:
        r"""
        Get the total number of function evaluations.

        Returns
        -------
        int
            Number of function evaluations performed
        """
    def best(
        self,
    ) -> tuple[builtins.list[builtins.float], builtins.float] | None:
        r"""
        Get the current best point and value found so far.

        Returns
        -------
        tuple[list[float], float] | None
            (best_point, best_value) or None if no valid evaluations yet
        """
    def current_position(self) -> builtins.list[builtins.float]:
        r"""
        Get the current parameter position.

        Returns
        -------
        list[float]
            Current parameter vector
        """
    def __repr__(self) -> builtins.str: ...
    def __str__(self) -> builtins.str: ...

@typing.final
class CMAES:
    r"""
    Covariance Matrix Adaptation Evolution Strategy optimiser.
    """
    def __new__(cls) -> CMAES:
        r"""
        Create a CMA-ES optimiser with library defaults.
        """
    def with_max_iter(self, max_iter: builtins.int) -> CMAES:
        r"""
        Limit the number of iterations/generations before termination.
        """
    def with_threshold(self, threshold: builtins.float) -> CMAES:
        r"""
        Set the stopping threshold on the best objective value.
        """
    def with_step_size(self, step_size: builtins.float) -> CMAES:
        r"""
        Set the initial global step-size (standard deviation).
        """
    def with_patience(self, patience: typing.Any) -> CMAES:
        r"""
        Abort the run if no improvement occurs for the given wall-clock duration.

        Parameters
        ----------
        patience : float or timedelta
            Either seconds (float) or a timedelta object
        """
    def with_population_size(self, population_size: builtins.int) -> CMAES:
        r"""
        Specify the number of offspring evaluated per generation.
        """
    def with_seed(self, seed: builtins.int) -> CMAES:
        r"""
        Initialise the internal RNG for reproducible runs.
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem starting from the provided mean vector.
        """
    def init(
        self,
        initial: typing.Sequence[builtins.float],
        bounds: typing.Sequence[tuple[builtins.float, builtins.float]] | None = None,
    ) -> CMAESState:
        r"""
        Initialize ask-tell optimization state.

        Returns a CMAESState object that can be used for incremental optimization
        via the ask-tell interface.

        Parameters
        ----------
        initial : list[float]
            Initial mean vector for the search distribution
        bounds : list[tuple[float, float]], optional
            Parameter bounds as [(lower, upper), ...]. If None, unbounded.

        Returns
        -------
        CMAESState
            State object for ask-tell optimization

        Examples
        --------
        >>> optimiser = diffid.CMAES()
        >>> state = optimiser.init(initial=[1.0, 2.0])
        >>> while True:
        ...     result = state.ask()
        ...     if isinstance(result, diffid.Done):
        ...         break
        ...     values = [evaluate(pt) for pt in result.points]
        ...     state.tell(values)
        """

@typing.final
class CMAESState:
    r"""
    Ask-tell state for incremental CMA-ES optimization.

    This state object allows step-by-step control over the optimization process.
    Use `ask()` to get a population of points to evaluate, and `tell()` to provide results.

    Examples
    --------
    >>> optimiser = diffid.CMAES().with_max_iter(100)
    >>> state = optimiser.init(initial=[1.0, 2.0])
    >>> while True:
    ...     result = state.ask()
    ...     if isinstance(result, diffid.Done):
    ...         print(f"Final result: {result.result}")
    ...         break
    ...     values = [f(pt) for pt in result.points]
    ...     state.tell(values)
    """
    def ask(self) -> typing.Any:
        r"""
        Get the next action: evaluate points or optimization complete.

        Returns
        -------
        Evaluate | Done
            Either Evaluate(points) requiring function evaluations,
            or Done(result) indicating completion.

        Notes
        -----
        CMA-ES evaluates a population of points each iteration. The number
        of points returned depends on the population_size setting.
        """
    def tell(self, results: typing.Sequence[builtins.float]) -> None:
        r"""
        Provide evaluation results for the requested population of points.

        Parameters
        ----------
        results : list[float]
            List of objective function values corresponding to the points
            from the last ask() call. Must match the number of points.

        Raises
        ------
        TellError
            If called after optimization has terminated or if wrong number
            of results provided
        EvaluationError
            If evaluations failed or contained invalid values
        """
    def iterations(self) -> builtins.int:
        r"""
        Get the current iteration (generation) count.

        Returns
        -------
        int
            Number of generations completed
        """
    def evaluations(self) -> builtins.int:
        r"""
        Get the total number of function evaluations.

        Returns
        -------
        int
            Number of function evaluations performed
        """
    def best(
        self,
    ) -> tuple[builtins.list[builtins.float], builtins.float] | None:
        r"""
        Get the current best point and value found so far.

        Returns
        -------
        tuple[list[float], float] | None
            (best_point, best_value) or None if no valid evaluations yet
        """
    def mean(self) -> builtins.list[builtins.float]:
        r"""
        Get the current mean of the search distribution.

        Returns
        -------
        list[float]
            Current mean vector
        """
    def sigma(self) -> builtins.float:
        r"""
        Get the current step size (sigma).

        Returns
        -------
        float
            Current global step size
        """
    def __repr__(self) -> builtins.str: ...
    def __str__(self) -> builtins.str: ...

@typing.final
class CostMetric:
    @property
    def name(self) -> builtins.str:
        r"""
        Name of the cost metric.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class DiffsolBuilder:
    r"""
    Differential equation solver builder.
    """
    def __new__(cls) -> DiffsolBuilder:
        r"""
        Create an empty differential solver builder.
        """
    def __copy__(self) -> DiffsolBuilder: ...
    def __deepcopy__(self, _memo: dict) -> DiffsolBuilder: ...
    def with_diffsl(self, dsl: builtins.str) -> DiffsolBuilder:
        r"""
        Register the DiffSL program describing the system dynamics.
        """
    def with_data(self, data: numpy.typing.NDArray[numpy.float64]) -> DiffsolBuilder:
        r"""
        Attach observed data used to fit the differential equation.

        The first column must contain the time samples (t_span) and the remaining
        columns the observed trajectories.
        """
    def remove_data(self) -> DiffsolBuilder:
        r"""
        Remove any previously attached data along with its time span.
        """
    def with_backend(self, backend: builtins.str) -> DiffsolBuilder:
        r"""
        Choose whether to use dense or sparse diffusion solvers.
        """
    def with_parallel(self, parallel: builtins.bool | None = None) -> DiffsolBuilder:
        r"""
        Opt into parallel proposal generation when supported by the backend.
        """
    def with_config(
        self, config: typing.Mapping[builtins.str, builtins.float]
    ) -> DiffsolBuilder: ...
    def with_tolerances(
        self, rtol: builtins.float, atol: builtins.float
    ) -> DiffsolBuilder:
        r"""
        Adjust the relative and absolute integration tolerances.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> DiffsolBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def clear_parameters(self) -> DiffsolBuilder:
        r"""
        Clear all previously registered parameters while preserving other configuration.
        """
    def with_cost(self, cost: CostMetric) -> DiffsolBuilder:
        r"""
        Select the error metric used to compare simulated and observed data.
        """
    def remove_cost(self) -> DiffsolBuilder:
        r"""
        Reset the cost metric to the default sum of squared errors.
        """
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> DiffsolBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimise` omits one.
        """
    def build(self) -> Problem:
        r"""
        Create a `Problem` representing the differential solver model.
        """

@typing.final
class Done:
    r"""
    Optimisation/sampling is complete with final results.

    This is returned by `ask()` when the algorithm has terminated.
    Access the results via the `result` attribute.

    Examples
    --------
    >>> while True:
    ...     result = state.ask()
    ...     if isinstance(result, diffid.Done):
    ...         print(f"Optimisation complete: {result.result}")
    ...         break
    """
    @property
    def result(self) -> typing.Any: ...
    def __repr__(self) -> builtins.str: ...
    def __str__(self) -> builtins.str: ...

@typing.final
class Evaluate:
    r"""
    Request to evaluate objective function at specific points.

    This is returned by `ask()` when the optimiser/sampler needs function
    evaluations. Call `tell()` with the results after evaluation.

    Examples
    --------
    >>> state = optimiser.init(problem, initial=[1.0, 2.0])
    >>> result = state.ask()
    >>> if isinstance(result, diffid.Evaluate):
    ...     values = [problem.evaluate(pt) for pt in result.points]
    ...     state.tell(values)
    """
    @property
    def points(self) -> builtins.list[builtins.list[builtins.float]]: ...
    def __new__(
        cls, points: typing.Sequence[typing.Sequence[builtins.float]]
    ) -> Evaluate: ...
    def __repr__(self) -> builtins.str: ...
    def __str__(self) -> builtins.str: ...

@typing.final
class NelderMead:
    r"""
    Classic simplex-based direct search optimiser.
    """
    def __new__(cls) -> NelderMead:
        r"""
        Create a Nelder-Mead optimiser with default coefficients.
        """
    def with_step_size(self, step_size: builtins.float) -> NelderMead:
        r"""
        Set the initial global step-size (standard deviation).
        """
    def with_max_iter(self, max_iter: builtins.int) -> NelderMead:
        r"""
        Limit the number of simplex iterations.
        """
    def with_threshold(self, threshold: builtins.float) -> NelderMead:
        r"""
        Set the stopping threshold on simplex size or objective reduction.
        """
    def with_position_tolerance(self, tolerance: builtins.float) -> NelderMead:
        r"""
        Stop once simplex vertices fall within the supplied positional tolerance.
        """
    def with_max_evaluations(self, max_evaluations: builtins.int) -> NelderMead:
        r"""
        Abort after evaluating the objective `max_evaluations` times.
        """
    def with_coefficients(
        self,
        alpha: builtins.float,
        gamma: builtins.float,
        rho: builtins.float,
        sigma: builtins.float,
    ) -> NelderMead:
        r"""
        Override the reflection, expansion, contraction, and shrink coefficients.
        """
    def with_patience(self, patience: typing.Any) -> NelderMead:
        r"""
        Abort if the objective fails to improve within the allotted time.

        Parameters
        ----------
        patience : float or timedelta
            Either seconds (float) or a timedelta object
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem starting from the provided initial simplex centre.
        """
    def init(
        self,
        initial: typing.Sequence[builtins.float],
        bounds: typing.Sequence[tuple[builtins.float, builtins.float]] | None = None,
    ) -> NelderMeadState:
        r"""
        Initialize ask-tell optimization state.

        Returns a NelderMeadState object that can be used for incremental optimization
        via the ask-tell interface.

        Parameters
        ----------
        initial : list[float]
            Initial parameter vector (simplex center)
        bounds : list[tuple[float, float]], optional
            Parameter bounds as [(lower, upper), ...]. If None, unbounded.

        Returns
        -------
        NelderMeadState
            State object for ask-tell optimization

        Examples
        --------
        >>> optimiser = diffid.NelderMead()
        >>> state = optimiser.init(initial=[1.0, 2.0])
        >>> while True:
        ...     result = state.ask()
        ...     if isinstance(result, diffid.Done):
        ...         break
        ...     values = [evaluate(pt) for pt in result.points]
        ...     state.tell(values)
        """

@typing.final
class NelderMeadState:
    r"""
    Ask-tell state for incremental Nelder-Mead optimization.

    This state object allows step-by-step control over the optimization process.
    Use `ask()` to get points to evaluate, and `tell()` to provide results.

    Examples
    --------
    >>> optimiser = diffid.NelderMead().with_max_iter(100)
    >>> state = optimiser.init(initial=[1.0, 2.0])
    >>> while True:
    ...     result = state.ask()
    ...     if isinstance(result, diffid.Done):
    ...         print(f"Final result: {result.result}")
    ...         break
    ...     values = [f(pt) for pt in result.points]
    ...     state.tell(values)
    """
    def ask(self) -> typing.Any:
        r"""
        Get the next action: evaluate points or optimization complete.

        Returns
        -------
        Evaluate | Done
            Either Evaluate(points) requiring function evaluations,
            or Done(result) indicating completion.
        """
    def tell(self, result: builtins.float) -> None:
        r"""
        Provide evaluation result (scalar value) for the requested point.

        Parameters
        ----------
        result : float
            Scalar objective function value

        Raises
        ------
        TellError
            If called after optimization has terminated
        EvaluationError
            If the evaluation failed or contained invalid values
        """
    def iterations(self) -> builtins.int:
        r"""
        Get the current iteration count.

        Returns
        -------
        int
            Number of iterations completed
        """
    def evaluations(self) -> builtins.int:
        r"""
        Get the total number of function evaluations.

        Returns
        -------
        int
            Number of function evaluations performed
        """
    def best(
        self,
    ) -> tuple[builtins.list[builtins.float], builtins.float] | None:
        r"""
        Get the current best point and value from the simplex.

        Returns
        -------
        tuple[list[float], float] | None
            (best_point, best_value) or None if no valid evaluations yet
        """
    def __repr__(self) -> builtins.str: ...
    def __str__(self) -> builtins.str: ...

@typing.final
class NestedSamplesIterator:
    r"""
    Iterator for NestedSamples posterior
    """
    def __iter__(self) -> NestedSamplesIterator: ...
    def __next__(
        self,
    ) -> (
        tuple[builtins.list[builtins.float], builtins.float, builtins.float] | None
    ): ...

@typing.final
class OptimisationResults:
    r"""
    Container for optimiser outputs and diagnostic metadata.
    """
    @property
    def x(self) -> numpy.typing.NDArray[numpy.float64]:
        r"""
        Decision vector corresponding to the best-found objective value.

        Returns
        -------
        numpy.ndarray
            Best parameter vector as a NumPy array
        """
    @property
    def value(self) -> builtins.float:
        r"""
        Objective value evaluated at `x`.
        """
    @property
    def iterations(self) -> builtins.int:
        r"""
        Number of iterations performed by the optimiser.
        """
    @property
    def evaluations(self) -> builtins.int:
        r"""
        Total number of objective function evaluations.
        """
    @property
    def time(self) -> datetime.timedelta:
        r"""
        Total number of objective function evaluations.
        """
    @property
    def success(self) -> builtins.bool:
        r"""
        Whether the run satisfied its convergence criteria.
        """
    @property
    def message(self) -> builtins.str:
        r"""
        Human-readable status message summarising the termination state.
        """
    @property
    def termination_reason(self) -> builtins.str:
        r"""
        Structured termination flag describing why the run ended.
        """
    @property
    def final_simplex(self) -> builtins.list[builtins.list[builtins.float]]:
        r"""
        Simplex vertices at termination, when provided by the optimiser.
        """
    @property
    def final_simplex_values(self) -> builtins.list[builtins.float]:
        r"""
        Objective values corresponding to `final_simplex`.
        """
    @property
    def covariance(
        self,
    ) -> builtins.list[builtins.list[builtins.float]] | None:
        r"""
        Estimated covariance of the search distribution, if available.
        """
    def __repr__(self) -> builtins.str:
        r"""
        Render a concise summary of the optimisation outcome.
        """
    def __str__(self) -> builtins.str:
        r"""
        Return a human-readable summary of the result.
        """
    def __bool__(self) -> builtins.bool:
        r"""
        Return truthiness based on optimization success.

        Allows using `if result:` instead of `if result.success:`.
        """

@typing.final
class Problem:
    r"""
    Executable optimisation problem wrapping the Diffid core implementation.
    """
    def evaluate(self, x: typing.Sequence[builtins.float]) -> builtins.float:
        r"""
        Evaluate the configured objective function at `x`.
        """
    def evaluate_gradient(
        self, x: typing.Sequence[builtins.float]
    ) -> builtins.list[builtins.float] | None:
        r"""
        Evaluate the gradient of the objective function at `x` if available.
        """
    def optimise(
        self,
        initial: typing.Sequence[builtins.float] | None = None,
        optimiser: NelderMead | CMAES | Adam | None = None,
    ) -> OptimisationResults:
        r"""
        Solve the problem starting from `initial` using the supplied optimiser.
        """
    def sample(
        self,
        initial: typing.Sequence[builtins.float] | None = None,
        sampler: MetropolisHastings | DynamicNestedSampler | None = None,
    ) -> typing.Any:
        r"""
        Sample from the problem starting from `initial` using the supplied sampler.
        """
    def get_config(self, _key: builtins.str) -> builtins.float | None:
        r"""
        Return the numeric configuration value stored under `key` if present.
        """
    def dimension(self) -> builtins.int:
        r"""
        Return the number of parameters the problem expects.
        """
    def bounds(self) -> builtins.list[tuple[builtins.float, builtins.float]]:
        r"""
        Return the parameter bounds for the problem as a list of (lower, upper) tuples.
        """
    def parameters(
        self,
    ) -> builtins.list[
        tuple[
            builtins.str,
            builtins.float,
            tuple[builtins.float, builtins.float] | None,
        ]
    ]: ...
    def initial_values(self) -> builtins.list[builtins.float]: ...
    def default_parameters(self) -> builtins.list[builtins.float]:
        r"""
        Return the default parameter vector implied by the builder.
        """
    def config(self) -> builtins.dict[builtins.str, builtins.float]:
        r"""
        Return a copy of the problem configuration dictionary.
        """
    def __call__(self, x: typing.Sequence[builtins.float]) -> builtins.float:
        r"""
        Call the problem as a function (shorthand for evaluate).

        Allows using `problem(x)` instead of `problem.evaluate(x)`.
        """
    def __repr__(self) -> builtins.str:
        r"""
        Return a detailed string representation of the problem.
        """
    def __str__(self) -> builtins.str:
        r"""
        Return a concise string representation of the problem.
        """

@typing.final
class SamplesIterator:
    r"""
    Iterator for Samples chains
    """
    def __iter__(self) -> SamplesIterator: ...
    def __next__(
        self,
    ) -> builtins.list[builtins.list[builtins.float]] | None: ...

@typing.final
class ScalarBuilder:
    r"""
    High-level builder for optimisation `Problem` instances exposed to Python.
    """
    def __new__(cls) -> ScalarBuilder:
        r"""
        Create an empty builder with no objective, parameters, or default optimiser.
        """
    def __copy__(self) -> ScalarBuilder: ...
    def __deepcopy__(self, _memo: dict) -> ScalarBuilder: ...
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> ScalarBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimise` omits one.
        """
    def with_objective(self, obj: typing.Any) -> ScalarBuilder:
        r"""
        Attach the objective function callable executed during optimisation.
        """
    def with_gradient(self, obj: typing.Any) -> ScalarBuilder:
        r"""
        Attach the gradient callable returning derivatives of the objective.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> ScalarBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def build(self) -> Problem:
        r"""
        Finalize the builder into an executable `Problem`.
        """

@typing.final
class VectorBuilder:
    r"""
    Time-series problem builder for vector-valued objectives.
    """
    def __new__(cls) -> VectorBuilder:
        r"""
        Create an empty vector problem builder.
        """
    def __copy__(self) -> VectorBuilder: ...
    def __deepcopy__(self, _memo: dict) -> VectorBuilder: ...
    def with_objective(self, objective: typing.Any) -> VectorBuilder:
        r"""
        Register a callable that produces predictions matching the data shape.

        The callable should accept a parameter vector and return a numpy array
        of the same shape as the observed data.
        """
    def with_data(self, data: numpy.typing.NDArray[numpy.float64]) -> VectorBuilder:
        r"""
        Attach observed data used to fit the model.

        The data should be a 1D numpy array. The shape will be inferred
        from the data length.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> VectorBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def with_cost(self, cost: CostMetric) -> VectorBuilder:
        r"""
        Select the error metric used to compare predictions and observed data.
        """
    def remove_cost(self) -> VectorBuilder:
        r"""
        Reset the cost metric to the default sum of squared errors.
        """
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> VectorBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimise` omits one.
        """
    def with_config(self, key: builtins.str, value: builtins.float) -> VectorBuilder:
        r"""
        Attach an arbitrary configuration value to the problem.
        """
    def build(self) -> Problem:
        r"""
        Create a `Problem` representing the vector optimisation model.
        """

def GaussianNLL(
    variance: builtins.float, weight: builtins.float = 1.0
) -> CostMetric: ...
def RMSE(weight: builtins.float = 1.0) -> CostMetric: ...
def SSE(weight: builtins.float = 1.0) -> CostMetric: ...
def builder_factory_py() -> ScalarBuilder:
    r"""
    Return a convenience factory for creating `Builder` instances.
    """
