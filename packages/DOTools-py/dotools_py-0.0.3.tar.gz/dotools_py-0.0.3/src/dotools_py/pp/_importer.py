import os
import subprocess
import uuid
from datetime import date
from pathlib import Path
from beartype import beartype
from beartype.typing import Literal, Dict
import anndata as ad
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from dotools_py import logger
from dotools_py.utils import convert_path, get_paths_utils


def _qc_vln(
    adata: ad.AnnData,
    title: str = "ViolinPlots - Quality Metrics",
    path: str | Path = None,
    filename: str = "ViolinPlots.png",
    stats: list = ("total_counts", "n_genes_by_counts", "pct_counts_mt"),
    colors: str | list = "lightsteelblue",
) -> None:
    """Violin Plots showing basic QC stats.

    Generate ViolinPlots to show the distribution of total counts, number of genes and percentage of
    mitochondrial genes.

    :param adata: annotated dt matrix.
    :param title: title of the Plot.
    :param path: path to figure folder.
    :param filename: name of the file.
    :param stats: `.obs` column name to plot.
    :param colors: colors for the violinplots.
    :return:
    """
    if isinstance(stats, tuple):
        stats = list(stats)

    assert all(col in list(adata.obs.columns) for col in stats), "column name in col_obs missing in adata.obs"
    assert len(stats) == 3, "Expected 3 variables to plot: total_counts, n_genes_by_counts, pct_counts_mt"

    if isinstance(colors, str):
        colors = [colors]
    if len(colors) == 1:
        colors = colors * 3

    fig, axs = plt.subplots(1, 3, figsize=(10, 6))
    for idx in range(3):
        vln = sns.violinplot(adata.obs[stats[idx]], ax=axs[idx], color=colors[idx])
        vln.set_xticklabels([f"Median = {np.floor(np.median(adata.obs[stats[idx]]))}"], fontweight="bold")
        vln.set_title("")
    plt.suptitle(title, fontsize=30, fontweight="bold")

    if path is not None:
        plt.savefig(convert_path(path) / filename, bbox_inches="tight")
        plt.close(fig)
    else:
        return


def _filter_quantiles(
    adata: ad.AnnData,
    low: int | None = None,
    high: int | None = None,
) -> ad.AnnData:
    """Filter cells based on total nUMI counts using quantiles.

    :param adata: annotated dt matrix
    :param low: lower quantile
    :param high: upper quantile
    :return: annotated dt matrix
    """
    counts = adata.obs["total_counts"]
    mask = np.ones(adata.n_obs, dtype=bool)
    if low:
        mask &= counts > np.percentile(counts, low)
    if high:
        mask &= counts < np.percentile(counts, high)
    return adata[mask, :].copy()


# def _run_scdblfinder(
#     adata: ad.AnnData,
#     batch_key: str | None = None,
# ) -> None:
#     """Find doublets.
#
#     The inference is performed using `scDblFinder <https://github.com/plger/scDblFinder>`_ in R.
#
#     :param adata: annotated anndata matrix
#     :param batch_key: `.obs` column name with batch information. Required if the anndata contain more than 1 sample.
#     :return:
#     """
#     import polars
#
#     logger.info("Finding Neotypic doublets")
#     rscript = get_paths_utils("_run_scDblFinder.R")
#     tmpdir_path = Path("/tmp") / f"scDblFinder_{uuid.uuid4().hex}"
#     tmpdir_path.mkdir(parents=True, exist_ok=False)
#     adata.write(tmpdir_path / "adata_tmp.h5ad")
#
#     logger.info("Running scDblFinder")
#     cmd = ["Rscript", rscript, "--input=" + str(tmpdir_path) + "/adata_tmp.h5ad", "--out=" + str(tmpdir_path) + "/"]
#     if batch_key:
#         cmd += ["--name=" + batch_key]
#     subprocess.call(cmd)
#
#     doublets = polars.read_csv(tmpdir_path / "scDblFinder_inference.csv", infer_schema_length=0)
#     doublets = doublets.to_pandas()
#     doublets = doublets.set_index(adata.obs_names)  # Avoid ImplicitModificationWarning
#     adata.obs[["doublet_class", "doublet_score"]] = doublets.values
#     shutil.rmtree(tmpdir_path)
#     return


@beartype
def find_doublets(
    adata: ad.AnnData | pd.DataFrame,
    batch_key: str | None = None,
    cluster_key: str | bool | None = None,
    doublet_rate: int = None,
    scdblfinder_metric: Literal['merror', 'logloss', 'auc', 'aucpr'] = "logloss",
    method: Literal["scDblFinder", "DoubletDetection", "Scrublet", "Ovrlpy"] = "scDblFinder",
    ovrlpy_keys: Dict = None,
    ovrlpy_report_path: str | Path = None,
    random_state: int = 0,
) -> None:
    """Detect doublets in scRNAseq and iST.

    Parameters
    ----------
    adata:
        Annotated data matrix or a pandas DataFrame if method is set to `Ovrlpy`.
    batch_key
        Column in `adata.obs` with batch information. If omitted, doublets will be searched for with all cells together.
        If given, doublets will be searched for independently for each sample, which is preferable if they represent
        different captures.
    cluster_key
        Column in `adata.obs` with clustering information. This is used to make doublets more efficiently.
        Alternatively, if `cluster_key=True`, fast clustering will be performed. If `cluster_key` is None or False,
        purely random artificial doublets will be generated.
    doublet_rate
        The expected doublet rate, i.e. the proportion of the cells expected to be doublets.
        If omitted, will be calculated automatically for scDblFinder and will be set to 0.05 for Scrublet.
    scdblfinder_metric
        Error metric to optimize during training (e.g. 'merror', 'logloss', 'auc', 'aucpr').
    method
        Library to use for detecting doublets. For scRNA-seq data the available methods are:
        `scDblFinder <https://f1000research.com/articles/10-979/v2>`_,
        `DoubletDetection <https://zenodo.org/records/14827937>`_, and
        `Scrublet <https://www.sciencedirect.com/science/article/pii/S2405471218304745>`_.
        For Spatial Transcriptomics at single cell resolution, like Xenium the avaialble methods are:
        `Ovrlpy <https://ovrlpy.readthedocs.io/latest/>`_ (Allow the detection of vertical doublets in image based ST).
    ovrlpy_keys
        Dictionary with the following keys: `gene_key`, `x_key`, `y_key` and `z_key` indicating the name of the column
        in the dataframe with the gene names and the x, y and z coordinate.
    ovrlpy_report_path
        Directory where the quality control plots and the ovrlpy object will be saved.
    random_state
        Seed for random number generator

    Returns
    -------
    None
        Returns `None`. Sets the following fields:

    `adata.obs['doublet_class']` : :class:`pandas.Series` (dtype `str`)
        Class indicating predicted doublet status
    `adata.obs['doublet_score']` : :class:`pandas.Series` (dtype `float`)
        Doublet scores for each observed transcriptome

    Examples
    --------
    >>> import dotools_py as do
    >>> adata = do.dt.example_10x_processed()
    >>> find_doublets(adata, batch_key="batch", method="scDblFinder")
    >>> adata.obs[["doublet_class", "doublet_score"]].head()
                                  doublet_class doublet_score
    CAAAGAATCAGATTGC-1-batch2       singlet      0.692706
    AGCTTCCCAGTCAACT-1-batch1       singlet      0.014858
    GAGAGGTTCCCTCTAG-1-batch1       singlet      0.172094
    CTAACTTCAGATCATC-1-batch1       singlet      0.092695
    CATGGTACAAACGGCA-1-batch1       singlet      0.237514

    """

    def py_none_to_r(obj):
        if obj is None:
            return r("NULL")  # evaluated at conversion time
        return obj

    if method == "scDblFinder":
        import anndata2ri
        from rpy2.robjects import r, conversion, globalenv, pandas2ri
        none_converter = conversion.Converter("None converter")
        none_converter.py2rpy.register(type(None), py_none_to_r)
        adata_copy = adata.copy()
        del adata_copy.raw, adata_copy.uns
        with conversion.localconverter(anndata2ri.converter + none_converter + pandas2ri.converter):
            r.assign("adata", adata_copy)
            r.assign("batch", batch_key)
            r.assign("cluster", cluster_key)
            r.assign("dbr", doublet_rate)
            r.assign("metric", scdblfinder_metric)
            r.assign("random_state", random_state)
            r(
                """
                library(scDblFinder)
                if (!suppressPackageStartupMessages(require(SingleCellExperiment))) {
                    stop("R dependecy SingleCellExperiment not found.")
                }
                set.seed(random_state)
                sce <- as(adata, "SingleCellExperiment")
                sce <- scDblFinder(sce, samples = batch, clusters=cluster, dbr=dbr, metric=metric, verbose = F)
                df <- data.frame(
                    scDblFinder.class = as.character(colData(sce)$scDblFinder.class),
                    scDblFinder.score = as.numeric(colData(sce)$scDblFinder.score),
                    stringsAsFactors = FALSE
                )
                """
            )
            doublets = globalenv["df"]
        doublets = doublets.set_index(adata.obs_names)
        adata.obs[["doublet_class", "doublet_score"]] = doublets.values
    elif method == "DoubletDetection":
        import doubletdetection
        clf = doubletdetection.BoostClassifier(
            n_iters=15, clustering_algorithm="leiden", standard_scaling=True, verbose=False, n_jobs=-1,
            random_state=random_state,
        )
        doublets = clf.fit(adata.X).predict()
        doublet_score = clf.doublet_score()
        mapped = np.full(doublets.shape, "singlet", dtype=object)
        mapped[doublets == 1.0] = "doublet"
        adata.obs["doublet_class"] = pd.Categorical(mapped, categories=["singlet", "doublet"])
        adata.obs["doublet_score"] = doublet_score
    elif method == "Scrublet":
        from scanpy.preprocessing import scrublet
        expected_doublet_rate = doublet_rate if doublet_rate is not None else 0.05
        scrublet(adata, expected_doublet_rate=expected_doublet_rate, random_state=random_state)
        adata.obs["doublet_class"] = adata.obs["predicted_doublet"].map({False: "singlet", True: "doublet"})
        del adata.obs["predicted_doublet"]
    elif method == "Ovrlpy":
        assert isinstance(adata, pd.DataFrame), ("To run Ovrlpy (Detection of doublets in scSpatialTranscriptomics "
                                                 "provide a DataFrame with X,Y,Z coordinates for features.")
        assert batch_key is None, "Ovrlpy cannot perform doublet detection across batches"
        assert ovrlpy_report_path is not None, "Provide path to save the report from the Ovrlpy inference"
        import ovrlpy
        import pickle
        available_cores = int(os.cpu_count() / 2)
        ovrlpy_keys = {} if ovrlpy_keys is None else ovrlpy_keys
        gene_key, x_key, y_key, z_key = (ovrlpy_keys.get("gene_key", "feature_name"),
                                         ovrlpy_keys.get("x_key", "x_location"),
                                         ovrlpy_keys.get("y_key", "y_location"),
                                         ovrlpy_keys.get("z_key", "z_location"))

        data = ovrlpy.Ovrlp(
            adata, n_workers=available_cores, random_state=random_state, gene_key=gene_key,
            coordinate_keys=(x_key, y_key, z_key),
        )
        data.analyse()

        # Save results in the report folder
        logger.info("Generating Report")
        os.makedirs(ovrlpy_report_path, exist_ok=True)
        _ = ovrlpy.plot_pseudocells(data)
        plt.savefig(convert_path(ovrlpy_report_path) / "Overview_Ovrlpy.pdf", bbox_inches="tight")
        plt.close()
        _ = ovrlpy.plot_signal_integrity(data, signal_threshold=3)
        plt.savefig(convert_path(ovrlpy_report_path) / "Integrity_Ovrlpy.pdf", bbox_inches="tight")
        plt.close()

        doublets = data.detect_doublets(min_signal=3, integrity_sigma=2)

        fig, ax = plt.subplots()
        _scatter = ax.scatter(doublets["x"], doublets["y"], c=doublets["integrity"], s=0.2, cmap="viridis")
        _ = ax.set_aspect("equal")
        _ = fig.colorbar(_scatter, ax=ax)
        plt.savefig(convert_path(ovrlpy_report_path) / "DoubletsIntegrity_Ovrlpy.pdf", bbox_inches="tight")
        plt.close()

        with open(convert_path(ovrlpy_report_path) / "ObjectOvrlpy.pickle", "wb") as file:
            pickle.dump(data, file)
        doublets.write_csv(convert_path(ovrlpy_report_path) / "SummaryDoublets.csv")
    else:
        raise Exception("Doublet detection tool available: scDblFinder, Scrublet and DoubletDetection")

    return None


def _normalise(
    adata: ad.AnnData,
    n_reads: int = 10_000,
) -> None:
    """Normalize raw counts.

    The input is an unnormalize anndata object. The dt in X will be log-normalize to 10,000 reads per cell.
    The returned anndata object will contain 3 layers:
    * counts: contains the raw unnormalized counts
    * logcounts: contains the log-normalize counts
    * scaled: contained the log-normalize counts scaled
    Additionally, the log-normalize counts will also be saved under the X attribute.

    :param adata: annData object
    :param n_reads: target number of reads per cell to normalize to. (Default  is **10,000**)
    :return: log-normalise anndata object
    """
    import scanpy as sc
    adata.layers["counts"] = adata.X.copy()  # Save raw counts
    sc.pp.normalize_total(adata, target_sum=n_reads)
    sc.pp.log1p(adata)
    adata.layers["logcounts"] = adata.X.copy()
    return

def log_normalize(
    adata: ad.AnnData,
    target_sum: int = 10_000,
) -> None:
    """Apply LogNormalization.

    The data in X will be log-normalize to 10,000 reads per cell.  The shifted logarithm works beneficial for
    stabilizing variance for subsequent dimensionality reduction and identification of differentially expressed genes.
    The returned anndata object will contain 3 layers:
    * counts: contains the raw un-normalized counts
    * logcounts: contains the log-normalize counts
    Additionally, the log-normalize counts will also be saved under the X attribute.

    Parameters
    ----------
    adata
        Annotated data matrix.
    target_sum
         Target number of reads per cell to normalize to.
    Returns
    -------
    Returns `None`. Changes will be performed inplace.

    """
    from scipy.sparse import issparse

    # LogNormalization should only be performed on raw counts
    matrix = adata.X.data if issparse(adata.X) else adata.X.flatten()
    if (matrix % 1 != 0).any():
        raise ValueError("The count matrix should only contain integers.")
    if (matrix < 0).any():
        raise ValueError("The count matrix should only contain non-negative values.")

    _normalise(adata, n_reads=target_sum)

    return None


def _qc_scrna(
    adata: ad.AnnData,
    ids: str,
    qc_path: str | Path | None = None,
    batch_key: str | None = None,
    min_genes_in_cell: int = 300,
    min_cells_with_genes: int = 5,
    cut_mt: int = 5,
    min_counts: int | None = None,
    max_counts: int | None = None,
    min_genes: int | None = None,
    max_genes: int | None = None,
    low_quantile: int | None = None,
    high_quantile: int | None = None,
    include_rbs: bool = True,
    remove_doublets: bool = False,
    doublet_tool: Literal["scDblFinder", "DoubletDetection", "Scrublet"] = "scDblFinder",
    metrics: bool = True,
    random_state: int = 0,
) -> ad.AnnData:
    """Basic QC.

    :param adata: annotated dt matrix.
    :param ids: id or name for the data.
    :param qc_path: path where to save the metric and the violin plots.
    :param batch_key: Column in `adata.obs` with sample information.
    :param min_genes_in_cell: minimum number of genes in a cell.
    :param min_cells_with_genes:  minimum number of cells expressing a gene.
    :param cut_mt: maximum number of mitochondrial content for cells.
    :param min_counts: minimum number of counts per cell.
    :param max_counts: maximum number of counts per cell.
    :param min_genes: minimum number of genes per cell.
    :param max_genes: maximum number of genes per cell.
    :param low_quantile: low quantile to filter genes and counts.
    :param high_quantile: upper quantile to filter genes and counts.
    :param include_rbs: calculate stats for ribosomal genes.
    :param remove_doublets: remove doublets.
    :param doublet_tool: doublet tool to use. Available scDblFinder, Scrublet and DoubletDetection.
    :param metrics: whether to generate a metrics file or not.
    :param random_state: Seed for random number generator.
    :return: annotated dt matrix
    """
    import scanpy as sc

    # Create a metrics file
    today = date.today().strftime("%y%m%d")
    metrics_filename = f"{today}_Metrics_{ids}.xlsx"
    df = pd.DataFrame([], columns=["QC_Step", "nCells", "nFeatures", "Comments"])
    df.loc[0] = ["Input_Shape", adata.shape[0], adata.shape[1], ""]

    # Compute Metrics
    mt_gene, ribo_gene = "mt-", ("rbs", "rpl")
    qc_metrics = ["mt", "ribo"] if include_rbs else ["mt"]
    adata.var["genenames"] = adata.var_names.str.lower()  # Generalise for any gene format
    adata.var["mt"] = adata.var["genenames"].str.startswith(mt_gene)  # Annotate mitochondria genes
    adata.var["ribo"] = adata.var["genenames"].str.startswith(ribo_gene)  # Annotate mitochondria genes
    sc.pp.calculate_qc_metrics(adata, qc_vars=qc_metrics, percent_top=None, log1p=True, inplace=True, parallel=True)

    # Vln Plots showing Metrics before qc
    _qc_vln(adata, title=f"PreQC for {ids}", path=qc_path, filename=f"Vln_PreQC_{ids}.svg")

    # Step 1 -
    logger.info("Remove Cells with low number of genes")
    sc.pp.filter_cells(adata, min_genes=min_genes_in_cell, inplace=True)
    df.loc[1] = ["Rm_Cells_lowGenes", adata.shape[0], adata.shape[1], f"Remove cells with <{min_genes_in_cell} genes"]

    # Step 2 -
    logger.info("Remove Genes lowly expressed")
    sc.pp.filter_genes(adata, min_cells=min_cells_with_genes, inplace=True)
    df.loc[2] = [
        "Rm_Genes_lowCells",
        adata.shape[0],
        adata.shape[1],
        f"Remove genes express in less than {min_cells_with_genes} cells",
    ]

    # Step 3 -
    logger.info("Remove cells with high MT-content")
    adata = adata[adata.obs.pct_counts_mt < cut_mt, :].copy()
    df.loc[3] = [
        "Rm_Cell_HighMT",
        adata.shape[0],
        adata.shape[1],
        f"Remove cells with >{cut_mt}% of Mitochondrial genes",
    ]

    # Step 4 -
    logger.info("Remove cells based on nUMI counts")
    assert (min_counts is None) != (low_quantile is None), "Set min_count or low_quantile"
    assert (max_counts is None) != (high_quantile is None), "Set max_count or high_quantile"

    if min_counts is not None:
        sc.pp.filter_cells(adata, min_counts=min_counts)
    if max_counts is not None:
        sc.pp.filter_cells(adata, max_counts=max_counts)
    if min_genes is not None:
        sc.pp.filter_cells(adata, min_genes=min_genes)
    if max_genes is not None:
        sc.pp.filter_cells(adata, max_genes=max_genes)

    # Apply quantile-based filtering (conditionally)
    adata = _filter_quantiles(adata, low_quantile, high_quantile)
    df.loc[4] = [
        "Rm_Cells_nUMI_nGenes",
        adata.shape[0],
        adata.shape[1],
        f"Remove cells based on nUMI counts[Absolute (Min/Max): {min_counts}/{max_counts}, "
        f"Quantile (low/high): {low_quantile}/{high_quantile}] and nFeatures [Absolute (Min/Max): "
        f"{min_genes}/{max_genes}]",
    ]

    # Step 5 -
    if remove_doublets:
        find_doublets(adata, batch_key=batch_key, method=doublet_tool, random_state=random_state)

        # if doublet_tool == "scDblFinder":
        #     adata.layers["counts"] = adata.X.copy()  # needed for scDblFinder
        #     _run_scdblfinder(adata, batch_key)
        # elif doublet_tool == "Scrublet":
        #     sc.pp.scrublet(adata)
        #     adata.obs["doublet_class"] = adata.obs["predicted_doublet"].map({False: "singlet", True: "doublet"})
        #     del adata.obs["predicted_doublet"]
        # elif doublet_tool == "DoubletDetection":
        #     clf = doubletdetection.BoostClassifier(
        #         n_iters=15,
        #         clustering_algorithm="leiden",
        #         standard_scaling=True,
        #         pseudocount=0.1,
        #         verbose=False,
        #         n_jobs=-1,
        #     )
        #     doublets = clf.fit(adata.X).predict()
        #     doublet_score = clf.doublet_score()
        #     mapped = np.full(doublets.shape, "singlet", dtype=object)
        #     mapped[doublets == 1.0] = "doublet"
        #     adata.obs["doublet_class"] = pd.Categorical(mapped, categories=["singlet", "doublet"])
        #     adata.obs["doublet_score"] = doublet_score
        # else:
        #     raise Exception("Doublet detection tool available: scDblFinder, Scrublet and DoubletDetection")

        n_doublets = adata.obs["doublet_class"].value_counts()["doublet"]
        adata = adata[adata.obs["doublet_class"] == "singlet"].copy()
        logger.info(f"Remove {n_doublets} doublets")
        df.loc[5] = ["Rm_Doublets", adata.shape[0], adata.shape[1], f"Remove neotypic doublets using {doublet_tool}"]

    # Save Metrics File
    if metrics:
        from dotools_py.utils import make_grid_spec
        from dotools_py.utility import get_hex_colormaps
        import matplotlib.lines as mlines
        df_plot = df.iloc[:, :-1].melt(id_vars="QC_Step")  # Exclude comments

        fig, gs = make_grid_spec(
            None or (8, 5), nrows=1, ncols=2, wspace=0.7 / 6, width_ratios=[6 - (0.9 + 0) + 0, 0.9]
        )

        ax = fig.add_subplot(gs[0])

        bp = sns.barplot(df_plot, hue="QC_Step", x="value", y="variable",
                         order=["nCells", "nFeatures"], hue_order=list(df["QC_Step"]),
                         palette="tab10", ax=ax, legend=False)

        for container in bp.containers:
            bp.bar_label(container, fmt='{:,.0f}')
        bp.set_title("Summary Quality Control", fontdict={"weight": "bold"})
        bp.set_ylabel("", fontsize=18)
        bp.set_xlabel("Counts", fontsize=18)
        bp.set_yticklabels(bp.get_yticklabels(), rotation=90, va="center", fontdict={"weight": "bold"})

        axs_legend = fig.add_subplot(gs[1])
        colors_dict = dict(zip(list(df["QC_Step"]), get_hex_colormaps("tab10"), strict=False))
        handles = []
        for lab, c in colors_dict.items():
            handles.append(mlines.Line2D([0], [0], marker=".", color=c, lw=0, label=lab,
                                         markerfacecolor=c, markeredgecolor=None, markersize=18))

        legend = axs_legend.legend(handles=handles, frameon=False, loc="center left", ncols=1, title="",
                                   prop={"size": 12, "weight": "bold"})
        legend.get_title().set_fontweight("bold")
        legend.get_title().set_fontsize(12 + 2)
        axs_legend.tick_params(axis="both", left=False, labelleft=False, labelright=False, bottom=False,
                               labelbottom=False)
        axs_legend.spines[["right", "left", "top", "bottom"]].set_visible(False)
        axs_legend.grid(visible=False)
        plt.savefig(os.path.join(qc_path, f"{today}_QC_Metrics{ids}.svg"), bbox_inches="tight")
        plt.close(fig)

        # Save Metric File
        df.to_excel(os.path.join(qc_path, metrics_filename), index=False)
    return adata


def quality_control(
    adata: ad.AnnData,
    batch_key: str | None = None,
    min_genes_in_cell: int = 300,
    min_cells_with_genes: int = 5,
    cut_mt: int = 5,
    min_counts: int | None = None,
    max_counts: int | None = None,
    min_genes: int | None = None,
    max_genes: int | None = None,
    low_quantile: int | None = None,
    high_quantile: int | None = None,
    include_rbs: bool = True,
    remove_doublets: bool = False,
    doublet_tool: Literal["scDblFinder", "DoubletDetection", "Scrublet"] = "scDblFinder",
    metrics: bool = True,
    qc_path: str | Path | None = None,
    random_state: int = 0,
) -> ad.AnnData:
    """Basic quality control for sc/snRNA-seq.

    For each sample in an AnnData object, several quality and filtering steps are applied:

    - Filter genes expressed in a low number of cells.
    - Filter cells with a low number of genes.
    - Filter cells with high mitochondrial content (recommended: 5% for scRNA, 3% for snRNA).
    - Filter cells based on nUMI and features using two modes:
        1. **Absolute filtering**: Sets absolute values for min/max UMI and features.
        2. **Quantile filtering**: Filters top/lower quantiles.
    - Remove doublets using scDblFinder, Scrublet, or DoubletDetection.

    An Excel sheet summarizing how many cells/genes were removed at each step will be generated,
    along with violin plots showing the distribution of `total_counts`, `n_genes_by_counts`,
    and `pct_mt_content` before and after QC.

    .. note::
        This function reproduces the quality control steps of :func:`dotools_py.pp.importer_py` but allows
        to provide an AnnData object as input.  This function assumes that `adata.X` contains raw counts.

    Parameters
    ----------
    adata
        Annotated data matrix with raw counts in `adata.X`.
    batch_key
        Column in `adata.obs` with sample information.
    min_genes_in_cell
        Minimum number of genes per cell.
    min_cells_with_genes
        Minimum number of cells expressing a gene.
    cut_mt
         Maximum percentage of mitochondrial genes per cell.
    min_counts
        Minimum number of counts per cell.
    max_counts
        Maximum number of counts per cell.
    min_genes
        Minimum number of genes per cell.
    max_genes
        Maximum number of genes per cell.
    low_quantile
        Low quantile to filter cells based on counts.
    high_quantile
        Upper quantile to filter cells based on counts.
    include_rbs
        Calculate statistics for ribosomal genes.
    remove_doublets
        Identify and remove doublets.
    doublet_tool
        Method to use for the removal of doublets.
    metrics
        Whether to compute statistics of how many cells and genes are remove in each step.
    qc_path
        Directory where the quality control plots and metrics are saved.
    random_state
        Seed for random number generator,

    Returns
    -------
    Returns a processed AnnData object.

    """
    from scipy.sparse import issparse

    matrix = adata.X.data if issparse(adata.X) else adata.X.flatten()
    if (matrix % 1 != 0).any():
        raise ValueError("The count matrix should only contain integers.")
    if (matrix < 0).any():
        raise ValueError("The count matrix should only contain non-negative values.")

    database = {}
    for batch_name in adata.obs[batch_key].unique():
        os.makedirs(convert_path(qc_path) / batch_name, exist_ok=True)
        adata_batch = adata[adata.obs[batch_key] == batch_name].copy()
        adata_batch = _qc_scrna(
            adata=adata_batch,
            ids=batch_name,
            batch_key=batch_key,
            min_genes_in_cell=min_genes_in_cell,
            min_cells_with_genes=min_cells_with_genes,
            cut_mt=cut_mt,
            min_counts=min_counts,
            max_counts=max_counts,
            min_genes=min_genes,
            max_genes=max_genes,
            low_quantile=low_quantile,
            high_quantile=high_quantile,
            include_rbs=include_rbs,
            remove_doublets=remove_doublets,
            doublet_tool=doublet_tool,
            metrics=metrics,
            qc_path=qc_path,
            random_state=random_state

        )
        database[batch_name] = adata_batch

    adata = ad.concat(
        database.values(), label=batch_key, keys=database.keys(), join="outer", index_unique="-", fill_value=0
    )
    return adata


def importer_py(
    paths: list,
    ids: list,
    metadata: dict | None = None,
    batch_key: str = "batch",
    remove_doublets: bool = True,
    doublet_tool: Literal["scDblFinder", "Scrublet", "DoubletDetection"] = "scDblFinder",
    min_genes_in_cell: int = 300,
    min_cells_with_genes: int = 5,
    cut_mt: int = 5,
    n_reads: int = 10_000,
    min_counts: int | None = None,
    max_counts: int | None = None,
    min_genes: int | None = None,
    max_genes: int | None = None,
    low_quantile: int | None = None,
    high_quantile: int | None = None,
    random_state: int = 0,
) -> ad.AnnData:
    """Quality control analysis for sc/snRNA.

    The input is a list with paths to H5 files generated with
    `CellRanger <https://www.10xgenomics.com/support/software/cell-ranger/latest>`_,
    `Cellbender <https://cellbender.readthedocs.io/en/latest/>`_, or
    `STARsolo <https://github.com/alexdobin/STAR>`_. A list of batch names for each sample must also be provided.
    Optionally, a dictionary with additional metadata can be passed. The order of batch names and metadata must
    match the order of the file paths.

    For each sample, several quality and filtering steps are applied:

    - Filter genes expressed in a low number of cells.
    - Filter cells with a low number of genes.
    - Filter cells with high mitochondrial content (recommended: 5% for scRNA, 3% for snRNA).
    - Filter cells based on nUMI and features using two modes:
        1. **Absolute filtering**: Sets absolute values for min/max UMI and features.
        2. **Quantile filtering**: Filters top/lower quantiles.
    - Remove doublets using scDblFinder, Scrublet, or DoubletDetection.

    An Excel sheet summarizing how many cells/genes were removed at each step will be generated,
    along with violin plots showing the distribution of `total_counts`, `n_genes_by_counts`,
    and `pct_mt_content` before and after QC. These outputs will be saved in the folder containing the H5 files.

    After QC, the data will be log-normalized and scaled. Highly variable genes and PCA will also be computed.

    :param paths: list with the path to the H5 files.
    :param ids: list with the batch name for each sample.
    :param metadata: dictionary with metadata information.
    :param batch_key: key in `.obs` for the batch information.
    :param remove_doublets: if set to True, neotypic doublets will be removed.
    :param doublet_tool: doublet tool to use. Available scDblFinder, Scrublet and DoubletDetection.
    :param min_genes_in_cell: minimum number of genes per cell.
    :param min_cells_with_genes: minimum cells expressing a genes.
    :param n_reads: target sum after normalization per cell.
    :param cut_mt: maximum percentage of mitochondrial genes per cell.
    :param min_counts:  minimum number of counts per cell.
    :param max_counts: maximum number of counts per cell.
    :param min_genes: minimum number of genes per cell.
    :param max_genes: maximum number of genes per cell.
    :param low_quantile: low quantile to filter cells based on counts.
    :param high_quantile: upper quantile to filter cells based on counts.
    :param random_state: seed for random number generator.
    :return: Returns an Annotated data matrix of shape `n_obs` x `n_vars` with all the samples concatenated.

    Example
    -------
    >>> import dotools_py as do
    >>> paths = ["/path/sample1", "/path/sample2"]
    >>> batchname = ["sample1", "sample2"]
    >>> metadata = {
    ...     "condition": ["WT", "KO"],
    ...     "age": ["3m", "3m"],
    ... }
    >>> adata = do.pp.importer_py(
    ...     paths=paths,
    ...     ids=batchname,
    ...     metadata=metadata,
    ...     batch_key="batch",
    ...     remove_doublets=True,
    ...     min_genes_in_cell=300,
    ...     min_cells_with_genes=5,
    ...     n_reads=10_000,
    ...     cut_mt=5,
    ...     high_quantile=95,
    ...     min_counts=500,
    ... )
    """
    import scanpy as sc

    # Checks
    assert isinstance(paths, list) and isinstance(ids, list), "Please provide a list of paths and ids"
    assert len(paths) == len(ids), f"Provided {len(paths)} paths and {len(ids)} ids"

    adata_dict = {}
    for idx, path in enumerate(paths):
        # Save QC Plots in the folder with raw dt
        qc_path = convert_path("/".join(path.split("/")[:-1]))

        logger.info(f"Reading {ids[idx]}")
        try:
            adata = sc.read_10x_h5(path)  # Works for 10x and CellBender and StarSolo?
        except IsADirectoryError:
            adata = sc.read_10x_mtx(path)  # Directory with .mtx and .tsv files

        adata.var_names_make_unique()

        # Add ID and Metadata
        adata.obs[batch_key] = ids[idx]
        if metadata:
            for key, value in metadata.items():
                adata.obs[key] = adata.obs[batch_key].map(dict(zip(ids, value, strict=False)))

        # Quality Control
        adata = _qc_scrna(
            adata=adata,
            ids=ids[idx],
            batch_key=batch_key,
            qc_path=qc_path,
            metrics=True,
            min_genes_in_cell=min_genes_in_cell,
            min_cells_with_genes=min_cells_with_genes,
            cut_mt=cut_mt,
            min_counts=min_counts,
            max_counts=max_counts,
            min_genes=min_genes,
            max_genes=max_genes,
            low_quantile=low_quantile,
            high_quantile=high_quantile,
            include_rbs=True,
            remove_doublets=remove_doublets,
            doublet_tool=doublet_tool,
            random_state=random_state
        )

        # Vln Plots showing Metrics before qc
        _qc_vln(adata, title=f"PostQC for {ids[idx]}", path=qc_path, filename=f"Vln_PostQC_{ids[idx]}.svg")

        adata_dict[ids[idx]] = adata

    logger.info("Concatenating samples")
    adata_concat = ad.concat(
        adata_dict.values(), label=batch_key, keys=adata_dict.keys(), join="outer", index_unique="-", fill_value=0
    )
    logger.info("Normalisation of the expression")
    _normalise(adata_concat, n_reads=n_reads)

    logger.info("Finding Highly Variable Genes shared across samples")
    sc.pp.highly_variable_genes(adata_concat, batch_key=batch_key)

    logger.info("Run PCA")
    hvg = adata_concat[:, adata_concat.var.highly_variable].copy()
    sc.pp.scale(hvg, zero_center=True)  # Scale only on HVGs to replicate Seurat Approach
    sc.pp.pca(hvg, random_state=random_state)  # PCA on Scaled HVGs
    adata_concat.obsm["X_pca"] = hvg.obsm["X_pca"].copy()  # Save in original object

    return adata_concat


def sctransform_normalize(
    adata: ad.AnnData,
    batch_key: str = None,
    layer: str = None
) -> None:
    """Normalization based on `SCTransform <https://github.com/satijalab/sctransform>`_.

    This function performs an alternative normalization based on the SCTransform.

    :param adata: AnnData object with counts in `X`.
    :param batch_key: obs metadata with batch information.
    :param layer: layer to use.
    :return: Returns None. The input AnnData object will have two new layers containing the SCT counts and normalize data.

    Example
    ------
    >>> import dotools_py as do
    >>> adata = do.dt.example_10x_processed()
    >>> adata
    AnnData object with n_obs × n_vars = 700 × 1851
    obs: 'batch', 'condition', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts',
         'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo',
         'pct_counts_ribo', 'n_genes', 'n_counts', 'doublet_class', 'doublet_score', 'leiden', 'cell_type',
         'autoAnnot', 'celltypist_conf_score', 'annotation', 'annotation_recluster'
    var: 'mean', 'std', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches',
         'highly_variable_intersection'
    uns: 'annotation_colors', 'annotation_recluster_colors', 'batch_colors', 'hvg', 'leiden', 'leiden_colors', 'log1p',
         'neighbors', 'pca', 'umap'
    obsm: 'X_CCA', 'X_pca', 'X_umap'
    varm: 'PCs'
    layers: 'counts', 'logcounts'
    obsp: 'connectivities', 'distances'
    >>>
    >>> do.pp.sctransform_normalize(adata, batch_key="batch", layer="counts")
    >>> adata
    AnnData object with n_obs × n_vars = 700 × 1181
    obs: 'batch', 'condition', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts',
         'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo',
         'pct_counts_ribo', 'n_genes', 'n_counts', 'doublet_class', 'doublet_score', 'leiden', 'cell_type',
         'autoAnnot', 'celltypist_conf_score', 'annotation', 'annotation_recluster'
    var: 'mean', 'std', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches',
         'highly_variable_intersection', 'SCT_rm'
    obsm: 'SCT_rm'
    varm: 'PCs'
    layers: 'counts', 'logcounts', 'SCT_norm', 'SCT_counts'
    obsp: 'connectivities', 'distances'
    """
    from scipy import sparse
    import polars

    rscript = get_paths_utils("_run_SCTransform.R")
    tmpdir_path = Path("/tmp") / f"SCTransform_{uuid.uuid4().hex}"
    tmpdir_path.mkdir(parents=True, exist_ok=False)

    logger.info("Preparing to transfer to R")
    adata_copy = adata.copy()
    if layer is not None:
        adata.X = adata.layers[layer].copy()
    del adata.uns
    del adata.obsm

    if batch_key is not None:
        adata_copy.obs["batch"] = adata_copy.obs[batch_key].copy()
    else:
        adata_copy.obs["batch"] = "batch1"
    adata_copy.write(tmpdir_path / "adata_tmp.h5ad")

    logger.info("Running SCTransform in R")
    subprocess.call(["Rscript", rscript, "--input=" + str(tmpdir_path) + "/", "--out=" + str(tmpdir_path) + "/"])

    raw_counts = polars.read_csv(os.path.join(tmpdir_path, "SCTransform_raw.csv"), infer_schema_length=0)
    raw_counts = raw_counts.to_pandas().astype(float)
    raw_counts = raw_counts.set_index(adata.obs_names)

    norm_counts = polars.read_csv(os.path.join(tmpdir_path, "SCTransform_norm.csv"), infer_schema_length=0)
    norm_counts = norm_counts.to_pandas().astype(float)
    norm_counts = norm_counts.set_index(adata.obs_names)

    # Transfer genes not kept during normalization to .obsm
    excluded_genes = [gene for gene in adata.var_names if gene not in norm_counts.columns]
    adata.var["SCT_rm"] = [True if gene in excluded_genes else False for gene in adata.var_names]
    adata.obsm["SCT_rm"] = adata[:, adata.var["SCT_rm"].values].X.toarray()
    adata = adata[:, ~adata.var["SCT_rm"].values]

    # Make sure we have the same order or barcodes and features
    norm_counts = norm_counts.reindex(index=adata.obs_names, columns=adata.var_names)
    raw_counts = raw_counts.reindex(index=adata.obs_names, columns=adata.var_names)

    adata.layers["SCT_norm"] = sparse.csr_matrix(norm_counts.values)
    adata.layers["SCT_counts"] = sparse.csr_matrix(raw_counts.values)
    return None
