{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/BattModels/smirk/blob/main/docs/smirk_demo.ipynb\">\n",
    "    <img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n",
    "</a>\n",
    "<a href=\"https://mybinder.org/v2/gh/BattModels/smirk/main?urlpath=%2Fdoc%2Ftree%2Fdocs%2Fsmirk_demo.ipynb\">\n",
    "    <img alt=\"Binder\" src=\"https://mybinder.org/badge_logo.svg\">\n",
    "</a>\n",
    "\n",
    "Molecular Foundation Models are demonstrating impressive performance, but current models use tokenizers that fail to represent *all* of chemistry; inherently limiting their performance. In particular,  [Atom-wise] tokenizers emit a single token for any [bracketed atom], triggering a combinatorial exposition of the vocabulary size. Capturing all variants of Carbon atoms would require 75,600 tokens, or nearly a quarter of the GPT-4o's vocabulary ([Wadell et al.][paper]).\n",
    "\n",
    "The problem is that most atoms are bracketed. Any element outside the organic subset, chiral centers, isotopes, or charged species are all encoded as bracketed atoms. Bracketed atoms encode the nuclear, electronic, and geometric features that are critical to numerous widely-used compounds, including:\n",
    "\n",
    "- [Cisplatin]: An effective chemotherapy drug on the World's Health Organizations List of Essential Medicines.\n",
    "    However, its isomer [Transplatin] is not an effective drug.\n",
    "- [Sodium pertechnetate]: A [radiopharmaceutical] used for thyroid imaging.\n",
    "- [Lithium Iron Phosphate]: A widely used cathode material for batteries powering everything from consumer electronics to electric vehicles. \n",
    "\n",
    "Smirk fixes this by tokenizing SMILES encodings all the way down to their constituent elements.\n",
    "Enabling the complete coverage of [OpenSMILES] with a vocabulary of *167* tokens.\n",
    "\n",
    "Check out the [paper] for all the details; otherwise, let's see it in action!\n",
    "\n",
    "[OpenSMILES]: http://opensmiles.org/\n",
    "[paper]: https://doi.org/10.1021/acs.jcim.5c01856\n",
    "[Atom-wise]: https://doi.org/10.1039/C8SC02339E\n",
    "[bracketed atom]: https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System#Atoms\n",
    "\n",
    "[Cisplatin]: https://en.wikipedia.org/wiki/Cisplatin\n",
    "[Transplatin]: https://en.wikipedia.org/wiki/Transplatin\n",
    "[Sodium pertechnetate]: https://en.wikipedia.org/wiki/Sodium_pertechnetate\n",
    "[radiopharmaceutical]: https://en.wikipedia.org/wiki/Radiopharmaceutical\n",
    "[Lithium Iron Phosphate]: https://en.wikipedia.org/wiki/Lithium_iron_phosphate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "ðŸ Installation is easy with pre-build binaries on [PyPI](https://pypi.org/project/smirk/) and [GitHub](https://github.com/BattModels/smirk/releases). Just run: `pip install smirk`\n",
    "\n",
    "> Installing from source? See [installing from source](./developer.md#installing-from-source) for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m pip install smirk transformers rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    "ðŸ¤— smirk subclasses Hugging Face's [PreTrainedTokenizerBase](#transformers.PreTrainedTokenizerBase) for seamless compatibility and leverages [Tokenizers] for raw rust-powered speed. No need to learn another framework; everything works out of the box ðŸŽ\n",
    "\n",
    "[Tokenizers]: https://huggingface.co/docs/tokenizers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smirk import SmirkTokenizerFast\n",
    "\n",
    "# Just import and tokenize!\n",
    "smirk = SmirkTokenizerFast()\n",
    "smirk(\"CC(=O)Nc1ccc(O)cc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Tokenization with Padding\n",
    "batch = smirk([\n",
    "    \"C[C@@H]1CCCCCCCCCCCCC(=O)C1\",\n",
    "    \"O=C(O)C[C@H](N)C(=O)N[C@H](C(=O)OC)Cc1ccccc1\",\n",
    "    \"CN(C)S[N][Re@OH18]([C][O])([C][O])([C][O])([C][O])[C][O]\"\n",
    "], padding=\"longest\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to molecules!\n",
    "smirk.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we don't add `[CLS]` and `[SEP]` tokens, but that's just a flag\n",
    "smirk_bert = SmirkTokenizerFast(template=\"[CLS] $0 [SEP]\")\n",
    "\" \".join(smirk_bert.tokenize(\"CNCCC(c1ccccc1)Oc2ccc(cc2)C(F)(F)F\", add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## What Makes Smirk Special?\n",
    "\n",
    "By fully decomposing the input molecule, `smirk` ensures complete coverage of the [OpenSMILES] specification. Any valid [OpenSMILES] encoding can be tokenized by `smirk` without emitting unknown tokens. Moreover, for non-bracketed atoms, the `smirk` tokenization is the same as an Atomwise tokenizer used by current molecular foundation models such as [MoLFormer].\n",
    "\n",
    "[OpenSMILES]: http://opensmiles.org/\n",
    "[MoLFormer]: https://doi.org/10.1038/s42256-022-00580-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import MolsToGridImage, rdMolDraw2D\n",
    "from IPython.display import SVG\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenizers being evaluated, see the paper for a more comphrensive study (30 tokenizers!)\n",
    "# Or try adding one of the other tokenziers evaluated in the paper\n",
    "tokenizers = {\n",
    "    \"smirk\": smirk,\n",
    "    \"molformer\": AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True),\n",
    "    \"GPT-4o\": AutoTokenizer.from_pretrained(\"Xenova/gpt-4o\"),\n",
    "}\n",
    "\n",
    "smi = [\n",
    "    \"Cl[Pt@SP1](Cl)([NH3])[NH3]\", # Cisplatin \n",
    "    \"Cl[Pt@SP2](Cl)([NH3])[NH3]\", # Transplatin\n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\", # Caffeine\n",
    "    \"[O-][99Tc](=O)(=O)=O.[Na+]\", # Sodium pertechnetate with radiotracer\n",
    "    \"[Ga+]$[As-]\", # Gallium arsenide\n",
    "    \"[OH2]\", # Water\n",
    "]\n",
    "\n",
    "def get_legend(smi:str, tokenizers:dict):\n",
    "    \"\"\"Helper function for creating legends\"\"\"\n",
    "    entries = []\n",
    "    for name, tok in tokenizers.items():\n",
    "        entries.append(f\"{name}: {' '.join(tok.tokenize(smi))}\")\n",
    "    return \"\\n\".join(entries)\n",
    "\n",
    "# Draw all molecules and tokenizations on a grid\n",
    "drawOptions = rdMolDraw2D.MolDrawOptions()\n",
    "drawOptions.fixedScale = 1\n",
    "drawOptions.centreMoleculesBeforeDrawing = True\n",
    "drawOptions.minFontSize = 6\n",
    "drawOptions.legendFontSize = 24\n",
    "drawOptions.legendFraction = 0.3\n",
    "MolsToGridImage(\n",
    "    [Chem.MolFromSmiles(smi) for smi in smi],\n",
    "    molsPerRow=2, subImgSize=(400,200),\n",
    "    legends=[get_legend(smi, tokenizers) for smi in smi],\n",
    "    drawOptions=drawOptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Smirk tokenized all molecules without a single unknown, whereas MoLFormer's Atomwise tokenizer emitted the unknown token for both [Cisplatin] and [Transplatin] (First row). Conversely, the Atomwise tokenizer emitted unknown tokens for the following:\n",
    "\n",
    "- Platinum chiral centers: `[Pt@SP1]` and `[Pt@SP2]`\n",
    "- Ammonia & Water with explicit hydrogens: `[NH3]` and `[OH2]`\n",
    "- Gallium ion: `[Ga+]`\n",
    "- Quadbond: `$`\n",
    "\n",
    "As a data-driven method, Atomwise tokenizers only know about the atoms seen during their training; fundamentally limiting their generalization ability.\n",
    "\n",
    "[Transplatin]: https://en.wikipedia.org/wiki/Transplatin\n",
    "[Cisplatin]: https://en.wikipedia.org/wiki/Cisplatin\n",
    "[Gallium arsenide]: https://en.wikipedia.org/wiki/Gallium_arsenide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Zero to Molecular Foundation Model with Smirk!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's train a small [RoBERTa] model on molecules from [QM9] using Hugging Face and smirk.\n",
    "\n",
    "[QM9]: https://doi.org/10.1021/ja902302h\n",
    "[RoBERTa]: https://doi.org/10.48550/ARXIV.1907.11692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output",
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m pip install accelerate datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# MoleculeNet's QM9 dataset. Normally this would be a larger (and unlabeled)\n",
    "# dataset. But for a demo, it's perfect\n",
    "dataset = load_dataset(\"csv\", \n",
    "    data_files=[\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\"],\n",
    ")[\"train\"].select_columns(\"smiles\").train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenizer the splits! For a larger dataset, this would be done on-the-fly\n",
    "dataset = dataset.map(smirk, input_columns=[\"smiles\"], desc=\"Tokenizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "> ðŸ’¡ huggingface/tokenizers may raise a warning about being forked as we've already used our tokenizers (this isn't a smirk issue).\n",
    "> It's harmless, but when actually training it's best to avoid tokenization until after the fork to benefit from the rust-level parallelism\n",
    "\n",
    "ðŸŽ‰ That's it! We've tokenized all of QM9 using smirk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training\n",
    "Once we've tokenized the dataset, training the model is just a matter of configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import Trainer, TrainingArguments, RobertaForMaskedLM, RobertaConfig, DataCollatorForLanguageModeling\n",
    "\n",
    "# A very small model for demonstrating training a molecular foundation model with smirk \n",
    "config = RobertaConfig(\n",
    "    vocab_size=len(smirk),\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    ")\n",
    "model = RobertaForMaskedLM(config)\n",
    "\n",
    "# Setup up the trainer to use our dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=smirk,\n",
    "    data_collator=DataCollatorForLanguageModeling(smirk), # The data collator needs to know about our tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
