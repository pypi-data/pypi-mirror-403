"""
Solving functionality for PyPSA networks.

Simplified to always use multi-period optimization for consistency.
"""

import time
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional


class NetworkSolver:
    """
    Simplified PyPSA network solver that always uses multi-period optimization.

    This ensures consistent behavior for both single-year and multi-year models.
    """

    def __init__(self, verbose: bool = False):
        """
        Initialize NetworkSolver.

        Args:
            verbose: Enable detailed logging output
        """
        self.verbose = verbose

        # Import PyPSA with error handling
        try:
            import pypsa

            self.pypsa = pypsa
        except ImportError as e:
            raise ImportError(
                "PyPSA is not installed or could not be imported. "
                "Please ensure it is installed correctly in the environment."
            ) from e

    def solve_network(
        self,
        network: "pypsa.Network",
        solver_name: str = "highs",
        solver_options: Optional[Dict[str, Any]] = None,
        discount_rate: Optional[float] = None,
        job_id: Optional[str] = None,
        conn=None,
        scenario_id: Optional[int] = None,
        constraint_applicator=None,
        custom_solver_config: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Solve PyPSA network and return results.

        Args:
            network: PyPSA Network object to solve
            solver_name: Solver to use (default: "highs"). Use "custom" for custom_solver_config.
            solver_options: Optional solver-specific options
            discount_rate: Optional discount rate for multi-period optimization
            job_id: Optional job ID for tracking
            custom_solver_config: Optional custom solver configuration when solver_name="custom"
                Format: {"solver": "actual_solver_name", "solver_options": {...}}
                Example: {"solver": "gurobi", "solver_options": {"Method": 2, "Crossover": 0}}

        Returns:
            Dictionary with solve results and metadata

        Raises:
            ImportError: If PyPSA is not available
            Exception: If solving fails
        """
        start_time = time.time()
        run_id = str(uuid.uuid4())

        try:
            # Extract PyPSA-specific options that shouldn't be passed to the solver
            pypsa_options = {}
            filtered_solver_options = solver_options.copy() if solver_options else {}
            
            # linearized_unit_commitment is a PyPSA option, not a solver option
            if filtered_solver_options.get('linearized_unit_commitment'):
                pypsa_options['linearized_unit_commitment'] = filtered_solver_options.pop('linearized_unit_commitment')
            
            # Get solver configuration (with filtered options)
            actual_solver_name, solver_config = self._get_solver_config(
                solver_name, filtered_solver_options if filtered_solver_options else None, custom_solver_config
            )

            # Resolve discount rate - fallback to 0.0 if None
            # Note: API layer (api.py) handles fetching from network_config before calling this
            effective_discount_rate = (
                discount_rate if discount_rate is not None else 0.0
            )

            years = list(network.investment_periods)

            # Calculate investment period weightings with discount rate
            self._calculate_investment_weightings(network, effective_discount_rate)

            # Set snapshot weightings after multi-period setup
            if conn:
                self._set_snapshot_weightings_after_multiperiod(conn, network)

            # Prepare optimization constraints - ONLY model constraints
            # Network constraints were already applied before solve in api.py
            extra_functionality = None
            model_constraints = []

            if conn and constraint_applicator:
                optimization_constraints = (
                    constraint_applicator.get_optimization_constraints(
                        conn, scenario_id
                    )
                )
                if optimization_constraints:
                    # Filter for model constraints only (network constraints already applied)
                    for constraint in optimization_constraints:
                        constraint_code = constraint.get("constraint_code", "")
                        constraint_type = self._detect_constraint_type(constraint_code)

                        if constraint_type == "model_constraint":
                            model_constraints.append(constraint)

                    if model_constraints:
                        extra_functionality = self._create_extra_functionality(
                            model_constraints, constraint_applicator
                        )

            # NOTE: Model constraints are applied DURING solve via extra_functionality
            # Network constraints were already applied to the network structure before solve

            # Build optimize kwargs
            optimize_kwargs = {
                'solver_name': actual_solver_name,
                'multi_investment_periods': True,
                'extra_functionality': extra_functionality,
            }
            
            # Add solver config if present
            if solver_config:
                optimize_kwargs.update(solver_config)
            
            # Add PyPSA-specific options (like linearized_unit_commitment)
            if pypsa_options:
                optimize_kwargs.update(pypsa_options)
            
            result = network.optimize(**optimize_kwargs)

            solve_time = time.time() - start_time

            # Extract solve results with comprehensive statistics
            solve_result = self._extract_solve_results(
                network, result, solve_time, actual_solver_name, run_id
            )

            # Calculate comprehensive network statistics (all years combined)
            if solve_result.get("success"):
                network_statistics = self._calculate_comprehensive_network_statistics(
                    network, solve_time, actual_solver_name
                )
                solve_result["network_statistics"] = network_statistics

                # Calculate year-based statistics for capacity expansion analysis
                year_statistics = self._calculate_statistics_by_year(
                    network, solve_time, actual_solver_name
                )
                solve_result["year_statistics"] = year_statistics
                solve_result["year_statistics_available"] = len(year_statistics) > 0

            return solve_result

        except Exception as e:
            solve_time = time.time() - start_time

            return {
                "success": False,
                "status": "failed",
                "error": str(e),
                "solve_time": solve_time,
                "solver_name": (
                    actual_solver_name
                    if "actual_solver_name" in locals()
                    else solver_name
                ),
                "run_id": run_id,
                "objective_value": None,
            }

    def _get_solver_config(
        self,
        solver_name: str,
        solver_options: Optional[Dict[str, Any]] = None,
        custom_solver_config: Optional[Dict[str, Any]] = None,
    ) -> tuple[str, Optional[Dict[str, Any]]]:
        """
        Get the actual solver name and options for solver configurations.

        Args:
            solver_name: The solver name (e.g., 'highs', 'gurobi', 'custom')
            solver_options: Optional additional solver options
            custom_solver_config: Custom solver configuration (from frontend)
                Format: {"solver": "actual_solver_name", "solver_options": {...}}

        Returns:
            Tuple of (actual_solver_name, solver_options_dict)
        """
        # Handle "custom" solver with custom configuration from frontend
        if solver_name == "custom" and custom_solver_config:
            if "solver" not in custom_solver_config:
                raise ValueError(
                    "custom_solver_config must contain 'solver' key with the actual solver name"
                )

            actual_solver = custom_solver_config["solver"]
            custom_options = custom_solver_config.get("solver_options", {})

            # Merge with any additional solver_options passed separately
            if solver_options:
                merged_options = {
                    "solver_options": {**custom_options, **solver_options}
                }
            else:
                merged_options = (
                    {"solver_options": custom_options} if custom_options else None
                )

            return actual_solver, merged_options

        # For all other cases, pass through solver name and options directly
        # The frontend is responsible for resolving presets and defaults
        if solver_options:
            return solver_name, {"solver_options": solver_options}
        
        return solver_name, None

    def _detect_constraint_type(self, constraint_code: str) -> str:
        """
        Detect if constraint is network-modification or model-constraint type.

        Args:
            constraint_code: The constraint code to analyze

        Returns:
            "model_constraint" or "network_modification"
        """
        # Type 2 indicators (model constraints) - need access to optimization model
        model_indicators = [
            "n.optimize.create_model()",
            "m.variables",
            "m.add_constraints",
            "gen_p =",
            "constraint_expr =",
            "LinearExpression",
            "linopy",
            "Generator-p",
            "lhs <=",
            "constraint_expr =",
        ]

        # Type 1 indicators (network modifications) - modify network directly
        network_indicators = [
            "n.generators.loc",
            "n.add(",
            "n.buses.",
            "n.lines.",
            "network.generators.loc",
            "network.add(",
            "network.buses.",
            "network.lines.",
        ]

        # Check for model constraint indicators first (more specific)
        if any(indicator in constraint_code for indicator in model_indicators):
            return "model_constraint"
        elif any(indicator in constraint_code for indicator in network_indicators):
            return "network_modification"
        else:
            # Default to network_modification for safety (existing behavior)
            return "network_modification"

    def _create_extra_functionality(
        self, optimization_constraints: list, constraint_applicator
    ) -> callable:
        """
        Create extra_functionality function for optimization-time constraints.

        This matches the old PyPSA solver's approach to applying constraints during optimization.

        Args:
            optimization_constraints: List of optimization constraint dictionaries
            constraint_applicator: ConstraintApplicator instance

        Returns:
            Function that can be passed to network.optimize(extra_functionality=...)
        """

        def extra_functionality(network, snapshots):
            """Apply optimization constraints during solve - matches old code structure"""
            try:
                # Apply each constraint in priority order
                sorted_constraints = sorted(
                    optimization_constraints, key=lambda x: x.get("priority", 0)
                )

                for constraint in sorted_constraints:
                    try:
                        constraint_applicator.apply_optimization_constraint(
                            network, snapshots, constraint
                        )
                    except Exception as e:
                        continue

            except Exception as e:
                pass  # Don't re-raise - let optimization continue

        return extra_functionality

    def _set_snapshot_weightings_after_multiperiod(
        self, conn, network: "pypsa.Network"
    ):
        """Set snapshot weightings AFTER multi-period setup - matches old code approach (single network per database)."""
        try:
            from pyconvexity.models import get_network_time_periods, get_network_info

            time_periods = get_network_time_periods(conn)
            if time_periods and len(network.snapshots) > 0:
                # Get network info to determine time interval
                network_info = get_network_info(conn)
                time_interval = network_info.get("time_interval", "1H")
                weight = self._parse_time_interval(time_interval)

                if weight is None:
                    weight = 1.0

                # Create weightings array - all snapshots get the same weight for this time resolution
                weightings = [weight] * len(time_periods)

                if len(weightings) == len(network.snapshots):
                    # Set all three columns like the old code - critical for proper objective calculation
                    network.snapshot_weightings.loc[:, "objective"] = weightings
                    network.snapshot_weightings.loc[:, "generators"] = weightings
                    network.snapshot_weightings.loc[:, "stores"] = weightings
        except Exception as e:
            pass  # Failed to set snapshot weightings

    def _parse_time_interval(self, time_interval: str) -> Optional[float]:
        """Parse time interval string to hours - handles multiple formats."""
        if not time_interval:
            return None

        try:
            # Clean up the string
            interval = time_interval.strip()

            # Handle ISO 8601 duration format (PT3H, PT30M, etc.)
            if interval.startswith("PT") and interval.endswith("H"):
                # Extract hours (e.g., 'PT3H' -> 3.0)
                hours_str = interval[2:-1]  # Remove 'PT' and 'H'
                return float(hours_str)
            elif interval.startswith("PT") and interval.endswith("M"):
                # Extract minutes (e.g., 'PT30M' -> 0.5)
                minutes_str = interval[2:-1]  # Remove 'PT' and 'M'
                return float(minutes_str) / 60.0
            elif interval.startswith("PT") and interval.endswith("S"):
                # Extract seconds (e.g., 'PT3600S' -> 1.0)
                seconds_str = interval[2:-1]  # Remove 'PT' and 'S'
                return float(seconds_str) / 3600.0

            # Handle simple frequency strings (3H, 2D, etc.)
            elif interval.endswith("H") or interval.endswith("h"):
                hours_str = interval[:-1]
                return float(hours_str) if hours_str else 1.0
            elif interval.endswith("D") or interval.endswith("d"):
                days_str = interval[:-1]
                return float(days_str) * 24 if days_str else 24.0
            elif interval.endswith("M") or interval.endswith("m"):
                minutes_str = interval[:-1]
                return float(minutes_str) / 60.0 if minutes_str else 1.0 / 60.0
            elif interval.endswith("S") or interval.endswith("s"):
                seconds_str = interval[:-1]
                return float(seconds_str) / 3600.0 if seconds_str else 1.0 / 3600.0

            # Try to parse as plain number (assume hours)
            else:
                return float(interval)

        except (ValueError, TypeError) as e:
            return None

    def _calculate_investment_weightings(
        self, network: "pypsa.Network", discount_rate: float
    ) -> None:
        """
        Calculate investment period weightings using discount rate - matching old PyPSA solver exactly.

        Args:
            network: PyPSA Network object
            discount_rate: Discount rate for NPV calculations
        """
        try:
            import pandas as pd

            if (
                not hasattr(network, "investment_periods")
                or len(network.investment_periods) == 0
            ):
                return

            years = network.investment_periods
            # Convert pandas Index to list for easier handling
            years_list = years.tolist() if hasattr(years, "tolist") else list(years)

            # For single year, use simple weighting of 1.0
            if len(years_list) == 1:
                # Single year case
                network.investment_period_weightings = pd.DataFrame(
                    {
                        "objective": pd.Series({years_list[0]: 1.0}),
                        "years": pd.Series({years_list[0]: 1}),
                    }
                )
            else:
                # Multi-year case - EXACTLY match old code logic
                # Get unique years from the network snapshots to determine period lengths
                if hasattr(network.snapshots, "year"):
                    snapshot_years = sorted(network.snapshots.year.unique())
                elif hasattr(network.snapshots, "get_level_values"):
                    # MultiIndex case - get years from 'period' level
                    snapshot_years = sorted(
                        network.snapshots.get_level_values("period").unique()
                    )
                else:
                    # Fallback: use investment periods as years
                    snapshot_years = years_list

                # Calculate years per period - EXACTLY matching old code
                years_diff = []
                for i, year in enumerate(years_list):
                    if i < len(years_list) - 1:
                        # Years between this period and the next
                        next_year = years_list[i + 1]
                        period_years = next_year - year
                    else:
                        # For the last period, calculate based on snapshot coverage
                        if snapshot_years:
                            # Find the last snapshot year that's >= current period year
                            last_snapshot_year = max(
                                [y for y in snapshot_years if y >= year]
                            )
                            period_years = last_snapshot_year - year + 1
                        else:
                            # Fallback: assume same length as previous period or 1
                            if len(years_diff) > 0:
                                period_years = years_diff[-1]  # Same as previous period
                            else:
                                period_years = 1

                    years_diff.append(period_years)

                # Create weightings DataFrame with years column
                weightings_df = pd.DataFrame(
                    {"years": pd.Series(years_diff, index=years_list)}
                )

                # Calculate objective weightings with discount rate - EXACTLY matching old code
                r = discount_rate
                T = 0  # Cumulative time tracker

                for period, nyears in weightings_df.years.items():
                    # Calculate discount factors for each year in this period
                    discounts = [(1 / (1 + r) ** t) for t in range(T, T + nyears)]
                    period_weighting = sum(discounts)
                    weightings_df.at[period, "objective"] = period_weighting
                    T += nyears  # Update cumulative time

                network.investment_period_weightings = weightings_df

        except Exception as e:
            pass  # Failed to calculate investment weightings

    def _extract_solve_results(
        self,
        network: "pypsa.Network",
        result: Any,
        solve_time: float,
        solver_name: str,
        run_id: str,
    ) -> Dict[str, Any]:
        """
        Extract solve results from PyPSA network.

        Args:
            network: Solved PyPSA Network object
            result: PyPSA solve result
            solve_time: Time taken to solve
            solver_name: Name of solver used
            run_id: Unique run identifier

        Returns:
            Dictionary with solve results and metadata
        """
        try:
            # Extract basic solve information
            status = getattr(result, "status", "unknown")
            objective_value = getattr(network, "objective", None)

            # Convert PyPSA result to dictionary format
            result_dict = self._convert_pypsa_result_to_dict(result)

            # Determine success based on multiple criteria
            success = self._determine_solve_success(
                result, network, status, objective_value
            )

            solve_result = {
                "success": success,
                "status": status,
                "solve_time": solve_time,
                "solver_name": solver_name,
                "run_id": run_id,
                "objective_value": objective_value,
                "pypsa_result": result_dict,
                "network_name": network.name,
                "num_buses": len(network.buses),
                "num_generators": len(network.generators),
                "num_loads": len(network.loads),
                "num_lines": len(network.lines),
                "num_links": len(network.links),
                "num_snapshots": len(network.snapshots),
            }

            # Add multi-period information if available
            if hasattr(network, "_available_years") and network._available_years:
                solve_result["years"] = network._available_years
                solve_result["multi_period"] = len(network._available_years) > 1

            return solve_result

        except Exception as e:
            return {
                "success": False,
                "status": "extraction_failed",
                "error": f"Failed to extract results: {e}",
                "solve_time": solve_time,
                "solver_name": solver_name,
                "run_id": run_id,
                "objective_value": None,
            }

    def _determine_solve_success(
        self,
        result: Any,
        network: "pypsa.Network",
        status: str,
        objective_value: Optional[float],
    ) -> bool:
        """
        Determine if solve was successful based on multiple criteria.

        PyPSA sometimes returns status='unknown' even for successful solves,
        so we need to check multiple indicators.
        """
        try:
            # Check explicit status first
            if status in ["optimal", "feasible"]:
                return True

            # Check termination condition
            if hasattr(result, "termination_condition"):
                term_condition = str(result.termination_condition).lower()
                if "optimal" in term_condition:
                    return True

            # Check if we have a valid objective value
            if objective_value is not None and not (
                objective_value == 0 and status == "unknown"
            ):
                return True

            # Check solver-specific success indicators
            if hasattr(result, "solver"):
                solver_info = result.solver
                if hasattr(solver_info, "termination_condition"):
                    term_condition = str(solver_info.termination_condition).lower()
                    if "optimal" in term_condition:
                        return True

            return False

        except Exception as e:
            return False

    def _convert_pypsa_result_to_dict(self, result) -> Dict[str, Any]:
        """
        Convert PyPSA result object to dictionary.

        Args:
            result: PyPSA solve result object

        Returns:
            Dictionary representation of the result
        """
        try:
            if result is None:
                return {"status": "no_result"}

            result_dict = {}

            # Extract common attributes
            for attr in ["status", "success", "termination_condition", "solver"]:
                if hasattr(result, attr):
                    value = getattr(result, attr)
                    # Convert to serializable format
                    if hasattr(value, "__dict__"):
                        result_dict[attr] = str(value)
                    else:
                        result_dict[attr] = value

            # Handle solver-specific information
            if hasattr(result, "solver_results"):
                solver_results = getattr(result, "solver_results")
                if hasattr(solver_results, "__dict__"):
                    result_dict["solver_results"] = str(solver_results)
                else:
                    result_dict["solver_results"] = solver_results

            return result_dict

        except Exception as e:
            return {"status": "conversion_failed", "error": str(e)}

    def _calculate_comprehensive_network_statistics(
        self, network: "pypsa.Network", solve_time: float, solver_name: str
    ) -> Dict[str, Any]:
        """Calculate comprehensive network statistics including PyPSA statistics and custom metrics"""
        try:
            # Initialize statistics structure
            statistics = {
                "core_summary": {},
                "pypsa_statistics": {},
                "custom_statistics": {},
                "runtime_info": {},
                "solver_info": {},
            }

            # Core summary statistics
            total_generation = 0
            total_demand = 0
            unserved_energy = 0

            # Calculate generation statistics
            if hasattr(network, "generators_t") and hasattr(network.generators_t, "p"):
                # Apply snapshot weightings to convert MW to MWh
                weightings = network.snapshot_weightings
                if isinstance(weightings, pd.DataFrame):
                    if "objective" in weightings.columns:
                        weighting_values = weightings["objective"].values
                    else:
                        weighting_values = weightings.iloc[:, 0].values
                else:
                    weighting_values = weightings.values

                total_generation = float(
                    (network.generators_t.p.values * weighting_values[:, None]).sum()
                )

                # Calculate unserved energy from UNMET_LOAD generators
                if hasattr(network, "generators") and hasattr(
                    network, "_component_type_map"
                ):
                    unmet_load_gen_names = [
                        name
                        for name, comp_type in network._component_type_map.items()
                        if comp_type == "UNMET_LOAD"
                    ]

                    for gen_name in unmet_load_gen_names:
                        if gen_name in network.generators_t.p.columns:
                            gen_output = float(
                                (
                                    network.generators_t.p[gen_name] * weighting_values
                                ).sum()
                            )
                            unserved_energy += gen_output

            # Calculate demand statistics
            if hasattr(network, "loads_t") and hasattr(network.loads_t, "p"):
                weightings = network.snapshot_weightings
                if isinstance(weightings, pd.DataFrame):
                    if "objective" in weightings.columns:
                        weighting_values = weightings["objective"].values
                    else:
                        weighting_values = weightings.iloc[:, 0].values
                else:
                    weighting_values = weightings.values

                total_demand = float(
                    (network.loads_t.p.values * weighting_values[:, None]).sum()
                )

            statistics["core_summary"] = {
                "total_generation_mwh": total_generation,
                "total_demand_mwh": total_demand,
                "total_cost": (
                    float(network.objective) if hasattr(network, "objective") else None
                ),
                "load_factor": (
                    (total_demand / (total_generation + 1e-6))
                    if total_generation > 0
                    else 0
                ),
                "unserved_energy_mwh": unserved_energy,
            }

            # Calculate PyPSA statistics
            try:
                pypsa_stats = network.statistics()
                if pypsa_stats is not None and not pypsa_stats.empty:
                    statistics["pypsa_statistics"] = self._convert_pypsa_result_to_dict(
                        pypsa_stats
                    )
                else:
                    statistics["pypsa_statistics"] = {}
            except Exception as e:
                statistics["pypsa_statistics"] = {}

            # Custom statistics - calculate detailed breakdowns
            total_cost = (
                float(network.objective) if hasattr(network, "objective") else 0.0
            )
            avg_price = (
                (total_cost / (total_generation + 1e-6))
                if total_generation > 0
                else None
            )
            unmet_load_percentage = (
                (unserved_energy / (total_demand + 1e-6)) * 100
                if total_demand > 0
                else 0
            )

            # Note: For solver statistics, we keep simplified approach since this is just for logging
            # The storage module will calculate proper totals from carrier statistics
            statistics["custom_statistics"] = {
                "total_capital_cost": 0.0,  # Will be calculated properly in storage module
                "total_operational_cost": total_cost,  # PyPSA objective (includes both capital and operational, discounted)
                "total_currency_cost": total_cost,
                "total_emissions_tons_co2": 0.0,  # Will be calculated properly in storage module
                "average_price_per_mwh": avg_price,
                "unmet_load_percentage": unmet_load_percentage,
                "max_unmet_load_hour_mw": 0.0,  # TODO: Calculate max hourly unmet load
            }

            # Runtime info
            unmet_load_count = 0
            if hasattr(network, "_component_type_map"):
                unmet_load_count = len(
                    [
                        name
                        for name, comp_type in network._component_type_map.items()
                        if comp_type == "UNMET_LOAD"
                    ]
                )

            statistics["runtime_info"] = {
                "solve_time_seconds": solve_time,
                "component_count": (
                    (
                        len(network.buses)
                        + len(network.generators)
                        + len(network.loads)
                        + len(network.lines)
                        + len(network.links)
                    )
                    if hasattr(network, "buses")
                    else 0
                ),
                "bus_count": len(network.buses) if hasattr(network, "buses") else 0,
                "generator_count": (
                    len(network.generators) if hasattr(network, "generators") else 0
                ),
                "unmet_load_count": unmet_load_count,
                "load_count": len(network.loads) if hasattr(network, "loads") else 0,
                "line_count": len(network.lines) if hasattr(network, "lines") else 0,
                "snapshot_count": (
                    len(network.snapshots) if hasattr(network, "snapshots") else 0
                ),
            }

            # Solver info
            statistics["solver_info"] = {
                "solver_name": solver_name,
                "termination_condition": (
                    "optimal" if hasattr(network, "objective") else "unknown"
                ),
                "objective_value": (
                    float(network.objective) if hasattr(network, "objective") else None
                ),
            }

            return statistics

        except Exception as e:
            return {
                "error": str(e),
                "core_summary": {},
                "pypsa_statistics": {},
                "custom_statistics": {},
                "runtime_info": {"solve_time_seconds": solve_time},
                "solver_info": {"solver_name": solver_name},
            }

    def _calculate_statistics_by_year(
        self, network: "pypsa.Network", solve_time: float, solver_name: str
    ) -> Dict[int, Dict[str, Any]]:
        """Calculate statistics for each year in the network"""
        try:
            # Extract years from network snapshots or manually extracted years
            if hasattr(network.snapshots, "year"):
                years = sorted(network.snapshots.year.unique())
            elif hasattr(network, "_available_years"):
                years = network._available_years
            elif hasattr(network.snapshots, "levels"):
                # Multi-period optimization - get years from period level
                period_values = network.snapshots.get_level_values(0)
                years = sorted(period_values.unique())
            else:
                # If no year info, skip year-based calculations
                return {}

            year_statistics = {}

            for year in years:
                try:
                    year_stats = self._calculate_network_statistics_for_year(
                        network, year, solve_time, solver_name
                    )
                    year_statistics[year] = year_stats
                except Exception as e:
                    continue

            return year_statistics

        except Exception as e:
            return {}

    def _calculate_network_statistics_for_year(
        self, network: "pypsa.Network", year: int, solve_time: float, solver_name: str
    ) -> Dict[str, Any]:
        """Calculate network statistics for a specific year"""
        try:
            # Initialize statistics structure
            statistics = {
                "core_summary": {},
                "custom_statistics": {},
                "runtime_info": {},
                "solver_info": {},
            }

            # Core summary statistics for this year
            total_generation = 0
            total_demand = 0
            unserved_energy = 0

            # Calculate generation statistics for this year
            if hasattr(network, "generators_t") and hasattr(network.generators_t, "p"):
                # Filter by year
                year_generation = self._filter_timeseries_by_year(
                    network.generators_t.p, network.snapshots, year
                )
                if year_generation is not None and not year_generation.empty:
                    # Apply snapshot weightings for this year
                    year_weightings = self._get_year_weightings(network, year)
                    if year_weightings is not None:
                        total_generation = float(
                            (year_generation.values * year_weightings[:, None]).sum()
                        )
                    else:
                        total_generation = float(year_generation.sum().sum())

                    # Calculate unserved energy for this year
                    if hasattr(network, "_component_type_map"):
                        unmet_load_gen_names = [
                            name
                            for name, comp_type in network._component_type_map.items()
                            if comp_type == "UNMET_LOAD"
                        ]

                        for gen_name in unmet_load_gen_names:
                            if gen_name in year_generation.columns:
                                if year_weightings is not None:
                                    gen_output = float(
                                        (
                                            year_generation[gen_name] * year_weightings
                                        ).sum()
                                    )
                                else:
                                    gen_output = float(year_generation[gen_name].sum())
                                unserved_energy += gen_output

            # Calculate demand statistics for this year
            if hasattr(network, "loads_t") and hasattr(network.loads_t, "p"):
                year_demand = self._filter_timeseries_by_year(
                    network.loads_t.p, network.snapshots, year
                )
                if year_demand is not None and not year_demand.empty:
                    year_weightings = self._get_year_weightings(network, year)
                    if year_weightings is not None:
                        total_demand = float(
                            (year_demand.values * year_weightings[:, None]).sum()
                        )
                    else:
                        total_demand = float(year_demand.sum().sum())

            statistics["core_summary"] = {
                "total_generation_mwh": total_generation,
                "total_demand_mwh": total_demand,
                "total_cost": None,  # Year-specific cost calculation would be complex
                "load_factor": (
                    (total_demand / (total_generation + 1e-6))
                    if total_generation > 0
                    else 0
                ),
                "unserved_energy_mwh": unserved_energy,
            }

            # Custom statistics
            unmet_load_percentage = (
                (unserved_energy / (total_demand + 1e-6)) * 100
                if total_demand > 0
                else 0
            )

            # Calculate year-specific carrier statistics
            year_carrier_stats = self._calculate_year_carrier_statistics(network, year)

            statistics["custom_statistics"] = {
                "unmet_load_percentage": unmet_load_percentage,
                "year": year,
                **year_carrier_stats,  # Include all carrier-specific statistics for this year
            }

            # Runtime info
            year_snapshot_count = self._count_year_snapshots(network.snapshots, year)

            statistics["runtime_info"] = {
                "solve_time_seconds": solve_time,
                "year": year,
                "snapshot_count": year_snapshot_count,
            }

            # Solver info
            statistics["solver_info"] = {"solver_name": solver_name, "year": year}

            return statistics

        except Exception as e:
            return {
                "error": str(e),
                "core_summary": {},
                "custom_statistics": {"year": year},
                "runtime_info": {"solve_time_seconds": solve_time, "year": year},
                "solver_info": {"solver_name": solver_name, "year": year},
            }

    def _filter_timeseries_by_year(
        self, timeseries_df: "pd.DataFrame", snapshots: "pd.Index", year: int
    ) -> "pd.DataFrame":
        """Filter timeseries data by year"""
        try:
            # Handle MultiIndex case (multi-period optimization)
            if hasattr(snapshots, "levels"):
                period_values = snapshots.get_level_values(0)
                year_mask = period_values == year
                if year_mask.any():
                    year_snapshots = snapshots[year_mask]
                    return timeseries_df.loc[year_snapshots]

            # Handle DatetimeIndex case (regular time series)
            elif hasattr(snapshots, "year"):
                year_mask = snapshots.year == year
                if year_mask.any():
                    return timeseries_df.loc[year_mask]

            # Fallback - return None if can't filter
            return None

        except Exception as e:
            return None

    def _get_year_weightings(self, network: "pypsa.Network", year: int) -> "np.ndarray":
        """Get snapshot weightings for a specific year"""
        try:
            # Filter snapshot weightings by year
            if hasattr(network.snapshots, "levels"):
                period_values = network.snapshots.get_level_values(0)
                year_mask = period_values == year
                if year_mask.any():
                    year_snapshots = network.snapshots[year_mask]
                    year_weightings = network.snapshot_weightings.loc[year_snapshots]
                    if isinstance(year_weightings, pd.DataFrame):
                        if "objective" in year_weightings.columns:
                            return year_weightings["objective"].values
                        else:
                            return year_weightings.iloc[:, 0].values
                    else:
                        return year_weightings.values

            elif hasattr(network.snapshots, "year"):
                year_mask = network.snapshots.year == year
                if year_mask.any():
                    year_weightings = network.snapshot_weightings.loc[year_mask]
                    if isinstance(year_weightings, pd.DataFrame):
                        if "objective" in year_weightings.columns:
                            return year_weightings["objective"].values
                        else:
                            return year_weightings.iloc[:, 0].values
                    else:
                        return year_weightings.values

            return None

        except Exception as e:
            return None

    def _count_year_snapshots(self, snapshots: "pd.Index", year: int) -> int:
        """Count snapshots for a specific year"""
        try:
            # Handle MultiIndex case
            if hasattr(snapshots, "levels"):
                period_values = snapshots.get_level_values(0)
                year_mask = period_values == year
                return year_mask.sum()

            # Handle DatetimeIndex case
            elif hasattr(snapshots, "year"):
                year_mask = snapshots.year == year
                return year_mask.sum()

            # Fallback
            return 0

        except Exception as e:
            return 0

    def _calculate_year_carrier_statistics(
        self, network: "pypsa.Network", year: int
    ) -> Dict[str, Any]:
        """Calculate carrier-specific statistics for a specific year"""
        # Note: This is a simplified implementation that doesn't have database access
        # The proper implementation should be done in the storage module where we have conn
        # For now, return empty dictionaries - the storage module will handle this properly
        return {
            "dispatch_by_carrier": {},
            "capacity_by_carrier": {},
            "emissions_by_carrier": {},
            "capital_cost_by_carrier": {},
            "operational_cost_by_carrier": {},
            "total_system_cost_by_carrier": {},
        }

    def _get_generator_carrier_name(self, generator_name: str) -> Optional[str]:
        """Get carrier name for a generator - simplified implementation"""
        # This is a simplified approach - in practice, this should query the database
        # or use the component type mapping from the network

        # Try to extract carrier from generator name patterns
        gen_lower = generator_name.lower()

        if "coal" in gen_lower:
            return "coal"
        elif "gas" in gen_lower or "ccgt" in gen_lower or "ocgt" in gen_lower:
            return "gas"
        elif "nuclear" in gen_lower:
            return "nuclear"
        elif "solar" in gen_lower or "pv" in gen_lower:
            return "solar"
        elif "wind" in gen_lower:
            return "wind"
        elif "hydro" in gen_lower:
            return "hydro"
        elif "biomass" in gen_lower:
            return "biomass"
        elif "battery" in gen_lower:
            return "battery"
        elif "unmet" in gen_lower:
            return "Unmet Load"
        else:
            # Default to generator name if no pattern matches
            return generator_name
