{"guide": {"name": "streaming-inputs", "category": "additional-features", "pretty_category": "Additional Features", "guide_index": 3, "absolute_index": 17, "pretty_name": "Streaming Inputs", "content": "# Streaming inputs\n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>Check out <a href=\"https://fastrtc.org/\">FastRTC</a>, our companion library for building low latency streaming web apps with a familiar Gradio syntax. </p></div>\n            </div>\n                \n\nIn the previous guide, we covered how to stream a sequence of outputs from an event handler. Gradio also allows you to stream images from a user's camera or audio chunks from their microphone **into** your event handler. This can be used to create real-time object detection apps or conversational chat applications with Gradio.\n\nCurrently, the `gr.Image` and the `gr.Audio` components support input streaming via the `stream` event.\nLet's create the simplest streaming app possible, which simply returns the webcam stream unmodified.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_img = gr.Image(label=\"Input\", sources=\"webcam\")\n        with gr.Column():\n            output_img = gr.Image(label=\"Output\")\n        input_img.stream(lambda s: s, input_img, output_img, time_limit=15, stream_every=0.1, concurrency_limit=30)\n\nif __name__ == \"__main__\":\n\n    demo.launch()\n\n```\n<gradio-app space='gradio/streaming_simple'></gradio-app>\n\nTry it out! The streaming event is triggered when the user starts recording. Under the hood, the webcam will take a photo every 0.1 seconds and send it to the server. The server will then return that image.\n\nThere are two unique keyword arguments for the `stream` event:\n\n* `time_limit` - This is the amount of time the gradio server will spend processing the event. Media streams are naturally unbounded so it's important to set a time limit so that one user does not hog the Gradio queue. The time limit only counts the time spent processing the stream, not the time spent waiting in the queue. The orange bar displayed at the bottom of the input image represents the remaining time. When the time limit expires, the user will automatically rejoin the queue.\n\n* `stream_every` - This is the frequency (in seconds) with which the stream will capture input and send it to the server. For demos like image detection or manipulation, setting a smaller value is desired to get a \"real-time\" effect. For demos like speech transcription, a higher value is useful so that the transcription algorithm has more context of what's being said.\n\n## A Realistic Image Demo\n\nLet's create a demo where a user can choose a filter to apply to their webcam stream. Users can choose from an edge-detection filter, a cartoon filter, or simply flipping the stream vertically.\n\n```python\nimport gradio as gr\nimport numpy as np\nimport cv2  \n\n\ndef transform_cv2(frame, transform):\n    if transform == \"cartoon\":\n        # prepare color\n        img_color = cv2.pyrDown(cv2.pyrDown(frame))\n        for _ in range(6):\n            img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n        img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n\n        # prepare edges\n        img_edges = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        img_edges = cv2.adaptiveThreshold(\n            cv2.medianBlur(img_edges, 7),\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY,\n            9,\n            2,\n        )\n        img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n        # combine color and edges\n        img = cv2.bitwise_and(img_color, img_edges)\n        return img\n    elif transform == \"edges\":\n        # perform edge detection\n        img = cv2.cvtColor(cv2.Canny(frame, 100, 200), cv2.COLOR_GRAY2BGR)\n        return img\n    else:\n        return np.flipud(frame)\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            transform = gr.Dropdown(\n                choices=[\"cartoon\", \"edges\", \"flip\"],\n                value=\"flip\",\n                label=\"Transformation\",\n            )\n            input_img = gr.Image(sources=[\"webcam\"], type=\"numpy\")\n        with gr.Column():\n            output_img = gr.Image(streaming=True)\n        dep = input_img.stream(\n            transform_cv2,\n            [input_img, transform],\n            [output_img],\n            time_limit=30,\n            stream_every=0.1,\n            concurrency_limit=30,\n        )\n\ndemo.launch()\n\n```\n<gradio-app space='gradio/streaming_filter'></gradio-app>\n\nYou will notice that if you change the filter value it will immediately take effect in the output stream. That is an important difference of stream events in comparison to other Gradio events. The input values of the stream can be changed while the stream is being processed. \n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>We set the \"streaming\" parameter of the image output component to be \"True\". Doing so lets the server automatically convert our output images into base64 format, a format that is efficient for streaming.</p></div>\n            </div>\n                \n\n## Unified Image Demos\n\nFor some image streaming demos, like the one above, we don't need to display separate input and output components. Our app would look cleaner if we could just display the modified output stream.\n\nWe can do so by just specifying the input image component as the output of the stream event.\n\n```python\nimport gradio as gr\nimport numpy as np\nimport cv2  \n\ndef transform_cv2(frame, transform):\n    if transform == \"cartoon\":\n        # prepare color\n        img_color = cv2.pyrDown(cv2.pyrDown(frame))\n        for _ in range(6):\n            img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n        img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n\n        # prepare edges\n        img_edges = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        img_edges = cv2.adaptiveThreshold(\n            cv2.medianBlur(img_edges, 7),\n            255,\n            cv2.ADAPTIVE_THRESH_MEAN_C,\n            cv2.THRESH_BINARY,\n            9,\n            2,\n        )\n        img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n        # combine color and edges\n        img = cv2.bitwise_and(img_color, img_edges)\n        return img\n    elif transform == \"edges\":\n        # perform edge detection\n        img = cv2.cvtColor(cv2.Canny(frame, 100, 200), cv2.COLOR_GRAY2BGR)\n        return img\n    else:\n        return np.flipud(frame)\n\n\ncss=\"\"\".my-group {max-width: 500px !important; max-height: 500px !important;}\n            .my-column {display: flex !important; justify-content: center !important; align-items: center !important};\"\"\"\n\nwith gr.Blocks() as demo:\n    with gr.Column(elem_classes=[\"my-column\"]):\n        with gr.Group(elem_classes=[\"my-group\"]):\n            transform = gr.Dropdown(choices=[\"cartoon\", \"edges\", \"flip\"],\n                                    value=\"flip\", label=\"Transformation\")\n            input_img = gr.Image(sources=[\"webcam\"], type=\"numpy\", streaming=True)\n    input_img.stream(transform_cv2, [input_img, transform], [input_img], time_limit=30, stream_every=0.1)\n\n\nif __name__ == \"__main__\":\n    demo.launch(css=css)\n\n```\n<gradio-app space='gradio/streaming_filter_unified'></gradio-app>\n\n## Keeping track of past inputs or outputs\n\nYour streaming function should be stateless. It should take the current input and return its corresponding output. However, there are cases where you may want to keep track of past inputs or outputs. For example, you may want to keep a buffer of the previous `k` inputs to improve the accuracy of your transcription demo. You can do this with Gradio's `gr.State()` component.\n\nLet's showcase this with a sample demo:\n\n```python\ndef transcribe_handler(current_audio, state, transcript):\n    next_text = transcribe(current_audio, history=state)\n    state.append(current_audio)\n    state = state[-3:]\n    return state, transcript + next_text\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            mic = gr.Audio(sources=\"microphone\")\n            state = gr.State(value=[])\n        with gr.Column():\n            transcript = gr.Textbox(label=\"Transcript\")\n    mic.stream(transcribe_handler, [mic, state, transcript], [state, transcript],\n               time_limit=10, stream_every=1)\n\n\ndemo.launch()\n```\n\n## End-to-End Examples\n\nFor an end-to-end example of streaming from the webcam, see the object detection from webcam [guide](/main/guides/object-detection-from-webcam-with-webrtc).", "tags": [], "spaces": [], "url": "/guides/streaming-inputs/", "contributor": null}}