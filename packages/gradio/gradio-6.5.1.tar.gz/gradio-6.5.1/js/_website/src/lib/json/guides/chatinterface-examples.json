{"guide": {"name": "chatinterface-examples", "category": "chatbots", "pretty_category": "Chatbots", "guide_index": 2, "absolute_index": 32, "pretty_name": "Chatinterface Examples", "content": "# Using Popular LLM libraries and APIs\n\n\n\nIn this Guide, we go through several examples of how to use `gr.ChatInterface` with popular LLM libraries and API providers.\n\nWe will cover the following libraries and API providers:\n\n* [Llama Index](#llama-index)\n* [LangChain](#lang-chain)\n* [OpenAI](#open-ai)\n* [Hugging Face `transformers`](#hugging-face-transformers)\n* [SambaNova](#samba-nova)\n* [Hyperbolic](#hyperbolic)\n* [Anthropic's Claude](#anthropics-claude)\n\nFor many LLM libraries and providers, there exist community-maintained integration libraries that make it even easier to spin up Gradio apps. We reference these libraries in the appropriate sections below.\n\n## Llama Index\n\nLet's start by using `llama-index` on top of `openai` to build a RAG chatbot on any text or PDF files that you can demo and share in less than 30 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)\n\n```python\n# This is a simple RAG chatbot built on top of Llama Index and Gradio. It allows you to upload any text or PDF files and ask questions about them!\n# Before running this, make sure you have exported your OpenAI API key as an environment variable:\n# export OPENAI_API_KEY=\"your-openai-api-key\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader  \nimport gradio as gr\n\ndef answer(message, history):\n    files = []\n    for msg in history:\n        if msg['role'] == \"user\" and isinstance(msg['content'], tuple):\n            files.append(msg['content'][0])\n    for file in message[\"files\"]:\n        files.append(file)\n\n    documents = SimpleDirectoryReader(input_files=files).load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    query_engine = index.as_query_engine()\n    return str(query_engine.query(message[\"text\"]))\n\ndemo = gr.ChatInterface(\n    answer,\n    title=\"Llama Index RAG Chatbot\",\n    description=\"Upload any text or pdf files and ask questions about them!\",\n    textbox=gr.MultimodalTextbox(file_types=[\".pdf\", \".txt\"]),\n    multimodal=True,\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n\n## LangChain\n\nHere's an example using `langchain` on top of `openai` to build a general-purpose chatbot. As before, you'll need to have an OpenAI key for this example.\n\n```python\n# This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n# Before running this, make sure you have exported your OpenAI API key as an environment variable:\n# export OPENAI_API_KEY=\"your-openai-api-key\"\n\nimport gradio as gr\nfrom langchain.messages import AIMessage, HumanMessage  \nfrom langchain_openai import ChatOpenAI  \n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\ndef predict(message, history):\n    history_langchain_format = []\n    for msg in history:\n        if msg[\"role\"] == \"user\":\n            history_langchain_format.append(HumanMessage(content=msg[\"content\"]))\n        elif msg[\"role\"] == \"assistant\":\n            history_langchain_format.append(AIMessage(content=msg[\"content\"]))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = model.invoke(history_langchain_format)\n    return gpt_response.content\n\n\ndemo = gr.ChatInterface(\n    predict,\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>For quick prototyping, the community-maintained <a href='https://github.com/AK391/langchain-gradio'>langchain-gradio repo</a>  makes it even easier to build chatbots on top of LangChain.</p></div>\n            </div>\n                \n\n## OpenAI\n\nOf course, we could also use the `openai` library directy. Here a similar example to the LangChain , but this time with streaming as well:\n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>For quick prototyping, the  <a href='https://github.com/gradio-app/openai-gradio'>openai-gradio library</a> makes it even easier to build chatbots on top of OpenAI models.</p></div>\n            </div>\n                \n\n\n## Hugging Face `transformers`\n\nOf course, in many cases you want to run a chatbot locally. Here's the equivalent example using the SmolLM2-135M-Instruct model using the Hugging Face `transformers` library.\n\n```python\n\nimport gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ndevice = \"cpu\"  # \"cuda\" or \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\n\ndef predict(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)  \n    outputs = model.generate(inputs, max_new_tokens=100, temperature=0.2, top_p=0.9, do_sample=True)\n    decoded = tokenizer.decode(outputs[0])\n    response = decoded.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n    return response\n\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n```\n\n## SambaNova\n\nThe SambaNova Cloud API provides access to full-precision open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the SambaNova API\n\n```python\n# This is a simple general-purpose chatbot built on top of SambaNova API. \n# Before running this, make sure you have exported your SambaNova API key as an environment variable:\n# export SAMBANOVA_API_KEY=\"your-sambanova-api-key\"\n\nimport os\nimport gradio as gr\nfrom openai import OpenAI\n\napi_key = os.getenv(\"SAMBANOVA_API_KEY\")\n\nclient = OpenAI(\n    base_url=\"https://api.sambanova.ai/v1/\",\n    api_key=api_key,\n)\n\ndef predict(message, history):\n    history.append({\"role\": \"user\", \"content\": message})\n    stream = client.chat.completions.create(messages=history, model=\"Meta-Llama-3.1-70B-Instruct-8k\", stream=True)\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk.choices[0].delta.content or \"\")\n        yield \"\".join(chunks)\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n\n```\n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>For quick prototyping, the  <a href='https://github.com/gradio-app/sambanova-gradio'>sambanova-gradio library</a> makes it even easier to build chatbots on top of SambaNova models.</p></div>\n            </div>\n                \n\n## Hyperbolic\n\nThe Hyperbolic AI API provides access to many open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the Hyperbolic\n\n```python\n# This is a simple general-purpose chatbot built on top of Hyperbolic API. \n# Before running this, make sure you have exported your Hyperbolic API key as an environment variable:\n# export HYPERBOLIC_API_KEY=\"your-hyperbolic-api-key\"\n\nimport os\nimport gradio as gr\nfrom openai import OpenAI\n\napi_key = os.getenv(\"HYPERBOLIC_API_KEY\")\n\nclient = OpenAI(\n    base_url=\"https://api.hyperbolic.xyz/v1/\",\n    api_key=api_key,\n)\n\ndef predict(message, history):\n    history.append({\"role\": \"user\", \"content\": message})\n    stream = client.chat.completions.create(messages=history, model=\"gpt-4o-mini\", stream=True)\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk.choices[0].delta.content or \"\")\n        yield \"\".join(chunks)\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n\n```\n            <div class='tip'>\n                <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n                    <path d=\"M15 14c.2-1 .7-1.7 1.5-2.5 1-.9 1.5-2.2 1.5-3.5A6 6 0 0 0 6 8c0 1 .2 2.2 1.5 3.5.7.7 1.3 1.5 1.5 2.5\"/>\n                    <path d=\"M9 18h6\"/>\n                    <path d=\"M10 22h4\"/>\n                </svg>\n                <div><p>For quick prototyping, the  <a href='https://github.com/HyperbolicLabs/hyperbolic-gradio'>hyperbolic-gradio library</a> makes it even easier to build chatbots on top of Hyperbolic models.</p></div>\n            </div>\n                \n\n\n## Anthropic's Claude \n\nAnthropic's Claude model can also be used via API. Here's a simple 20 questions-style game built on top of the Anthropic API:\n\n```python\n# This is a simple 20 questions-style game built on top of the Anthropic API.\n# Before running this, make sure you have exported your Anthropic API key as an environment variable:\n# export ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\nimport anthropic  \nimport gradio as gr\n\nclient = anthropic.Anthropic()\n\ndef predict(message, history):\n    keys_to_keep = [\"role\", \"content\"]\n    history = [{k: d[k] for k in keys_to_keep if k in d} for d in history]\n    history.append({\"role\": \"user\", \"content\": message})\n    if len(history) > 20:\n        history.append({\"role\": \"user\", \"content\": \"DONE\"})\n    output = client.messages.create(\n        messages=history,  \n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1000,\n        system=\"You are guessing an object that the user is thinking of. You can ask 10 yes/no questions. Keep asking questions until the user says DONE\"\n    )\n    return {\n        \"role\": \"assistant\",\n        \"content\": output.content[0].text,  \n        \"options\": [{\"value\": \"Yes\"}, {\"value\": \"No\"}]\n    }\n\nplaceholder = \"\"\"\n<center><h1>10 Questions</h1><br>Think of a person, place, or thing. I'll ask you 10 yes/no questions to try and guess it.\n</center>\n\"\"\"\n\ndemo = gr.ChatInterface(\n    predict,\n    examples=[\"Start!\"],\n    chatbot=gr.Chatbot(placeholder=placeholder),\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n\n\n", "tags": ["LLM", "CHATBOT", "API"], "spaces": [], "url": "/guides/chatinterface-examples/", "contributor": null}}