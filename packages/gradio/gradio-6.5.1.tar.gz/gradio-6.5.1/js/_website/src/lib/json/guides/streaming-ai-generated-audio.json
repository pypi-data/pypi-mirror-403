{"guide": {"name": "streaming-ai-generated-audio", "category": "streaming", "pretty_category": "Streaming", "guide_index": 1, "absolute_index": 43, "pretty_name": "Streaming Ai Generated Audio", "content": "# Streaming AI Generated Audio\n\n\n\nIn this guide, we'll build a novel AI application to showcase Gradio's audio output streaming. We're going to a build a talking [Magic 8 Ball](https://en.wikipedia.org/wiki/Magic_8_Ball) \ud83c\udfb1\n\nA Magic 8 Ball is a toy that answers any question after you shake it. Our application will do the same but it will also speak its response!\n\nWe won't cover all the implementation details in this blog post but the code is freely available on [Hugging Face Spaces](https://huggingface.co/spaces/gradio/magic-8-ball).\n\n## The Overview\n\nJust like the classic Magic 8 Ball, a user should ask it a question orally and then wait for a response. Under the hood, we'll use Whisper to transcribe the audio and then use an LLM to generate a magic-8-ball-style answer. Finally, we'll use Parler TTS to read the response aloud.\n\n## The UI\n\nFirst let's define the UI and put placeholders for all the python logic.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as block:\n    gr.HTML(\n        f\"\"\"\n        <h1 style='text-align: center;'> Magic 8 Ball \ud83c\udfb1 </h1>\n        <h3 style='text-align: center;'> Ask a question and receive wisdom </h3>\n        <p style='text-align: center;'> Powered by <a href=\"https://github.com/huggingface/parler-tts\"> Parler-TTS</a>\n        \"\"\"\n    )\n    with gr.Group():\n        with gr.Row():\n            audio_out = gr.Audio(label=\"Spoken Answer\", streaming=True, autoplay=True)\n            answer = gr.Textbox(label=\"Answer\")\n            state = gr.State()\n        with gr.Row():\n            audio_in = gr.Audio(label=\"Speak your question\", sources=\"microphone\", type=\"filepath\")\n\n    audio_in.stop_recording(generate_response, audio_in, [state, answer, audio_out])\\\n        .then(fn=read_response, inputs=state, outputs=[answer, audio_out])\n\nblock.launch()\n```\n\nWe're placing the output Audio and Textbox components and the input Audio component in separate rows. In order to stream the audio from the server, we'll set `streaming=True` in the output Audio component. We'll also set `autoplay=True` so that the audio plays as soon as it's ready.\nWe'll be using the Audio input component's `stop_recording` event to trigger our application's logic when a user stops recording from their microphone.\n\nWe're separating the logic into two parts. First, `generate_response` will take the recorded audio, transcribe it and generate a response with an LLM. We're going to store the response in a `gr.State` variable that then gets passed to the `read_response` function that generates the audio.\n\nWe're doing this in two parts because only `read_response` will require a GPU. Our app will run on Hugging Faces [ZeroGPU](https://huggingface.co/zero-gpu-explorers) which has time-based quotas. Since generating the response can be done with Hugging Face's Inference API, we shouldn't include that code in our GPU function as it will needlessly use our GPU quota.\n\n## The Logic\n\nAs mentioned above, we'll use [Hugging Face's Inference API](https://huggingface.co/docs/huggingface_hub/guides/inference) to transcribe the audio and generate a response from an LLM. After instantiating the client, I use the `automatic_speech_recognition` method (this automatically uses Whisper running on Hugging Face's Inference Servers) to transcribe the audio. Then I pass the question to an LLM (Mistal-7B-Instruct) to generate a response. We are prompting the LLM to act like a magic 8 ball with the system message.\n\nOur `generate_response` function will also send empty updates to the output textbox and audio components (returning `None`). \nThis is because I want the Gradio progress tracker to be displayed over the components but I don't want to display the answer until the audio is ready.\n\n\n```python\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(token=os.getenv(\"HF_TOKEN\"))\n\ndef generate_response(audio):\n    gr.Info(\"Transcribing Audio\", duration=5)\n    question = client.automatic_speech_recognition(audio).text\n\n    messages = [{\"role\": \"system\", \"content\": (\"You are a magic 8 ball.\"\n                                              \"Someone will present to you a situation or question and your job \"\n                                              \"is to answer with a cryptic adage or proverb such as \"\n                                              \"'curiosity killed the cat' or 'The early bird gets the worm'.\"\n                                              \"Keep your answers short and do not include the phrase 'Magic 8 Ball' in your response. If the question does not make sense or is off-topic, say 'Foolish questions get foolish answers.'\"\n                                              \"For example, 'Magic 8 Ball, should I get a dog?', 'A dog is ready for you but are you ready for the dog?'\")},\n                {\"role\": \"user\", \"content\": f\"Magic 8 Ball please answer this question -  {question}\"}]\n    \n    response = client.chat_completion(messages, max_tokens=64, seed=random.randint(1, 5000),\n                                      model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n\n    response = response.choices[0].message.content.replace(\"Magic 8 Ball\", \"\").replace(\":\", \"\")\n    return response, None, None\n```\n\n\nNow that we have our text response, we'll read it aloud with Parler TTS. The `read_response` function will be a python generator that yields the next chunk of audio as it's ready.\n\n\nWe'll be using the [Mini v0.1](https://huggingface.co/parler-tts/parler_tts_mini_v0.1) for the feature extraction but the [Jenny fine tuned version](https://huggingface.co/parler-tts/parler-tts-mini-jenny-30H) for the voice. This is so that the voice is consistent across generations.\n\n\nStreaming audio with transformers requires a custom Streamer class. You can see the implementation [here](https://huggingface.co/spaces/gradio/magic-8-ball/blob/main/streamer.py). Additionally, we'll convert the output to bytes so that it can be streamed faster from the backend. \n\n\n```python\nfrom streamer import ParlerTTSStreamer\nfrom transformers import AutoTokenizer, AutoFeatureExtractor, set_seed\nimport numpy as np\nimport spaces\nimport torch\nfrom threading import Thread\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if device != \"cpu\" else torch.float32\n\nrepo_id = \"parler-tts/parler_tts_mini_v0.1\"\n\njenny_repo_id = \"ylacombe/parler-tts-mini-jenny-30H\"\n\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\n    jenny_repo_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(repo_id)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(repo_id)\n\nsampling_rate = model.audio_encoder.config.sampling_rate\nframe_rate = model.audio_encoder.config.frame_rate\n\n@spaces.GPU\ndef read_response(answer):\n\n    play_steps_in_s = 2.0\n    play_steps = int(frame_rate * play_steps_in_s)\n\n    description = \"Jenny speaks at an average pace with a calm delivery in a very confined sounding environment with clear audio quality.\"\n    description_tokens = tokenizer(description, return_tensors=\"pt\").to(device)\n\n    streamer = ParlerTTSStreamer(model, device=device, play_steps=play_steps)\n    prompt = tokenizer(answer, return_tensors=\"pt\").to(device)\n\n    generation_kwargs = dict(\n        input_ids=description_tokens.input_ids,\n        prompt_input_ids=prompt.input_ids,\n        streamer=streamer,\n        do_sample=True,\n        temperature=1.0,\n        min_new_tokens=10,\n    )\n\n    set_seed(42)\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    for new_audio in streamer:\n        print(f\"Sample of length: {round(new_audio.shape[0] / sampling_rate, 2)} seconds\")\n        yield answer, numpy_to_mp3(new_audio, sampling_rate=sampling_rate)\n```\n\n## Conclusion\n\nYou can see our final application [here](https://huggingface.co/spaces/gradio/magic-8-ball)!\n\n\n", "tags": ["AUDIO", "STREAMING"], "spaces": [], "url": "/guides/streaming-ai-generated-audio/", "contributor": null}}