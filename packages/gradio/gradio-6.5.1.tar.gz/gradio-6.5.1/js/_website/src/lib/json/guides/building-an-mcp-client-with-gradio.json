{"guide": {"name": "building-an-mcp-client-with-gradio", "category": "mcp", "pretty_category": "Mcp", "guide_index": 3, "absolute_index": 65, "pretty_name": "Building An Mcp Client With Gradio", "content": "# Using the Gradio Chatbot as an MCP Client\n\nThis guide will walk you through a Model Context Protocol (MCP) Client and Server implementation with Gradio. You'll build a Gradio Chatbot that uses Anthropic's Claude API to respond to user messages, but also, as an MCP Client, generates images (by connecting to an MCP Server, which is a separate Gradio app). \n\n<video src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp-guides.mp4\" style=\"width:100%\" controls preload> </video>\n\n## What is MCP?\n\nThe Model Context Protocol (MCP) standardizes how applications provide context to LLMs. It allows Claude to interact with external tools, like image generators, file systems, or APIs, etc.\n\n## Prerequisites\n\n- Python 3.10+\n- An Anthropic API key\n- Basic understanding of Python programming\n\n## Setup\n\nFirst, install the required packages:\n\n```bash\npip install gradio anthropic mcp\n```\n\nCreate a `.env` file in your project directory and add your Anthropic API key:\n\n```\nANTHROPIC_API_KEY=your_api_key_here\n```\n\n## Part 1: Building the MCP Server\n\nThe server provides tools that Claude can use. In this example, we'll create a server that generates images through [a HuggingFace space](https://huggingface.co/spaces/ysharma/SanaSprint).\n\nCreate a file named `gradio_mcp_server.py`:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nimport json\nimport sys\nimport io\nimport time\nfrom gradio_client import Client\n\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\nsys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')\n\nmcp = FastMCP(\"huggingface_spaces_image_display\")\n\n@mcp.tool()\nasync def generate_image(prompt: str, width: int = 512, height: int = 512) -> str:\n    \"\"\"Generate an image using SanaSprint model.\n    \n    Args:\n        prompt: Text prompt describing the image to generate\n        width: Image width (default: 512)\n        height: Image height (default: 512)\n    \"\"\"\n    client = Client(\"https://ysharma-sanasprint.hf.space/\")\n    \n    try:\n        result = client.predict(\n            prompt,\n            \"0.6B\",\n            0,\n            True,\n            width,\n            height,\n            4.0,\n            2,\n            api_name=\"/infer\"\n        )\n        \n        if isinstance(result, list) and len(result) >= 1:\n            image_data = result[0]\n            if isinstance(image_data, dict) and \"url\" in image_data:\n                return json.dumps({\n                    \"type\": \"image\",\n                    \"url\": image_data[\"url\"],\n                    \"message\": f\"Generated image for prompt: {prompt}\"\n                })\n        \n        return json.dumps({\n            \"type\": \"error\",\n            \"message\": \"Failed to generate image\"\n        })\n        \n    except Exception as e:\n        return json.dumps({\n            \"type\": \"error\",\n            \"message\": f\"Error generating image: {str(e)}\"\n        })\n\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')\n```\n\n### What this server does:\n\n1. It creates an MCP server that exposes a `generate_image` tool\n2. The tool connects to the SanaSprint model hosted on HuggingFace Spaces\n3. It handles the asynchronous nature of image generation by polling for results\n4. When an image is ready, it returns the URL in a structured JSON format\n\n## Part 2: Building the MCP Client with Gradio\n\nNow let's create a Gradio chat interface as MCP Client that connects Claude to our MCP server.\n\nCreate a file named `app.py`:\n\n```python\nimport asyncio\nimport os\nimport json\nfrom typing import List, Dict, Any, Union\nfrom contextlib import AsyncExitStack\n\nimport gradio as gr\nfrom gradio.components.chatbot import ChatMessage\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nloop = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\n\nclass MCPClientWrapper:\n    def __init__(self):\n        self.session = None\n        self.exit_stack = None\n        self.anthropic = Anthropic()\n        self.tools = []\n    \n    def connect(self, server_path: str) -> str:\n        return loop.run_until_complete(self._connect(server_path))\n    \n    async def _connect(self, server_path: str) -> str:\n        if self.exit_stack:\n            await self.exit_stack.aclose()\n        \n        self.exit_stack = AsyncExitStack()\n        \n        is_python = server_path.endswith('.py')\n        command = \"python\" if is_python else \"node\"\n        \n        server_params = StdioServerParameters(\n            command=command,\n            args=[server_path],\n            env={\"PYTHONIOENCODING\": \"utf-8\", \"PYTHONUNBUFFERED\": \"1\"}\n        )\n        \n        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n        self.stdio, self.write = stdio_transport\n        \n        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n        await self.session.initialize()\n        \n        response = await self.session.list_tools()\n        self.tools = [{ \n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"input_schema\": tool.inputSchema\n        } for tool in response.tools]\n        \n        tool_names = [tool[\"name\"] for tool in self.tools]\n        return f\"Connected to MCP server. Available tools: {', '.join(tool_names)}\"\n    \n    def process_message(self, message: str, history: List[Union[Dict[str, Any], ChatMessage]]) -> tuple:\n        if not self.session:\n            return history + [\n                {\"role\": \"user\", \"content\": message}, \n                {\"role\": \"assistant\", \"content\": \"Please connect to an MCP server first.\"}\n            ], gr.Textbox(value=\"\")\n        \n        new_messages = loop.run_until_complete(self._process_query(message, history))\n        return history + [{\"role\": \"user\", \"content\": message}] + new_messages, gr.Textbox(value=\"\")\n    \n    async def _process_query(self, message: str, history: List[Union[Dict[str, Any], ChatMessage]]):\n        claude_messages = []\n        for msg in history:\n            if isinstance(msg, ChatMessage):\n                role, content = msg.role, msg.content\n            else:\n                role, content = msg.get(\"role\"), msg.get(\"content\")\n            \n            if role in [\"user\", \"assistant\", \"system\"]:\n                claude_messages.append({\"role\": role, \"content\": content})\n        \n        claude_messages.append({\"role\": \"user\", \"content\": message})\n        \n        response = self.anthropic.messages.create(\n            model=\"claude-3-5-sonnet-20241022\",\n            max_tokens=1000,\n            messages=claude_messages,\n            tools=self.tools\n        )\n\n        result_messages = []\n        \n        for content in response.content:\n            if content.type == 'text':\n                result_messages.append({\n                    \"role\": \"assistant\", \n                    \"content\": content.text\n                })\n                \n            elif content.type == 'tool_use':\n                tool_name = content.name\n                tool_args = content.input\n                \n                result_messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": f\"I'll use the {tool_name} tool to help answer your question.\",\n                    \"metadata\": {\n                        \"title\": f\"Using tool: {tool_name}\",\n                        \"log\": f\"Parameters: {json.dumps(tool_args, ensure_ascii=True)}\",\n                        \"status\": \"pending\",\n                        \"id\": f\"tool_call_{tool_name}\"\n                    }\n                })\n                \n                result_messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": \"```json\\n\" + json.dumps(tool_args, indent=2, ensure_ascii=True) + \"\\n```\",\n                    \"metadata\": {\n                        \"parent_id\": f\"tool_call_{tool_name}\",\n                        \"id\": f\"params_{tool_name}\",\n                        \"title\": \"Tool Parameters\"\n                    }\n                })\n                \n                result = await self.session.call_tool(tool_name, tool_args)\n                \n                if result_messages and \"metadata\" in result_messages[-2]:\n                    result_messages[-2][\"metadata\"][\"status\"] = \"done\"\n                \n                result_messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": \"Here are the results from the tool:\",\n                    \"metadata\": {\n                        \"title\": f\"Tool Result for {tool_name}\",\n                        \"status\": \"done\",\n                        \"id\": f\"result_{tool_name}\"\n                    }\n                })\n                \n                result_content = result.content\n                if isinstance(result_content, list):\n                    result_content = \"\\n\".join(str(item) for item in result_content)\n                \n                try:\n                    result_json = json.loads(result_content)\n                    if isinstance(result_json, dict) and \"type\" in result_json:\n                        if result_json[\"type\"] == \"image\" and \"url\" in result_json:\n                            result_messages.append({\n                                \"role\": \"assistant\",\n                                \"content\": {\"path\": result_json[\"url\"], \"alt_text\": result_json.get(\"message\", \"Generated image\")},\n                                \"metadata\": {\n                                    \"parent_id\": f\"result_{tool_name}\",\n                                    \"id\": f\"image_{tool_name}\",\n                                    \"title\": \"Generated Image\"\n                                }\n                            })\n                        else:\n                            result_messages.append({\n                                \"role\": \"assistant\",\n                                \"content\": \"```\\n\" + result_content + \"\\n```\",\n                                \"metadata\": {\n                                    \"parent_id\": f\"result_{tool_name}\",\n                                    \"id\": f\"raw_result_{tool_name}\",\n                                    \"title\": \"Raw Output\"\n                                }\n                            })\n                except:\n                    result_messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": \"```\\n\" + result_content + \"\\n```\",\n                        \"metadata\": {\n                            \"parent_id\": f\"result_{tool_name}\",\n                            \"id\": f\"raw_result_{tool_name}\",\n                            \"title\": \"Raw Output\"\n                        }\n                    })\n                \n                claude_messages.append({\"role\": \"user\", \"content\": f\"Tool result for {tool_name}: {result_content}\"})\n                next_response = self.anthropic.messages.create(\n                    model=\"claude-3-5-sonnet-20241022\",\n                    max_tokens=1000,\n                    messages=claude_messages,\n                )\n                \n                if next_response.content and next_response.content[0].type == 'text':\n                    result_messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": next_response.content[0].text\n                    })\n\n        return result_messages\n\nclient = MCPClientWrapper()\n\ndef gradio_interface():\n    with gr.Blocks(title=\"MCP Weather Client\") as demo:\n        gr.Markdown(\"# MCP Weather Assistant\")\n        gr.Markdown(\"Connect to your MCP weather server and chat with the assistant\")\n        \n        with gr.Row(equal_height=True):\n            with gr.Column(scale=4):\n                server_path = gr.Textbox(\n                    label=\"Server Script Path\",\n                    placeholder=\"Enter path to server script (e.g., weather.py)\",\n                    value=\"gradio_mcp_server.py\"\n                )\n            with gr.Column(scale=1):\n                connect_btn = gr.Button(\"Connect\")\n        \n        status = gr.Textbox(label=\"Connection Status\", interactive=False)\n        \n        chatbot = gr.Chatbot(\n            value=[], \n            height=500,\n            show_copy_button=True,\n            avatar_images=(\"\ud83d\udc64\", \"\ud83e\udd16\")\n        )\n        \n        with gr.Row(equal_height=True):\n            msg = gr.Textbox(\n                label=\"Your Question\",\n                placeholder=\"Ask about weather or alerts (e.g., What's the weather in New York?)\",\n                scale=4\n            )\n            clear_btn = gr.Button(\"Clear Chat\", scale=1)\n        \n        connect_btn.click(client.connect, inputs=server_path, outputs=status)\n        msg.submit(client.process_message, [msg, chatbot], [chatbot, msg])\n        clear_btn.click(lambda: [], None, chatbot)\n        \n    return demo\n\nif __name__ == \"__main__\":\n    if not os.getenv(\"ANTHROPIC_API_KEY\"):\n        print(\"Warning: ANTHROPIC_API_KEY not found in environment. Please set it in your .env file.\")\n    \n    interface = gradio_interface()\n    interface.launch(debug=True)\n```\n\n### What this MCP Client does:\n\n- Creates a friendly Gradio chat interface for user interaction\n- Connects to the MCP server you specify\n- Handles conversation history and message formatting\n- Makes call to Claude API with tool definitions\n- Processes tool usage requests from Claude\n- Displays images and other tool outputs in the chat\n- Sends tool results back to Claude for interpretation\n\n## Running the Application\n\nTo run your MCP application:\n\n- Start a terminal window and run the MCP Client:\n   ```bash\n   python app.py\n   ```\n- Open the Gradio interface at the URL shown (typically http://127.0.0.1:7860)\n- In the Gradio interface, you'll see a field for the MCP Server path. It should default to `gradio_mcp_server.py`.\n- Click \"Connect\" to establish the connection to the MCP server.\n- You should see a message indicating the server connection was successful.\n\n## Example Usage\n\nNow you can chat with Claude and it will be able to generate images based on your descriptions.\n\nTry prompts like:\n- \"Can you generate an image of a mountain landscape at sunset?\"\n- \"Create an image of a cool tabby cat\"\n- \"Generate a picture of a panda wearing sunglasses\"\n\nClaude will recognize these as image generation requests and automatically use the `generate_image` tool from your MCP server.\n\n\n## How it Works\n\nHere's the high-level flow of what happens during a chat session:\n\n1. Your prompt enters the Gradio interface\n2. The client forwards your prompt to Claude\n3. Claude analyzes the prompt and decides to use the `generate_image` tool\n4. The client sends the tool call to the MCP server\n5. The server calls the external image generation API\n6. The image URL is returned to the client\n7. The client sends the image URL back to Claude\n8. Claude provides a response that references the generated image\n9. The Gradio chat interface displays both Claude's response and the image\n\n\n## Next Steps\n\nNow that you have a working MCP system, here are some ideas to extend it:\n\n- Add more tools to your server\n- Improve error handling \n- Add private Huggingface Spaces with authentication for secure tool access\n- Create custom tools that connect to your own APIs or services\n- Implement streaming responses for better user experience\n\n## Conclusion\n\nCongratulations! You've successfully built an MCP Client and Server that allows Claude to generate images based on text prompts. This is just the beginning of what you can do with Gradio and MCP. This guide enables you to build complex AI applications that can use Claude or any other powerful LLM to interact with virtually any external tool or service.\n\nRead our other Guide on using [Gradio apps as MCP Servers](./building-mcp-server-with-gradio).\n", "tags": [], "spaces": [], "url": "/guides/building-an-mcp-client-with-gradio/", "contributor": null}}