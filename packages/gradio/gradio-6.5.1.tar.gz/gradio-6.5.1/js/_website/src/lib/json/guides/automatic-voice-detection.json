{"guide": {"name": "automatic-voice-detection", "category": "streaming", "pretty_category": "Streaming", "guide_index": 6, "absolute_index": 48, "pretty_name": "Automatic Voice Detection", "content": "# Multimodal Gradio App Powered by Groq with Automatic Speech Detection\n\n\n\n## Introduction\nModern voice applications should feel natural and responsive, moving beyond the traditional \"click-to-record\" pattern. By combining Groq's fast inference capabilities with automatic speech detection, we can create a more intuitive interaction model where users can simply start talking whenever they want to engage with the AI.\n\n> Credits: VAD and Gradio code inspired by [WillHeld's Diva-audio-chat](https://huggingface.co/spaces/WillHeld/diva-audio-chat/tree/main).\n\nIn this tutorial, you will learn how to create a multimodal Gradio and Groq app that has automatic speech detection. You can also watch the full video tutorial which includes a demo of the application:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/azXaioGdm2Q\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n## Background\n\nMany voice apps currently work by the user clicking record, speaking, then stopping the recording. While this can be a powerful demo, the most natural mode of interaction with voice requires the app to dynamically detect when the user is speaking, so they can talk back and forth without having to continually click a record button. \n\nCreating a natural interaction with voice and text requires a dynamic and low-latency response. Thus, we need both automatic voice detection and fast inference. With @ricky0123/vad-web powering speech detection and Groq powering the LLM, both of these requirements are met. Groq provides a lightning fast response, and Gradio allows for easy creation of impressively functional apps.\n\nThis tutorial shows you how to build a calorie tracking app where you speak to an AI that automatically detects when you start and stop your response, and provides its own text response back to guide you with questions that allow it to give a calorie estimate of your last meal.\n\n## Key Components\n\n- **Gradio**: Provides the web interface and audio handling capabilities\n- **@ricky0123/vad-web**: Handles voice activity detection\n- **Groq**: Powers fast LLM inference for natural conversations\n- **Whisper**: Transcribes speech to text\n\n### Setting Up the Environment\n\nFirst, let\u2019s install and import our essential libraries and set up a client for using the Groq API. Here\u2019s how to do it:\n\n`requirements.txt`\n```\ngradio\ngroq\nnumpy\nsoundfile\nlibrosa\nspaces\nxxhash\ndatasets\n```\n\n`app.py`\n```python\nimport groq\nimport gradio as gr\nimport soundfile as sf\nfrom dataclasses import dataclass, field\nimport os\n\n# Initialize Groq client securely\napi_key = os.environ.get(\"GROQ_API_KEY\")\nif not api_key:\n    raise ValueError(\"Please set the GROQ_API_KEY environment variable.\")\nclient = groq.Client(api_key=api_key)\n```\n\nHere, we\u2019re pulling in key libraries to interact with the Groq API, build a sleek UI with Gradio, and handle audio data. We\u2019re accessing the Groq API key securely with a key stored in an environment variable, which is a security best practice for avoiding leaking the API key.\n\n---\n\n### State Management for Seamless Conversations\n\nWe need a way to keep track of our conversation history, so the chatbot remembers past interactions, and manage other states like whether recording is currently active. To do this, let\u2019s create an `AppState` class:\n\n```python\n@dataclass\nclass AppState:\n    conversation: list = field(default_factory=list)\n    stopped: bool = False\n    model_outs: Any = None\n```\n\nOur `AppState` class is a handy tool for managing conversation history and tracking whether recording is on or off. Each instance will have its own fresh list of conversations, making sure chat history is isolated to each session. \n\n---\n\n### Transcribing Audio with Whisper on Groq\n\nNext, we\u2019ll create a function to transcribe the user\u2019s audio input into text using Whisper, a powerful transcription model hosted on Groq. This transcription will also help us determine whether there\u2019s meaningful speech in the input. Here\u2019s how:\n\n```python\ndef transcribe_audio(client, file_name):\n    if file_name is None:\n        return None\n\n    try:\n        with open(file_name, \"rb\") as audio_file:\n            response = client.audio.transcriptions.with_raw_response.create(\n                model=\"whisper-large-v3-turbo\",\n                file=(\"audio.wav\", audio_file),\n                response_format=\"verbose_json\",\n            )\n            completion = process_whisper_response(response.parse())\n            return completion\n    except Exception as e:\n        print(f\"Error in transcription: {e}\")\n        return f\"Error in transcription: {str(e)}\"\n```\n\nThis function opens the audio file and sends it to Groq\u2019s Whisper model for transcription, requesting detailed JSON output. verbose_json is needed to get information to determine if speech was included in the audio. We also handle any potential errors so our app doesn\u2019t fully crash if there\u2019s an issue with the API request. \n\n```python\ndef process_whisper_response(completion):\n    \"\"\"\n    Process Whisper transcription response and return text or null based on no_speech_prob\n    \n    Args:\n        completion: Whisper transcription response object\n        \n    Returns:\n        str or None: Transcribed text if no_speech_prob <= 0.7, otherwise None\n    \"\"\"\n    if completion.segments and len(completion.segments) > 0:\n        no_speech_prob = completion.segments[0].get('no_speech_prob', 0)\n        print(\"No speech prob:\", no_speech_prob)\n\n        if no_speech_prob > 0.7:\n            return None\n            \n        return completion.text.strip()\n    \n    return None\n```\n\nWe also need to interpret the audio data response. The process_whisper_response function takes the resulting completion from Whisper and checks if the audio was just background noise or had actual speaking that was transcribed. It uses a threshold of 0.7 to interpret the no_speech_prob, and will return None if there was no speech. Otherwise, it will return the text transcript of the conversational response from the human.\n\n\n---\n\n### Adding Conversational Intelligence with LLM Integration\n\nOur chatbot needs to provide intelligent, friendly responses that flow naturally. We\u2019ll use a Groq-hosted Llama-3.2 for this:\n\n```python\ndef generate_chat_completion(client, history):\n    messages = []\n    messages.append(\n        {\n            \"role\": \"system\",\n            \"content\": \"In conversation with the user, ask questions to estimate and provide (1) total calories, (2) protein, carbs, and fat in grams, (3) fiber and sugar content. Only ask *one question at a time*. Be conversational and natural.\",\n        }\n    )\n\n    for message in history:\n        messages.append(message)\n\n    try:\n        completion = client.chat.completions.create(\n            model=\"llama-3.2-11b-vision-preview\",\n            messages=messages,\n        )\n        return completion.choices[0].message.content\n    except Exception as e:\n        return f\"Error in generating chat completion: {str(e)}\"\n```\n\nWe\u2019re defining a system prompt to guide the chatbot\u2019s behavior, ensuring it asks one question at a time and keeps things conversational. This setup also includes error handling to ensure the app gracefully manages any issues.\n\n---\n\n### Voice Activity Detection for Hands-Free Interaction\n\nTo make our chatbot hands-free, we\u2019ll add Voice Activity Detection (VAD) to automatically detect when someone starts or stops speaking. Here\u2019s how to implement it using ONNX in JavaScript:\n\n```javascript\nasync function main() {\n  const script1 = document.createElement(\"script\");\n  script1.src = \"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js\";\n  document.head.appendChild(script1)\n  const script2 = document.createElement(\"script\");\n  script2.onload = async () =>  {\n    console.log(\"vad loaded\");\n    var record = document.querySelector('.record-button');\n    record.textContent = \"Just Start Talking!\"\n    \n    const myvad = await vad.MicVAD.new({\n      onSpeechStart: () => {\n        var record = document.querySelector('.record-button');\n        var player = document.querySelector('#streaming-out')\n        if (record != null && (player == null || player.paused)) {\n          record.click();\n        }\n      },\n      onSpeechEnd: (audio) => {\n        var stop = document.querySelector('.stop-button');\n        if (stop != null) {\n          stop.click();\n        }\n      }\n    })\n    myvad.start()\n  }\n  script2.src = \"https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js\";\n}\n```\n\nThis script loads our VAD model and sets up functions to start and stop recording automatically. When the user starts speaking, it triggers the recording, and when they stop, it ends the recording.\n\n---\n\n### Building a User Interface with Gradio\n\nNow, let\u2019s create an intuitive and visually appealing user interface with Gradio. This interface will include an audio input for capturing voice, a chat window for displaying responses, and state management to keep things synchronized.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        input_audio = gr.Audio(\n            label=\"Input Audio\",\n            sources=[\"microphone\"],\n            type=\"numpy\",\n            streaming=False,\n            waveform_options=gr.WaveformOptions(waveform_color=\"#B83A4B\"),\n        )\n    with gr.Row():\n        chatbot = gr.Chatbot(label=\"Conversation\")\n    state = gr.State(value=AppState())\ndemo.launch(theme=theme, js=js)\n```\n\nIn this code block, we\u2019re using Gradio\u2019s `Blocks` API to create an interface with an audio input, a chat display, and an application state manager. The color customization for the waveform adds a nice visual touch.\n\n---\n\n### Handling Recording and Responses\n\nFinally, let\u2019s link the recording and response components to ensure the app reacts smoothly to user inputs and provides responses in real-time.\n\n```python\n    stream = input_audio.start_recording(\n        process_audio,\n        [input_audio, state],\n        [input_audio, state],\n    )\n    respond = input_audio.stop_recording(\n        response, [state, input_audio], [state, chatbot]\n    )\n```\n\nThese lines set up event listeners for starting and stopping the recording, processing the audio input, and generating responses. By linking these events, we create a cohesive experience where users can simply talk, and the chatbot handles the rest.\n\n---\n\n## Summary\n\n1. When you open the app, the VAD system automatically initializes and starts listening for speech\n2. As soon as you start talking, it triggers the recording automatically\n3. When you stop speaking, the recording ends and:\n   - The audio is transcribed using Whisper\n   - The transcribed text is sent to the LLM\n   - The LLM generates a response about calorie tracking\n   - The response is displayed in the chat interface\n4. This creates a natural back-and-forth conversation where you can simply talk about your meals and get instant feedback on nutritional content\n\nThis app demonstrates how to create a natural voice interface that feels responsive and intuitive. By combining Groq's fast inference with automatic speech detection, we've eliminated the need for manual recording controls while maintaining high-quality interactions. The result is a practical calorie tracking assistant that users can simply talk to as naturally as they would to a human nutritionist.\n\nLink to GitHub repository: [Groq Gradio Basics](https://github.com/bklieger-groq/gradio-groq-basics/tree/main/calorie-tracker)", "tags": ["AUDIO", "STREAMING", "CHATBOTS", "VOICE"], "spaces": [], "url": "/guides/automatic-voice-detection/", "contributor": null}}