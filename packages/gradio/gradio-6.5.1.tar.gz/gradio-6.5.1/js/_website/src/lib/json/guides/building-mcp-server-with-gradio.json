{"guide": {"name": "building-mcp-server-with-gradio", "category": "mcp", "pretty_category": "Mcp", "guide_index": 1, "absolute_index": 63, "pretty_name": "Building Mcp Server With Gradio", "content": "# Building an MCP Server with Gradio\n\n\n\nIn this guide, we will describe how to launch your Gradio app so that it functions as an MCP Server.\n\nPunchline: it's as simple as setting `mcp_server=True` in `.launch()`. \n\n### Prerequisites\n\nIf not already installed, please install Gradio with the MCP extra:\n\n```bash\npip install \"gradio[mcp]\"\n```\n\nThis will install the necessary dependencies, including the `mcp` package. Also, you will need an LLM application that supports tool calling using the MCP protocol, such as Claude Desktop, Cursor, or Cline (these are known as \"MCP Clients\").\n\n## What is an MCP Server?\n\nAn MCP (Model Control Protocol) server is a standardized way to expose tools so that they can be used by  LLMs. A tool can provide an LLM functionality that it does not have natively, such as the ability to generate images or calculate the prime factors of a number. \n\n## Example: Counting Letters in a Word\n\nLLMs are famously not great at counting the number of letters in a word (e.g. the number of \"r\"-s in \"strawberry\"). But what if we equip them with a tool to help? Let's start by writing a simple Gradio app that counts the number of letters in a word or phrase:\n\n```python\nimport gradio as gr\n\ndef letter_counter(word, letter):\n    \"\"\"\n    Count the number of occurrences of a letter in a word or text.\n\n    Args:\n        word (str): The input text to search through\n        letter (str): The letter to search for\n\n    Returns:\n        str: A message indicating how many times the letter appears\n    \"\"\"\n    word = word.lower()\n    letter = letter.lower()\n    count = word.count(letter)\n    return count\n\ndemo = gr.Interface(\n    fn=letter_counter,\n    inputs=[gr.Textbox(\"strawberry\"), gr.Textbox(\"r\")],\n    outputs=[gr.Number()],\n    title=\"Letter Counter\",\n    description=\"Enter text and a letter to count how many times the letter appears in the text.\",\n    api_name=\"predict\"\n)\n\nif __name__ == \"__main__\":\n    demo.launch(mcp_server=True)\n\n```\n\nNotice that we have: (1) included a detailed docstring for our function, and (2) set `mcp_server=True` in `.launch()`. This is all that's needed for your Gradio app to serve as an MCP server! Now, when you run this app, it will:\n\n1. Start the regular Gradio web interface\n2. Start the MCP server\n3. Print the MCP server URL in the console\n\nThe MCP server will be accessible at:\n```\nhttp://your-server:port/gradio_api/mcp/\n```\n\nGradio automatically converts the `letter_counter` function into an MCP tool that can be used by LLMs. The docstring of the function and the type hints of arguments will be used to generate the description of the tool and its parameters. The name of the function will be used as the name of your tool. Any initial values you provide to your input components (e.g. \"strawberry\" and \"r\" in the `gr.Textbox` components above) will be used as the default values if your LLM doesn't specify a value for that particular input parameter.\n\nNow, all you need to do is add this URL endpoint to your MCP Client (e.g. Claude Desktop, Cursor, or Cline), which typically means pasting this config in the settings:\n\n```\n{\n  \"mcpServers\": {\n    \"gradio\": {\n      \"url\": \"http://your-server:port/gradio_api/mcp/\"\n    }\n  }\n}\n```\n\n(By the way, you can find the exact config to copy-paste by going to the \"View API\" link in the footer of your Gradio app, and then clicking on \"MCP\").\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-mcp.png)\n\n## Key features of the Gradio <> MCP Integration\n\n1. **Tool Conversion**: Each API endpoint in your Gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. To view the tools and schemas, visit http://your-server:port/gradio_api/mcp/schema or go to the \"View API\" link in the footer of your Gradio app, and then click on \"MCP\".\n\n\n2. **Environment variable support**. There are two ways to enable the MCP server functionality:\n\n*  Using the `mcp_server` parameter, as shown above:\n   ```python\n   demo.launch(mcp_server=True)\n   ```\n\n* Using environment variables:\n   ```bash\n   export GRADIO_MCP_SERVER=True\n   ```\n\n3. **File Handling**: The Gradio MCP server automatically handles file data conversions, including:\n   - Processing image files and returning them in the correct format\n   - Managing temporary file storage\n\n    By default, the Gradio MCP server accepts input images and files as full URLs (\"http://...\" or \"https:/...\"). For convenience, an additional STDIO-based MCP server is also generated, which can be used to upload files to any remote Gradio app and which returns a URL that can be used for subsequent tool calls.\n\n4. **Hosted MCP Servers on \udb40\udc20\ud83e\udd17 Spaces**: You can publish your Gradio application for free on Hugging Face Spaces, which will allow you to have a free hosted MCP server. Here's an example of such a Space: https://huggingface.co/spaces/abidlabs/mcp-tools. Notice that you can add this config to your MCP Client to start using the tools from this Space immediately:\n\n```\n{\n  \"mcpServers\": {\n    \"gradio\": {\n      \"url\": \"https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/\"\n    }\n  }\n}\n```\n\n<video src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/mcp_guide1.mp4\" style=\"width:100%\" controls preload> </video>\n\n\n## Converting an Existing Space\n\nIf there's an existing Space that you'd like to use an MCP server, you'll need to do three things:\n\n1. First, [duplicate the Space](https://huggingface.co/docs/hub/en/spaces-more-ways-to-create#duplicating-a-space) if it is not your own Space. This will allow you to make changes to the app. If the Space requires a GPU, set the hardware of the duplicated Space to be same as the original Space. You can make it either a public Space or a private Space, since it is possible to use either as an MCP server, as described below.\n2. Then, add docstrings to the functions that you'd like the LLM to be able to call as a tool. The docstring should be in the same format as the example code above.\n3. Finally, add `mcp_server=True` in `.launch()`.\n\nThat's it!\n\n## Private Spaces\n\nYou can use either a public Space or a private Space as an MCP server. If you'd like to use a private Space as an MCP server (or a ZeroGPU Space with your own quota), then you will need to provide your [Hugging Face token](https://huggingface.co/settings/token) when you make your request. To do this, simply add it as a header in your config like this:\n\n```\n{\n  \"mcpServers\": {\n    \"gradio\": {\n      \"url\": \"https://abidlabs-mcp-tools.hf.space/gradio_api/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <YOUR-HUGGING-FACE-TOKEN>\"\n      }\n    }\n  }\n}\n```\n\n## Authentication and Credentials\n\nYou may wish to authenticate users more precisely or let them provide other kinds of credentials or tokens in order to provide a custom experience for different users. \n\nGradio allows you to access the underlying `starlette.Request` that has made the tool call, which means that you can access headers, originating IP address, or any other information that is part of the network request. To do this, simply add a parameter in your function of the type `gr.Request`, and Gradio will automatically inject the request object as the parameter.\n\nHere's an example:\n\n```py\nimport gradio as gr\n\ndef echo_headers(x, request: gr.Request):\n    return str(dict(request.headers))\n\ngr.Interface(echo_headers, \"textbox\", \"textbox\").launch(mcp_server=True)\n```\n\nThis MCP server will simply ignore the user's input and echo back all of the headers from a user's request. One can build more complex apps using the same idea. See the [docs on `gr.Request`](https://www.gradio.app/main/docs/gradio/request) for more information (note that only the core Starlette attributes of the `gr.Request` object will be present, attributes such as Gradio's `.session_hash` will not be present).\n\n### Using the gr.Header class\n\nA common pattern in MCP server development is to use authentication headers to call services on behalf of your users. Instead of using a `gr.Request` object like in the example above, you can use a `gr.Header` argument. Gradio will automatically extract that header from the incoming request (if it exists) and pass it to your function.\n\nIn the example below, the `X-API-Token` header is extracted from the incoming request and passed in as the `x_api_token` argument to `make_api_request_on_behalf_of_user`.\n\nThe benefit of using `gr.Header` is that the MCP connection docs will automatically display the headers you need to supply when connecting to the server! See the image below:\n\n```python\nimport gradio as gr\n\ndef make_api_request_on_behalf_of_user(prompt: str, x_api_token: gr.Header):\n    \"\"\"Make a request to everyone's favorite API.\n    Args:\n        prompt: The prompt to send to the API.\n    Returns:\n        The response from the API.\n    Raises:\n        AssertionError: If the API token is not valid.\n    \"\"\"\n    return \"Hello from the API\" if not x_api_token else \"Hello from the API with token!\"\n\n\ndemo = gr.Interface(\n    make_api_request_on_behalf_of_user,\n    [\n        gr.Textbox(label=\"Prompt\"),\n    ],\n    gr.Textbox(label=\"Response\"),\n)\n\ndemo.launch(mcp_server=True)\n```\n\n![MCP Header Connection Page](https://github.com/user-attachments/assets/e264eedf-a91a-476b-880d-5be0d5934134)\n\n### Sending Progress Updates\n\nThe Gradio MCP server automatically sends progress updates to your MCP Client based on the queue in the Gradio application. If you'd like to send custom progress updates, you can do so using the same mechanism as you would use to display progress updates in the UI of your Gradio app: by using the `gr.Progress` class!\n\nHere's an example of how to do this:\n\n```python\nimport gradio as gr\nimport time\n\ndef slow_text_reverser(text: str, progress=gr.Progress()):\n    for i in range(len(text)):\n        progress(i / len(text), desc=\"Reversing text\")\n        time.sleep(0.3)\n    return text[::-1]\n\n\ndemo = gr.Interface(slow_text_reverser, gr.Textbox(\"Hello, world!\"), gr.Textbox(), api_name=\"predict\")\n\nif __name__ == \"__main__\":\n    demo.launch(mcp_server=True)\n\n```\n\n[Here are the docs](https://www.gradio.app/docs/gradio/progress) for the `gr.Progress` class, which can also automatically track `tqdm` calls.\n\nNote: by default, progress notifications are enabled for all MCP tools, even if the corresponding Gradio functions do not include a `gr.Progress`. However, this can add some overhead to the MCP tool (typically ~500ms). To disable progress notification, you can set `queue=False` in your Gradio event handler to skip the overhead related to subscribing to the queue's progress updates.\n\n\n## Modifying Tool Descriptions\n\nGradio automatically sets the tool name based on the name of your function, and the description from the docstring of your function. But you may want to change how the description appears to your LLM. You can do this by using the `api_description` parameter in `Interface`, `ChatInterface`, or any event listener. This parameter takes three different kinds of values:\n\n* `None` (default): the tool description is automatically created from the docstring of the function (or its parent's docstring if it does not have a docstring but inherits from a method that does.)\n* `False`: no tool description appears to the LLM.\n* `str`: an arbitrary string to use as the tool description.\n\nIn addition to modifying the tool descriptions, you can also toggle which tools appear to the LLM. You can do this by setting the `show_api` parameter, which is by default `True`. Setting it to `False` hides the endpoint from the API docs and from the MCP server. If you expose multiple tools, users of your app will also be able to toggle which tools they'd like to add to their MCP server by checking boxes in the \"view MCP or API\" panel.\n\nHere's an example that shows the `api_description` and `show_api` parameters in actions:\n\n```python\nimport numpy as np\nimport gradio as gr\nfrom pathlib import Path\nimport os\nfrom PIL import Image\n\ndef prime_factors(n: str):\n    \"\"\"\n    Compute the prime factorization of a positive integer.\n\n    Args:\n        n (str): The integer to factorize. Must be greater than 1.\n    \"\"\"\n    n_int = int(n)\n    if n_int <= 1:\n        raise ValueError(\"Input must be an integer greater than 1.\")\n\n    factors = []\n    while n_int % 2 == 0:\n        factors.append(2)\n        n_int //= 2\n\n    divisor = 3\n    while divisor * divisor <= n_int:\n        while n_int % divisor == 0:\n            factors.append(divisor)\n            n_int //= divisor\n        divisor += 2\n\n    if n_int > 1:\n        factors.append(n_int)\n\n    return factors\n\n\ndef generate_cheetah_image():\n    \"\"\"\n    Generate a cheetah image.\n\n    Returns:\n        The generated cheetah image.\n    \"\"\"\n    return Path(os.path.dirname(__file__)) / \"cheetah.jpg\"\n\n\ndef image_orientation(image: Image.Image) -> str:\n    \"\"\"\n    Returns whether image is portrait or landscape.\n\n    Args:\n        image (Image.Image): The image to check.\n\n    Returns:\n        str: \"Portrait\" if image is portrait, \"Landscape\" if image is landscape.\n    \"\"\"\n    return \"Portrait\" if image.height > image.width else \"Landscape\"\n\n\ndef sepia(input_img):\n    \"\"\"\n    Apply a sepia filter to the input image.\n\n    Args:\n        input_img (np.array): The input image to apply the sepia filter to.\n\n    Returns:\n        The sepia filtered image.\n    \"\"\"\n    sepia_filter = np.array([\n        [0.393, 0.769, 0.189],\n        [0.349, 0.686, 0.168],\n        [0.272, 0.534, 0.131]\n    ])\n    sepia_img = input_img.dot(sepia_filter.T)\n    sepia_img /= sepia_img.max()\n    return sepia_img\n\n\n\ndemo = gr.TabbedInterface(\n    [\n        gr.Interface(prime_factors, gr.Textbox(\"1001\"), gr.Textbox()),\n        gr.Interface(generate_cheetah_image, None, gr.Image(), api_description=\"Generates a cheetah image. No arguments are required.\"),\n        gr.Interface(image_orientation, gr.Image(type=\"pil\"), gr.Textbox(), api_visibility=\"private\"),\n        gr.Interface(sepia, gr.Image(), gr.Image(), api_description=False),\n    ],\n    [\n        \"Prime Factors\",\n        \"Cheetah Image\",\n        \"Image Orientation Checker\",\n        \"Sepia Filter\",\n    ]\n)\n\nif __name__ == \"__main__\":\n    demo.launch(mcp_server=True)\n\n```\n\n\n\n## MCP Resources and Prompts\n\nIn addition to tools (which execute functions generally and are the default for any function exposed through the Gradio MCP integration), MCP supports two other important primitives: **resources** (for exposing data) and **prompts** (for defining reusable templates). Gradio provides decorators to easily create MCP servers with all three capabilities.\n\n\n### Creating MCP Resources\n\nUse the `@gr.mcp.resource` decorator on any function to expose data through your Gradio app. Resources can be static (always available at a fixed URI) or templated (with parameters in the URI).\n\n```python\n\"\"\"\nAdapts the FastMCP quickstart example to work with Gradio's MCP integration.\n\"\"\"\nimport gradio as gr\n\n\n@gr.mcp.tool()  # Not needed as functions are registered as tools by default\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n@gr.mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n@gr.mcp.prompt()\ndef greet_user(name: str, style: str = \"friendly\") -> str:\n    \"\"\"Generate a greeting prompt\"\"\"\n    styles = {\n        \"friendly\": \"Please write a warm, friendly greeting\",\n        \"formal\": \"Please write a formal, professional greeting\", \n        \"casual\": \"Please write a casual, relaxed greeting\",\n    }\n\n    return f\"{styles.get(style, styles['friendly'])} for someone named {name}.\"\n\n\ndemo = gr.TabbedInterface(\n    [\n        gr.Interface(add, [gr.Number(value=1), gr.Number(value=2)], gr.Number()),\n        gr.Interface(get_greeting, gr.Textbox(\"Abubakar\"), gr.Textbox()),\n        gr.Interface(greet_user, [gr.Textbox(\"Abubakar\"), gr.Dropdown(choices=[\"friendly\", \"formal\", \"casual\"])], gr.Textbox()),\n    ],\n    [\n        \"Add\",\n        \"Get Greeting\",\n        \"Greet User\",\n    ]\n)\n\n\nif __name__ == \"__main__\":\n    demo.launch(mcp_server=True)\n\n```\n\nIn this example:\n- The `get_greeting` function is exposed as a resource with a URI template `greeting://{name}`\n- When an MCP client requests `greeting://Alice`, it receives \"Hello, Alice!\"\n- Resources can also return images and other types of files or binary data. In order to return non-text data, you should specify the `mime_type` parameter in `@gr.mcp.resource()` and return a Base64 string from your function.\n\n### Creating MCP Prompts  \n\nPrompts help standardize how users interact with your tools. They're especially useful for complex workflows that require specific formatting or multiple steps.\n\nThe `greet_user` function in the example above is decorated with `@gr.mcp.prompt()`, which:\n- Makes it available as a prompt template in MCP clients\n- Accepts parameters (`name` and `style`) to customize the output\n- Returns a structured prompt that guides the LLM's behavior\n\n\n## Adding MCP-Only Functions\n\nSo far, all of our MCP tools, resources, or prompts have corresponded to event listeners in the UI. This works well for functions that directly update the UI, but may not work if you wish to expose a \"pure logic\" function that should return raw data (e.g. a JSON object) without directly causing a UI update.\n\nIn order to expose such an MCP tool, you can create a pure Gradio API endpoint using `gr.api` (see [full docs here](https://www.gradio.app/main/docs/gradio/api)). Here's an example of creating an MCP tool that slices a list:\n\n```python\nimport gradio as gr\n\ndef slice_list(lst: list, start: int, end: int) -> list:\n    \"\"\"\n    A tool that slices a list given a start and end index.\n    Args:\n        lst: The list to slice.\n        start: The start index.\n        end: The end index.\n    Returns:\n        The sliced list.\n    \"\"\"\n    return lst[start:end]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n        This is a demo of a MCP-only tool.\n        This tool slices a list.\n        This tool is MCP-only, so it does not have a UI.\n        \"\"\"\n    )\n    gr.api(\n        slice_list\n    )\n\n_, url, _ = demo.launch(mcp_server=True)\n```\n\nNote that if you use this approach, your function signature must be fully typed, including the return value, as these signature are used to determine the typing information for the MCP tool.\n\n## Gradio with FastMCP\n\nIn some cases, you may decide not to use Gradio's built-in integration and instead manually create an FastMCP Server that calls a Gradio app. This approach is useful when you want to:\n\n- Store state / identify users between calls instead of treating every tool call completely independently\n- Start the Gradio app MCP server when a tool is called (if you are running multiple Gradio apps locally and want to save memory / GPU)\n\nThis is very doable thanks to the [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) and the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)'s `FastMCP` class. Here's an example of creating a custom MCP server that connects to various Gradio apps hosted on [HuggingFace Spaces](https://huggingface.co/spaces) using the `stdio` protocol:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom gradio_client import Client\nimport sys\nimport io\nimport json \n\nmcp = FastMCP(\"gradio-spaces\")\n\nclients = {}\n\ndef get_client(space_id: str) -> Client:\n    \"\"\"Get or create a Gradio client for the specified space.\"\"\"\n    if space_id not in clients:\n        clients[space_id] = Client(space_id)\n    return clients[space_id]\n\n\n@mcp.tool()\nasync def generate_image(prompt: str, space_id: str = \"ysharma/SanaSprint\") -> str:\n    \"\"\"Generate an image using Flux.\n    \n    Args:\n        prompt: Text prompt describing the image to generate\n        space_id: HuggingFace Space ID to use \n    \"\"\"\n    client = get_client(space_id)\n    result = client.predict(\n            prompt=prompt,\n            model_size=\"1.6B\",\n            seed=0,\n            randomize_seed=True,\n            width=1024,\n            height=1024,\n            guidance_scale=4.5,\n            num_inference_steps=2,\n            api_name=\"/infer\"\n    )\n    return result\n\n\n@mcp.tool()\nasync def run_dia_tts(prompt: str, space_id: str = \"ysharma/Dia-1.6B\") -> str:\n    \"\"\"Text-to-Speech Synthesis.\n    \n    Args:\n        prompt: Text prompt describing the conversation between speakers S1, S2\n        space_id: HuggingFace Space ID to use \n    \"\"\"\n    client = get_client(space_id)\n    result = client.predict(\n            text_input=f\"\"\"{prompt}\"\"\",\n            audio_prompt_input=None, \n            max_new_tokens=3072,\n            cfg_scale=3,\n            temperature=1.3,\n            top_p=0.95,\n            cfg_filter_top_k=30,\n            speed_factor=0.94,\n            api_name=\"/generate_audio\"\n    )\n    return result\n\n\nif __name__ == \"__main__\":\n    import sys\n    import io\n    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n    \n    mcp.run(transport='stdio')\n```\n\nThis server exposes two tools:\n1. `run_dia_tts` - Generates a conversation for the given transcript in the form of `[S1]first-sentence. [S2]second-sentence. [S1]...`\n2. `generate_image` - Generates images using a fast text-to-image model\n\nTo use this MCP Server with Claude Desktop (as MCP Client):\n\n1. Save the code to a file (e.g., `gradio_mcp_server.py`)\n2. Install the required dependencies: `pip install mcp gradio-client`\n3. Configure Claude Desktop to use your server by editing the configuration file at `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n    \"mcpServers\": {\n        \"gradio-spaces\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"/absolute/path/to/gradio_mcp_server.py\"\n            ]\n        }\n    }\n}\n```\n\n4. Restart Claude Desktop\n\nNow, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.\n\n\n## Troubleshooting your MCP Servers\n\nThe MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.\n\nHere are some things that may help:\n\n**1. Ensure that you've provided type hints and valid docstrings for your functions**\n\nAs mentioned earlier, Gradio reads the docstrings for your functions and the type hints of input arguments to generate the description of the tool and parameters. A valid function and docstring looks like this (note the \"Args:\" block with indented parameter names underneath):\n\n```py\ndef image_orientation(image: Image.Image) -> str:\n    \"\"\"\n    Returns whether image is portrait or landscape.\n\n    Args:\n        image (Image.Image): The image to check.\n    \"\"\"\n    return \"Portrait\" if image.height > image.width else \"Landscape\"\n```\n\nNote: You can preview the schema that is created for your MCP server by visiting the `http://your-server:port/gradio_api/mcp/schema` URL.\n\n**2. Try accepting input arguments as `str`**\n\nSome MCP Clients do not recognize parameters that are numeric or other complex types, but all of the MCP Clients that we've tested accept `str` input parameters. When in doubt, change your input parameter to be a `str` and then cast to a specific type in the function, as in this example:\n\n```py\ndef prime_factors(n: str):\n    \"\"\"\n    Compute the prime factorization of a positive integer.\n\n    Args:\n        n (str): The integer to factorize. Must be greater than 1.\n    \"\"\"\n    n_int = int(n)\n    if n_int <= 1:\n        raise ValueError(\"Input must be an integer greater than 1.\")\n\n    factors = []\n    while n_int % 2 == 0:\n        factors.append(2)\n        n_int //= 2\n\n    divisor = 3\n    while divisor * divisor <= n_int:\n        while n_int % divisor == 0:\n            factors.append(divisor)\n            n_int //= divisor\n        divisor += 2\n\n    if n_int > 1:\n        factors.append(n_int)\n\n    return factors\n```\n\n**3. Ensure that your MCP Client Supports Streamable HTTP**\n\nSome MCP Clients do not yet support streamable HTTP-based MCP Servers. In those cases, you can use a tool such as [mcp-remote](https://github.com/geelen/mcp-remote). First install [Node.js](https://nodejs.org/en/download/). Then, add the following to your own MCP Client config:\n\n```\n{\n  \"mcpServers\": {\n    \"gradio\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"http://your-server:port/gradio_api/mcp/\"\n      ]\n    }\n  }\n}\n```\n\n**4. Restart your MCP Client and MCP Server**\n\nSome MCP Clients require you to restart them every time you update the MCP configuration. Other times, if the connection between the MCP Client and servers breaks, you might need to restart the MCP server. If all else fails, try restarting both your MCP Client and MCP Servers!\n\n", "tags": ["MCP", "TOOL", "LLM", "SERVER"], "spaces": [], "url": "/guides/building-mcp-server-with-gradio/", "contributor": null}}