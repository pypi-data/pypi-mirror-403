"""
ç´¢å¼•ç®¡ç†å™¨ï¼ˆæ¶æ§‹ Bï¼‰
ä½¿ç”¨ SQLite FTS5 é€²è¡Œå…¨æ–‡æª¢ç´¢
"""
import sqlite3
import hashlib
import sys
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing


def _log(msg: str):
    """è¼¸å‡ºåˆ° stderrï¼ˆé¿å…å¹²æ“¾ MCP stdio é€šè¨Šï¼‰"""
    print(msg, file=sys.stderr)


class IndexManager:
    """
    SQLite FTS5 ç´¢å¼•ç®¡ç†å™¨ï¼ˆæ¶æ§‹ Bï¼‰
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self._init_connection()
        self._init_tables()

    def _init_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£ç·š"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row

    def _init_tables(self):
        """å»ºç«‹ç´¢å¼•è¡¨"""
        # å…ˆå»ºç«‹åŸºç¤è¡¨
        self.conn.executescript("""
            -- ä¾†æºè¡¨ï¼ˆå¤–éƒ¨ç›®éŒ„è¨»å†Šï¼‰
            CREATE TABLE IF NOT EXISTS sources (
                id INTEGER PRIMARY KEY,
                path TEXT UNIQUE,
                source_type TEXT,
                category TEXT,
                recursive BOOLEAN DEFAULT 1,
                last_sync TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- è¨˜æ†¶ç´¢å¼•è¡¨ï¼ˆåŸºç¤çµæ§‹ï¼‰
            CREATE TABLE IF NOT EXISTS memories (
                id TEXT PRIMARY KEY,
                title TEXT,
                file_path TEXT UNIQUE,
                speaker TEXT CHECK(speaker IN ('user', 'ai', 'external')),
                authority INTEGER CHECK(authority BETWEEN 1 AND 10),
                quality INTEGER CHECK(quality BETWEEN 1 AND 5),
                captured_at TIMESTAMP NOT NULL,
                updated TIMESTAMP NOT NULL,
                tags TEXT,
                keywords TEXT,
                content_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- å…¨æ–‡æª¢ç´¢ç´¢å¼•ï¼ˆFTS5ï¼‰
            CREATE VIRTUAL TABLE IF NOT EXISTS memories_fts USING fts5(
                id UNINDEXED,
                title,
                content,
                tokenize='unicode61 remove_diacritics 2'
            );

            -- å¯¦é«”è¡¨ï¼ˆçŸ¥è­˜åœ–è­œç¯€é»ï¼‰
            CREATE TABLE IF NOT EXISTS entities (
                id TEXT PRIMARY KEY,
                type TEXT NOT NULL,
                canonical_name TEXT NOT NULL,
                properties TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            -- åˆ¥åè¡¨ï¼ˆå¯¦é«”çš„å¤šç¨®ç¨±å‘¼ï¼‰
            CREATE TABLE IF NOT EXISTS aliases (
                id INTEGER PRIMARY KEY,
                alias TEXT NOT NULL,
                entity_id TEXT NOT NULL,
                confidence REAL DEFAULT 1.0,
                source TEXT DEFAULT 'user_confirmed',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE
            );

            -- é—œä¿‚è¡¨ï¼ˆå¯¦é«”ä¹‹é–“çš„é€£çµï¼‰
            CREATE TABLE IF NOT EXISTS relations (
                id INTEGER PRIMARY KEY,
                from_id TEXT NOT NULL,
                relation TEXT NOT NULL,
                to_id TEXT NOT NULL,
                properties TEXT,
                confidence REAL DEFAULT 1.0,
                source TEXT DEFAULT 'user_confirmed',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (from_id) REFERENCES entities(id) ON DELETE CASCADE,
                FOREIGN KEY (to_id) REFERENCES entities(id) ON DELETE CASCADE
            );

            -- è¨˜æ†¶-å¯¦é«”é—œè¯è¡¨
            CREATE TABLE IF NOT EXISTS memory_entities (
                memory_id TEXT NOT NULL,
                entity_id TEXT NOT NULL,
                relation TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (memory_id, entity_id, relation),
                FOREIGN KEY (memory_id) REFERENCES memories(id) ON DELETE CASCADE,
                FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE
            );

            -- å¾…ç¢ºèªä½‡åˆ—ï¼ˆå¯¦é«”è§£æç”¨ï¼‰
            CREATE TABLE IF NOT EXISTS pending_confirmations (
                id INTEGER PRIMARY KEY,
                memory_id TEXT,
                extracted_name TEXT NOT NULL,
                suggested_entity_id TEXT,
                suggested_type TEXT,
                context TEXT,
                confidence REAL DEFAULT 0.5,
                status TEXT DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                resolved_at TIMESTAMP,
                FOREIGN KEY (memory_id) REFERENCES memories(id) ON DELETE CASCADE,
                FOREIGN KEY (suggested_entity_id) REFERENCES entities(id) ON DELETE SET NULL
            );

            -- åŸºç¤è¤‡åˆç´¢å¼•
            CREATE INDEX IF NOT EXISTS idx_speaker_authority
                ON memories(speaker, authority DESC);
            CREATE INDEX IF NOT EXISTS idx_updated
                ON memories(updated DESC);
            CREATE INDEX IF NOT EXISTS idx_quality_updated
                ON memories(quality DESC, updated DESC);
            CREATE INDEX IF NOT EXISTS idx_tags
                ON memories(tags);
            
            -- å¯¦é«”ç›¸é—œç´¢å¼•
            CREATE INDEX IF NOT EXISTS idx_entities_type
                ON entities(type);
            CREATE INDEX IF NOT EXISTS idx_entities_name
                ON entities(canonical_name);
            CREATE INDEX IF NOT EXISTS idx_aliases_alias
                ON aliases(alias);
            CREATE INDEX IF NOT EXISTS idx_aliases_entity
                ON aliases(entity_id);
            CREATE INDEX IF NOT EXISTS idx_relations_from
                ON relations(from_id);
            CREATE INDEX IF NOT EXISTS idx_relations_to
                ON relations(to_id);
            CREATE INDEX IF NOT EXISTS idx_relations_type
                ON relations(relation);
            CREATE INDEX IF NOT EXISTS idx_memory_entities_memory
                ON memory_entities(memory_id);
            CREATE INDEX IF NOT EXISTS idx_memory_entities_entity
                ON memory_entities(entity_id);
            CREATE INDEX IF NOT EXISTS idx_pending_status
                ON pending_confirmations(status);
            CREATE INDEX IF NOT EXISTS idx_pending_memory
                ON pending_confirmations(memory_id);
        """)
        self.conn.commit()
        
        # é·ç§»ï¼šç‚ºç¾æœ‰è¡¨æ–°å¢æ¬„ä½ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self._migrate_tables()
    
    def _migrate_tables(self):
        """è³‡æ–™åº«é·ç§»ï¼šæ–°å¢æ¬„ä½"""
        cursor = self.conn.execute("PRAGMA table_info(memories)")
        columns = {row[1] for row in cursor.fetchall()}
        
        migrations = [
            ("source_id", "INTEGER"),
            ("storage_mode", "TEXT DEFAULT 'internal'"),
            ("mtime", "INTEGER"),
            ("remind_at", "TIMESTAMP"),
        ]
        
        for col_name, col_type in migrations:
            if col_name not in columns:
                try:
                    self.conn.execute(f"ALTER TABLE memories ADD COLUMN {col_name} {col_type}")
                    self.conn.commit()
                except sqlite3.OperationalError:
                    pass
        
        # å»ºç«‹æ–°æ¬„ä½çš„ç´¢å¼•ï¼ˆé·ç§»å¾Œï¼‰
        try:
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_source_id ON memories(source_id)")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_storage_mode ON memories(storage_mode)")
            self.conn.commit()
        except sqlite3.OperationalError:
            pass

    def update(self, memory_id: str, file_path: str, metadata: Dict, content: str, auto_commit: bool = True):
        """æ›´æ–°ç´¢å¼•ï¼ˆæ”¯æ´æ‰¹æ¬¡æ¨¡å¼ï¼‰
        
        Args:
            memory_id: è¨˜æ†¶ ID
            file_path: æª”æ¡ˆè·¯å¾‘
            metadata: å…ƒè³‡æ–™
            content: å…§å®¹
            auto_commit: æ˜¯å¦è‡ªå‹• commitï¼ˆæ‰¹æ¬¡æ¨¡å¼è¨­ç‚º Falseï¼‰
        """
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        
        # è™•ç† keywordsï¼ˆå¯èƒ½æ˜¯å­—ä¸²æˆ–åˆ—è¡¨ï¼‰
        keywords = metadata.get('keywords', '')
        if isinstance(keywords, list):
            keywords = ','.join(str(k) for k in keywords if k)

        self.conn.execute("""
            INSERT OR REPLACE INTO memories
            (id, title, file_path, speaker, authority, quality, captured_at, updated, tags, keywords, content_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            memory_id,
            metadata.get('title', ''),
            file_path,
            metadata.get('speaker', 'external'),
            metadata.get('authority', 3),
            metadata.get('quality', 3),
            metadata.get('captured_at', ''),
            metadata.get('updated', ''),
            ','.join(metadata.get('tags', [])),
            keywords,
            content_hash
        ))

        self.conn.execute("""
            INSERT OR REPLACE INTO memories_fts (id, title, content)
            VALUES (?, ?, ?)
        """, (memory_id, metadata.get('title', ''), content))

        if auto_commit:
            self.conn.commit()
    
    def batch_update(self, memories: List[Tuple[str, str, Dict, str]], batch_size: int = 100):
        """æ‰¹æ¬¡æ›´æ–°ç´¢å¼•
        
        Args:
            memories: [(memory_id, file_path, metadata, content), ...]
            batch_size: æ¯æ‰¹æ¬¡å¤§å°
        """
        total = len(memories)
        
        for i in range(0, total, batch_size):
            batch = memories[i:i+batch_size]
            
            for memory_id, file_path, metadata, content in batch:
                self.update(memory_id, file_path, metadata, content, auto_commit=False)
            
            self.conn.commit()
            
            if (i + batch_size) % 500 == 0 or (i + batch_size) >= total:
                _log(f"  å·²ç´¢å¼• {min(i + batch_size, total)} / {total} ç­†...")

    def search(self, query: str, filters: Optional[Dict] = None, limit: int = 10) -> List[Dict]:
        """å…¨æ–‡æª¢ç´¢ï¼ˆFTS5 å„ªå…ˆï¼ŒLIKE fallback ç”¨æ–¼ä¸­æ–‡ï¼‰"""
        results = self._search_fts(query, filters, limit)
        
        # FTS5 ç„¡çµæœæ™‚ï¼Œå˜—è©¦ LIKE æŸ¥è©¢ï¼ˆæ”¯æ´ä¸­æ–‡ï¼‰
        if not results:
            results = self._search_like(query, filters, limit)
        
        return results
    
    def _search_fts(self, query: str, filters: Optional[Dict] = None, limit: int = 10) -> List[Dict]:
        """FTS5 å…¨æ–‡æª¢ç´¢"""
        sql = """
            SELECT
                m.*,
                snippet(memories_fts, 2, '<mark>', '</mark>', '...', 64) as snippet,
                rank
            FROM memories m
            JOIN memories_fts fts ON m.id = fts.id
            WHERE fts.content MATCH ?
        """
        params = [query]
        sql = self._add_filters(sql, params, filters)
        sql += " ORDER BY rank LIMIT ?"
        params.append(limit)

        try:
            cursor = self.conn.execute(sql, params)
            rows = cursor.fetchall()
        except Exception as e:
            # FTS5 æœå°‹å¤±æ•—ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡æŸ¥è©¢ï¼‰ï¼Œè¿”å›ç©ºè®“ LIKE fallback
            return []
        
        return self._rows_to_results(rows)
    
    def _search_like(self, query: str, filters: Optional[Dict] = None, limit: int = 10) -> List[Dict]:
        """LIKE æ¨¡ç³Šæœå°‹ï¼ˆæ”¯æ´ä¸­æ–‡ï¼‰"""
        sql = """
            SELECT
                m.*,
                '' as snippet,
                0 as rank
            FROM memories m
            JOIN memories_fts fts ON m.id = fts.id
            WHERE (fts.title LIKE ? OR fts.content LIKE ?)
        """
        like_pattern = f'%{query}%'
        params = [like_pattern, like_pattern]
        sql = self._add_filters(sql, params, filters)
        sql += " ORDER BY m.captured_at DESC LIMIT ?"
        params.append(limit)

        try:
            cursor = self.conn.execute(sql, params)
            rows = cursor.fetchall()
        except Exception as e:
            _log(f"LIKE search error: {e}")
            return []
        
        return self._rows_to_results(rows)
    
    def _add_filters(self, sql: str, params: list, filters: Optional[Dict]) -> str:
        """åŠ å…¥éæ¿¾æ¢ä»¶åˆ° SQLï¼Œè¿”å›ä¿®æ”¹å¾Œçš„ SQL"""
        if not filters:
            return sql
        
        if 'speaker' in filters:
            sql += " AND m.speaker = ?"
            params.append(filters['speaker'])

        if 'authority' in filters:
            if isinstance(filters['authority'], dict):
                if 'gte' in filters['authority']:
                    sql += " AND m.authority >= ?"
                    params.append(filters['authority']['gte'])
                if 'lte' in filters['authority']:
                    sql += " AND m.authority <= ?"
                    params.append(filters['authority']['lte'])
            else:
                sql += " AND m.authority >= ?"
                params.append(filters['authority'])

        if 'quality' in filters:
            if isinstance(filters['quality'], dict):
                if 'gte' in filters['quality']:
                    sql += " AND m.quality >= ?"
                    params.append(filters['quality']['gte'])
            else:
                sql += " AND m.quality >= ?"
                params.append(filters['quality'])

        if 'date_range' in filters:
            start, end = filters['date_range']
            sql += " AND m.captured_at BETWEEN ? AND ?"
            params.extend([start, end])

        if 'tags' in filters:
            for tag in filters['tags']:
                sql += " AND m.tags LIKE ?"
                params.append(f'%{tag}%')
        
        return sql
    
    def _rows_to_results(self, rows) -> List[Dict]:
        """è½‰æ›è³‡æ–™åˆ—ç‚ºçµæœ"""
        results = []
        for row in rows:
            results.append({
                'id': row['id'],
                'title': row['title'],
                'file_path': row['file_path'],
                'metadata': {
                    'speaker': row['speaker'],
                    'authority': row['authority'],
                    'quality': row['quality'],
                    'captured_at': row['captured_at'],
                    'updated': row['updated'],
                    'tags': row['tags'].split(',') if row['tags'] else [],
                    'keywords': row['keywords']
                },
                'snippet': row['snippet'] if len(row) > 12 else ''
            })
        return results

    def get_file_path(self, memory_id: str) -> Optional[str]:
        """å–å¾—è¨˜æ†¶æª”æ¡ˆè·¯å¾‘"""
        cursor = self.conn.execute(
            "SELECT file_path FROM memories WHERE id = ?",
            (memory_id,)
        )
        row = cursor.fetchone()
        return row['file_path'] if row else None

    def delete(self, memory_id: str):
        """åˆªé™¤ç´¢å¼•"""
        self.conn.execute("DELETE FROM memories WHERE id = ?", (memory_id,))
        self.conn.execute("DELETE FROM memories_fts WHERE id = ?", (memory_id,))
        self.conn.commit()

    def rebuild(self, memory_manager, parallel: bool = True, workers: int = None):
        """é‡å»ºç´¢å¼•ï¼ˆæ”¯æ´å¹³è¡Œè™•ç†ï¼‰
        
        Args:
            memory_manager: è¨˜æ†¶ç®¡ç†å™¨
            parallel: æ˜¯å¦ä½¿ç”¨å¹³è¡Œè™•ç†
            workers: å·¥ä½œé€²ç¨‹æ•¸ï¼ˆNone å‰‡è‡ªå‹•ï¼‰
        """
        _log("ğŸ—‘ï¸  æ¸…ç©ºç¾æœ‰ç´¢å¼•...")
        self.conn.execute("DELETE FROM memories")
        self.conn.execute("DELETE FROM memories_fts")
        self.conn.commit()

        _log("ğŸ“‚ æƒæè¨˜æ†¶æª”æ¡ˆ...")
        memory_list = memory_manager.list_memories(limit=999999)
        total = len(memory_list)
        _log(f"   æ‰¾åˆ° {total} ç­†è¨˜æ†¶")
        
        if not memory_list:
            _log("ç´¢å¼•é‡å»ºå®Œæˆï¼šå…± 0 ç­†è¨˜æ†¶")
            return

        if parallel and total > 100:
            _log(f"ğŸš€ ä½¿ç”¨å¹³è¡Œè™•ç†æ¨¡å¼...")
            self._rebuild_parallel(memory_manager, memory_list, workers)
        else:
            _log(f"ğŸ“ ä½¿ç”¨å¾ªåºè™•ç†æ¨¡å¼...")
            self._rebuild_sequential(memory_manager, memory_list)

        _log(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆï¼šå…± {total} ç­†è¨˜æ†¶")
    
    def _rebuild_sequential(self, memory_manager, memory_list: List[Dict]):
        """å¾ªåºé‡å»ºç´¢å¼•"""
        total = len(memory_list)
        
        for i, mem in enumerate(memory_list, 1):
            full_memory = memory_manager.read_memory(mem['id'])
            if full_memory:
                self.update(
                    full_memory['id'],
                    full_memory['file_path'],
                    full_memory['metadata'],
                    full_memory['content'],
                    auto_commit=False
                )
            
            if i % 100 == 0 or i == total:
                self.conn.commit()
                _log(f"  å·²ç´¢å¼• {i} / {total} ç­†...")
        
        self.conn.commit()
    
    def _rebuild_parallel(self, memory_manager, memory_list: List[Dict], workers: int = None):
        """å¹³è¡Œé‡å»ºç´¢å¼•
        
        ç­–ç•¥ï¼š
        1. å¤šé€²ç¨‹å¹³è¡Œè®€å–æª”æ¡ˆï¼ˆI/O å¯†é›†ï¼‰
        2. ä¸»é€²ç¨‹æ‰¹æ¬¡å¯«å…¥ SQLiteï¼ˆå–®åŸ·è¡Œç·’é–å®šï¼‰
        """
        if workers is None:
            workers = min(multiprocessing.cpu_count(), 8)
        
        _log(f"   ä½¿ç”¨ {workers} å€‹å·¥ä½œé€²ç¨‹")
        
        # å¹³è¡Œè®€å–æª”æ¡ˆ
        memories_data = []
        completed = 0
        total = len(memory_list)
        
        with ProcessPoolExecutor(max_workers=workers) as executor:
            # æäº¤æ‰€æœ‰è®€å–ä»»å‹™
            future_to_mem = {
                executor.submit(_read_memory_worker, mem['id'], mem['file_path']): mem
                for mem in memory_list
            }
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_mem):
                completed += 1
                if completed % 500 == 0:
                    _log(f"  å·²è®€å– {completed} / {total} ç­†...")
                
                try:
                    result = future.result()
                    if result:
                        memories_data.append(result)
                except Exception as e:
                    mem = future_to_mem[future]
                    _log(f"  âš ï¸  è®€å–å¤±æ•—: {mem['id']} - {e}")
        
        _log(f"  å·²è®€å– {len(memories_data)} ç­†è¨˜æ†¶")
        
        # æ‰¹æ¬¡å¯«å…¥ç´¢å¼•
        _log(f"ğŸ’¾ æ‰¹æ¬¡å¯«å…¥ç´¢å¼•...")
        self.batch_update(memories_data, batch_size=100)

    def get_all_tags(self) -> Dict:
        """å–å¾—æ‰€æœ‰ tags"""
        cursor = self.conn.execute(
            "SELECT tags FROM memories WHERE tags != '' AND tags IS NOT NULL"
        )
        all_tags = set()
        for row in cursor:
            if row['tags']:
                for tag in row['tags'].split(','):
                    tag = tag.strip()
                    if tag:
                        all_tags.add(tag)
        
        return {
            'tags': sorted(list(all_tags)),
            'total': len(all_tags)
        }

    def browse_by_category(self, category: str, limit: int = 50) -> List[Dict]:
        """ç€è¦½åˆ†é¡"""
        sql = """
            SELECT id, title, file_path, speaker, authority, quality,
                   captured_at, updated, tags, keywords
            FROM memories
            WHERE tags LIKE ? OR file_path LIKE ?
            ORDER BY updated DESC
            LIMIT ?
        """
        pattern = f'%{category}%'
        cursor = self.conn.execute(sql, (pattern, pattern, limit))
        
        results = []
        for row in cursor:
            results.append({
                'id': row['id'],
                'title': row['title'],
                'file_path': row['file_path'],
                'metadata': {
                    'speaker': row['speaker'],
                    'authority': row['authority'],
                    'quality': row['quality'],
                    'captured_at': row['captured_at'],
                    'updated': row['updated'],
                    'tags': row['tags'].split(',') if row['tags'] else [],
                    'keywords': row['keywords']
                }
            })
        return results

    def get_categories(self) -> Dict:
        """å–å¾—æ‰€æœ‰åˆ†é¡"""
        cursor = self.conn.execute("""
            SELECT DISTINCT
                CASE
                    WHEN file_path LIKE '%èªç¾©è¨˜æ†¶%' THEN 'èªç¾©è¨˜æ†¶'
                    WHEN file_path LIKE '%æƒ…ç¯€è¨˜æ†¶%' THEN 'æƒ…ç¯€è¨˜æ†¶'
                    WHEN file_path LIKE '%ç¨‹åºè¨˜æ†¶%' THEN 'ç¨‹åºè¨˜æ†¶'
                    ELSE 'å…¶ä»–'
                END as category,
                COUNT(*) as count
            FROM memories
            GROUP BY category
        """)
        
        categories = {}
        for row in cursor:
            categories[row['category']] = row['count']
        
        return {
            'categories': list(categories.keys()),
            'counts': categories,
            'total': sum(categories.values())
        }

    def get_stats(self) -> Dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total,
                COUNT(CASE WHEN speaker = 'user' THEN 1 END) as user_memories,
                COUNT(CASE WHEN speaker = 'ai' THEN 1 END) as ai_memories,
                COUNT(CASE WHEN quality >= 4 THEN 1 END) as high_quality,
                AVG(authority) as avg_authority
            FROM memories
        """)
        row = cursor.fetchone()

        return {
            'total_memories': row['total'],
            'user_memories': row['user_memories'],
            'ai_memories': row['ai_memories'],
            'high_quality_memories': row['high_quality'],
            'avg_authority': round(row['avg_authority'], 2) if row['avg_authority'] else 0
        }

    # ===== æé†’åŠŸèƒ½ =====
    
    def set_reminder(self, memory_id: str, remind_at: str) -> bool:
        """
        è¨­å®šè¨˜æ†¶æé†’æ™‚é–“
        
        Args:
            memory_id: è¨˜æ†¶ ID
            remind_at: æé†’æ™‚é–“ï¼ˆISO æ ¼å¼ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        cursor = self.conn.execute(
            "SELECT id FROM memories WHERE id = ?", (memory_id,)
        )
        if not cursor.fetchone():
            return False
        
        self.conn.execute(
            "UPDATE memories SET remind_at = ? WHERE id = ?",
            (remind_at, memory_id)
        )
        self.conn.commit()
        return True
    
    def clear_reminder(self, memory_id: str) -> bool:
        """æ¸…é™¤è¨˜æ†¶æé†’"""
        self.conn.execute(
            "UPDATE memories SET remind_at = NULL WHERE id = ?",
            (memory_id,)
        )
        self.conn.commit()
        return True
    
    def get_reminders(self, include_past: bool = False, limit: int = 50) -> List[Dict]:
        """
        å–å¾—å¾…æé†’äº‹é …
        
        Args:
            include_past: æ˜¯å¦åŒ…å«éæœŸæé†’
            limit: æ•¸é‡ä¸Šé™
        
        Returns:
            æé†’åˆ—è¡¨
        """
        from datetime import datetime
        now = datetime.now().isoformat()
        
        if include_past:
            cursor = self.conn.execute("""
                SELECT id, title, file_path, remind_at, updated
                FROM memories
                WHERE remind_at IS NOT NULL
                ORDER BY remind_at ASC
                LIMIT ?
            """, (limit,))
        else:
            cursor = self.conn.execute("""
                SELECT id, title, file_path, remind_at, updated
                FROM memories
                WHERE remind_at IS NOT NULL AND remind_at >= ?
                ORDER BY remind_at ASC
                LIMIT ?
            """, (now, limit))
        
        results = []
        for row in cursor:
            remind_at = row['remind_at']
            is_past = remind_at < now if remind_at else False
            
            results.append({
                'id': row['id'],
                'title': row['title'],
                'file_path': row['file_path'],
                'remind_at': remind_at,
                'updated': row['updated'],
                'is_past': is_past
            })
        
        return results
    
    def get_due_reminders(self) -> List[Dict]:
        """å–å¾—å·²åˆ°æœŸä½†å°šæœªè™•ç†çš„æé†’"""
        from datetime import datetime
        now = datetime.now().isoformat()
        
        cursor = self.conn.execute("""
            SELECT id, title, file_path, remind_at, updated
            FROM memories
            WHERE remind_at IS NOT NULL AND remind_at <= ?
            ORDER BY remind_at ASC
        """, (now,))
        
        results = []
        for row in cursor:
            results.append({
                'id': row['id'],
                'title': row['title'],
                'file_path': row['file_path'],
                'remind_at': row['remind_at'],
                'updated': row['updated']
            })
        
        return results

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£ç·š"""
        if self.conn:
            self.conn.close()

    def __del__(self):
        self.close()



# ===== å·¥ä½œé€²ç¨‹å‡½æ•¸ï¼ˆå¿…é ˆåœ¨æ¨¡çµ„å±¤ç´šï¼‰ =====

def _read_memory_worker(memory_id: str, file_path: str) -> Optional[Tuple[str, str, Dict, str]]:
    """å·¥ä½œé€²ç¨‹ï¼šè®€å–å–®å€‹è¨˜æ†¶æª”æ¡ˆ
    
    Args:
        memory_id: è¨˜æ†¶ IDï¼ˆå¯ç‚º Noneï¼Œæœƒå¾ metadata ä¸­è®€å–ï¼‰
        file_path: æª”æ¡ˆè·¯å¾‘
    
    Returns:
        (memory_id, file_path, metadata, content) æˆ– None
    """
    try:
        from pathlib import Path
        import yaml
        
        path = Path(file_path)
        if not path.exists():
            return None
        
        text = path.read_text(encoding='utf-8')
        
        # è§£æ frontmatter
        if not text.startswith('---'):
            return None
        
        parts = text.split('---', 2)
        if len(parts) < 3:
            return None
        
        metadata = yaml.safe_load(parts[1]) or {}
        content = parts[2].strip()
        
        # å¾ metadata ä¸­å–å¾— memory_idï¼ˆå¦‚æœæœªå‚³å…¥ï¼‰
        actual_memory_id = memory_id or metadata.get('id', '')
        if not actual_memory_id:
            return None
        
        return (actual_memory_id, file_path, metadata, content)
    
    except Exception:
        return None

