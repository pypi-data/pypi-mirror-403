"""
Google Chat ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†ä»‹é¢
"""
import sys
from pathlib import Path
from typing import Dict, List, Optional

from timeless_memory import get_home

from .auth import ChatAuthManager
from .downloader import ChatDownloader
from .converter import ChatConverter
from .parallel_converter import ParallelChatConverter


def _log(msg: str):
    """è¼¸å‡ºåˆ° stderrï¼ˆé¿å…å¹²æ“¾ MCP stdio é€šè¨Šï¼‰"""
    print(msg, file=sys.stderr)


class ChatManager:
    """Google Chat æ•´åˆç®¡ç†å™¨"""
    
    def __init__(self, home_path: Optional[Path] = None):
        """
        Args:
            home_path: TIMELESS_HOME è·¯å¾‘ï¼ˆNone å‰‡ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼‰
        """
        self.home = Path(home_path) if home_path else get_home()
        self.sources_dir = self.home / "sources" / "google-chat"
        self.data_dir = self.home / "data" / "google-chat"
        
        # åˆå§‹åŒ–å­ç®¡ç†å™¨
        self.auth_manager = ChatAuthManager(self.sources_dir)
        self.downloader = ChatDownloader(self.sources_dir, self.auth_manager)
        self.converter = ChatConverter(self.sources_dir, self.data_dir)
        self.parallel_converter = ParallelChatConverter(self.sources_dir, self.data_dir)
    
    def sync(
        self,
        space_id: Optional[str] = None,
        skip_dm: bool = True,
        max_workers: int = 5,
        full: bool = False,
        parallel: bool = True,
        overlap_days: int = 1
    ) -> Dict:
        """
        å®Œæ•´åŒæ­¥æµç¨‹ï¼šä¸‹è¼‰ + è½‰æ› + è‡ªå‹•ç´¢å¼•
        
        Args:
            space_id: æŒ‡å®š Space IDï¼ˆNone å‰‡è™•ç†å…¨éƒ¨ï¼‰
            skip_dm: æ˜¯å¦è·³é DM
            max_workers: ä¸¦è¡Œç·šç¨‹æ•¸
            full: æ˜¯å¦å…¨é‡ä¸‹è¼‰ï¼ˆFalse å‰‡å¢é‡æ›´æ–°ï¼‰
            parallel: æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œè½‰æ›ï¼ˆé è¨­ Trueï¼‰
            overlap_days: å¢é‡ä¸‹è¼‰æ™‚å›æº¯å¤©æ•¸ï¼ˆé è¨­ 1 å¤©ï¼‰
        
        Returns:
            dict: åŒæ­¥çµæœï¼ˆåŒ…å« agent_todosï¼‰
        """
        _log("\n" + "="*60)
        _log("ğŸ”„ é–‹å§‹ Google Chat å®Œæ•´åŒæ­¥")
        _log("="*60)
        
        # æ­¥é©Ÿ 1: ä¸‹è¼‰ï¼ˆå·²ç¶“æ˜¯ä¸¦è¡Œï¼‰
        _log("\nğŸ“¥ æ­¥é©Ÿ 1/3: ä¸‹è¼‰è¨Šæ¯")
        download_result = self.download(
            space_id=space_id,
            skip_dm=skip_dm,
            incremental=not full,
            max_workers=max_workers,
            overlap_days=overlap_days
        )
        
        if not download_result.get("success"):
            return download_result
        
        # æ­¥é©Ÿ 2: è½‰æ›ï¼ˆå¯é¸ä¸¦è¡Œï¼‰
        _log("\nğŸ“ æ­¥é©Ÿ 2/3: è½‰æ›ç‚ºè¨˜æ†¶")
        
        # å¦‚æœæŒ‡å®šäº† space_idï¼Œéœ€è¦æ‰¾åˆ°å°æ‡‰çš„ space_name
        space_name = None
        if space_id:
            spaces = self.downloader.list_spaces()
            for space in spaces:
                if space.get("space_id") == space_id:
                    space_name = space.get("display_name")
                    break
        
        # é¸æ“‡è½‰æ›å™¨
        if parallel and not space_id:  # å¤šå€‹ Space æ‰ä½¿ç”¨ä¸¦è¡Œ
            convert_result = self.parallel_converter.convert(
                space_name=space_name,
                max_workers=max_workers
            )
        else:
            convert_result = self.converter.convert(space_name=space_name)
        
        if not convert_result.get("success"):
            return convert_result
        
        # æ­¥é©Ÿ 3: å»ºç«‹ç´¢å¼•
        _log("\nğŸ” æ­¥é©Ÿ 3/3: å»ºç«‹ç´¢å¼•")
        
        # å„ªå…ˆä½¿ç”¨å¢é‡ç´¢å¼•ï¼ˆæœ‰ new_files æ™‚ï¼‰
        new_files = convert_result.get("new_files", [])
        new_memories = convert_result.get("new_memories", 0)
        
        if new_files:
            # å¢é‡ç´¢å¼•
            index_result = self._incremental_index(convert_result)
        elif new_memories > 0:
            # æ²’æœ‰ new_files ä½†æœ‰æ–°è¨˜æ†¶ï¼Œåšå®Œæ•´é‡å»ºç´¢å¼•
            _log("  âš ï¸ æ²’æœ‰æ–°æª”æ¡ˆåˆ—è¡¨ï¼ŒåŸ·è¡Œå®Œæ•´é‡å»ºç´¢å¼•...")
            index_result = self._full_rebuild_index()
        else:
            # æ²’æœ‰æ–°è¨˜æ†¶ï¼Œè·³éç´¢å¼•
            _log("  â­ï¸ ç„¡æ–°è¨˜æ†¶ï¼Œè·³éç´¢å¼•")
            index_result = {"success": True, "indexed_count": 0}
        
        # åˆä½µçµæœ
        _log("\n" + "="*60)
        _log("âœ… åŒæ­¥å®Œæˆ")
        _log("="*60)
        
        # å»ºç«‹ Agent TODO åˆ—è¡¨
        agent_todos = self._build_agent_todos(convert_result)
        
        # é¡¯ç¤º TODOï¼ˆconsole è¼¸å‡ºï¼‰
        self._print_agent_todos(agent_todos, convert_result)
        
        return {
            "success": True,
            "download": download_result,
            "convert": convert_result,
            "index": index_result,
            "message": f"åŒæ­¥å®Œæˆï¼šæ–°å¢ {new_memories} å€‹è¨˜æ†¶ï¼Œç´¢å¼• {index_result.get('indexed_count', 0)} ç­†",
            "agent_todos": agent_todos
        }
    
    def _full_rebuild_index(self) -> Dict:
        """å®Œæ•´é‡å»ºç´¢å¼•"""
        try:
            from timeless_memory.core import get_managers
            
            memory_manager, index_manager, _, _, _, _ = get_managers(quiet=True)
            
            _log("  é‡å»ºå®Œæ•´ç´¢å¼•...")
            index_manager.rebuild(memory_manager)
            stats = index_manager.get_stats()
            
            _log(f"  âœ… å®Œæˆé‡å»ºï¼Œå…± {stats.get('total_memories', 0)} ç­†")
            
            return {
                "success": True,
                "indexed_count": stats.get("total_memories", 0),
                "method": "full_rebuild"
            }
        except Exception as e:
            _log(f"  âŒ é‡å»ºç´¢å¼•å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "indexed_count": 0
            }
    
    def _build_agent_todos(self, convert_result: Dict) -> List[Dict]:
        """å»ºç«‹ Agent TODO åˆ—è¡¨"""
        from timeless_memory import get_home, get_index_dir
        
        todos = []
        home = get_home()
        index_dir = get_index_dir()
        data_dir = home / "data" / "google-chat"
        
        # TODO 1: åˆ†æä¸¦å»ºç«‹çŸ¥è­˜åœ–è­œ
        todos.append({
            "id": 1,
            "task": "åˆ†æå»ºç«‹çŸ¥è­˜åœ–è­œ",
            "description": "å¾èŠå¤©è¨˜éŒ„æå–äººç‰©ã€å°ˆæ¡ˆè³‡è¨Šï¼Œå»ºç«‹å¯¦é«”å’Œé—œè¯",
            "steps": [
                "åŸ·è¡Œ chat(action='analyze') åˆ†æèŠå¤©è³‡æ–™",
                "æ ¹æ“šåˆ†æçµæœå»ºç«‹äººç‰©å¯¦é«”ï¼šentity(action='batch_create', entities=[...])",
                "å»ºç«‹å°ˆæ¡ˆå¯¦é«”ï¼šentity(action='batch_create', entities=[...])",
                "å»ºç«‹äººç‰©-å°ˆæ¡ˆé—œè¯ï¼šrelation(action='batch_create', relations=[...])"
            ],
            "priority": "high"
        })
        
        # TODO 2: å»ºç«‹æ¯æœˆèŠå¤©å®¤ç´¢å¼•
        spaces = []
        if data_dir.exists():
            spaces = [d.name for d in data_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]
        
        chat_index_dir = index_dir / "èŠå¤©å®¤"
        
        todos.append({
            "id": 2,
            "task": "å»ºç«‹æ¯æœˆèŠå¤©å®¤ç´¢å¼•",
            "description": f"ç‚ºæ¯å€‹èŠå¤©å®¤å»ºç«‹æœˆåº¦æ‘˜è¦ç´¢å¼•æª”æ¡ˆ",
            "index_directory": str(chat_index_dir),
            "file_format": "monthly-summary-{èŠå¤©å®¤åç¨±}-YYYY-MM.md",
            "spaces": spaces,
            "steps": [
                f"å»ºç«‹ç´¢å¼•ç›®éŒ„: {chat_index_dir}",
                "å°æ¯å€‹èŠå¤©å®¤ã€æ¯å€‹æœˆä»½ï¼š",
                "  - è®€å–è©²æœˆæ‰€æœ‰ daily-*.md æª”æ¡ˆ",
                "  - æ‘˜è¦é‡é»è¨è«–å…§å®¹",
                "  - ç”Ÿæˆ monthly-summary-{space}-YYYY-MM.md"
            ],
            "priority": "medium"
        })
        
        return todos
    
    def _print_agent_todos(self, todos: List[Dict], convert_result: Dict):
        """è¼¸å‡º Agent TODO åˆ° console"""
        from timeless_memory import get_index_dir
        
        _log("\nğŸ“‹ Agent TODOï¼ˆä¸‹ä¸€æ­¥å·¥ä½œï¼‰:\n")
        
        for todo in todos:
            _log(f"{todo['id']}ï¸âƒ£ {todo['task']}:")
            _log(f"   {todo['description']}")
            
            if "steps" in todo:
                _log("   æ­¥é©Ÿ:")
                for step in todo["steps"]:
                    _log(f"     - {step}")
            
            if "index_directory" in todo:
                _log(f"   ç´¢å¼•ç›®éŒ„: {todo['index_directory']}")
            
            if "spaces" in todo and todo["spaces"]:
                _log(f"   èŠå¤©å®¤: {', '.join(todo['spaces'][:5])}")
                if len(todo["spaces"]) > 5:
                    _log(f"           ... é‚„æœ‰ {len(todo['spaces']) - 5} å€‹")
            
            _log("")
    
    def _incremental_index(self, convert_result: Dict) -> Dict:
        """å¢é‡ç´¢å¼•æ–°è½‰æ›çš„è¨˜æ†¶
        
        Args:
            convert_result: è½‰æ›çµæœï¼ˆåŒ…å«æ–°æª”æ¡ˆåˆ—è¡¨ï¼‰
        
        Returns:
            dict: ç´¢å¼•çµæœ
        """
        try:
            from timeless_memory.core import get_managers
            
            # å–å¾—ç®¡ç†å™¨
            memory_manager, index_manager, _, _, _, _ = get_managers(quiet=True)
            
            # å–å¾—æ–°è½‰æ›çš„æª”æ¡ˆ
            new_files = convert_result.get("new_files", [])
            updated_files = convert_result.get("updated_files", [])
            all_files = new_files + updated_files
            
            if not all_files:
                _log("  â­ï¸ ç„¡æ–°æª”æ¡ˆéœ€è¦ç´¢å¼•")
                return {"success": True, "indexed_count": 0}
            
            _log(f"  æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆéœ€è¦ç´¢å¼•")
            
            # è®€å–ä¸¦æ‰¹æ¬¡ç´¢å¼•
            from concurrent.futures import ProcessPoolExecutor, as_completed
            from timeless_memory.core.index_manager import _read_memory_worker
            import multiprocessing
            
            workers = min(multiprocessing.cpu_count(), 8)
            _log(f"  ä½¿ç”¨ {workers} å€‹å·¥ä½œé€²ç¨‹å¹³è¡Œè®€å–")
            
            memories_data = []
            completed = 0
            total = len(all_files)
            
            with ProcessPoolExecutor(max_workers=workers) as executor:
                # æäº¤è®€å–ä»»å‹™
                future_to_file = {
                    executor.submit(_read_memory_worker, None, str(file_path)): file_path
                    for file_path in all_files
                }
                
                # æ”¶é›†çµæœ
                for future in as_completed(future_to_file):
                    completed += 1
                    if completed % 100 == 0:
                        _log(f"  å·²è®€å– {completed} / {total} ç­†...")
                    
                    try:
                        result = future.result()
                        if result:
                            memories_data.append(result)
                    except Exception as e:
                        file_path = future_to_file[future]
                        _log(f"  âš ï¸ è®€å–å¤±æ•—: {file_path} - {e}")
            
            _log(f"  å·²è®€å– {len(memories_data)} ç­†è¨˜æ†¶")
            
            # æ‰¹æ¬¡å¯«å…¥ç´¢å¼•
            _log(f"  æ‰¹æ¬¡å¯«å…¥ç´¢å¼•...")
            index_manager.batch_update(memories_data, batch_size=100)
            
            _log(f"  âœ… å®Œæˆç´¢å¼• {len(memories_data)} ç­†")
            
            return {
                "success": True,
                "indexed_count": len(memories_data),
                "total_files": total
            }
        
        except Exception as e:
            _log(f"  âŒ ç´¢å¼•å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "indexed_count": 0
            }
    
    def download(
        self,
        space_id: Optional[str] = None,
        skip_dm: bool = True,
        incremental: bool = True,
        max_workers: int = 5,
        overlap_days: int = 1
    ) -> Dict:
        """
        ä¸‹è¼‰åŸå§‹è³‡æ–™
        
        Args:
            space_id: æŒ‡å®š Space ID
            skip_dm: æ˜¯å¦è·³é DM
            incremental: æ˜¯å¦å¢é‡æ›´æ–°
            max_workers: ä¸¦è¡Œç·šç¨‹æ•¸
            overlap_days: å¢é‡ä¸‹è¼‰æ™‚å›æº¯å¤©æ•¸
        """
        return self.downloader.download(
            space_id=space_id,
            skip_dm=skip_dm,
            incremental=incremental,
            max_workers=max_workers,
            overlap_days=overlap_days
        )
    
    def convert(self, space_name: Optional[str] = None, parallel: bool = False, max_workers: int = 5) -> Dict:
        """
        è½‰æ›å·²ä¸‹è¼‰çš„è³‡æ–™ç‚ºè¨˜æ†¶
        
        Args:
            space_name: æŒ‡å®š Space åç¨±
            parallel: æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œè½‰æ›
            max_workers: ä¸¦è¡Œç·šç¨‹æ•¸
        """
        if parallel and not space_name:
            return self.parallel_converter.convert(
                space_name=space_name,
                max_workers=max_workers
            )
        else:
            return self.converter.convert(space_name=space_name)
    
    def list_spaces(self, refresh: bool = False) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰ Spaces
        
        Args:
            refresh: æ˜¯å¦å¼·åˆ¶é‡æ–°å¾ API å–å¾—
        """
        return self.downloader.list_spaces(refresh=refresh)
    
    def status(self) -> Dict:
        """
        æŸ¥çœ‹åŒæ­¥ç‹€æ…‹
        
        Returns:
            dict: ç‹€æ…‹è³‡è¨Š
        """
        # æª¢æŸ¥èªè­‰ç‹€æ…‹
        is_authenticated = self.auth_manager.is_authenticated()
        
        # çµ±è¨ˆä¾†æºç›®éŒ„
        source_spaces = []
        source_count = 0
        if self.sources_dir.exists():
            for item in self.sources_dir.iterdir():
                if item.is_dir() and not item.name.startswith("."):
                    source_spaces.append(item.name)
                    source_count += 1
        
        # çµ±è¨ˆè¨˜æ†¶ç›®éŒ„
        memory_spaces = []
        memory_count = 0
        total_memories = 0
        if self.data_dir.exists():
            for item in self.data_dir.iterdir():
                if item.is_dir() and not item.name.startswith("."):
                    memory_spaces.append(item.name)
                    memory_count += 1
                    # è¨ˆç®—è¨˜æ†¶æ•¸é‡
                    md_files = list(item.glob("*.md"))
                    total_memories += len(md_files)
        
        # è®€å–åŒæ­¥ç‹€æ…‹
        sync_state_file = self.data_dir.parent / ".google-chat-sync.json"
        last_sync = None
        if sync_state_file.exists():
            import json
            with open(sync_state_file, "r") as f:
                state = json.load(f)
                last_sync = state.get("last_sync")
        
        return {
            "authenticated": is_authenticated,
            "sources_dir": str(self.sources_dir),
            "data_dir": str(self.data_dir),
            "source_spaces": source_count,
            "memory_spaces": memory_count,
            "total_memories": total_memories,
            "last_sync": last_sync,
            "credentials_file": str(self.auth_manager.credentials_file),
            "token_file": str(self.auth_manager.token_file)
        }
    
    def init_auth(self) -> Dict:
        """
        åˆå§‹åŒ– OAuth èªè­‰
        
        Returns:
            dict: èªè­‰çµæœ
        """
        return self.auth_manager.init_auth()
    
    def analyze(self, include_content: bool = False) -> Dict:
        """
        åˆ†æ Google Chat è³‡æ–™ï¼Œæå–äººç‰©å’Œå°ˆæ¡ˆè³‡è¨Š
        ç”¨æ–¼è¼”åŠ© agent å»ºç«‹çŸ¥è­˜åœ–è­œ
        
        Args:
            include_content: æ˜¯å¦åŒ…å«è©³ç´°çš„å°æ‡‰è³‡æ–™ï¼ˆè¼ƒå¤§ï¼‰
        
        Returns:
            dict: åˆ†æçµæœï¼ŒåŒ…å« user_ids, user_mentions, projects
        """
        import re
        import yaml
        from collections import defaultdict, Counter
        
        if not self.data_dir.exists():
            return {
                "success": False,
                "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ sync"
            }
        
        md_files = list(self.data_dir.rglob("*.md"))
        if not md_files:
            return {
                "success": False,
                "error": "æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¨˜æ†¶æª”æ¡ˆ"
            }
        
        # çµ±è¨ˆè³‡æ–™
        all_user_ids = set()
        user_id_speak_count = Counter()  # user_id ç™¼è¨€æ¬¡æ•¸
        user_id_mention_names = defaultdict(Counter)  # user_id -> {æåŠçš„åå­—: æ¬¡æ•¸}
        mention_name_user_ids = defaultdict(set)  # æåŠçš„åå­— -> {ç™¼è¨€è€… user_ids}
        projects = {}
        space_participants = defaultdict(set)
        space_message_count = defaultdict(int)
        
        for md_file in md_files:
            try:
                text = md_file.read_text(encoding='utf-8')
                
                # è§£æ frontmatter
                if not text.startswith('---'):
                    continue
                parts = text.split('---', 2)
                if len(parts) < 3:
                    continue
                
                try:
                    metadata = yaml.safe_load(parts[1]) or {}
                except:
                    continue
                
                content = parts[2]
                space_name = metadata.get('space_name', '')
                participants = metadata.get('participants', [])
                message_count = metadata.get('message_count', 0)
                
                # æ”¶é›† user_ids
                for uid in participants:
                    all_user_ids.add(str(uid))
                    space_participants[space_name].add(str(uid))
                
                space_message_count[space_name] += message_count
                
                # åˆ†æå°ˆæ¡ˆï¼ˆå¾ space_nameï¼‰
                project_match = re.search(r'(P\d+)', space_name)
                if project_match:
                    project_code = project_match.group(1)
                    if project_code not in projects:
                        # æå–å°ˆæ¡ˆåç¨±
                        project_name = re.sub(r'P\d+[_\s]*', '', space_name).strip()
                        project_name = re.sub(r'_AAQA.*', '', project_name)
                        projects[project_code] = {
                            'code': project_code,
                            'name': project_name,
                            'spaces': set(),
                            'participants': set()
                        }
                    projects[project_code]['spaces'].add(space_name)
                    projects[project_code]['participants'].update([str(p) for p in participants])
                
                # åˆ†æç™¼è¨€è€…å’Œ @mentions çš„å°æ‡‰
                # æ ¼å¼: [æ™‚é–“] user_id: è¨Šæ¯å…§å®¹
                message_pattern = r'\[[\d\-:\s]+\]\s+(\d+):\s*(.+?)(?=\n\[[\d\-:\s]+\]|\Z)'
                messages = re.findall(message_pattern, content, re.DOTALL)
                
                for speaker_id, message_text in messages:
                    user_id_speak_count[speaker_id] += 1
                    all_user_ids.add(speaker_id)
                    
                    # æå– @mentions
                    mention_pattern = r'@([a-zA-Z\u4e00-\u9fff][a-zA-Z\u4e00-\u9fff\s]{0,15}?)(?:\s|$|ï¼Œ|,|\n|ï¼š)'
                    mentions = re.findall(mention_pattern, message_text)
                    
                    for mention in mentions:
                        mention = mention.strip()
                        if mention and len(mention) >= 2:
                            user_id_mention_names[speaker_id][mention] += 1
                            mention_name_user_ids[mention].add(speaker_id)
            
            except Exception:
                continue
        
        # æ¨æ¸¬ user_id -> äººåå°æ‡‰
        # ç­–ç•¥ï¼šå¦‚æœä¸€å€‹åå­—è¢«æŸå€‹ user_id ç‰¹åˆ¥å¸¸æåŠï¼Œå¯èƒ½æ˜¯é‚£å€‹äººçš„åŒäº‹
        # æ›´å¥½çš„ç­–ç•¥ï¼šçœ‹èª°ã€Œè‡ªç¨±ã€æˆ–åœ¨ç°½åä¸­ä½¿ç”¨æŸå€‹åå­—
        
        # æ‰¾å‡ºé«˜ä¿¡å¿ƒåº¦çš„å°æ‡‰ï¼ˆåŸºæ–¼å…±ç¾åˆ†æï¼‰
        high_confidence_mappings = {}
        for mention_name, speaker_ids in mention_name_user_ids.items():
            # æ’é™¤å¤ªçŸ­æˆ–å¤ªé•·çš„åå­—
            if len(mention_name) < 2 or len(mention_name) > 10:
                continue
            # æ’é™¤ç´”æ•¸å­—
            if mention_name.isdigit():
                continue
            # å¦‚æœåªæœ‰ä¸€å€‹äººæåˆ°é€™å€‹åå­—ï¼Œå¯èƒ½æ˜¯å…§éƒ¨ç¾¤çµ„
            if len(speaker_ids) == 1:
                # å¯èƒ½æ˜¯é‚£å€‹äººè‡ªå·±çš„åå­—ï¼Œæˆ–æ˜¯åªæœ‰ä¸€å€‹äººèªè­˜
                pass
        
        # è½‰æ›çµæœ
        result = {
            "success": True,
            "summary": {
                "total_files": len(md_files),
                "total_user_ids": len(all_user_ids),
                "total_mentions": len(mention_name_user_ids),
                "total_projects": len(projects),
                "total_spaces": len(space_participants)
            },
            "user_ids": sorted(list(all_user_ids)),
            "top_speakers": [
                {"user_id": uid, "message_count": count}
                for uid, count in user_id_speak_count.most_common(30)
            ],
            "projects": {
                k: {
                    'code': v['code'],
                    'name': v['name'],
                    'spaces': sorted(list(v['spaces'])),
                    'participant_count': len(v['participants']),
                    'participants': sorted(list(v['participants']))
                } for k, v in projects.items()
            },
            "spaces": {
                name: {
                    'participant_count': len(uids),
                    'message_count': space_message_count.get(name, 0)
                } for name, uids in space_participants.items()
            },
            "mention_names": sorted(list(mention_name_user_ids.keys()))
        }
        
        # å¦‚æœéœ€è¦è©³ç´°è³‡æ–™
        if include_content:
            result["user_mention_details"] = {
                uid: dict(mentions) 
                for uid, mentions in user_id_mention_names.items()
            }
            result["mention_speakers"] = {
                name: sorted(list(uids))
                for name, uids in mention_name_user_ids.items()
            }
        
        return result
    
    def get_month_data(
        self,
        space_name: str,
        year_month: str
    ) -> Dict:
        """
        å–å¾—æŒ‡å®šèŠå¤©å®¤çš„æœˆåº¦è³‡æ–™ï¼ˆä¾› agent ç”Ÿæˆæ‘˜è¦ç”¨ï¼‰
        
        Agent å·¥ä½œæµç¨‹ï¼š
        1. å‘¼å« chat(action='list_months') å–å¾—å¯ç”¨çš„èŠå¤©å®¤/æœˆä»½åˆ—è¡¨
        2. å‘¼å« chat(action='get_month_data', space_name=..., year_month=...) å–å¾—è©²æœˆè³‡æ–™
        3. Agent ç”¨ LLM ç”Ÿæˆæ‘˜è¦
        4. å‘¼å« chat(action='save_summary', ...) å„²å­˜æ‘˜è¦
        
        Args:
            space_name: èŠå¤©å®¤åç¨±
            year_month: å¹´æœˆ (YYYY-MM æ ¼å¼)
        
        Returns:
            dict: è©²æœˆçš„å°è©±è³‡æ–™ï¼Œä¾› agent ç”Ÿæˆæ‘˜è¦
        """
        import re
        import yaml
        from pathlib import Path
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ sync"}
        
        # æ‰¾è©²èŠå¤©å®¤è©²æœˆçš„æ‰€æœ‰ daily æª”æ¡ˆ
        space_dir = self.data_dir / space_name
        if not space_dir.exists():
            return {"success": False, "error": f"èŠå¤©å®¤ä¸å­˜åœ¨: {space_name}"}
        
        pattern = f"daily-*-{year_month}-*.md"
        daily_files = sorted(space_dir.glob(pattern))
        
        if not daily_files:
            return {
                "success": False,
                "error": f"æ²’æœ‰æ‰¾åˆ° {space_name} åœ¨ {year_month} çš„è³‡æ–™"
            }
        
        # æ”¶é›†è³‡æ–™
        days = []
        total_messages = 0
        all_participants = set()
        
        for daily_file in daily_files:
            text = daily_file.read_text(encoding='utf-8')
            
            # è§£æ frontmatter
            metadata = {}
            content = text
            if text.startswith('---'):
                parts = text.split('---', 2)
                if len(parts) >= 3:
                    try:
                        metadata = yaml.safe_load(parts[1]) or {}
                    except:
                        pass
                    content = parts[2]
            
            msg_count = metadata.get('message_count', 0)
            total_messages += msg_count
            
            for p in metadata.get('participants', []):
                all_participants.add(str(p))
            
            # æå–æ—¥æœŸ
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', daily_file.name)
            date_str = date_match.group(1) if date_match else daily_file.name
            
            # å…§å®¹æˆªæ–·ï¼ˆé¿å…éé•·ï¼‰
            content_lines = content.strip().split('\n')
            if len(content_lines) > 50:
                content = '\n'.join(content_lines[:50]) + f"\n\n... (é‚„æœ‰ {len(content_lines) - 50} è¡Œ)"
            
            days.append({
                "date": date_str,
                "message_count": msg_count,
                "content": content.strip()
            })
        
        return {
            "success": True,
            "space_name": space_name,
            "year_month": year_month,
            "total_days": len(days),
            "total_messages": total_messages,
            "participant_count": len(all_participants),
            "participants": sorted(list(all_participants)),
            "days": days,
            "agent_instruction": "è«‹æ ¹æ“šä»¥ä¸Šæ¯æ—¥å°è©±å…§å®¹ï¼Œç”Ÿæˆä¸€ä»½æœˆåº¦æ‘˜è¦ï¼ŒåŒ…å«ï¼šé‡é»è¨è«–ä¸»é¡Œã€é‡è¦æ±ºç­–ã€å¾…è¾¦äº‹é …ç­‰ã€‚å®Œæˆå¾Œå‘¼å« chat(action='save_summary') å„²å­˜ã€‚"
        }
    
    def list_months(self, check_updates: bool = True) -> Dict:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„èŠå¤©å®¤/æœˆä»½çµ„åˆï¼Œæ¨™è¨˜éœ€è¦å»ºç«‹æˆ–æ›´æ–°æ‘˜è¦çš„é …ç›®
        
        å¢é‡æ›´æ–°é‚è¼¯ï¼š
        - missing: å®Œå…¨æ²’æœ‰æ‘˜è¦çš„æœˆä»½
        - outdated: æœ‰æ‘˜è¦ä½†è³‡æ–™æœ‰æ›´æ–°ï¼ˆdaily æª”æ¡ˆæ¯”æ‘˜è¦æ–°ï¼‰
        
        Args:
            check_updates: æ˜¯å¦æª¢æŸ¥éœ€è¦æ›´æ–°çš„æ‘˜è¦ï¼ˆæ¯”è¼ƒæª”æ¡ˆæ™‚é–“ï¼‰
        
        Returns:
            dict: èŠå¤©å®¤ -> æœˆä»½åˆ—è¡¨ï¼Œä»¥åŠéœ€è¦è™•ç†çš„é …ç›®
        """
        import re
        import os
        from collections import defaultdict
        from timeless_memory import get_index_dir
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨"}
        
        # æ”¶é›†æ‰€æœ‰ daily æª”æ¡ˆåŠå…¶æœ€å¾Œä¿®æ”¹æ™‚é–“
        # space -> month -> latest_mtime
        space_month_mtime = defaultdict(lambda: defaultdict(float))
        space_months = defaultdict(set)
        
        for md_file in self.data_dir.rglob("daily-*.md"):
            match = re.match(r'daily-(.+)-(\d{4}-\d{2})-\d{2}\.md$', md_file.name)
            if match:
                space = md_file.parent.name
                month = match.group(2)
                space_months[space].add(month)
                
                if check_updates:
                    mtime = os.path.getmtime(md_file)
                    if mtime > space_month_mtime[space][month]:
                        space_month_mtime[space][month] = mtime
        
        # æª¢æŸ¥å·²å­˜åœ¨çš„æ‘˜è¦åŠå…¶ç”Ÿæˆæ™‚é–“
        index_dir = get_index_dir() / "èŠå¤©å®¤"
        # (space, month) -> summary_mtime
        existing_summaries = {}
        
        if index_dir.exists():
            for summary_file in index_dir.glob("monthly-summary-*.md"):
                match = re.match(r'monthly-summary-(.+)-(\d{4}-\d{2})\.md$', summary_file.name)
                if match:
                    space = match.group(1)
                    month = match.group(2)
                    existing_summaries[(space, month)] = os.path.getmtime(summary_file)
        
        # æ•´ç†çµæœ
        result = {}
        missing = []  # å®Œå…¨æ²’æœ‰æ‘˜è¦
        outdated = []  # æœ‰æ‘˜è¦ä½†è³‡æ–™æœ‰æ›´æ–°
        
        for space, months in sorted(space_months.items()):
            month_status = {}
            for month in sorted(months):
                key = (space, month)
                if key not in existing_summaries:
                    month_status[month] = "missing"
                    missing.append({"space": space, "month": month})
                elif check_updates:
                    data_mtime = space_month_mtime[space][month]
                    summary_mtime = existing_summaries[key]
                    if data_mtime > summary_mtime:
                        month_status[month] = "outdated"
                        outdated.append({
                            "space": space,
                            "month": month,
                            "reason": "è³‡æ–™æœ‰æ›´æ–°"
                        })
                    else:
                        month_status[month] = "up_to_date"
                else:
                    month_status[month] = "exists"
            
            result[space] = {
                "months": month_status,
                "total_months": len(months)
            }
        
        # åˆä½µéœ€è¦è™•ç†çš„é …ç›®ï¼ˆmissing + outdatedï¼‰
        needs_update = missing + outdated
        
        return {
            "success": True,
            "spaces": result,
            "total_spaces": len(result),
            "missing_summaries": missing[:10],
            "outdated_summaries": outdated[:10],
            "needs_update": needs_update[:20],
            "total_missing": len(missing),
            "total_outdated": len(outdated),
            "total_needs_update": len(needs_update),
            "index_dir": str(index_dir),
            "agent_instruction": (
                "needs_update åˆ—å‡ºéœ€è¦å»ºç«‹æˆ–æ›´æ–°çš„æ‘˜è¦ã€‚"
                "ä½¿ç”¨ chat(action='get_month_data', space_name=..., year_month=...) å–å¾—è³‡æ–™ï¼Œ"
                "ç”Ÿæˆæ‘˜è¦å¾Œç”¨ chat(action='save_summary') å„²å­˜ã€‚"
            )
        }
    
    def save_summary(
        self,
        space_name: str,
        year_month: str,
        summary_content: str
    ) -> Dict:
        """
        å„²å­˜ agent ç”Ÿæˆçš„æœˆåº¦æ‘˜è¦
        
        Args:
            space_name: èŠå¤©å®¤åç¨±
            year_month: å¹´æœˆ (YYYY-MM æ ¼å¼)
            summary_content: æ‘˜è¦å…§å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
        
        Returns:
            dict: å„²å­˜çµæœ
        """
        from pathlib import Path
        from datetime import datetime
        from timeless_memory import get_index_dir
        
        # è¨­å®šè¼¸å‡ºç›®éŒ„
        out_dir = get_index_dir() / "èŠå¤©å®¤"
        out_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæª”æ¡ˆ
        summary_file = out_dir / f"monthly-summary-{space_name}-{year_month}.md"
        
        # åŠ ä¸Š frontmatter
        full_content = f"""---
space_name: {space_name}
month: {year_month}
generated_at: {datetime.now().isoformat()}
generated_by: agent
---

{summary_content}
"""
        
        summary_file.write_text(full_content, encoding='utf-8')
        
        return {
            "success": True,
            "file": str(summary_file),
            "space_name": space_name,
            "year_month": year_month,
            "message": f"å·²å„²å­˜æ‘˜è¦: {summary_file.name}"
        }
    
    def extract_users_for_entities(self) -> Dict:
        """
        å¾èŠå¤©è¨˜éŒ„æå– user_id â†’ é¡¯ç¤ºåç¨±å°æ‡‰
        ç›´æ¥è¼¸å‡ºå¯é¤µçµ¦ entity(batch_create) çš„æ ¼å¼
        
        Returns:
            dict: åŒ…å« entities é™£åˆ—ï¼Œå¯ç›´æ¥ç”¨æ–¼ batch_create
        """
        import re
        import yaml
        from collections import defaultdict, Counter
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨"}
        
        md_files = list(self.data_dir.rglob("*.md"))
        if not md_files:
            return {"success": False, "error": "æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¨˜æ†¶æª”æ¡ˆ"}
        
        # åˆ†æè³‡æ–™
        user_id_names = defaultdict(Counter)  # user_id -> {å¯èƒ½çš„åå­—: æ¬¡æ•¸}
        user_id_spaces = defaultdict(set)  # user_id -> åƒèˆ‡çš„èŠå¤©å®¤
        user_id_speak_count = Counter()  # user_id ç™¼è¨€æ¬¡æ•¸
        
        for md_file in md_files:
            try:
                text = md_file.read_text(encoding='utf-8')
                
                if not text.startswith('---'):
                    continue
                parts = text.split('---', 2)
                if len(parts) < 3:
                    continue
                
                try:
                    metadata = yaml.safe_load(parts[1]) or {}
                except:
                    continue
                
                content = parts[2]
                space_name = metadata.get('space_name', '')
                participants = [str(p) for p in metadata.get('participants', [])]
                
                for uid in participants:
                    user_id_spaces[uid].add(space_name)
                
                # åˆ†æè¨Šæ¯ä¸­çš„ç°½åæ¨¡å¼
                # å¾ˆå¤šäººæœƒåœ¨è¨Šæ¯çµå°¾ç°½åï¼Œæˆ–åœ¨å°è©±ä¸­è‡ªç¨±
                message_pattern = r'\[[\d\-:\s]+\]\s+(\d+):\s*(.+?)(?=\n\[[\d\-:\s]+\]|\Z)'
                messages = re.findall(message_pattern, content, re.DOTALL)
                
                for speaker_id, message_text in messages:
                    user_id_speak_count[speaker_id] += 1
                    
                    # ç°½åæ¨¡å¼ï¼šçµå°¾æœ‰åå­—
                    # ä¾‹å¦‚ï¼šã€Œå¥½çš„ï¼Œè¬è¬ - å°æ˜ã€æˆ–ã€Œby å°æ˜ã€
                    sig_patterns = [
                        r'[-â€”]\s*([a-zA-Z\u4e00-\u9fff]{2,8})\s*$',
                        r'by\s+([a-zA-Z\u4e00-\u9fff]{2,8})\s*$',
                        r'from\s+([a-zA-Z\u4e00-\u9fff]{2,8})\s*$',
                    ]
                    for pat in sig_patterns:
                        sig_match = re.search(pat, message_text.strip(), re.IGNORECASE)
                        if sig_match:
                            name = sig_match.group(1).strip()
                            if len(name) >= 2:
                                user_id_names[speaker_id][name] += 5  # ç°½åæ¬Šé‡è¼ƒé«˜
                
            except Exception:
                continue
        
        # å»ºç«‹å¯¦é«”åˆ—è¡¨
        entities = []
        unmapped = []
        
        for user_id, speak_count in user_id_speak_count.most_common():
            possible_names = user_id_names.get(user_id, {})
            spaces = list(user_id_spaces.get(user_id, []))
            
            if possible_names:
                # å–æœ€é«˜åˆ†çš„åå­—
                best_name, score = possible_names.most_common(1)[0]
                confidence = min(score / 10, 1.0)  # æ­£è¦åŒ–åˆ° 0-1
                
                entities.append({
                    "entity_type": "person",
                    "name": best_name,
                    "aliases": [user_id],  # Chat ID ä½œç‚ºåˆ¥å
                    "properties": {
                        "chat_id": user_id,
                        "speak_count": speak_count,
                        "confidence": confidence,
                        "spaces": spaces[:5]  # åªä¿ç•™å‰ 5 å€‹
                    }
                })
            else:
                unmapped.append({
                    "user_id": user_id,
                    "speak_count": speak_count,
                    "spaces": spaces[:3]
                })
        
        return {
            "success": True,
            "entities": entities,
            "unmapped": unmapped[:30],  # åªé¡¯ç¤ºå‰ 30 å€‹æœªå°æ‡‰
            "summary": {
                "total_user_ids": len(user_id_speak_count),
                "mapped": len(entities),
                "unmapped": len(unmapped)
            },
            "usage": "ä½¿ç”¨ entity(action='batch_create', entities=<entities>) å»ºç«‹å¯¦é«”"
        }

    def get_user_context(self, user_id: str, limit: int = 10) -> Dict:
        """
        å–å¾—ç‰¹å®š user_id çš„ä¸Šä¸‹æ–‡è³‡è¨Š
        ç”¨æ–¼è¼”åŠ© agent åˆ¤æ–·é€™å€‹ user_id å°æ‡‰çš„äººå
        
        Args:
            user_id: Google Chat User ID
            limit: è¿”å›çš„è¨Šæ¯æ•¸é‡ä¸Šé™
        
        Returns:
            dict: è©² user_id çš„ä¸Šä¸‹æ–‡è³‡è¨Š
        """
        import re
        import yaml
        from collections import Counter
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨"}
        
        md_files = list(self.data_dir.rglob("*.md"))
        
        messages = []  # è©² user çš„ç™¼è¨€
        mentioned_by = Counter()  # è¢«èª°æåˆ°
        mentioned_names = Counter()  # æåˆ°çš„åå­—
        spaces = set()  # åƒèˆ‡çš„èŠå¤©å®¤
        
        for md_file in md_files:
            try:
                text = md_file.read_text(encoding='utf-8')
                
                if not text.startswith('---'):
                    continue
                parts = text.split('---', 2)
                if len(parts) < 3:
                    continue
                
                try:
                    metadata = yaml.safe_load(parts[1]) or {}
                except:
                    continue
                
                content = parts[2]
                space_name = metadata.get('space_name', '')
                participants = [str(p) for p in metadata.get('participants', [])]
                
                if user_id in participants:
                    spaces.add(space_name)
                
                # æ‰¾é€™å€‹ user çš„ç™¼è¨€
                message_pattern = rf'\[([\d\-:\s]+)\]\s+{user_id}:\s*(.+?)(?=\n\[[\d\-:\s]+\]|\Z)'
                user_messages = re.findall(message_pattern, content, re.DOTALL)
                
                for timestamp, msg_text in user_messages:
                    if len(messages) < limit:
                        messages.append({
                            "time": timestamp.strip(),
                            "space": space_name,
                            "text": msg_text.strip()[:200]  # æˆªæ–·
                        })
                    
                    # é€™å€‹ user æåˆ°çš„åå­—
                    mention_pattern = r'@([a-zA-Z\u4e00-\u9fff][a-zA-Z\u4e00-\u9fff\s]{0,15}?)(?:\s|$|ï¼Œ|,|\n)'
                    mentions = re.findall(mention_pattern, msg_text)
                    for m in mentions:
                        m = m.strip()
                        if m and len(m) >= 2:
                            mentioned_names[m] += 1
                
                # æ‰¾èª°æåˆ°é€™å€‹ userï¼ˆé€šé ID æåŠè¼ƒå°‘ï¼Œé€™è£¡ç•¥éï¼‰
            
            except Exception:
                continue
        
        return {
            "success": True,
            "user_id": user_id,
            "spaces": sorted(list(spaces)),
            "space_count": len(spaces),
            "sample_messages": messages[:limit],
            "mentioned_names": [
                {"name": name, "count": count}
                for name, count in mentioned_names.most_common(20)
            ]
        }
    
    def list_mentions(self, limit: int = 50) -> Dict:
        """
        åˆ—å‡ºæ‰€æœ‰è¢« @ æåŠçš„åå­—ï¼ˆä¾› agent å»ºç«‹äººç‰©å¯¦é«”ï¼‰
        
        é€™æ˜¯çµ¦ agent ä½¿ç”¨çš„å·¥å…·ï¼š
        1. åˆ—å‡ºæ‰€æœ‰ @ æåŠçš„åå­—å’Œæ¬¡æ•¸
        2. Agent æ ¹æ“šçµæœå»ºç«‹äººç‰©å¯¦é«”
        
        Args:
            limit: è¿”å›çš„åå­—æ•¸é‡ä¸Šé™
        
        Returns:
            dict: æ‰€æœ‰è¢«æåŠçš„åå­—åŠçµ±è¨ˆ
        """
        import re
        import yaml
        from collections import Counter, defaultdict
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ sync"}
        
        md_files = list(self.data_dir.rglob("*.md"))
        if not md_files:
            return {"success": False, "error": "æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¨˜æ†¶æª”æ¡ˆ"}
        
        # çµ±è¨ˆ @ æåŠ
        mention_counts = Counter()  # åå­— -> ç¸½æ¬¡æ•¸
        mention_speakers = defaultdict(set)  # åå­— -> æåŠè€… user_ids
        mention_spaces = defaultdict(set)  # åå­— -> å‡ºç¾çš„èŠå¤©å®¤
        
        for md_file in md_files:
            try:
                text = md_file.read_text(encoding='utf-8')
                
                if not text.startswith('---'):
                    continue
                parts = text.split('---', 2)
                if len(parts) < 3:
                    continue
                
                try:
                    metadata = yaml.safe_load(parts[1]) or {}
                except:
                    continue
                
                content = parts[2]
                space_name = metadata.get('space_name', '')
                
                # è§£æè¨Šæ¯ï¼š[æ™‚é–“] user_id: å…§å®¹
                message_pattern = r'\[[\d\-:\s]+\]\s+(\d+):\s*(.+?)(?=\n\[[\d\-:\s]+\]|\Z)'
                messages = re.findall(message_pattern, content, re.DOTALL)
                
                for speaker_id, message_text in messages:
                    # æå– @ æåŠï¼ˆæ”¯æ´ä¸­è‹±æ–‡ï¼‰
                    mention_pattern = r'@([a-zA-Z\u4e00-\u9fff][a-zA-Z0-9\u4e00-\u9fff_\-\s]{0,20}?)(?:\s|$|ï¼Œ|,|\n|ï¼š|:|ã€)'
                    mentions = re.findall(mention_pattern, message_text)
                    
                    for name in mentions:
                        name = name.strip()
                        # éæ¿¾ç„¡æ•ˆåå­—
                        if not name or len(name) < 2 or len(name) > 15:
                            continue
                        if name.isdigit():
                            continue
                        # æ’é™¤å¸¸è¦‹éäººåé—œéµå­—
                        if name.lower() in ['all', 'here', 'channel', 'everyone']:
                            continue
                        
                        mention_counts[name] += 1
                        mention_speakers[name].add(speaker_id)
                        mention_spaces[name].add(space_name)
            
            except Exception:
                continue
        
        # æ•´ç†çµæœ
        results = []
        for name, count in mention_counts.most_common(limit):
            results.append({
                "name": name,
                "mention_count": count,
                "mentioned_by_count": len(mention_speakers[name]),
                "space_count": len(mention_spaces[name]),
                "spaces": sorted(list(mention_spaces[name]))[:5]  # åªåˆ—å‰ 5 å€‹
            })
        
        return {
            "success": True,
            "total_unique_names": len(mention_counts),
            "mentions": results,
            "agent_instruction": (
                "ä»¥ä¸Šæ˜¯èŠå¤©è¨˜éŒ„ä¸­è¢« @ æåŠçš„åå­—ã€‚"
                "ä½¿ç”¨ chat(action='search_mention', name='xxx') å–å¾—è©²åå­—çš„ä¸Šä¸‹æ–‡ï¼Œ"
                "åˆ¤æ–·å°æ‡‰çš„ user_id å¾Œå»ºç«‹äººç‰©å¯¦é«”ã€‚"
            )
        }
    
    def search_mention(self, name: str, limit: int = 10) -> Dict:
        """
        æœå°‹ç‰¹å®š @ åå­—çš„ä¸Šä¸‹æ–‡ï¼ˆä¾› agent åˆ¤æ–· user_id å°æ‡‰ï¼‰
        
        é€™æ˜¯çµ¦ agent ä½¿ç”¨çš„å·¥å…·ï¼š
        1. æœå°‹æ‰€æœ‰ @name çš„è¨Šæ¯
        2. é¡¯ç¤ºä¸Šä¸‹æ–‡ï¼ˆèª°ç™¼è¨€ã€åœ¨å“ªå€‹èŠå¤©å®¤ï¼‰
        3. Agent æ ¹æ“šä¸Šä¸‹æ–‡æ¨æ–· user_id â†’ äººåå°æ‡‰
        
        Args:
            name: è¦æœå°‹çš„åå­—
            limit: è¿”å›çš„è¨Šæ¯æ•¸é‡ä¸Šé™
        
        Returns:
            dict: è©²åå­—çš„æ‰€æœ‰æåŠä¸Šä¸‹æ–‡
        """
        import re
        import yaml
        from collections import Counter
        
        if not self.data_dir.exists():
            return {"success": False, "error": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨"}
        
        if not name:
            return {"success": False, "error": "è«‹æä¾›è¦æœå°‹çš„åå­—"}
        
        md_files = list(self.data_dir.rglob("*.md"))
        
        # æ”¶é›†çµæœ
        contexts = []  # ä¸Šä¸‹æ–‡åˆ—è¡¨
        speaker_counts = Counter()  # èª°æåˆ°é€™å€‹åå­—
        spaces = set()  # å‡ºç¾çš„èŠå¤©å®¤
        
        for md_file in md_files:
            try:
                text = md_file.read_text(encoding='utf-8')
                
                if not text.startswith('---'):
                    continue
                parts = text.split('---', 2)
                if len(parts) < 3:
                    continue
                
                try:
                    metadata = yaml.safe_load(parts[1]) or {}
                except:
                    continue
                
                content = parts[2]
                space_name = metadata.get('space_name', '')
                date_str = metadata.get('date', '')
                
                # è§£æè¨Šæ¯
                message_pattern = r'\[([\d\-:\s]+)\]\s+(\d+):\s*(.+?)(?=\n\[[\d\-:\s]+\]|\Z)'
                messages = re.findall(message_pattern, content, re.DOTALL)
                
                for timestamp, speaker_id, message_text in messages:
                    # æª¢æŸ¥æ˜¯å¦æåˆ°é€™å€‹åå­—
                    if f'@{name}' in message_text or f'@ {name}' in message_text:
                        speaker_counts[speaker_id] += 1
                        spaces.add(space_name)
                        
                        if len(contexts) < limit:
                            contexts.append({
                                "date": date_str,
                                "time": timestamp.strip(),
                                "space": space_name,
                                "speaker_id": speaker_id,
                                "message": message_text.strip()[:300]  # æˆªæ–·
                            })
            
            except Exception:
                continue
        
        # åˆ†æï¼šèª°æœ€å¸¸æåˆ°é€™å€‹åå­—ï¼Ÿ
        top_speakers = [
            {"user_id": uid, "count": count}
            for uid, count in speaker_counts.most_common(10)
        ]
        
        return {
            "success": True,
            "name": name,
            "total_mentions": sum(speaker_counts.values()),
            "mentioned_by": top_speakers,
            "spaces": sorted(list(spaces)),
            "contexts": contexts,
            "agent_instruction": (
                f"ä»¥ä¸Šæ˜¯ @{name} çš„æ‰€æœ‰æåŠä¸Šä¸‹æ–‡ã€‚"
                "åˆ†æ 'mentioned_by' å¯ä»¥æ¨æ–·èª°æœ€å¸¸æåˆ°é€™å€‹äººã€‚"
                "åˆ†æ 'contexts' ä¸­çš„å°è©±å…§å®¹ï¼Œåˆ¤æ–·é€™å€‹åå­—å°æ‡‰çš„ user_idã€‚"
                "ç¢ºå®šå¾Œç”¨ entity(action='create', ...) å»ºç«‹äººç‰©å¯¦é«”ã€‚"
            )
        }
