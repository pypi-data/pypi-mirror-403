"""
Self-Evolving RLM Framework
===========================

LLMs that improve reasoning through usage without human supervision.
Based on R-Zero (arXiv:2508.05004) and EvolveR architectures.

Key Concepts:
- Challenger-Solver co-evolutionary loop
- Self-generated training data
- Pseudo-labeling with majority voting
- Experience-driven improvement

WARNING: Full training requires significant compute (8+ GPUs).
This module provides inference-time self-improvement patterns.
"""

from __future__ import annotations

import time
import json
import hashlib
import random
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Callable, Tuple
from enum import Enum
import threading


class EvolutionStrategy(Enum):
    """Self-evolution strategies."""
    SELF_REFINE = "self_refine"      # Single model self-improvement
    CHALLENGER_SOLVER = "challenger_solver"  # R-Zero style co-evolution
    MULTI_AGENT = "multi_agent"      # MAE style multi-agent evolution
    EXPERIENCE_REPLAY = "experience_replay"  # EvolveR style experience-driven


@dataclass
class EvolutionMetrics:
    """Metrics for tracking self-improvement."""
    total_evolutions: int = 0
    successful_solves: int = 0
    failed_solves: int = 0
    average_solve_time: float = 0.0
    improvement_rate: float = 0.0  # % improvement over baseline
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_evolutions": self.total_evolutions,
            "successful_solves": self.successful_solves,
            "failed_solves": self.failed_solves,
            "success_rate": self.success_rate,
            "average_solve_time": self.average_solve_time,
            "improvement_rate": self.improvement_rate,
        }
    
    @property
    def success_rate(self) -> float:
        total = self.successful_solves + self.failed_solves
        return self.successful_solves / total if total > 0 else 0.0


@dataclass
class Challenge:
    """A challenge generated by the Challenger model."""
    id: str
    problem: str
    difficulty: float  # 0.0-1.0
    domain: str
    created_at: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Solution:
    """A solution attempt by the Solver model."""
    challenge_id: str
    reasoning: str
    answer: str
    confidence: float  # 0.0-1.0
    latency: float
    success: Optional[bool] = None
    feedback: Optional[str] = None


@dataclass
class ExperienceEntry:
    """Experience entry for replay buffer."""
    challenge: Challenge
    solutions: List[Solution]
    best_solution: Optional[Solution] = None
    timestamp: float = field(default_factory=time.time)


class SelfEvolvingRLM:
    """
    Self-Evolving RLM with Challenger-Solver dynamics.
    
    Implements inference-time self-improvement patterns based on R-Zero.
    For full training, see the R-Zero GitHub repository.
    
    Example:
        >>> from rlm_toolkit.evolve import SelfEvolvingRLM
        >>> from rlm_toolkit.providers import OllamaProvider
        >>> 
        >>> evolve = SelfEvolvingRLM(
        ...     provider=OllamaProvider("llama3"),
        ...     strategy=EvolutionStrategy.SELF_REFINE
        ... )
        >>> answer = evolve.solve("What is 2+2?")
        >>> print(evolve.get_metrics())
    """
    
    def __init__(
        self,
        provider,  # LLMProvider instance
        strategy: EvolutionStrategy = EvolutionStrategy.SELF_REFINE,
        max_refinements: int = 3,
        experience_buffer_size: int = 1000,
        confidence_threshold: float = 0.8,
        verifier: Optional[Callable[[str, str], bool]] = None,
    ):
        """
        Initialize self-evolving RLM.
        
        Args:
            provider: LLM provider for generation
            strategy: Evolution strategy to use
            max_refinements: Maximum self-refinement iterations
            experience_buffer_size: Size of experience replay buffer
            confidence_threshold: Threshold for accepting solutions
            verifier: Optional callable(answer, expected) -> bool for verification
        """
        self.provider = provider
        self.strategy = strategy
        self.max_refinements = max_refinements
        self.confidence_threshold = confidence_threshold
        self.verifier = verifier
        
        # Metrics
        self._metrics = EvolutionMetrics()
        self._lock = threading.Lock()
        
        # Experience buffer (for experience replay strategy)
        self._experience_buffer: List[ExperienceEntry] = []
        self._buffer_size = experience_buffer_size
        
        # Prompts
        self._challenger_prompt = self._get_challenger_prompt()
        self._solver_prompt = self._get_solver_prompt()
        self._refine_prompt = self._get_refine_prompt()
    
    def _get_challenger_prompt(self) -> str:
        """Prompt for generating challenging problems."""
        return """You are the Challenger in a self-evolving AI system.
Your job is to generate challenging problems that push the Solver's boundaries.

Generate a problem that is:
1. Solvable but requires reasoning
2. At the edge of current AI capabilities
3. Has a verifiable answer

Domain: {domain}
Difficulty level: {difficulty}/10

Output format:
PROBLEM: [the problem statement]
EXPECTED_ANSWER: [the expected answer]
"""
    
    def _get_solver_prompt(self) -> str:
        """Prompt for solving problems."""
        return """You are the Solver in a self-evolving AI system.
Your job is to solve problems step by step with careful reasoning.

Problem: {problem}

Think through this step by step:
1. Understand the problem
2. Break it down into sub-problems
3. Solve each part
4. Verify your answer

Output format:
REASONING: [your step-by-step reasoning]
ANSWER: [your final answer]
CONFIDENCE: [0.0-1.0 how confident you are]
"""
    
    def _get_refine_prompt(self) -> str:
        """Prompt for self-refinement."""
        return """You are refining a previous answer to improve it.

Problem: {problem}
Previous answer: {previous_answer}
Previous reasoning: {previous_reasoning}
Feedback: {feedback}

Improve your answer by:
1. Identifying weaknesses in previous reasoning
2. Correcting any errors
3. Providing clearer explanation

Output format:
REASONING: [improved step-by-step reasoning]
ANSWER: [improved final answer]
CONFIDENCE: [0.0-1.0 how confident you are]
"""
    
    def solve(
        self,
        problem: str,
        expected_answer: Optional[str] = None,
        domain: str = "general",
    ) -> Solution:
        """
        Solve a problem using the configured evolution strategy.
        
        Args:
            problem: Problem to solve
            expected_answer: Optional expected answer for verification
            domain: Problem domain
            
        Returns:
            Solution with reasoning, answer, and confidence
        """
        start_time = time.perf_counter()
        
        if self.strategy == EvolutionStrategy.SELF_REFINE:
            solution = self._solve_with_self_refine(problem)
        elif self.strategy == EvolutionStrategy.CHALLENGER_SOLVER:
            solution = self._solve_with_challenger_solver(problem, domain)
        elif self.strategy == EvolutionStrategy.EXPERIENCE_REPLAY:
            solution = self._solve_with_experience_replay(problem)
        else:
            # Default to simple solve
            solution = self._simple_solve(problem)
        
        solution.latency = time.perf_counter() - start_time
        
        # Verify if verifier provided
        if expected_answer and self.verifier:
            solution.success = self.verifier(solution.answer, expected_answer)
        elif expected_answer:
            solution.success = solution.answer.strip().lower() == expected_answer.strip().lower()
        
        # Update metrics
        with self._lock:
            self._metrics.total_evolutions += 1
            if solution.success:
                self._metrics.successful_solves += 1
            else:
                self._metrics.failed_solves += 1
            
            # Update average latency
            n = self._metrics.total_evolutions
            self._metrics.average_solve_time = (
                (self._metrics.average_solve_time * (n - 1) + solution.latency) / n
            )
        
        return solution
    
    def _simple_solve(self, problem: str) -> Solution:
        """Simple single-shot solve."""
        prompt = self._solver_prompt.format(problem=problem)
        response = self.provider.generate(prompt)
        
        return self._parse_solution(response.content, problem)
    
    def _solve_with_self_refine(self, problem: str) -> Solution:
        """Solve with iterative self-refinement."""
        # Initial solve
        solution = self._simple_solve(problem)
        
        # Self-refine until confident or max iterations
        for i in range(self.max_refinements):
            if solution.confidence >= self.confidence_threshold:
                break
            
            # Generate feedback
            feedback = self._generate_self_feedback(problem, solution)
            
            # Refine
            prompt = self._refine_prompt.format(
                problem=problem,
                previous_answer=solution.answer,
                previous_reasoning=solution.reasoning,
                feedback=feedback,
            )
            response = self.provider.generate(prompt)
            solution = self._parse_solution(response.content, problem)
        
        return solution
    
    def _solve_with_challenger_solver(self, problem: str, domain: str) -> Solution:
        """Solve using Challenger-Solver dynamics."""
        # First, solve the given problem
        solution = self._solve_with_self_refine(problem)
        
        # Store experience
        challenge = Challenge(
            id=hashlib.sha256(problem.encode()).hexdigest()[:16],
            problem=problem,
            difficulty=1.0 - solution.confidence,  # Harder if less confident
            domain=domain,
        )
        
        experience = ExperienceEntry(
            challenge=challenge,
            solutions=[solution],
            best_solution=solution,
        )
        self._add_to_experience_buffer(experience)
        
        return solution
    
    def _solve_with_experience_replay(self, problem: str) -> Solution:
        """Solve using experience from similar past problems."""
        # Find similar experiences
        similar = self._find_similar_experiences(problem, k=3)
        
        if similar:
            # Use past experiences as examples
            examples = "\n".join([
                f"Example {i+1}:\nProblem: {exp.challenge.problem}\nSolution: {exp.best_solution.answer}"
                for i, exp in enumerate(similar) if exp.best_solution
            ])
            
            prompt = f"""Learn from these examples and solve the new problem:

{examples}

Now solve this new problem:
{problem}

{self._solver_prompt.format(problem=problem)}"""
            
            response = self.provider.generate(prompt)
            return self._parse_solution(response.content, problem)
        else:
            return self._solve_with_self_refine(problem)
    
    def _generate_self_feedback(self, problem: str, solution: Solution) -> str:
        """Generate self-feedback for refinement."""
        prompt = f"""Review this solution and provide constructive feedback:

Problem: {problem}
Reasoning: {solution.reasoning}
Answer: {solution.answer}
Confidence: {solution.confidence}

What are the weaknesses? What could be improved?
Provide specific, actionable feedback."""
        
        response = self.provider.generate(prompt)
        return response.content
    
    def _parse_solution(self, response: str, problem: str) -> Solution:
        """Parse solution from LLM response."""
        # Extract components using simple parsing
        reasoning = ""
        answer = ""
        confidence = 0.5
        
        lines = response.split("\n")
        current_section = None
        
        for line in lines:
            if line.startswith("REASONING:"):
                current_section = "reasoning"
                reasoning = line.replace("REASONING:", "").strip()
            elif line.startswith("ANSWER:"):
                current_section = "answer"
                answer = line.replace("ANSWER:", "").strip()
            elif line.startswith("CONFIDENCE:"):
                try:
                    confidence = float(line.replace("CONFIDENCE:", "").strip())
                except ValueError:
                    pass
            elif current_section == "reasoning":
                reasoning += " " + line.strip()
            elif current_section == "answer":
                answer += " " + line.strip()
        
        # Fallback if parsing failed
        if not answer:
            answer = response.strip()
        
        return Solution(
            challenge_id=hashlib.sha256(problem.encode()).hexdigest()[:16],
            reasoning=reasoning.strip(),
            answer=answer.strip(),
            confidence=min(max(confidence, 0.0), 1.0),
            latency=0.0,
        )
    
    def _add_to_experience_buffer(self, experience: ExperienceEntry) -> None:
        """Add experience to replay buffer with eviction."""
        self._experience_buffer.append(experience)
        
        # Evict oldest if over capacity
        if len(self._experience_buffer) > self._buffer_size:
            self._experience_buffer = self._experience_buffer[-self._buffer_size:]
    
    def _find_similar_experiences(self, problem: str, k: int = 3) -> List[ExperienceEntry]:
        """Find similar past experiences (simple word overlap)."""
        problem_words = set(problem.lower().split())
        
        scored = []
        for exp in self._experience_buffer:
            exp_words = set(exp.challenge.problem.lower().split())
            overlap = len(problem_words & exp_words)
            if overlap > 0:
                scored.append((overlap, exp))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [exp for _, exp in scored[:k]]
    
    def generate_challenge(
        self,
        domain: str = "math",
        difficulty: int = 5,
    ) -> Challenge:
        """
        Generate a new challenge using the Challenger role.
        
        Args:
            domain: Problem domain (math, logic, code, etc.)
            difficulty: Difficulty level 1-10
            
        Returns:
            Generated Challenge
        """
        prompt = self._challenger_prompt.format(
            domain=domain,
            difficulty=difficulty,
        )
        response = self.provider.generate(prompt)
        
        # Parse challenge
        problem = ""
        expected = ""
        
        for line in response.content.split("\n"):
            if line.startswith("PROBLEM:"):
                problem = line.replace("PROBLEM:", "").strip()
            elif line.startswith("EXPECTED_ANSWER:"):
                expected = line.replace("EXPECTED_ANSWER:", "").strip()
        
        return Challenge(
            id=hashlib.sha256(f"{problem}:{time.time()}".encode()).hexdigest()[:16],
            problem=problem or response.content,
            difficulty=difficulty / 10.0,
            domain=domain,
            metadata={"expected_answer": expected},
        )
    
    def training_loop(
        self,
        iterations: int = 10,
        domain: str = "math",
        on_iteration: Optional[Callable[[int, Challenge, Solution], None]] = None,
    ) -> EvolutionMetrics:
        """
        Run a self-improvement training loop.
        
        This is a simplified version of R-Zero's training loop for inference-time use.
        For full training with gradient updates, use the R-Zero repository.
        
        Args:
            iterations: Number of challenge-solve iterations
            domain: Domain for challenges
            on_iteration: Optional callback(iteration, challenge, solution)
            
        Returns:
            EvolutionMetrics after training
        """
        for i in range(iterations):
            # Generate adaptive difficulty
            difficulty = min(1 + i // 2, 10)  # Gradually increase
            
            # Challenger generates problem
            challenge = self.generate_challenge(domain=domain, difficulty=difficulty)
            
            # Solver attempts solution
            expected = challenge.metadata.get("expected_answer")
            solution = self.solve(challenge.problem, expected_answer=expected, domain=domain)
            
            # Callback
            if on_iteration:
                on_iteration(i, challenge, solution)
        
        return self.get_metrics()
    
    def get_metrics(self) -> EvolutionMetrics:
        """Get current evolution metrics."""
        with self._lock:
            return EvolutionMetrics(
                total_evolutions=self._metrics.total_evolutions,
                successful_solves=self._metrics.successful_solves,
                failed_solves=self._metrics.failed_solves,
                average_solve_time=self._metrics.average_solve_time,
                improvement_rate=self._metrics.improvement_rate,
            )
    
    def get_experience_count(self) -> int:
        """Get number of experiences in buffer."""
        return len(self._experience_buffer)


# Convenience factory
def create_self_evolving_rlm(
    provider,
    strategy: str = "self_refine",
    **kwargs
) -> SelfEvolvingRLM:
    """
    Create a self-evolving RLM instance.
    
    Args:
        provider: LLM provider instance
        strategy: "self_refine", "challenger_solver", "multi_agent", or "experience_replay"
        **kwargs: Additional SelfEvolvingRLM arguments
        
    Returns:
        Configured SelfEvolvingRLM
    """
    strategy_map = {
        "self_refine": EvolutionStrategy.SELF_REFINE,
        "challenger_solver": EvolutionStrategy.CHALLENGER_SOLVER,
        "multi_agent": EvolutionStrategy.MULTI_AGENT,
        "experience_replay": EvolutionStrategy.EXPERIENCE_REPLAY,
    }
    
    return SelfEvolvingRLM(
        provider=provider,
        strategy=strategy_map.get(strategy, EvolutionStrategy.SELF_REFINE),
        **kwargs
    )


class TrainingDataGenerator:
    """
    Self-generated training data for future fine-tuning.
    
    Based on R-Zero's pseudo-labeling approach with majority voting.
    Generates (challenge, solution) pairs for fine-tuning datasets.
    
    Example:
        >>> generator = TrainingDataGenerator(evolving_rlm)
        >>> dataset = generator.generate(size=100, domain="math")
        >>> generator.save_jsonl("training_data.jsonl")
    """
    
    def __init__(self, evolving_rlm: SelfEvolvingRLM, num_attempts: int = 3):
        """
        Initialize training data generator.
        
        Args:
            evolving_rlm: SelfEvolvingRLM instance
            num_attempts: Number of solution attempts for majority voting
        """
        self.rlm = evolving_rlm
        self.num_attempts = num_attempts
        self._generated_data: List[Dict[str, Any]] = []
    
    def generate(
        self,
        size: int = 100,
        domain: str = "math",
        min_difficulty: int = 1,
        max_difficulty: int = 10,
        on_progress: Optional[Callable[[int, int], None]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Generate training data with pseudo-labeling.
        
        Args:
            size: Number of training examples to generate
            domain: Problem domain
            min_difficulty: Minimum difficulty level
            max_difficulty: Maximum difficulty level
            on_progress: Optional callback(current, total)
            
        Returns:
            List of training data dicts with keys: problem, solution, reasoning, confidence
        """
        data = []
        
        for i in range(size):
            # Generate challenge with random difficulty
            difficulty = random.randint(min_difficulty, max_difficulty)
            challenge = self.rlm.generate_challenge(domain=domain, difficulty=difficulty)
            
            # Multiple solution attempts
            solutions = []
            for _ in range(self.num_attempts):
                solution = self.rlm.solve(
                    challenge.problem,
                    expected_answer=challenge.metadata.get("expected_answer"),
                    domain=domain,
                )
                solutions.append(solution)
            
            # Majority voting for best answer
            best = self._majority_vote(solutions)
            
            if best and best.confidence >= 0.5:  # Only include confident solutions
                entry = {
                    "problem": challenge.problem,
                    "solution": best.answer,
                    "reasoning": best.reasoning,
                    "confidence": best.confidence,
                    "domain": domain,
                    "difficulty": difficulty,
                    "expected_answer": challenge.metadata.get("expected_answer"),
                }
                data.append(entry)
            
            if on_progress:
                on_progress(i + 1, size)
        
        self._generated_data.extend(data)
        return data
    
    def _majority_vote(self, solutions: List[Solution]) -> Optional[Solution]:
        """Select best solution via majority voting."""
        if not solutions:
            return None
        
        # Count answer frequencies
        answer_counts: Dict[str, int] = {}
        answer_to_solution: Dict[str, Solution] = {}
        
        for sol in solutions:
            answer = sol.answer.strip().lower()
            answer_counts[answer] = answer_counts.get(answer, 0) + 1
            # Keep highest confidence solution for each answer
            if answer not in answer_to_solution or sol.confidence > answer_to_solution[answer].confidence:
                answer_to_solution[answer] = sol
        
        # Find most common answer
        most_common = max(answer_counts.items(), key=lambda x: x[1])
        return answer_to_solution.get(most_common[0])
    
    def save_jsonl(self, path: str) -> int:
        """
        Save generated data to JSONL file.
        
        Args:
            path: Output file path
            
        Returns:
            Number of entries saved
        """
        with open(path, 'w') as f:
            for entry in self._generated_data:
                f.write(json.dumps(entry) + "\n")
        
        return len(self._generated_data)
    
    def get_dataset_stats(self) -> Dict[str, Any]:
        """Get statistics about generated dataset."""
        if not self._generated_data:
            return {"size": 0}
        
        confidences = [d["confidence"] for d in self._generated_data]
        difficulties = [d["difficulty"] for d in self._generated_data]
        
        return {
            "size": len(self._generated_data),
            "avg_confidence": sum(confidences) / len(confidences),
            "avg_difficulty": sum(difficulties) / len(difficulties),
            "domains": list(set(d["domain"] for d in self._generated_data)),
        }
    
    def clear(self) -> None:
        """Clear generated data."""
        self._generated_data.clear()
