# Web server configuration
HOST=0.0.0.0
PORT=8080

# Skip logging for redirection responses
SKIP_REDIRECTION_LOGGING=true

# Application mode. If the value is "dev", it will enable uvicorn reload
ENV_MODE=development

# Authentication secret, HTTP bearer token header is required if set
AUTH_SECRET=

# Observability backend
# OBSERVABILITY_BACKEND=langfuse

# Langfuse configuration
# LANGFUSE_SECRET_KEY=
# LANGFUSE_PUBLIC_KEY=
# LANGFUSE_HOST=http://langfuse-web:3000

# Langsmith configuration
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=
# LANGSMITH_PROJECT=default
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# Database type.
# If the value is "postgres", then it will require Postgresql related environment variables.
# If the value is "sqlite", then you can configure optional file path via SQLITE_DB_PATH
MEMORY_BACKEND=postgres

# If DATABASE_TYPE=sqlite (Optional)
SQLITE_DB_PATH=

# If DATABASE_TYPE=postgres
POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_HOST=
POSTGRES_PORT=
POSTGRES_DB=agents
# Connection pool settings
# Rule: POOL_SIZE should be < (max_connections - reserved_for_superuser - other_apps)
# With max_connections=181, reserve ~30 for superuser/monitoring, leaves ~150 for app
POSTGRES_POOL_SIZE=100
POSTGRES_MIN_SIZE=5
POSTGRES_MAX_IDLE=120
POSTGRES_POOL_TIMEOUT=45.0
POSTGRES_RECONNECT_TIMEOUT=60.0
# Maximum lifetime of a connection (seconds). Old connections are automatically recycled.
POSTGRES_MAX_LIFETIME=300.0
# Number of background workers for pool maintenance
# POSTGRES_NUM_WORKERS=3
# PostgreSQL query timeouts (milliseconds) - CRITICAL for preventing stuck connections
# Maximum time a query can run. Should be > LLM latency (30s) + buffer for retries/tools.
POSTGRES_STATEMENT_TIMEOUT=120000
# Maximum time to wait for locks. If same thread_id used concurrently, second request waits.
POSTGRES_LOCK_TIMEOUT=45000
# Maximum idle time in transaction. Should be > LLM latency to not kill active requests.
POSTGRES_IDLE_IN_TRANSACTION_SESSION_TIMEOUT=120000

# Agent URL: used in Streamlit app - if not set, defaults to http://{HOST}:{PORT}
# AGENT_URL=http://0.0.0.0:8080

# Use a fake model for testing
USE_FAKE_MODEL=false

# OpenAI Settings
OPENAI_API_KEY=
OPENAI_API_BASE_URL=
OPENAI_API_VERSION=
OPENAI_MODEL_NAME=

# Azure OpenAI Settings
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_VERSION=
AZURE_OPENAI_MODEL_NAME=
AZURE_OPENAI_DEPLOYMENT_NAME=

# Anthropic Settings
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL_NAME=

 # Google VertexAI Settings
GOOGLE_VERTEXAI_MODEL_NAME=
GOOGLE_VERTEXAI_API_KEY=

# Google GenAI Settings
GOOGLE_GENAI_MODEL_NAME=
GOOGLE_GENAI_API_KEY=

# Bedrock Settings
AWS_BEDROCK_MODEL_NAME=

# DeepSeek Settings
DEEPSEEK_MODEL_NAME=
DEEPSEEK_API_KEY=

# Ollama Settings
OLLAMA_MODEL_NAME=
OLLAMA_BASE_URL=

# Openrouter
OPENROUTER_API_KEY=

# Model configurations - use multiline format with line continuation for readability
# It can be useful if you want to use different models for different agents or use different models from different providers.
MODEL_CONFIGS={"router":{"provider":"azure_openai","model_name":"gpt-4o","openai_api_key":"your-azure-key-here","azure_endpoint":"https://your-resource.openai.azure.com/","openai_api_version":"2024-12-01-preview","deployment_name":"gpt-4o-deployment"},"assistant":{"provider":"azure_openai","model_name":"gpt-4o-mini","openai_api_key":"your-azure-key-here","azure_endpoint":"https://your-resource.openai.azure.com/","openai_api_version":"2024-12-01-preview","deployment_name":"gpt-4o-mini-deployment"},"analyzer":{"provider":"google_genai","model_name":"gemini-pro","api_key":"your-google-key-here","temperature":0.7}}
MODEL_CONFIGS_BASE64=
MODEL_CONFIGS_PATH=

# Amazon Bedrock Knowledge Base ID
AWS_KB_ID=

CHECK_INTERRUPTS=
