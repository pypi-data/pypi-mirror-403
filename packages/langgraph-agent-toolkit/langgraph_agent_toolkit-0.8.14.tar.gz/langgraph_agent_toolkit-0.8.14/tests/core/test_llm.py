import pytest
from langchain.chat_models.base import _ConfigurableModel
from langchain_community.chat_models import FakeListChatModel
from langchain_core.runnables import RunnableSerializable
from langchain_openai import ChatOpenAI

from langgraph_agent_toolkit.core.models.factory import CompletionModelFactory
from langgraph_agent_toolkit.schema.models import ModelProvider


def test_get_model_openai_compatible():
    # Fix: Pass the base URL directly in kwargs instead of relying on settings
    model = CompletionModelFactory.create(
        ModelProvider.OPENAI, model_name="gpt-4", openai_api_key="test_key", openai_api_base="http://api.example.com"
    )
    assert isinstance(model, (ChatOpenAI, RunnableSerializable, _ConfigurableModel))
    assert model.model_name == "gpt-4"
    assert model.streaming is True
    assert model.openai_api_base == "http://api.example.com"
    assert model.openai_api_key.get_secret_value() == "test_key"


def test_get_model_openai_compatible_missing_config():
    # Fix: No need to patch settings here since we're testing a specific condition
    with pytest.raises(ValueError, match="Model name must be provided for non-fake models"):
        CompletionModelFactory.create(ModelProvider.OPENAI)


def test_get_model_fake():
    model = CompletionModelFactory.create(ModelProvider.FAKE)
    assert isinstance(model, FakeListChatModel)
    assert model.responses == ["This is a test response from the fake model."]


def test_get_model_invalid():
    # Invalid provider string raises ValueError when converting to ModelProvider enum
    with pytest.raises(ValueError, match="is not a valid ModelProvider"):
        CompletionModelFactory.create("invalid_model", model_name=None)  # type: ignore
