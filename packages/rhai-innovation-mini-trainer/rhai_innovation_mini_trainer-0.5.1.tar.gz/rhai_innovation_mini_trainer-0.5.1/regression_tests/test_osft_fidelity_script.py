"""
Test script to verify OSFT decomposition and reconstruction fidelity.

This script tests that when a model is created with distributed OSFT initialization,
the reconstructed parameters from the decomposed SVD parts are identical to the
original untouched model parameters (within numerical tolerance).

Usage:
    # Single GPU
    python test_osft_fidelity_script.py

    # Multiple GPUs (to test distributed SVD computation)
    torchrun --nnodes=1 --nproc-per-node=4 test_osft_fidelity_script.py
"""

import torch
import torch.distributed as dist
import torch.nn as nn
import os
import time
from mini_trainer.setup_model_for_training import setup_model
from mini_trainer.utils import init_distributed_environment, log_rank_0
from mini_trainer.osft_utils import get_osft_target_parameters
import typer


app = typer.Typer()


def load_original_model(model_name_or_path, use_liger_kernels=False):
    """Load the original model without OSFT decomposition."""
    rank = int(os.environ.get("RANK", 0))

    log_rank_0("Loading original model (no OSFT)...")
    original_model = setup_model(
        model_name_or_path=model_name_or_path,
        use_liger_kernels=use_liger_kernels,
        osft=False,  # No OSFT
        local_rank=rank,
    )
    return original_model


def load_osft_model(model_name_or_path, use_liger_kernels=False):
    """Load the model with distributed OSFT initialization.
    
    Note: We use float64 for upcast_dtype and output_dtype to ensure accurate
    orthogonality and rank checks. With float32 and especially bfloat16, 
    numerical precision issues make it very difficult to verify orthogonality.
    As matrix size increases, even float64 may not be sufficient for perfect
    orthogonality validation.
    """
    rank = int(os.environ.get("RANK", 0))

    log_rank_0("Loading OSFT model (with distributed SVD decomposition)...")
    osft_model = setup_model(
        model_name_or_path=model_name_or_path,
        use_liger_kernels=use_liger_kernels,
        osft=True,  # Enable OSFT
        local_rank=rank,
        upcast_dtype=torch.float64,  # Use float64 for better numerical stability
        output_dtype=torch.float64,  # Maintain precision in output
    )
    return osft_model


def compare_parameters(original_model, osft_model, tolerance=1e-5):
    """
    Compare parameters between original model and reconstructed OSFT model.

    Args:
        original_model: Model without OSFT decomposition
        osft_model: Model with OSFT decomposition
        tolerance: Numerical tolerance for comparison

    Returns:
        dict: Results of comparison including statistics
    """
    results = {
        "total_params_compared": 0,
        "identical_params": 0,
        "close_params": 0,
        "different_params": 0,
        "max_difference": 0.0,
        "avg_difference": 0.0,
        "differences": [],
        "param_details": [],
    }

    # # Get parameters that should be decomposed
    # if hasattr(osft_model, "osft_config"):
    #     target_params = get_osft_target_parameters(osft_model, osft_model.osft_config)
    #     target_param_names = set(name for name, _ in target_params)
    # else:
    #     target_param_names = set()

    # Compare all parameters
    # original_params = dict(original_model.named_parameters())

    # # this is how we need to compare the parameters
    # ignored_params = []
    # for orig_name, original_param in original_model.state_dict():
    #     if orig_name not in osft_model.name_mapping:
    #         # save this for logging purposes later
    #         ignored_params += [orig_name]
    #         continue

    #     reconstructed_param = osft_model._reconstruct_weight(orig_name)
    #     # now we can compare original_param with reconstructed_param

    ignored_params = []
    for orig_name, original_param in original_model.state_dict().items():
        name = orig_name
        if orig_name not in osft_model.name_mapping:
            # this one isnt in the svd params so we ignore
            ignored_params += [orig_name]
            continue

        # This parameter was decomposed, so reconstruct it
        try:
            comparison_param = osft_model._reconstruct_weight(orig_name)
            param_type = "reconstructed"
        except Exception as e:
            # If reconstruction fails, skip this parameter
            results["param_details"].append(
                {"name": orig_name, "type": "reconstruction_failed", "error": str(e)}
            )
            continue

        # upcast the original_param to reduce information loss from comparison
        if comparison_param.dtype != original_param.dtype:
            original_param = original_param.to(comparison_param.dtype)

        # Compare parameters
        if comparison_param.shape != original_param.shape:
            results["param_details"].append(
                {
                    "name": orig_name,
                    "type": param_type,
                    "status": "shape_mismatch",
                    "original_shape": original_param.shape,
                    "comparison_shape": comparison_param.shape,
                }
            )
            results["different_params"] += 1
            continue

        # Calculate difference
        diff = torch.abs(comparison_param - original_param)
        max_diff = diff.max().item()
        mean_diff = diff.mean().item()

        results["total_params_compared"] += 1
        results["differences"].append(max_diff)
        results["max_difference"] = max(results["max_difference"], max_diff)

        # Classify the difference
        if torch.equal(comparison_param, original_param):
            status = "identical"
            results["identical_params"] += 1
        elif torch.allclose(
            comparison_param, original_param, atol=tolerance, rtol=tolerance
        ):
            status = "close"
            results["close_params"] += 1
        else:
            status = "different"
            results["different_params"] += 1

        results["param_details"].append(
            {
                "name": name,
                "type": param_type,
                "status": status,
                "max_diff": max_diff,
                "mean_diff": mean_diff,
                "shape": tuple(original_param.shape),
            }
        )

    # Calculate average difference
    if results["differences"]:
        results["avg_difference"] = sum(results["differences"]) / len(
            results["differences"]
        )

    print_results(results, tolerance)
    dist.breakpoint()
    return results


def print_results(results, tolerance):
    """Print comparison results in a formatted way."""
    print(f"\n{'=' * 60}")
    print(f"OSFT RECONSTRUCTION FIDELITY TEST RESULTS")
    print(f"{'=' * 60}")

    print(f"Parameters compared: {results['total_params_compared']}")
    print(f"Tolerance: {tolerance:.2e}")
    print(f"")

    print(f"ðŸ“Š Summary:")
    print(f"  âœ… Identical parameters: {results['identical_params']}")
    print(f"  ðŸ” Close parameters (within tolerance): {results['close_params']}")
    print(f"  âŒ Different parameters: {results['different_params']}")
    print(f"")

    print(f"ðŸ“ˆ Numerical Statistics:")
    print(f"  Max difference: {results['max_difference']:.2e}")
    print(f"  Average difference: {results['avg_difference']:.2e}")
    print(f"")

    # Show parameter breakdown by type
    reconstructed_count = sum(
        1 for p in results["param_details"] if p["type"] == "reconstructed"
    )
    direct_count = sum(1 for p in results["param_details"] if p["type"] == "direct")

    print(f"ðŸ”§ Parameter Types:")
    print(f"  Reconstructed from OSFT: {reconstructed_count}")
    print(f"  Direct comparison: {direct_count}")
    print(f"")

    # Show worst differences for reconstructed parameters
    reconstructed_params = [
        p
        for p in results["param_details"]
        if p["type"] == "reconstructed" and "max_diff" in p
    ]
    if reconstructed_params:
        print(f"ðŸš¨ Worst Reconstructed Parameter Differences:")
        worst_params = sorted(
            reconstructed_params, key=lambda x: x["max_diff"], reverse=True
        )[:5]
        for param in worst_params:
            print(
                f"  {param['name']}: {param['max_diff']:.2e} (status: {param['status']})"
            )
        print(f"")

    # Overall result
    success_rate = (
        (
            (results["identical_params"] + results["close_params"])
            / results["total_params_compared"]
            * 100
        )
        if results["total_params_compared"] > 0
        else 0
    )

    print(f"ðŸŽ¯ Overall Result:")
    if results["different_params"] == 0:
        print(f"  âœ… SUCCESS: All parameters match within tolerance!")
    else:
        print(f"  âš ï¸  PARTIAL SUCCESS: {success_rate:.1f}% of parameters match")

    print(f"  Success rate: {success_rate:.1f}%")
    print(f"{'=' * 60}")


def compare_params(original_model: nn.Module, osft_model: nn.Module):
    # here we can compare the original parameters
    for k, orig_w in original_model.state_dict().items():
        if k not in osft_model.name_mapping:
            print("we dont do this one")
            continue


@app.command()
def test_reconstruction_fidelity(
    model_name_or_path: str = typer.Option(
        "Qwen/Qwen2.5-1.5B-Instruct", help="Model name or path"
    ),
    use_liger_kernels: bool = typer.Option(False, help="Whether to use liger kernels"),
    tolerance: float = typer.Option(1e-5, help="Numerical tolerance for comparison"),
    verbose: bool = typer.Option(False, help="Show detailed parameter comparison"),
):
    """Test OSFT reconstruction fidelity."""

    # Initialize distributed environment
    init_distributed_environment()

    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))

    log_rank_0(f"ðŸ§ª Testing OSFT reconstruction fidelity with {world_size} ranks")
    log_rank_0(f"Model: {model_name_or_path}")
    log_rank_0(f"Tolerance: {tolerance:.2e}")

    # Load OSFT model
    start_time = time.time()
    osft_model = load_osft_model(model_name_or_path, use_liger_kernels)
    osft_time = time.time() - start_time
    log_rank_0(f"â±ï¸ OSFT model loaded in {osft_time:.2f}s")

    # Compare parameters (only on rank 0 to avoid redundant work)
    if rank == 0:
        # Load original model
        start_time = time.time()
        original_model = load_original_model(model_name_or_path, use_liger_kernels)
        device_0 = torch.device("cuda", 0)
        original_model = original_model.to(device_0)
        original_time = time.time() - start_time

        log_rank_0(f"â±ï¸ Original model loaded in {original_time:.2f}s")

        log_rank_0("ðŸ” Comparing parameters...")
        start_time = time.time()
        results = compare_parameters(original_model, osft_model, tolerance)
        comparison_time = time.time() - start_time
        log_rank_0(f"â±ï¸ Parameter comparison completed in {comparison_time:.2f}s")

        # Print results
        print_results(results, tolerance)

        # Show detailed results if requested
        if verbose:
            print(f"\nðŸ“‹ Detailed Parameter Comparison:")
            for param in results["param_details"]:
                if "max_diff" in param:
                    print(
                        f"  {param['name']} ({param['type']}): {param['status']} - "
                        f"max_diff={param['max_diff']:.2e}, mean_diff={param['mean_diff']:.2e}"
                    )
                else:
                    print(f"  {param['name']} ({param['type']}): {param['status']}")

    # Synchronize all processes
    if world_size > 1:
        dist.barrier()

    log_rank_0("ðŸŽ‰ OSFT reconstruction fidelity test completed!")


if __name__ == "__main__":
    app()
