import dataclasses
from typing import List, Union, Optional, Any, Dict, Literal
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import polars as pl
import pandas as pd

from mars.core.base import MarsBaseEstimator
from mars.analysis.report import MarsProfileReport
from mars.analysis.config import MarsProfileConfig
from mars.feature.binner import MarsNativeBinner
from mars.utils.logger import logger
from mars.utils.decorators import time_it # , monitor_os_memory
from mars.utils.date import MarsDate

class MarsDataProfiler(MarsBaseEstimator):
    """
    [MarsDataProfiler] åŸºäº Polars çš„é«˜æ€§èƒ½å¤šç»´æ•°æ®ç”»åƒå·¥å…·ã€‚

    ä¸“ä¸ºå¤§è§„æ¨¡é£æ§å»ºæ¨¡æ•°æ®é›†è®¾è®¡ã€‚å®ƒä½œä¸ºåˆ†ææµç¨‹çš„å…¥å£ï¼Œå°è£…äº†ä»
    æ•°æ®è´¨é‡è¯Šæ–­ã€ç»Ÿè®¡å€¼è®¡ç®—åˆ°å¯è§†åŒ–ç”Ÿæˆçš„å…¨é“¾è·¯é€»è¾‘ã€‚

    ä¸»è¦åŠŸèƒ½ (Key Features)
    -----------------------
    1. **å…¨é‡æŒ‡æ ‡æ¦‚è§ˆ (Overview)**:
       - è®¡ç®— Missing/Zero/Unique/Top1 ç­‰åŸºç¡€ DQ æŒ‡æ ‡ã€‚
       - è‡ªåŠ¨è¯†åˆ«å¹¶è®¡ç®—æ•°å€¼åˆ—çš„ç»Ÿè®¡åˆ†å¸ƒ (Mean, Std, Quantiles)ã€‚
    
    2. **è¿·ä½ åˆ†å¸ƒå›¾ (Sparklines)**:
       - åœ¨æŠ¥å‘Šä¸­ç”Ÿæˆ Unicode å­—ç¬¦ç”» (å¦‚  â–‚â–…â–‡â–ˆ)ã€‚
       - **å¯è§†åŒ–é€»è¾‘**: è‡ªåŠ¨é‡‡æ · (é»˜è®¤20wè¡Œ) -> å‰”é™¤å¼‚å¸¸å€¼ -> ç­‰å®½åˆ†ç®± -> å­—ç¬¦æ˜ å°„ã€‚
       - æ”¯æŒé€šè¿‡ Config è°ƒæ•´åˆ†ç®±ç²¾åº¦å’Œé‡‡æ ·ç‡ã€‚

    3. **å¤šç»´è¶‹åŠ¿åˆ†æ (Trend Analysis)**:
       - æ”¯æŒæŒ‰æ—¶é—´ (Month/Vintage) æˆ–å®¢ç¾¤ (Segment) è¿›è¡Œåˆ†ç»„åˆ†æã€‚
       - è‡ªåŠ¨è®¡ç®—ç»„é—´ç¨³å®šæ€§æŒ‡æ ‡ (Variance/CV)ã€‚

    Attributes
    ----------
    df : pl.DataFrame
        å†…éƒ¨å­˜å‚¨çš„ Polars DataFrame (å·²è½¬æ¢ä¸º Polars æ ¼å¼)ã€‚
    features : List[str]
        æœ€ç»ˆç¡®å®šçš„å¾…åˆ†æç‰¹å¾åˆ—è¡¨ (ç»è¿‡ exclude_features å’Œ include_dtypes ç­›é€‰å)ã€‚
    config : MarsProfileConfig
        å…¨å±€é…ç½®å¯¹è±¡ã€‚æ§åˆ¶è®¡ç®—å“ªäº›æŒ‡æ ‡ã€æ˜¯å¦ç”»å›¾ç­‰ã€‚
        è¯¦è§ `mars.analysis.config.MarsProfileConfig`ã€‚
    custom_missing : List[Any]
        è‡ªå®šä¹‰ç¼ºå¤±å€¼åˆ—è¡¨ (å¦‚ -999, 'null')ã€‚åœ¨è®¡ç®— missing_rate æ—¶è®¡å…¥åˆ†å­ï¼Œ
        åœ¨è®¡ç®—ç»Ÿè®¡åˆ†å¸ƒ (Mean/Std) æ—¶è¢«å‰”é™¤ã€‚
    special_values : List[Any]
        è‡ªå®šä¹‰ç‰¹æ®Šå€¼åˆ—è¡¨ã€‚è¿™äº›å€¼è¢«è§†ä¸ºæœ‰æ•ˆå€¼å‚ä¸ DQ ç»Ÿè®¡ (å¦‚ Top1)ï¼Œ
        ä½†åœ¨è®¡ç®—æ•°å€¼åˆ†å¸ƒ (Sparkline/Mean/PSI) æ—¶ä¼šè¢«å‰”é™¤ã€‚
    psi_cv_ignore_threshold : float
        PSI ç¨³å®šæ€§è®¡ç®—çš„é—¨æ§é˜ˆå€¼ (é»˜è®¤ 0.05)ã€‚
        ç”¨äºè§£å†³ "å°åŸºæ•°é™·é˜±"ï¼šè‹¥æŸç‰¹å¾åœ¨æ‰€æœ‰åˆ†ç»„ä¸‹çš„ PSI æœ€å¤§å€¼ä»å°äºè¯¥é˜ˆå€¼ï¼Œ
        åˆ™è®¤ä¸ºå…¶å¤„äºç»å¯¹ç¨³å®šåŒºï¼Œå¼ºåˆ¶å°† Group CV ç½®ä¸º 0ï¼Œé¿å…å¾®å°æ³¢åŠ¨è§¦å‘è¯¯æŠ¥ã€‚

    Examples
    --------
    >>> # 1. åŸºç¡€ç”¨æ³•
    >>> profiler = MarsDataProfiler(df)
    >>> report = profiler.generate_profile()
    >>> report.show_overview()

    >>> # 2. é«˜çº§ç”¨æ³•ï¼šè‡ªå®šä¹‰ç¼ºå¤±å€¼ + æŒ‰æœˆåˆ†ç»„ + å…³é—­ç”»å›¾(æé€Ÿ)
    >>> profiler = MarsDataProfiler(df, custom_missing_values=[-999, "unknown"])
    >>> report = profiler.generate_profile(
    ...     profile_by="month",
    ...     config_overrides={"enable_sparkline": False, "stat_metrics": ["psi", "mean", "min", "max"]}
    ... )
    """

    def __init__(
        self, 
        df: Union[pl.DataFrame, pd.DataFrame], 
        features: Optional[List[str]] = None,
        *,
        exclude_features: Optional[List[str]] = None,
        include_dtypes: Union[type, pl.DataType, List[Union[type, pl.DataType]], None] = None,
        
        custom_missing_values: Optional[List[Union[int, float, str]]] = None,
        custom_special_values: Optional[List[Any]] = None,
        
        overview_batch_size: int = 500,  # æ–°å¢ï¼šæ§åˆ¶æ¦‚è§ˆè®¡ç®—çš„æ‰¹å¤§å°
        
        psi_batch_size: int = 50, 
        psi_n_bins: int = 10,           
        psi_bin_method: Literal["quantile", "uniform"] = "quantile", 
        psi_cv_ignore_threshold: float = 0.05,
        
        sample_frac: Optional[float] = None, 
        
        config: Optional[MarsProfileConfig] = None,
    ) -> None:
        """
        Parameters
        ----------
        df : Union[pl.DataFrame, pd.DataFrame]
            è¾“å…¥æ•°æ®é›†ã€‚ä¼šè‡ªåŠ¨è½¬æ¢ä¸º Polars æ ¼å¼ä»¥åˆ©ç”¨å…¶å‘é‡åŒ–è®¡ç®—ä¼˜åŠ¿ã€‚
        features : List[str], optional
            æŒ‡å®šè¦åˆ†æçš„åˆ—ååˆ—è¡¨ã€‚å¦‚æœä¸º Noneï¼Œåˆ™åˆ†ææ‰€æœ‰åˆ—ã€‚
        exclude_features : List[str], optional
            [é»‘åå•] æŒ‡å®šè¦æ’é™¤çš„åˆ—ååˆ—è¡¨ã€‚
            é€»è¾‘ä¼˜å…ˆçº§: final_features = (features or all_cols) - exclude_features
        include_dtypes : List[pl.DataType], optional
            [ç±»å‹ç™½åå•] ä»…åŒ…å«æŒ‡å®šæ•°æ®ç±»å‹çš„åˆ—è¿›è¡Œåˆ†æã€‚
            æ”¯æŒ Python åŸç”Ÿç±»å‹å’Œ Polars ç±»å‹ã€‚ä¾‹å¦‚: [int, pl.Int64, pl.Float64] åªåˆ†ææ•°å€¼åˆ—ã€‚
        custom_missing_values : List[Union[int, float, str]], optional
            æŒ‡å®šè‡ªå®šä¹‰ç¼ºå¤±å€¼åˆ—è¡¨ã€‚ä¾‹å¦‚: [-999, "unknown", "\\N"]ã€‚
        custom_special_values : List[Any], optional
            æŒ‡å®šè‡ªå®šä¹‰ç‰¹æ®Šå€¼åˆ—è¡¨ (å¦‚æç«¯å€¼)ã€‚è¿™äº›å€¼åœ¨è®¡ç®—åˆ†å¸ƒå›¾æ—¶ä¼šè¢«å•ç‹¬å¤„ç†ã€‚
        psi_batch_size : int, optional
            è®¡ç®— PSI æ—¶çš„ç‰¹å¾æ‰¹å¤„ç†å¤§å°ã€‚é»˜è®¤ä¸º 50ã€‚
        psi_n_bins : int, optional
            è®¡ç®— PSI æ—¶çš„åˆ†ç®±æ•°é‡ã€‚é»˜è®¤ä¸º 10ã€‚
        psi_bin_method : str, optional
            è®¡ç®— PSI æ—¶çš„åˆ†ç®±æ–¹æ³•ã€‚æ”¯æŒ "quantile" æˆ– "uniform"ã€‚é»˜è®¤ä¸º "quantile"ã€‚
        psi_cv_ignore_threshold : float, optional
            PSI ç¨³å®šæ€§è®¡ç®—çš„é—¨æ§é˜ˆå€¼ã€‚é»˜è®¤ 0.01ã€‚
            å½“æŸç‰¹å¾çš„å¹³å‡ PSI å°äºè¯¥å€¼æ—¶ï¼Œå¼ºåˆ¶å°† group_cv ç½®ä¸º 0ï¼Œé¿å…"å°åŸºæ•°é™·é˜±"ã€‚
        sample_frac : float, optional
            å¦‚æœæŒ‡å®šï¼Œåˆ™å¯¹è¾“å…¥æ•°æ®è¿›è¡Œéšæœºé‡‡æ ·ï¼Œé‡‡æ ·æ¯”ä¾‹ä¸ºè¯¥å€¼ (0~1ä¹‹é—´)ã€‚
            æ•°æ®é‡éå¸¸å¤§æ—¶å¯ç”¨ä»¥æå‡åˆ†æé€Ÿåº¦ï¼Œä½†ä¼šç‰ºç‰²ä¸€å®šç²¾åº¦ã€‚
        config : MarsProfileConfig, optional
            é…ç½®å¯¹è±¡ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ã€‚
        
        """
        super().__init__()
        # 1. æ•°æ®æ¥å…¥ä¸é‡‡æ ·
        self.df = self._ensure_polars_dataframe(df)
        if sample_frac is not None and 0 < sample_frac < 1.0:
            logger.warning(f"ğŸ² Data is sampled (frac={sample_frac}). Metrics are estimates.")
            self.df = self.df.sample(fraction=sample_frac, shuffle=True)

        self.config = config if config else MarsProfileConfig()
        
        # 2. å€¼å¤„ç†é…ç½®
        self.custom_missing = custom_missing_values if custom_missing_values else []
        self.special_values = custom_special_values if custom_special_values else []
        
        # 3. PSI é…ç½®
        self.psi_batch_size = psi_batch_size
        self.psi_n_bins = psi_n_bins
        self.psi_bin_method = psi_bin_method
        self.psi_cv_ignore_threshold = psi_cv_ignore_threshold

        # 4. ç‰¹å¾ç­›é€‰é€»è¾‘ 
        # Step A: åˆå§‹èŒƒå›´
        candidates = features if features else self.df.columns
            
        # Step B: é»‘åå•å‰”é™¤
        if exclude_features:
            exclude_set = set(exclude_features)
            candidates = [c for c in candidates if c not in exclude_set]

        # Step C: ç±»å‹ç™½åå• (æ”¯æŒ PythonåŸç”Ÿç±»å‹ + Polarsç±»å‹)
        # ---------------------------------------------------------
        if include_dtypes:
            import polars.selectors as cs
            
            # 1. å½’ä¸€åŒ–ä¸ºåˆ—è¡¨
            if not isinstance(include_dtypes, list):
                raw_dtypes = [include_dtypes]
            else:
                raw_dtypes = include_dtypes
            
            # 2. ç±»å‹æ˜ å°„ï¼šPython Type -> Polars Abstract Type
            target_dtypes = []
            for t in raw_dtypes:
                # --- Python Native Mapping ---
                if t is int:
                    target_dtypes.append(pl.Integer) # åŒ¹é…æ‰€æœ‰æ•´å‹ (Int8~64, UInt)
                elif t is float:
                    target_dtypes.append(pl.Float)   # åŒ¹é…æ‰€æœ‰æµ®ç‚¹ (Float32/64)
                elif t is str:
                    target_dtypes.append(pl.String)  # åŒ¹é… String/Utf8
                elif t is bool:
                    target_dtypes.append(pl.Boolean)
                elif t is list:
                    target_dtypes.append(pl.List)
                # --- Polars Type Pass-through ---
                else:
                    target_dtypes.append(t)
            
            # 3. æ™ºèƒ½é€‰æ‹©
            try:
                # åˆ©ç”¨ Selectors è¿›è¡Œå®½å®¹åŒ¹é…
                dtype_selector = cs.by_dtype(target_dtypes)
                # åªåœ¨ candidates èŒƒå›´å†…ç­›é€‰
                matched_cols = self.df.select(pl.col(candidates)).select(dtype_selector).columns
                candidates = matched_cols
                
            except Exception as e:
                logger.error(f"Type filtering failed: {e}")
                # é™çº§ç­–ç•¥: ç®€å•çš„åŒ…å«åˆ¤æ–­ (ä»…å¯¹ Polars ç±»å‹æœ‰æ•ˆ)
                candidates = [c for c in candidates if self.df.schema[c] in target_dtypes]

        if not candidates:
            raise ValueError("No features selected after filtering.")
        
        self._dtype_map = self.df.schema
        self.overview_batch_size = overview_batch_size
            
        self.features = candidates
        
    @time_it
    def generate_profile(
        self, 
        profile_by: Optional[str] = None, 
        *,
        dt_col: Optional[str] = None,
        config_overrides: Optional[Dict[str, Any]] = None
    ) -> MarsProfileReport:
        """
        [Core] æ‰§è¡Œæ•°æ®ç”»åƒåˆ†æç®¡é“ï¼Œç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Šã€‚

        è¯¥æ–¹æ³•ä¼šè‡ªåŠ¨è®¡ç®—ä¸¤ç±»æŒ‡æ ‡ï¼š
        1. **Overview (å…¨é‡æ¦‚è§ˆ)**: åŒ…å«æ•°æ®åˆ†å¸ƒ(Sparkline)ã€DQæŒ‡æ ‡ã€ç»Ÿè®¡æŒ‡æ ‡ã€‚ä¸æ¶‰åŠåˆ†ç»„ã€‚
        2. **Trends (åˆ†ç»„è¶‹åŠ¿)**: å¦‚æœæŒ‡å®šäº† `profile_by`ï¼Œä¼šè®¡ç®—å„é¡¹æŒ‡æ ‡éšè¯¥ç»´åº¦çš„å˜åŒ–ã€‚
        
        **æ—¥æœŸèšåˆåŠŸèƒ½ (New)**:
        å¦‚æœæŒ‡å®šäº† `dt_col`ï¼Œ`profile_by` å¯ç›´æ¥ä¼ å…¥ "day", "week", "month"ã€‚
        ç¨‹åºä¼šè‡ªåŠ¨åŸºäº `dt_col` ç”Ÿæˆå¯¹åº”çš„æ—¶é—´ç²’åº¦åˆ—è¿›è¡Œåˆ†ç»„åˆ†æã€‚

        Parameters
        ----------
        profile_by : str, optional
            åˆ†ç»„ç»´åº¦ã€‚
            - è‹¥æä¾› `dt_col`: å¯é€‰ "day", "week", "month"ã€‚
            - è‹¥æœªæä¾› `dt_col`: å¿…é¡»æ˜¯æ•°æ®ä¸­å·²å­˜åœ¨çš„åˆ—åã€‚
            - None: ä»…ç”Ÿæˆ Overview å’Œ Total è¶‹åŠ¿åˆ—ã€‚
            - å¦‚æœæ˜¯è‡ªåŠ¨èšåˆï¼ŒOverview è¡¨ä¸­ä¸ä¼šåŒ…å«è¿™ä¸ªä¸´æ—¶çš„æ—¥æœŸåˆ†ç»„åˆ—ï¼Œåªä¼šåœ¨ Trends è¡¨ä¸­ä½“ç°ã€‚
        
        dt_col : str, optional
            æŒ‡å®šæ—¥æœŸ/æ—¶é—´åˆ—åã€‚ç”¨äºé…åˆ `profile_by` è¿›è¡Œè‡ªåŠ¨æ—¶é—´èšåˆã€‚

        config_overrides : Dict[str, Any], optional
            ä¸´æ—¶è¦†ç›– `MarsProfileConfig` ä¸­çš„é»˜è®¤é…ç½®ã€‚æ”¯æŒä»¥ä¸‹é…ç½®é¡¹ï¼š

            **1. è®¡ç®—èŒƒå›´ (Metrics)**
            
            * ``stat_metrics`` (List[str]): éœ€è¦è®¡ç®—çš„ç»Ÿè®¡æŒ‡æ ‡ã€‚
              å¯é€‰å€¼: "psi", "mean", "std", "min", "max", "p25", "median", "p75", "skew", "kurtosis"ã€‚
            * ``dq_metrics`` (List[str]): éœ€è¦è®¡ç®—çš„æ•°æ®è´¨é‡æŒ‡æ ‡ã€‚
              å¯é€‰å€¼: "missing", "zeros", "unique", "top1"ã€‚

            **2. å¯è§†åŒ– (Visualization)**
            
            * ``enable_sparkline`` (bool): æ˜¯å¦è®¡ç®—å­—ç¬¦ç”»å½¢å¼çš„è¿·ä½ åˆ†å¸ƒå›¾ (é»˜è®¤ True)ã€‚
            * ``sparkline_sample_size`` (int): è®¡ç®—åˆ†å¸ƒå›¾æ—¶çš„é‡‡æ ·è¡Œæ•°ã€‚
            * ``sparkline_bins`` (int): åˆ†å¸ƒå›¾çš„åˆ†ç®±ç²¾åº¦ã€‚

        Returns
        -------
        MarsProfileReport
            åŒ…å«æ¦‚è§ˆè¡¨å’Œè¶‹åŠ¿è¡¨çš„æŠ¥å‘Šå¯¹è±¡å®¹å™¨ã€‚

        Examples
        --------
        >>> # 1. åŸºç¡€ç”¨æ³•ï¼šç”Ÿæˆå¹¶æŸ¥çœ‹æŠ¥å‘Š
        >>> profiler = MarsDataProfiler(df)
        >>> report = profiler.generate_profile()
        
        >>> # æ‹¿åˆ° report åæ€ä¹ˆç”¨ï¼Ÿ
        >>> report # åœ¨ Jupyter ä¸­æ˜¾ç¤ºæŠ¥å‘Šçš„ç”¨æ³•
        >>> report.show_overview()  # åœ¨ Jupyter ä¸­æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        >>> report.write_excel("my_analysis.xlsx")  # å¯¼å‡º Excel

        >>> # 2. é«˜çº§ç”¨æ³•ï¼šæŒ‰æœˆåˆ†ç»„åˆ†æ
        >>> report = profiler.generate_profile(profile_by="month")
        >>> report.show_trend("mean") # æŸ¥çœ‹å‡å€¼éšæœˆä»½çš„å˜åŒ–è¶‹åŠ¿
        
        >>> # 3. è·å–åº•å±‚æ•°æ® (å¯ä»¥ç”¨äºè‡ªåŠ¨åŒ–ç‰¹å¾ç­›é€‰)
        >>> # è¿”å›å€¼ç»“æ„:
        >>> # overview: DataFrame (å…¨é‡æ¦‚è§ˆ)
        >>> # dq_tables: Dict[str, DataFrame] (DQ æŒ‡æ ‡è¶‹åŠ¿è¡¨å­—å…¸)
        >>> # stat_tables: Dict[str, DataFrame] (ç»Ÿè®¡æŒ‡æ ‡è¶‹åŠ¿è¡¨å­—å…¸)
        >>> overview, dq_tables, stat_tables = report.get_profile_data()
        
        >>> # ç¤ºä¾‹: ç­›é€‰å‡ºç¼ºå¤±ç‡ > 90% çš„ç‰¹å¾åˆ—è¡¨
        >>> high_missing_cols = overview.filter(pl.col("missing_rate") > 0.9)["feature"].to_list()
        """
        # 1. åŠ¨æ€é…ç½®åˆå¹¶
        run_config: MarsProfileConfig = self.config
        if config_overrides:
            run_config = dataclasses.replace(self.config, **config_overrides)

        # ---------------------------------------------------------------------
        # 2. æ„å»ºåˆ†æä¸Šä¸‹æ–‡ (Context Setup)
        #    å†³å®šæœ¬æ¬¡è¿è¡Œä½¿ç”¨çš„æ•°æ®é›† (working_df) å’Œ åˆ†ç»„åˆ— (group_col)
        # ---------------------------------------------------------------------
        working_df = self.df
        group_col = profile_by

        # è‡ªåŠ¨æ—¥æœŸèšåˆ
        if dt_col and profile_by in ["day", "week", "month"]:
            if dt_col not in self.df.columns:
                raise ValueError(f"dt_col '{dt_col}' not found in DataFrame.")
            
            # è°ƒç”¨ MarsDate å·¥å…·ç±»ç”Ÿæˆ Polars è¡¨è¾¾å¼
            # æ— è®º dt_col æ˜¯ "20230101"(str), 20230101(int) è¿˜æ˜¯ "2023/01/01"
            # MarsDate éƒ½èƒ½é€šè¿‡ smart_parse_expr è‡ªåŠ¨å¤„ç†å¹¶æˆªæ–­
            if profile_by == "day":
                date_expr = MarsDate.dt2day(dt_col)
            elif profile_by == "week":
                date_expr = MarsDate.dt2week(dt_col)
            elif profile_by == "month":
                date_expr = MarsDate.dt2month(dt_col)
            else:
                raise ValueError(f"Unsupported time grain: {profile_by}")

            # ç”Ÿæˆä¸´æ—¶åˆ†ç»„åˆ—å, é¿å…ä¸ç°æœ‰åˆ—å†²çª
            temp_group_col = f"_mars_auto_{profile_by}"
            
            # ç”ŸæˆåŒ…å«ä¸´æ—¶åˆ—çš„ working_df (Zero-Copy æœºåˆ¶ä¸‹å¼€é”€å¾ˆå°)
            working_df = self.df.with_columns(date_expr.alias(temp_group_col))
            group_col = temp_group_col
            
            logger.info(f"ğŸ“… Auto-aggregating '{dt_col}' by '{profile_by}' using MarsDate -> '{group_col}'")

        elif dt_col is None and profile_by is not None:
            # å¸¸è§„æ¨¡å¼ï¼šprofile_by å¿…é¡»æ˜¯ç°æœ‰åˆ—
            if profile_by not in self.df.columns:
                raise ValueError(f"Column '{profile_by}' not found. Did you forget to set `dt_col`?")
        
        # 3. è®¡ç®—å…¨é‡æ¦‚è§ˆ (Overview) 
        #    Overview æ€»æ˜¯åŸºäºåŸå§‹ self.df (æˆ– working_dfï¼Œä¸å½±å“ç»“æœ)
        overview_df: pl.DataFrame = self._calculate_overview(run_config)

        # 4. è®¡ç®—è¶‹åŠ¿è¡¨ (Trend Tables)
        #    å¿…é¡»ä¼ å…¥ context_df=working_dfï¼Œå› ä¸ºå®ƒåŒ…å«äº†å¯èƒ½æ–°ç”Ÿæˆçš„ group_col
        dq_tables: Dict[str, pl.DataFrame] = {}
        
        # 4.1 DQ Trends
        for m in run_config.dq_metrics:
            dq_tables[m] = self._generate_pivot_report(m, group_col, context_df=working_df)

        # 4.2 Stats Trends
        stat_tables: Dict[str, pl.DataFrame] = {}
        for m in run_config.stat_metrics:
            # a. Pivot
            pivot: pl.DataFrame = self._generate_pivot_report(m, group_col, context_df=working_df)
            # b. Stability (CV/Var) - ä»…åœ¨æœ‰åˆ†ç»„æ—¶è®¡ç®—
            if group_col:
                pivot = self._add_stability_metrics(pivot, exclude_cols=["feature", "dtype", "total"])
            stat_tables[m] = pivot
            
        # 4.3 PSI Trend
        if group_col and ("psi" in run_config.stat_metrics):
            try:
                # ä¼ å…¥ working_df ä»¥æ”¯æŒä¸´æ—¶æ—¶é—´åˆ—
                psi_df = self._get_psi_trend(group_col, context_df=working_df)
                if not psi_df.is_empty():
                    stat_tables["psi"] = psi_df
            except Exception as e:
                logger.warning(f"âš ï¸ PSI calculation skipped due to error: {e}")

        return MarsProfileReport(
            overview=self._format_output(overview_df),
            dq_tables=self._format_output(dq_tables),
            stats_tables=self._format_output(stat_tables)
        )
        
    # ==========================================================================
    # Overview Pipeline 
    # ==========================================================================
    def _calculate_overview(self, config: MarsProfileConfig) -> pl.DataFrame:
        """
        [Internal] è®¡ç®—å…¨é‡æ¦‚è§ˆå¤§å®½è¡¨ (Overview Table)ã€‚

        è¯¥æ–¹æ³•é‡‡ç”¨ **One-Pass (å•æ¬¡æ‰«æ)** ç­–ç•¥ï¼Œé€šè¿‡æ„å»ºå‘é‡åŒ–è¡¨è¾¾å¼ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆDQ + Statsï¼‰ï¼Œ
        å¹¶è‡ªåŠ¨æ‹¼æ¥å…ƒæ•°æ®ã€åˆ†å¸ƒå›¾ (Sparklines)ï¼Œæœ€åå¯¹åˆ—é¡ºåºå’Œæ•°æ®ç±»å‹è¿›è¡Œæ ‡å‡†åŒ–æ•´å½¢ã€‚

        Parameters
        ----------
        config : MarsProfileConfig
            é…ç½®å¯¹è±¡ã€‚æ§åˆ¶è®¡ç®—å“ªäº›ç»Ÿè®¡æŒ‡æ ‡ (stat_metrics) ä»¥åŠæ˜¯å¦ç”Ÿæˆåˆ†å¸ƒå›¾ (enable_sparkline)ã€‚

        Returns
        -------
        pl.DataFrame
            æ¦‚è§ˆå®½è¡¨ã€‚
            - Index: feature (ç‰¹å¾å)
            - Columns: dtype, distribution, missing_rate, top1_value, mean, ...
        """
        cols = self.features
        
        # 1. å‘é‡åŒ–è®¡ç®—æ‰€æœ‰åŸºç¡€æŒ‡æ ‡ (One-Pass)
        stats: pl.DataFrame = self._analyze_cols_vectorized(cols, config)
        
        # 2. æ‹¼æ¥ dtype ä¿¡æ¯
        dtype_df: pl.DataFrame = self._get_feature_dtypes()
        stats = stats.join(dtype_df, on="feature", how="left")
        
        # 3. [Feature] è®¡ç®—è¿·ä½ åˆ†å¸ƒå›¾ (Sparklines)
        if config.enable_sparkline:
            sparkline_df: pl.DataFrame = self._compute_all_sparklines(cols, config)
            if not sparkline_df.is_empty():
                stats = stats.join(sparkline_df, on="feature", how="left")
        
        # 4. æ˜¾å¼æŒ‡å®šåˆ—é¡ºåºï¼šFeature -> Dtype -> Distribution -> DQ -> Stats
        ideal_order: List[str] = [
            "feature", "dtype", 
            "distribution",  
            "missing_rate", "zeros_rate", "unique_rate", 
            "top1_ratio", "top1_value"
        ] + config.stat_metrics
        
        # å®¹é”™ï¼šåªé€‰æ‹©å®é™…å­˜åœ¨çš„åˆ—å¹¶ä¿æŒ ideal_order çš„é¡ºåº
        final_cols: List[str] = []
        seen = set()
        for c in ideal_order:
            if c in stats.columns and c not in seen:
                final_cols.append(c)
                seen.add(c)
        
        # å¦‚æœè¿˜æœ‰å…¶ä»–æœªå®šä¹‰çš„åˆ—ï¼Œæ”¾åˆ°æœ€å
        remaining_cols = [c for c in stats.columns if c not in seen]
        
        return stats.select(final_cols + remaining_cols).sort(["dtype", "feature"])
    
    def _analyze_cols_vectorized(self, cols: List[str], config: Optional[MarsProfileConfig] = None) -> pl.DataFrame:
        """
        [Internal] å…¨é‡æŒ‡æ ‡å‘é‡åŒ–è®¡ç®—å¼•æ“ (Overview æ ¸å¿ƒ)ã€‚
        
        è¯¥æ–¹æ³•é€šè¿‡â€œåˆ†æ‰¹æ¬¡å‘é‡åŒ– (Batch Vectorization)â€ç­–ç•¥ï¼Œå¹³è¡¡äº† Polars çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›
        ä¸æŸ¥è¯¢ä¼˜åŒ–å™¨ (Query Planner) çš„è§£æå¼€é”€ã€‚

        Core Logic
        ---------------------
        1. **åˆ†æ‰¹æ‰§è¡Œ (Batching)**: 
           é’ˆå¯¹é«˜ç»´æ•°æ®ï¼ˆå¦‚ 5000+ åˆ—ï¼‰ï¼Œå¦‚æœä¸€æ¬¡æ€§ç”Ÿæˆæ•°ä¸‡ä¸ªè¡¨è¾¾å¼ï¼ŒPolars çš„ Query Planner 
           ä¼šå› ä¸ºé€»è¾‘å›¾è¿‡äºå¤æ‚è€Œå¯¼è‡´è§£ææ—¶é—´å‘ˆæŒ‡æ•°çº§ä¸Šå‡ã€‚é€šè¿‡ `overview_batch_size` 
           å°†ç‰¹å¾åˆ†å—å¤„ç†ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…è¿™ç§â€œé€»è¾‘å›¾çˆ†ç‚¸â€ã€‚

        2. **One-Pass èšåˆ**:
           åœ¨æ¯ä¸ªæ‰¹æ¬¡å†…éƒ¨ï¼Œé€šè¿‡æ„å»ºå·¨å¤§çš„è¡¨è¾¾å¼åˆ—è¡¨ï¼Œå®ç°å•æ¬¡æ‰«æ (Single Scan) è®¡ç®—è¯¥æ‰¹æ¬¡
           æ‰€æœ‰åˆ—çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆMissing, Mean, Max ç­‰ï¼‰ã€‚

        3. **æ•´å½¢é‡æ„ (Reshape)**:
           - **Unpivot (å®½å˜é•¿)**: å°†èšåˆåçš„å•è¡Œç»“æœï¼ˆæå®½ï¼‰å±•å¼€ä¸ºé•¿è¡¨æ ¼å¼ã€‚
           - **Metadata Parsing**: åˆ©ç”¨å­—ç¬¦ä¸²åˆ†å‰²è§£æå‡º `feature` å’Œ `metric` å…ƒæ•°æ®ã€‚
           - **Pivot (é•¿å˜å®½)**: å°†æŒ‡æ ‡é‡æ–°é€è§†ä¸ºæ ‡å‡†çš„ç”»åƒæ ¼å¼ã€‚

        4. **ç»“æœåˆå¹¶**:
           å°†å„æ‰¹æ¬¡çš„è®¡ç®—ç»“æœé€šè¿‡ `pl.concat` è¿›è¡Œå‚ç›´åˆå¹¶ï¼Œå¹¶ç»Ÿä¸€è¿›è¡Œç±»å‹è½¬æ¢ã€‚

        Parameters
        ----------
        cols : List[str]
            å¾…è®¡ç®—æŒ‡æ ‡çš„ç‰¹å¾åç§°åˆ—è¡¨ã€‚
        config : MarsProfileConfig, optional
            é…ç½®å¯¹è±¡ã€‚æ§åˆ¶å…·ä½“çš„ç»Ÿè®¡æŒ‡æ ‡è®¡ç®—èŒƒå›´ã€‚

        Returns
        -------
        pl.DataFrame
            åŒ…å«æ‰€æœ‰ç‰¹å¾ç»Ÿè®¡æŒ‡æ ‡çš„ç”»åƒå®½è¡¨ã€‚
            ç»“æ„: [feature, metric1, metric2, ...]
        """
        if not cols: 
            return pl.DataFrame()
            
        cfg = config if config else self.config
        all_batches: List[pl.DataFrame] = []

        # è·å–æ‰¹æ¬¡å¤§å°é…ç½®
        batch_size = self.overview_batch_size

        # 1. å¼€å¯æ‰¹æ¬¡è¿­ä»£
        for i in range(0, len(cols), batch_size):
            batch_cols = cols[i : i + batch_size]
            all_exprs = []
            
            # 2. æ„å»ºå½“å‰æ‰¹æ¬¡çš„è¡¨è¾¾å¼æ± 
            for col in batch_cols:
                # è·å–è¯¥åˆ—åœ¨ Config ä¸‹çš„æ‰€æœ‰åŸºç¡€æŒ‡æ ‡è¡¨è¾¾å¼
                base_exprs = self._build_expressions(col, cfg)
                for expr in base_exprs:
                    # è·å–è¡¨è¾¾å¼çš„åŸå§‹åç§° (å¦‚ "mean", "missing_rate")
                    metric_name = expr.meta.output_name()
                    # å…³é”®æ­¥éª¤ï¼šä½¿ç”¨ ::: ä½œä¸ºåˆ†éš”ç¬¦ç¼–ç å…ƒæ•°æ®ï¼Œä¾¿äºåç»­è§£æ
                    all_exprs.append(expr.alias(f"{col}:::{metric_name}"))

            # 3. æ‰§è¡Œæ‰¹æ¬¡èšåˆ (è®¡ç®— 1 è¡Œç»“æœ)
            batch_raw = self.df.select(all_exprs)
            
            # 4. ç«‹å³æ‰§è¡Œ Reshape æ“ä½œï¼Œé‡Šæ”¾å†…å­˜å¹¶é™ç»´
            # unpivot: [1 row x (Batch * Metrics) cols] -> [(Batch * Metrics) rows x 2 cols]
            batch_long = batch_raw.unpivot(variable_name="temp_id", value_name="value")
            
            # 5. è§£æç¼–ç åœ¨ temp_id ä¸­çš„ç‰¹å¾åå’ŒæŒ‡æ ‡å
            batch_pivoted = (
                batch_long
                .with_columns(
                    # å¿«é€Ÿå­—ç¬¦ä¸²åˆ†å‰²ï¼šä» "age:::mean" æå– ["age", "mean"]
                    pl.col("temp_id").str.split_exact(":::", 1)
                    .struct.rename_fields(["feature", "metric"])
                    .alias("meta")
                )
                .unnest("meta")
                # é€è§†ï¼šå°† metric åˆ—çš„å€¼è½¬ä¸ºç»“æœè¡¨çš„åˆ—å
                .pivot(on="metric", index="feature", values="value", aggregate_function="first")
            )
            all_batches.append(batch_pivoted)

        # 6. å‚ç›´åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœé›† (Horizontal Partitioning Concatenation)
        pivoted = pl.concat(all_batches)
        
        # 7. ç±»å‹æ ‡å‡†åŒ–ï¼šå°†æŒ‡æ ‡åˆ—ç»Ÿä¸€è½¬ä¸º Float64
        # æ’é™¤ feature (String) å’Œ top1_value (Mixed String)
        cols_to_cast = [c for c in pivoted.columns if c not in ["feature", "top1_value"]]
        
        if cols_to_cast:
            pivoted = pivoted.with_columns([
                pl.col(c).cast(pl.Float64, strict=False) for c in cols_to_cast
            ])
            
        return pivoted
    
    

    def _compute_all_sparklines(self, cols: List[str], config: MarsProfileConfig) -> pl.DataFrame:
        """
        [Internal] æ‰¹é‡è®¡ç®—æ•°å€¼åˆ—çš„è¿·ä½ åˆ†å¸ƒå›¾ (Polars Native V3 + å¹¶è¡Œä¼˜åŒ–ç‰ˆ)ã€‚

        [ä¼˜åŒ–]
        1. **Single-Sample Strategy**: æ”¹ä¸ºå•æ¬¡æ•´ä½“é‡‡æ ·ï¼Œé¿å…é«˜ç»´åœºæ™¯ä¸‹æ•°åƒæ¬¡é‡å¤é‡‡æ ·äº§ç”Ÿçš„ I/O ç“¶é¢ˆã€‚
        2. **Thread-Level Parallelism**: å¼•å…¥ ThreadPoolExecutorã€‚ç”±äº Polars åº•å±‚é‡Šæ”¾ GILï¼Œ
        å¹¶è¡Œè®¡ç®— 5000+ åˆ—çš„ç›´æ–¹å›¾å¯è·å¾—è¿‘ä¹çº¿æ€§çš„åŠ é€Ÿã€‚

        ä½¿ç”¨ Polars åŸç”Ÿ API (`series.hist`) è¿›è¡Œç›´æ–¹å›¾ç»Ÿè®¡ï¼Œå¹¶æ˜ å°„ä¸º Unicode å­—ç¬¦ç”»ã€‚
        ç›¸æ¯” Numpy æ–¹æ¡ˆï¼Œå‡å°‘äº†æ•°æ®æ‹·è´ï¼Œå¹¶åœ¨å¤„ç†ç¼ºå¤±å€¼å’Œè¾¹ç¼˜æƒ…å†µæ—¶æ›´åŠ é²æ£’ã€‚

        **åˆ†å¸ƒå›¾ç¬¦å·è¯´æ˜ (Visual Representation)**:
        -----------------------------------------
        * **æ­£å¸¸åˆ†å¸ƒ**: ä½¿ç”¨ Unicode æ–¹å—å­—ç¬¦è¡¨ç¤ºé¢‘ç‡é«˜ä½ (å¦‚ ``_ â–‚â–…â–‡â–ˆ``)ã€‚
        - **0 å€¼**: å¼ºåˆ¶ä½¿ç”¨ä¸‹åˆ’çº¿ ``_`` ä½œä¸ºåŸºå‡†çº¿ï¼Œç¡®ä¿è§†è§‰å ä½ã€‚
        - **é 0 å€¼**: ä½¿ç”¨ 2/8 åˆ° 8/8 é«˜åº¦çš„æ–¹å— (``â–‚`` åˆ° ``â–ˆ``)ï¼Œè·³è¿‡ 1/8 å—ä»¥å¢å¼ºå¯è§†æ€§ã€‚
        
        * **æ— æœ‰æ•ˆæ•°æ®**: æ˜¾ç¤ºå…¨ä¸‹åˆ’çº¿ ``________``ã€‚
        - åœºæ™¯: åŸå§‹åˆ—å…¨ä¸º Null/NaNï¼Œæˆ–è€…æ‰€æœ‰å€¼éƒ½åœ¨ `custom_missing` åˆ—è¡¨ä¸­ã€‚
        
        * **é€»è¾‘æ— åˆ†å¸ƒ**: æ˜¾ç¤ºå…¨ä¸‹åˆ’çº¿ ``________`` (å¹¶è®°å½• Debug æ—¥å¿—)ã€‚
        - åœºæ™¯: æ•°æ®å­˜åœ¨ (len>0) ä½†æ— æ³•æ„å»ºç›´æ–¹å›¾ (å¦‚å…¨ä¸ºæ— ç©·å¤§ Inf)ã€‚
        
        * **å•ä¸€å€¼ (Constant)**: æ˜¾ç¤ºå±…ä¸­æ–¹å— ``____â–ˆ____``ã€‚
        - åœºæ™¯: æ–¹å·®ä¸º 0ï¼Œæ‰€æœ‰æœ‰æ•ˆå€¼éƒ½ç›¸ç­‰ã€‚
        
        * **è®¡ç®—å¼‚å¸¸**: æ˜¾ç¤º ``ERR``ã€‚

        Parameters
        ----------
        cols : List[str]
            å¾…è®¡ç®—çš„åˆ—ååˆ—è¡¨ã€‚æ–¹æ³•å†…éƒ¨ä¼šè‡ªåŠ¨ç­›é€‰å‡ºæ•°å€¼å‹åˆ—ã€‚
        config : MarsProfileConfig
            åŒ…å« `sparkline_sample_size` (é‡‡æ ·æ•°) å’Œ `sparkline_bins` (å­—ç¬¦ç”»é•¿åº¦/åˆ†ç®±æ•°) é…ç½®ã€‚

        Returns
        -------
        pl.DataFrame
            åŒ…å« [feature, distribution] çš„ä¸¤åˆ— DataFrameã€‚
        """
        # 1. ç­›é€‰æ•°å€¼åˆ— (éæ•°å€¼åˆ—æ— æ³•ç”»åˆ†å¸ƒå›¾)
        num_cols: List[str] = [c for c in cols if self._is_numeric(c)]
        if not num_cols:
            return pl.DataFrame(
                {"feature": [], "distribution": []}, 
                schema={"feature": pl.String, "distribution": pl.String}
            )

        # 2. é‡‡æ · (Sampling) - [æ€§èƒ½ä¼˜åŒ–: å•æ¬¡æ•´ä½“é‡‡æ ·]
        limit_n = config.sparkline_sample_size
        
        # ç­–ç•¥ï¼šå…ˆ Select ç›®æ ‡åˆ— -> æ‰§è¡Œå•æ¬¡å…¨åˆ—é‡‡æ ·
        # è¿™æ ·é¿å…äº†åœ¨å¾ªç¯ä¸­åå¤è°ƒç”¨ sample() å¸¦æ¥çš„å†…å­˜æ´—ç‰Œå¼€é”€
        df_subset = self.df.select(num_cols)
        if df_subset.height > limit_n:
            sample_df = df_subset.sample(n=limit_n, with_replacement=False)
        else:
            sample_df = df_subset

        # é¢„åŠ è½½å‚æ•°
        bars = ['_', '\u2582', '\u2583', '\u2584', '\u2585', '\u2586', '\u2587', '\u2588']
        n_bins: int = config.sparkline_bins

        # 3. å®šä¹‰å•åˆ—å¤„ç†å‡½æ•° (ç”¨äºå¹¶è¡Œæ˜ å°„)
        def _process_column(col: str) -> Dict[str, str]:
            dist_str: str = "-" 
            try:
                # è·å–æ¸…æ´—é€»è¾‘ï¼ˆæ’é™¤ -999 ç­‰è‡ªå®šä¹‰ç¼ºå¤±å€¼ï¼‰
                exclude_vals = self._get_values_to_exclude(col)
                target_s: pl.Series = sample_df[col]
                
                # --- A. æ•°æ®æ¸…æ´— ---
                if target_s.dtype in [pl.Float32, pl.Float64]:
                    target_s = target_s.filter(target_s.is_not_nan())
                
                if exclude_vals:
                    target_s = target_s.filter(~target_s.is_in(exclude_vals))
                
                s: pl.Series = target_s.drop_nulls()
                
                # --- B. è¾¹ç•Œæ£€æŸ¥ ---
                if s.len() == 0:
                    dist_str = "_" * n_bins 
                elif s.len() == 1 or s.min() == s.max():
                    dist_str = "____â–ˆ____" 
                else:
                    # --- C. æ ¸å¿ƒè®¡ç®— (Polars Hist) ---
                    hist_df: pl.DataFrame = s.hist(bin_count=n_bins)
                    counts: List[int] = hist_df.get_column(hist_df.columns[-1]).to_list()
                    
                    # --- D. å­—ç¬¦æ˜ å°„ç®—æ³• ---
                    max_count = max(counts)
                    if max_count == 0:
                        dist_str = "_" * n_bins
                    else:
                        chars = []
                        for c in counts:
                            if c == 0:
                                chars.append(bars[0])
                            else:
                                idx = int(c / max_count * (len(bars) - 2)) + 1
                                idx = min(idx, len(bars) - 1)
                                chars.append(bars[idx])
                        dist_str = "".join(chars)
                        
            except Exception as e:
                logger.error(f"Sparkline calculation failed for feature '{col}': {str(e)}")
                dist_str = "ERR"
                
            return {"feature": col, "distribution": dist_str}

        # 4. [æ€§èƒ½ä¼˜åŒ–: å¤šæ ¸å¹¶å‘æ‰§è¡Œ]
        # pl.thread_pool_size() ä¼šè‡ªåŠ¨è¯†åˆ«å½“å‰ç³»ç»Ÿçš„é€»è¾‘æ ¸å¿ƒæ•°ã€‚
        with ThreadPoolExecutor(max_workers=pl.thread_pool_size()-1) as executor:
            results = list(executor.map(_process_column, num_cols))

        return pl.DataFrame(
            results, 
            schema={"feature": pl.String, "distribution": pl.String}
        )
        
    # ==========================================================================
    # Trends & Pivot Pipeline 
    # ==========================================================================
    def _generate_pivot_report(
        self, metric: str, 
        group_col: Optional[str], 
        context_df: Optional[pl.DataFrame] = None
    ) -> pl.DataFrame:
        """
        [Internal] ç”ŸæˆæŒ‡å®šæŒ‡æ ‡çš„åˆ†ç»„è¶‹åŠ¿é€è§†è¡¨ (Pivot Table)ã€‚

        è¯¥æ–¹æ³•è´Ÿè´£è®¡ç®—å•ä¸€æŒ‡æ ‡ï¼ˆå¦‚ 'mean'ï¼‰åœ¨ä¸åŒæ—¶é—´åˆ‡ç‰‡æˆ–å®¢ç¾¤ä¸‹çš„å˜åŒ–è¶‹åŠ¿ï¼Œ
        å¹¶å°†ç»“æœæ•´å½¢ä¸º "ç‰¹å¾ x åˆ†ç»„" çš„çŸ©é˜µæ ¼å¼ã€‚

        Core Logic
        ---------------------
        ç”±äº Polars æ˜¯åˆ—å¼å­˜å‚¨ (Column-oriented)ï¼Œç»Ÿè®¡è®¡ç®—é€šå¸¸äº§å‡º "1è¡Œ x Nç‰¹å¾" çš„ç»“æœã€‚
        ä¸ºäº†ç”Ÿæˆå¯è¯»çš„æŠ¥å‘Šï¼Œéœ€è¦è¿›è¡Œ **è½¬ç½® (Transpose)** æ“ä½œï¼š
        1. **Total è®¡ç®—**: å¯¹å…¨é‡æ•°æ®èšåˆ -> è½¬ç½® -> å¾—åˆ° [feature, total] åˆ—ã€‚
        2. **Group è®¡ç®—**: æŒ‰ `group_col` èšåˆ -> è½¬ç½® -> å¾—åˆ° [feature, group_val_1, group_val_2...] åˆ—ã€‚
        3. **åˆå¹¶**: å°† Total åˆ—ä¸ Group åˆ—é€šè¿‡ feature JOINï¼Œå½¢æˆæœ€ç»ˆå®½è¡¨ã€‚

        Parameters
        ----------
        metric : str
            å¾…è®¡ç®—çš„æŒ‡æ ‡åç§° (ä¾‹å¦‚ 'missing', 'mean', 'max')ã€‚
            å¿…é¡»èƒ½å¤Ÿè¢« `_get_single_metric_expr` è§£æã€‚
        group_col : str, optional
            åˆ†ç»„åˆ—å (ä¾‹å¦‚ 'month', 'vintage')ã€‚
            - å¦‚æœä¸º Noneï¼Œåˆ™åªè®¡ç®—å¹¶è¿”å› Total åˆ—ã€‚
            - å¦‚æœå­˜åœ¨ï¼Œç»“æœè¡¨å°†åŒ…å«è¯¥åˆ†ç»„ä¸‹åˆ—çš„æ‰€æœ‰å–å€¼ä½œä¸ºåˆ—åã€‚
        context_df : pl.DataFrame, optional
            ä¸Šä¸‹æ–‡æ•°æ®é›†ã€‚
            é€šå¸¸ä¼ å…¥åŒ…å«ä¸´æ—¶ç”Ÿæˆçš„è‡ªåŠ¨èšåˆæ—¶é—´åˆ—ï¼ˆå¦‚ '_mars_auto_month'ï¼‰çš„ DataFrameã€‚
            å¦‚æœä¸º Noneï¼Œåˆ™é»˜è®¤ä½¿ç”¨ `self.df`ã€‚

        Returns
        -------
        pl.DataFrame
            é€è§†åçš„è¶‹åŠ¿å®½è¡¨ã€‚
            - Index: feature (ç‰¹å¾å)
            - Columns: feature, dtype, [group_val_1, ...], total
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ä¸Šä¸‹æ–‡ DFï¼Œå¦åˆ™å›é€€åˆ° self.df
        target_df = context_df if context_df is not None else self.df
        
        target_cols = [c for c in self.features if c != group_col]
        if not target_cols: 
            return pl.DataFrame()

        # 1. è®¡ç®— Total åˆ— (å…¨å±€èšåˆ)
        total_exprs = [self._get_single_metric_expr(c, metric).alias(c) for c in target_cols]
        total_row = target_df.select(total_exprs)
        # Transpose: [1, n_feats] -> [n_feats, 1]
        total_df = total_row.transpose(include_header=True, header_name="feature", column_names=["total"])

        # 2. å‡†å¤‡åŸºç¡€è¡¨ (feature + dtype + total)
        dtype_df = self._get_feature_dtypes()
        base_df = total_df.join(dtype_df, on="feature", how="left")
        
        # Case A: æ²¡æœ‰åˆ†ç»„ -> ç›´æ¥è¿”å› Total è¡¨
        if group_col is None:
            return base_df.select(["feature", "dtype", "total"]).sort(["dtype", "feature"])

        # Case B: æœ‰åˆ†ç»„ -> è®¡ç®— Pivot å¹¶ Join
        agg_exprs = [self._get_single_metric_expr(c, metric).alias(c) for c in target_cols]
        # GroupBy -> Agg -> Sort
        #   ç»“æœå½¢çŠ¶: Mä¸ªåˆ†ç»„ x Nä¸ªç‰¹å¾
        grouped =target_df.group_by(group_col).agg(agg_exprs).sort(group_col)
        grouped = grouped.with_columns(pl.col(group_col).cast(pl.String))
        
        # å†æ¬¡è½¬ç½®
        #   è¾“å…¥: 
        #   month  | age | income
        #   202301 | 25  | 10000
        #   202302 | 26  | 12000
        #
        #   è¾“å‡º (Transposeå):
        #   feature | 202301 | 202302
        #   age     | 25     | 26
        #   income  | 10000  | 12000
        pivot_df = grouped.transpose(include_header=True, header_name="feature", column_names=group_col)

        # 3. Join Together
        result = base_df.join(pivot_df, on="feature", how="left")
        
        # 4. è°ƒæ•´åˆ—é¡ºåº: feature, dtype, ...groups..., total
        fixed = {"feature", "dtype", "total"}
        groups = [c for c in result.columns if c not in fixed]
        final_order = ["feature", "dtype"] + groups + ["total"]
        
        return result.select(final_order).sort(["dtype", "feature"])
    
    def _add_stability_metrics(self, df: pl.DataFrame, exclude_cols: List[str]) -> pl.DataFrame:
        """
        [Internal] è®¡ç®—è¡Œçº§ç¨³å®šæ€§æŒ‡æ ‡ï¼šæ–¹å·® (Var) å’Œ å˜å¼‚ç³»æ•° (CV)ã€‚
        
        åˆ©ç”¨ Polars çš„ list ç®—å­è¿›è¡Œæ°´å¹³èšåˆ (Horizontal Aggregation)ã€‚
        
        æˆ‘ä»¬æƒ³è®¡ç®—æ¯ä¸€è¡Œ (æ¯ä¸ªç‰¹å¾) åœ¨ä¸åŒ group é—´çš„æ³¢åŠ¨ã€‚
        Polars ä¸»è¦æ˜¯åˆ—å¼è®¡ç®—ï¼Œè¡Œè®¡ç®—æ¯”è¾ƒéº»çƒ¦ã€‚
        è¿™é‡Œç”¨äº†ä¸€ä¸ªæŠ€å·§ï¼š`concat_list`ã€‚
        æŠŠæ‰€æœ‰æœˆä»½åˆ—çš„å€¼ï¼Œåˆå¹¶æˆä¸€åˆ— list: [25, 26, ...]
        ç„¶åç›´æ¥å¯¹è¿™ä¸ª list åˆ—ç®— std å’Œ meanã€‚
        
        Parameters
        ----------
        df : pl.DataFrame
            åŒ…å«åˆ†ç»„æ•°æ®çš„é€è§†è¡¨ã€‚
        exclude_cols : List[str]
            éœ€è¦æ’é™¤çš„éæ•°æ®åˆ— (å¦‚ feature, dtype)ã€‚

        Returns
        -------
        pl.DataFrame
            å¢åŠ äº† group_var å’Œ group_cv åˆ—çš„ DataFrameã€‚
        """
        if df.is_empty(): return df
        
        # é”å®šçº¯åˆ†ç»„åˆ— (æ’é™¤ feature, dtype, total)
        calc_cols = [
            c for c in df.columns 
            if c not in exclude_cols and df[c].dtype in [pl.Float64, pl.Float32]
        ]
        if not calc_cols: return df

        epsilon = 1e-9 # é˜²æ­¢é™¤ä»¥0
        
        return (
            df
            .with_columns(pl.concat_list(calc_cols).alias("_tmp")) # å°†åˆ†ç»„åˆ—å‹ç¼©ä¸º List
            .with_columns([
                pl.col("_tmp").list.mean().fill_null(0).alias("group_mean"),
                pl.col("_tmp").list.var().fill_null(0).alias("group_var"),
                (pl.col("_tmp").list.std() / (pl.col("_tmp").list.mean().abs() + epsilon)).fill_null(0).alias("group_cv")
            ])
            .drop("_tmp")
            # è°ƒæ•´åˆ—é¡ºåº: feature, dtype, groups..., total, var, cv
            .select(["feature", "dtype"] + calc_cols + ["total", "group_mean", "group_var", "group_cv"])
        )
        
    # ==========================================================================
    # PSI Pipeline
    # ==========================================================================
    @time_it
    def _get_psi_trend(
        self, 
        group_col: str, 
        features: Optional[List[str]] = None,
        context_df: Optional[pl.DataFrame] = None
    ) -> pl.DataFrame:
        """
        [Internal] è®¡ç®—ç‰¹å¾åœ¨åˆ†ç»„ç»´åº¦ä¸Šçš„ PSI (ç¾¤ä½“ç¨³å®šæ€§æŒ‡æ ‡) è¶‹åŠ¿åŠç¨³å®šæ€§ç»Ÿè®¡ã€‚

        è¯¥æ–¹æ³•åˆ©ç”¨ Polars çš„ Streaming å¼•æ“å’Œ Lazy APIï¼Œé«˜æ•ˆåœ°è®¡ç®—æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾
        éšæ—¶é—´æˆ–å®¢ç¾¤çš„å˜åŒ–è¶‹åŠ¿ã€‚

        Core Logic
        ---------------------
        1. **åŸºå‡†é€‰æ‹© (Baseline)**: 
           è‡ªåŠ¨é€‰å– `group_col` ä¸­å€¼æœ€å°çš„åˆ†ç»„ï¼ˆä¾‹å¦‚æœ€æ—©çš„æœˆä»½ï¼‰ä½œä¸ºåŸºå‡†åˆ†å¸ƒ (Expected)ï¼Œ
           å…¶ä»–æ‰€æœ‰åˆ†ç»„ä½œä¸ºå®é™…åˆ†å¸ƒ (Actual) è¿›è¡Œå¯¹æ¯”è®¡ç®—ã€‚

        2. **åˆ†ç®±ç­–ç•¥ (Binning)**:
           - æ•°å€¼ç‰¹å¾: ä½¿ç”¨ `MarsNativeBinner` è¿›è¡Œåˆ†ç®± (é»˜è®¤ Quantile)ã€‚
           - ç±»åˆ«ç‰¹å¾: ç›´æ¥æŒ‰ç±»åˆ«å€¼è¿›è¡Œåˆ†å¸ƒç»Ÿè®¡ã€‚

        3. **ç¨³å®šæ€§æŒ‡æ ‡ä¸é—¨æ§æœºåˆ¶ (Stability & Gating)**:
           è®¡ç®— PSI åºåˆ—çš„å‡å€¼ (Mean) å’Œå˜å¼‚ç³»æ•° (CV)ã€‚
           **æ³¨æ„**: ä¸ºäº†è§£å†³ "å°åŸºæ•°é™·é˜±" (å³ PSI æ•°å€¼æå°æ—¶ï¼Œå¾®å°çš„æŠ–åŠ¨å¯¼è‡´ CV è™šé«˜)ï¼Œ
           å¼•å…¥äº† `psi_cv_ignore_threshold` (åœ¨ __init__ ä¸­å®šä¹‰):
           - **é€»è¾‘**: å¦‚æœæŸç‰¹å¾åœ¨**æ‰€æœ‰åˆ†ç»„**ä¸‹çš„ PSI æœ€å¤§å€¼ (`group_max`) éƒ½å°äºè¯¥é˜ˆå€¼ï¼Œ
             åˆ™è®¤ä¸ºè¯¥ç‰¹å¾å¤„äº"ç»å¯¹ç¨³å®šåŒº/å™ªå£°åŒº"ï¼Œå¼ºåˆ¶å°†å…¶ `group_cv` ç½®ä¸º 0ã€‚
           - åªæœ‰å½“å†å²æ•°æ®ä¸­è‡³å°‘æœ‰ä¸€æ¬¡ PSI çªç ´é˜ˆå€¼æ—¶ï¼Œæ‰è®¡ç®—å¹¶å±•ç¤ºçœŸå®çš„ CVã€‚

        Parameters
        ----------
        group_col : str
            åˆ†ç»„åˆ—åã€‚é€šå¸¸æ˜¯æ—¶é—´åˆ— (å¦‚ 'month') æˆ– Vintage åˆ—ã€‚
        features : List[str], optional
            æŒ‡å®šéœ€è¦è®¡ç®— PSI çš„ç‰¹å¾å­é›†ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è®¡ç®— `self.features` ä¸­çš„æ‰€æœ‰åˆ—ã€‚
        context_df : pl.DataFrame, optional
            ä¸Šä¸‹æ–‡æ•°æ®é›†ã€‚é€šå¸¸ä¼ å…¥åŒ…å«ä¸´æ—¶ç”Ÿæˆçš„è‡ªåŠ¨èšåˆæ—¶é—´åˆ—çš„ DataFrameã€‚
            å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨å®ä¾‹å†…éƒ¨çš„ `self.df`ã€‚

        Returns
        -------
        pl.DataFrame
            PSI è¶‹åŠ¿å®½è¡¨ (Pivot Table)ã€‚
            ç»“æ„: [feature, dtype, group_val_1, group_val_2, ..., total, group_mean, group_var, group_cv]
            
        Notes
        -----
        - æ¶æ„ä¼˜åŒ–: é‡‡ç”¨ "Lazy Batching" ç­–ç•¥ã€‚åˆ†æ‰¹å°†ç‰¹å¾æ¨å…¥ Lazy ç®¡é“ï¼Œ
          å¹¶åœ¨ `_calc_psi_from_stats` ä¸­å®Œæˆæœ€ç»ˆåˆå¹¶ï¼Œå‡å°‘äº†é¢‘ç¹ collect å¸¦æ¥çš„å†…å­˜ç¢ç‰‡ã€‚
          
        Benchmarks:
        ---------
        - æ€§èƒ½: åœ¨ 5000 åˆ— x 20 ä¸‡è¡Œè§„æ¨¡ä¸‹ (i7-14700, 64G RAM), é‡‡ç”¨å…¨ Lazy æµæ°´çº¿å
        å®ç°çº¦ 20% çš„åŠ é€Ÿ (150s -> 120s)ã€‚
        - ç“¶é¢ˆ: è®¡ç®—ç“¶é¢ˆå·²ä»å†…å­˜å®ä¾‹åŒ–è½¬å‘çº¯é€»è¾‘è¿ç®—, æœ‰æ•ˆåˆ©ç”¨äº†å¤šæ ¸å¹¶è¡Œèƒ½åŠ›ã€‚
        """
        target_df: pl.DataFrame = context_df if context_df is not None else self.df
        
        # ==============================================================================
        # ğŸ›¡ï¸ [æ–°å¢] å†…å­˜ä¿æŠ¤ç†”æ–­æœºåˆ¶ (Sanity Check)
        # ==============================================================================
        # PSI çŸ©é˜µæ˜¯é€šè¿‡ Cross Join ç”Ÿæˆéª¨æ¶çš„ã€‚
        # å¦‚æœ group_col è¯¯ä¼ äº†é«˜åŸºæ•°ä¸»é”®(å¦‚ user_id)ï¼Œä¼šå¯¼è‡´ (N_feat * N_bins * N_users) çš„å†…å­˜çˆ†ç‚¸ã€‚
        # è®¾å®šä¸€ä¸ªå®‰å…¨é˜ˆå€¼ï¼Œä¾‹å¦‚ 1000 (è¶³ä»¥è¦†ç›–å‡ åå¹´çš„æœˆä»½æˆ–å¸¸ç”¨çš„ Segment)
        MAX_PSI_GROUPS = 1000  
        
        # å¿«é€Ÿæ£€æŸ¥åˆ†ç»„æ•°é‡ (ä½¿ç”¨ approx_n_unique æé€Ÿä¼°ç®—ï¼Œæˆ–è€…ç›´æ¥ count unique)
        n_groups = target_df.select(pl.col(group_col).n_unique()).item()
        
        if n_groups > MAX_PSI_GROUPS:
            logger.error(f"âŒ PSI Calculation aborted: Column '{group_col}' has {n_groups} unique values.")
            logger.error(f"   Threshold is {MAX_PSI_GROUPS}. Did you accidentally group by an ID column?")
            # è¿”å›ç©ºè¡¨ï¼Œé¿å…ç¨‹åº Crashï¼Œè®©æŠ¥å‘Šå…¶ä»–éƒ¨åˆ†èƒ½æ­£å¸¸ç”Ÿæˆ
            return pl.DataFrame()
        # ==============================================================================
        
        # 1. ç¡®å®šè®¡ç®—èŒƒå›´
        candidates = features if features else self.features
        candidates = [c for c in candidates if c != group_col]
        
        if not candidates:
            return pl.DataFrame()

        num_cols = [c for c in candidates if self._is_numeric(c)]
        cat_cols = [c for c in candidates if c not in num_cols]

        try:
            baseline_group = target_df.select(pl.col(group_col).min()).item()
        except Exception:
            return pl.DataFrame()

        psi_result_parts = []
        common_schema_order = [group_col, "feature", "total", "psi"]
        
        BATCH_SIZE = self.psi_batch_size 
        # ==============================================================================
        # ğŸŸ¢ è·¯ä¸€ï¼šæ•°å€¼ç‰¹å¾ PSI 
        # ==============================================================================
        if num_cols:
            try:
                numeric_missing = [v for v in self.custom_missing if isinstance(v, (int, float)) and not isinstance(v, bool)]
                numeric_special = [v for v in self.special_values if isinstance(v, (int, float)) and not isinstance(v, bool)]
                
                # 1. Fit Global
                binner = MarsNativeBinner(
                    features=num_cols,
                    method=self.psi_bin_method, 
                    n_bins=self.psi_n_bins,          
                    special_values=numeric_special,
                    missing_values=numeric_missing,
                    remove_empty_bins=False     
                )
                binner.fit(target_df)
                
                # é¢„æ„å»ºéª¨æ¶æ‰€éœ€çš„ Bin IDs
                possible_bins = list(range(self.psi_n_bins)) + [-1]
                if numeric_special:
                    possible_bins.extend([-3 - i for i in range(len(numeric_special))])
                b_ids = pl.DataFrame({"bin_id": possible_bins}, schema={"bin_id": pl.Int16})

                # 2. åˆ†æ‰¹å¤„ç† Loop
                for i in range(0, len(num_cols), BATCH_SIZE):
                    batch_cols = num_cols[i : i + BATCH_SIZE]
                    
                    # --- Local Scope Start ---
                    
                    # A. Transform (Input: Eager, Output: Lazy)
                    cols_needed = batch_cols + [group_col]
                    
                    # è¿™é‡Œç›´æ¥ä¼ å…¥ Eager DataFrame Slice
                    # Polars çš„ select æ˜¯é›¶æ‹·è´çš„ï¼Œä¸ä¼šå¤åˆ¶æ•°æ®ï¼Œæ‰€ä»¥è¿™é‡Œå¾ˆå¿«ä¸”å†…å­˜å®‰å…¨
                    df_batch_input = target_df.select(cols_needed)
                    
                    # å¼€å¯ lazy=Trueï¼Œè®© transform å†…éƒ¨è½¬ä¸º lazy æ¨¡å¼æ‰§è¡Œé€»è¾‘ï¼Œé¿å…ç”Ÿæˆå·¨å¤§çš„ä¸­é—´ç»“æœçŸ©é˜µ
                    lf_binned: pl.LazyFrame = binner.transform(df_batch_input, return_type='index', lazy=True)
                    
                    # B. æ„å»ºå½“å‰æ‰¹æ¬¡çš„ Rename Map
                    feat_map_batch = {idx: name for idx, name in enumerate(batch_cols)}
                    bin_cols_batch = [f"{c}_bin" for c in batch_cols]
                    rename_map = {old: str(idx) for idx, old in enumerate(bin_cols_batch)}
                    
                    # C. [ä¿®æ”¹] å»æ‰ .collect()ï¼Œä¿æŒä¸º LazyFrame
                    lf_agg_stats_batch = (
                        lf_binned
                        .rename(rename_map)
                        .select([group_col] + list(rename_map.values()))
                        .unpivot(
                            index=[group_col],
                            on=list(rename_map.values()),
                            variable_name="feat_idx", 
                            value_name="bin_id"
                        )
                        .with_columns([
                            pl.col("feat_idx").cast(pl.Int16),
                            pl.col("bin_id").cast(pl.Int16) 
                        ])
                        .group_by([group_col, "feat_idx", "bin_id"])
                        .len()  # æ­¤æ—¶è¿”å›çš„æ˜¯ LazyFrame
                    )

                    # D. [ä¿®æ”¹] éª¨æ¶æ„å»ºä¹Ÿæ”¹ä¸º Lazy (å»æ‰ Eager çš„ DataFrame åˆ›å»º)
                    lf_f_ids = pl.LazyFrame({"feat_idx": list(feat_map_batch.keys())}, schema={"feat_idx": pl.Int16})
                    lf_b_ids = b_ids.lazy() # å‡è®¾ä¹‹å‰çš„ b_ids æ˜¯ eager çš„ï¼Œè½¬ä¸º lazy
                    lf_unique_bins_skel = lf_f_ids.join(lf_b_ids, how="cross")

                    # E. [è°ƒç”¨] ä¼ å…¥ LazyFrameï¼Œå¾—åˆ° LazyFrame ç»“æœ
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†ä¼ å…¥ skeletonï¼Œè®© _calc_psi_from_stats å†…éƒ¨è‡ªå·±ç”Ÿæˆå®Œæ•´çš„ [åˆ†ç»„xç‰¹å¾xåˆ†ç®±] éª¨æ¶
                    lf_psi_num_raw = self._calc_psi_from_stats(
                        stats_df=lf_agg_stats_batch, 
                        unique_bins_skel=lf_unique_bins_skel, 
                        group_col=group_col, 
                        baseline_group=baseline_group
                    )

                    # F. [è¿˜åŸ] ä¿æŒ Lazy é“¾æ¡
                    mapping_df = pl.LazyFrame({
                        "feat_idx": list(feat_map_batch.keys()),
                        "feature": list(feat_map_batch.values())
                    }, schema={"feat_idx": pl.Int16, "feature": pl.String})

                    psi_num_final = (
                        lf_psi_num_raw
                        .join(mapping_df, on="feat_idx", how="left")
                        .select(common_schema_order)
                        .collect(streaming=True) # åªæœ‰åœ¨æœ€ååˆå¹¶å‰æ‰ collect
                    )
                    
                    psi_result_parts.append(psi_num_final)
                    # --- Local Scope End ---

            except Exception as e:
                logger.error(f"Numeric PSI failed: {e}")

        # ==============================================================================
        # ğŸŸ¡ è·¯äºŒï¼šç±»åˆ«ç‰¹å¾ PSI 
        # ==============================================================================
        if cat_cols:
            try:
                # 1. æ„å»ºèšåˆç»Ÿè®¡ LazyFrame (ä¸ collect)
                lf_long_cat: pl.LazyFrame = (
                    target_df.lazy()
                    .select(cat_cols + [group_col])
                    .unpivot(
                        index=[group_col],
                        on=cat_cols,
                        variable_name="feature",
                        value_name="bin_id_raw"
                    )
                    .with_columns(
                        pl.col("bin_id_raw").fill_null("Missing").cast(pl.Utf8).alias("bin_id")
                    )
                    .group_by([group_col, "feature", "bin_id"])
                    .len()
                )

                # 2. ç”Ÿæˆ [ç‰¹å¾ x ç±»åˆ«] çš„å”¯ä¸€ç»„åˆéª¨æ¶ (Lazy)
                lf_unique_bins_cat = lf_long_cat.select(["feature", "bin_id"]).unique()

                # 3. è°ƒç”¨é‡æ„åçš„è®¡ç®—å‡½æ•° (ä¼ å…¥ LazyFrame)
                # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬ä¼˜åŒ–äº† _calc_psi_from_statsï¼Œä¸å†éœ€è¦æ‰‹åŠ¨ä¼ å…¥ skeleton_cat
                lf_psi_cat_raw = self._calc_psi_from_stats(
                    stats_df=lf_long_cat, 
                    unique_bins_skel=lf_unique_bins_cat, 
                    group_col=group_col, 
                    baseline_group=baseline_group
                )

                # 4. æ‰§è¡Œå¹¶å­˜å…¥ç»“æœé›†
                # åœ¨è¿™é‡Œè¿›è¡Œ collect æ˜¯ä¸ºäº†å°†ç»“æœå­˜å…¥ listï¼Œæ–¹ä¾¿æœ€åçš„ pl.concat
                psi_cat_final = (
                    lf_psi_cat_raw
                    .select(common_schema_order)
                    .collect(streaming=True)
                )
                psi_result_parts.append(psi_cat_final)

            except Exception as e:
                logger.error(f"Categorical PSI failed: {e}")

        # ==============================================================================
        # ğŸ åˆå¹¶ä¸æ•´å½¢
        # ==============================================================================
        if not psi_result_parts:
            return pl.DataFrame()

        final_long_psi: pl.DataFrame = pl.concat(psi_result_parts)

        # Pivot
        pivot_df = (
            final_long_psi
            .pivot(on=group_col, index=["feature", "total"], values="psi")
        )

        dtype_df = self._get_feature_dtypes()
        result = pivot_df.join(dtype_df, on="feature", how="left")
        
        raw_group_cols = [c for c in result.columns if c not in ["feature", "dtype", "total"]]
        psi_data_cols = sorted(raw_group_cols)
        
        if psi_data_cols:
            epsilon_stat = 1e-9
            result = (
                result
                .with_columns(pl.concat_list(psi_data_cols).alias("_tmp_psi_list"))
                .with_columns([
                    pl.col("_tmp_psi_list").list.mean().alias("group_mean"),
                    pl.col("_tmp_psi_list").list.max().fill_null(0).alias("group_max"),
                    
                    pl.col("_tmp_psi_list").list.var().fill_null(0).alias("group_var"),
                    pl.col("_tmp_psi_list").list.std().alias("_tmp_std") 
                ])
                .with_columns([
                    # åªæœ‰å½“å†å²ä¸Šå‡ºç°è¿‡çš„æœ€å¤§ PSI éƒ½å°äºé˜ˆå€¼æ—¶ï¼Œæ‰å¿½ç•¥æ³¢åŠ¨(CV=0)
                    # åªè¦æœ‰ä¸€æ¬¡ PSI è¶…è¿‡é˜ˆå€¼ï¼Œå°±è€è€å®å®è®¡ç®— CV
                    pl.when(pl.col("group_max") < self.psi_cv_ignore_threshold)
                    .then(pl.lit(0.0))
                    .otherwise(
                        (pl.col("_tmp_std") / (pl.col("group_mean") + epsilon_stat))
                    )
                    .fill_null(0)
                    .alias("group_cv")
                ])
                .drop(["_tmp_psi_list", "_tmp_std", "group_max"]) 
            )
            
            final_order = ["feature", "dtype"] + psi_data_cols + ["total", "group_mean", "group_var", "group_cv"]
            return result.select(final_order).sort("feature")
        else:
            return result.sort("feature")

    def _calc_psi_from_stats(
        self, 
        stats_df: pl.LazyFrame,  
        unique_bins_skel: pl.LazyFrame, 
        group_col: str, 
        baseline_group: Any
    ) -> pl.LazyFrame: # è¿”å› LazyFrame
        """
        [Math Core] åŸºäºèšåˆåçš„é¢‘æ¬¡è¡¨ (Count Table) è®¡ç®— PSIã€‚

        æ­¤æ–¹æ³•ä¸æ¥è§¦åŸå§‹æ˜ç»†æ•°æ®ï¼Œç›´æ¥åœ¨èšåˆåçš„ç»Ÿè®¡è¡¨ä¸Šè¿›è¡Œå‘é‡åŒ–è¿ç®—ï¼Œ
        æ˜¯ PSI è®¡ç®—é«˜æ€§èƒ½çš„æ ¸å¿ƒæ‰€åœ¨ã€‚

        Formula
        --------
        1. **Expected (E)**: åŸºå‡†ç»„ (Baseline) ä¸­å„ç®±çš„å æ¯”ã€‚
        2. **Actual (A)**: å½“å‰ç»„ (Group) ä¸­å„ç®±çš„å æ¯”ã€‚
        3. **PSI Contribution**: (A - E) * ln(A / E)
        4. **Sum**: å¯¹æ‰€æœ‰ç®±æ±‚å’Œå¾—åˆ°æœ€ç»ˆ PSIã€‚

        Parameters
        ----------
        stats_df : pl.LazyFrame
            èšåˆåçš„é¢‘æ¬¡ç»Ÿè®¡è¡¨ã€‚
            ç»“æ„å¿…é¡»åŒ…å«: ``[group_col, feature (or feat_idx), bin_id, len]``ã€‚
        skeleton : pl.LazyFrame
            (Group x Feature x Bin) çš„å…¨æ’åˆ—éª¨æ¶è¡¨ã€‚
            ç”¨äº Left Join ä»¥ç¡®ä¿è®¡æ•°ä¸º 0 çš„ç©ºç®±ä¸ä¼šä¸¢å¤±ï¼ˆä¼šè¢«å¡«å…… epsilonï¼‰ã€‚
        unique_bins_skel : pl.LazyFrame
            (Feature x Bin) çš„å”¯ä¸€ç»„åˆéª¨æ¶è¡¨ã€‚
            ç”¨äºè®¡ç®—å…¨é‡æ•°æ® (Total) çš„ PSIã€‚
        group_col : str
            åˆ†ç»„åˆ—çš„åç§° (ä¾‹å¦‚ 'month', 'vintage')ã€‚
        baseline_group : Any
            åŸºå‡†ç»„çš„å…·ä½“å–å€¼ (ä¾‹å¦‚ '2023-01')ã€‚
            è¯¥ç»„çš„æ•°æ®åˆ†å¸ƒå°†ä½œä¸º Expected åˆ†å¸ƒã€‚

        Returns
        -------
        pl.LazyFrame
            è®¡ç®—ç»“æœå®½è¡¨ï¼ŒåŒ…å« feature, group_psi, total_psi ç­‰ä¿¡æ¯ã€‚
            
        Implementation Details
        ----------------------
        - éª¨æ¶æœºåˆ¶: å‡½æ•°å†…éƒ¨é€šè¿‡ cross join åŠ¨æ€ç”Ÿæˆ [Group x Feature x Bin] çš„å…¨é‡ Lazy éª¨æ¶ï¼Œ
          å¼ºåˆ¶æ‰§è¡Œé›¶é¢‘æ ¼å¡«å…… (Epsilon filling)ï¼Œç¡®ä¿åœ¨æ•°æ®æ¼‚ç§»æç«¯ï¼ˆæŸåˆ†ç®±å®Œå…¨æ¶ˆå¤±ï¼‰æ—¶
          å…¬å¼ä»å…·å¤‡æ•°å€¼ç¨³å®šæ€§ã€‚
        - æ€§èƒ½: è¾“å…¥è¾“å‡ºå‡ä¸º pl.LazyFrameï¼Œå…è®¸ Polars ä¼˜åŒ–å™¨è¿›è¡Œè°“è¯ä¸‹æ¨å’Œå¹¶è¡ŒåŠ é€Ÿã€‚
        """
        feat_col = "feat_idx" if "feat_idx" in stats_df.collect_schema().names() else "feature"
        epsilon = 1e-6
        div_safe = 1e-9  # é˜²æ­¢åˆ†æ¯ä¸º 0 çš„ä¿é™©ç³»æ•°

        # 1. è‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„ [åˆ†ç»„ x ç‰¹å¾ x åˆ†ç®±] éª¨æ¶ (Lazy)
        # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªåˆ†ç»„é‡Œéƒ½æœ‰å®Œæ•´çš„ç‰¹å¾åˆ†ç®±åå•
        unique_groups = stats_df.select(group_col).unique()
        full_skeleton = unique_bins_skel.join(unique_groups, how="cross")

        # 2. è®¡ç®—åŸºå‡†åˆ†å¸ƒ (Expected)
        expected = (
            stats_df
            .filter(pl.col(group_col) == baseline_group)
            .with_columns(
                (pl.col("len") / (pl.col("len").sum().over(feat_col) + div_safe)).alias("E")
            )
            .select([feat_col, "bin_id", "E"])
        )

        # 3. è®¡ç®—å®é™…åˆ†å¸ƒ (Actual)
        actual = (
            stats_df
            .with_columns(
                (pl.col("len") / (pl.col("len").sum().over([group_col, feat_col]) + div_safe)).alias("A")
            )
            .select([group_col, feat_col, "bin_id", "A"])
        )

        # 4. è®¡ç®—å…¨é‡åˆ†å¸ƒ (Global Actual)
        global_actual = (
            stats_df
            .group_by([feat_col, "bin_id"])
            .agg(pl.col("len").sum().alias("total_len"))
            .with_columns(
                (pl.col("total_len") / (pl.col("total_len").sum().over(feat_col) + div_safe)).alias("A_global")
            )
            .select([feat_col, "bin_id", "A_global"])
        )

        # 5. æ ¸å¿ƒè®¡ç®—é€»è¾‘ (Left Join éª¨æ¶å¹¶å¡«å…… epsilon)
        # è®¡ç®—åˆ†ç»„ PSI
        psi_group = (
            full_skeleton
            .join(actual, on=[group_col, feat_col, "bin_id"], how="left")
            .join(expected, on=[feat_col, "bin_id"], how="left")
            .with_columns([
                pl.col("A").fill_null(epsilon),
                pl.col("E").fill_null(epsilon)
            ])
            .with_columns(
                ((pl.col("A") - pl.col("E")) * (pl.col("A") / pl.col("E")).log()).alias("psi_contrib")
            )
            .group_by([group_col, feat_col])
            .agg(pl.col("psi_contrib").sum().alias("psi"))
        )

        # è®¡ç®—å…¨é‡ PSI
        psi_total = (
            unique_bins_skel
            .join(global_actual, on=[feat_col, "bin_id"], how="left")
            .join(expected, on=[feat_col, "bin_id"], how="left")
            .with_columns([
                pl.col("A_global").fill_null(epsilon),
                pl.col("E").fill_null(epsilon)
            ])
            .with_columns(
                ((pl.col("A_global") - pl.col("E")) * (pl.col("A_global") / pl.col("E")).log()).alias("psi_contrib_total")
            )
            .group_by(feat_col)
            .agg(pl.col("psi_contrib_total").sum().alias("total"))
        )

        return psi_group.join(psi_total, on=feat_col, how="left")


    # =========================================================================
    # Expression Factories 
    # =========================================================================
    def _build_expressions(self, col: str, config: MarsProfileConfig) -> List[pl.Expr]:
        """[Factory] ä¸ºå•ä¸ªåˆ—ç”Ÿæˆæ‰€æœ‰ Overview æŒ‡æ ‡çš„è®¡ç®—è¡¨è¾¾å¼ã€‚"""
        return self._get_full_stats_exprs(col, config)
    
    def _get_full_stats_exprs(self, col: str, config: MarsProfileConfig) -> List[pl.Expr]:
        """
        [Factory] ä¸ºå•ä¸ªåˆ—æ„å»ºå…¨é‡ç»Ÿè®¡æŒ‡æ ‡çš„ Polars è¡¨è¾¾å¼åˆ—è¡¨ã€‚

        è¯¥æ–¹æ³•å°è£…äº† Overview è®¡ç®—çš„æ ¸å¿ƒç»†èŠ‚ï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®é€»è¾‘ï¼š
        1. **å¤§åŸºæ•°ä¼˜åŒ– (High Cardinality Opt)**: 
           å¯¹äºè¶…è¿‡ 100w è¡Œçš„æ•°æ®é›†ï¼Œè‡ªåŠ¨å°† `n_unique` åˆ‡æ¢ä¸º `approx_n_unique` (HyperLogLog)ï¼Œ
           åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æå¤§é™ä½å†…å­˜æ¶ˆè€—ã€‚
        2. **ä¼—æ•°æå– (Mode Extraction)**:
           é¢„å…ˆè®¡ç®— `value_counts` å¹¶æå– Top1 çš„ç»“æ„ä½“ï¼ŒåŒæ—¶è·å–å…¶ **Value** (è½¬æ¢ç±»å‹ä¸º String) 
           å’Œ **Ratio** (å æ¯”)ï¼Œç¡®ä¿æ•°æ®è´¨é‡å¯è§æ€§ã€‚
        3. **åŠ¨æ€æŒ‡æ ‡ç”Ÿæˆ**:
           æ ¹æ® Config ä¸­çš„ `stat_metrics` åŠ¨æ€ç”Ÿæˆå‡å€¼ã€æ–¹å·®ç­‰ç»Ÿè®¡è¡¨è¾¾å¼ï¼Œ
           å¹¶è‡ªåŠ¨åº”ç”¨ `_get_metric_expr` ä¸­çš„ç¼ºå¤±å€¼å‰”é™¤é€»è¾‘ã€‚

        Returns
        -------
        List[pl.Expr]
            åŒ…å«è¯¥åˆ—æ‰€æœ‰å¾…è®¡ç®—æŒ‡æ ‡çš„è¡¨è¾¾å¼åˆ—è¡¨ã€‚
        """
        
        total_len = pl.len()
        is_num = self._is_numeric(col)
        exprs = []

        # --- åŠ¨æ€ç”Ÿæˆ DQ æŒ‡æ ‡ (æ ¹æ® config.dq_metrics è¿‡æ»¤) ---
        dq_targets = config.dq_metrics
        
        if "missing" in dq_targets:
            # [æ ¸å¿ƒä¿®å¤] æ„å»º è”åˆç¼ºå¤±æ¡ä»¶: åŸç”Ÿ Null | (å¦‚æœæ˜¯æ•°å€¼åˆ™åŒ…å« NaN) | è‡ªå®šä¹‰ç¼ºå¤±å€¼
            missing_cond = pl.col(col).is_null()
            if is_num:
                missing_cond |= pl.col(col).is_nan()
            
            valid_missing = self._get_valid_missing(col)
            if valid_missing:
                missing_cond |= pl.col(col).is_in(valid_missing)
                
            exprs.append((missing_cond.sum() / total_len).alias("missing_rate"))
            
        if "zeros" in dq_targets:
            zeros_c = (pl.col(col) == 0).sum() if is_num else pl.lit(0, dtype=pl.UInt32)
            exprs.append((zeros_c / total_len).alias("zeros_rate"))
            
        if "unique" in dq_targets:
            if self.df.height > 1_000_000:
                unique_count_expr = pl.col(col).approx_n_unique()
            else:
                unique_count_expr = pl.col(col).n_unique()
            exprs.append((unique_count_expr / total_len).alias("unique_rate"))
            
        if "top1" in dq_targets:
            # é¢„è®¡ç®— Top1 ç»“æ„ä½“ (é¿å…é‡å¤å†™ value_counts é€»è¾‘)
            # value_counts è¿”å› struct: {col_name: value, count: int}
            top1_struct = pl.col(col).value_counts(sort=True).first()
            
            exprs.append(
                (
                    pl.col(col)                         # 1. é€‰ä¸­ç›®æ ‡åˆ— (å‡è®¾åˆ—åå« "city")
                    
                    .value_counts(sort=True)            # 2. ç»Ÿè®¡æ¯ä¸ªå€¼å‡ºç°çš„æ¬¡æ•°ï¼Œå¹¶æŒ‰æ¬¡æ•°ä»å¤šåˆ°å°‘æ’åº
                                                        #    è¿”å›æ•°æ®æ ¼å¼ (List[Struct]): 
                                                        #    [{"city": "åŒ—äº¬", "count": 100},  <-- ç¬¬1è¡Œ (æ¬¡æ•°æœ€å¤š)
                                                        #     {"city": "ä¸Šæµ·", "count": 80},   <-- ç¬¬2è¡Œ
                                                        #     ...]

                    .first()                            # 3. åªå–æ’åºåçš„ç¬¬ 1 è¡Œæ•°æ® (ä¹Ÿå°±æ˜¯ä¼—æ•°çš„é‚£ä¸€è¡Œ)
                                                        #    è¿”å›æ•°æ®æ ¼å¼ (Struct): 
                                                        #    {"city": "åŒ—äº¬", "count": 100}

                    .struct.field("count")              # 4. ä»è¿™ä¸ªç»“æ„ä½“(Struct)ä¸­ï¼Œåªæå– "count" è¿™ä¸ªå­—æ®µçš„å€¼
                                                        #    è¿”å›æ•°æ®: 100

                    / total_len                         # 5. é™¤ä»¥æ€»è¡Œæ•° (ä¾‹å¦‚æ€»å…±æœ‰ 1000 è¡Œ)
                                                        #    è®¡ç®—: 100 / 1000 = 0.1

                ).alias("top1_ratio")                    # 6. ç»™è¿™ä¸ªè®¡ç®—ç»“æœèµ·ä¸ªåå­—å« "top1_ratio"
            )
            
            # å¢åŠ  Top1 Value (ä¼—æ•°å…·ä½“å€¼)
            # å¼ºåˆ¶è½¬ä¸º Stringï¼Œé˜²æ­¢ä¸ Mean/Std ç­‰æ•°å€¼æŒ‡æ ‡åœ¨ pivot æ—¶å‘ç”Ÿç±»å‹å†²çª
            exprs.append(top1_struct.struct.field(col).cast(pl.Utf8).alias("top1_value"))

        # åŠ¨æ€ç»Ÿè®¡æŒ‡æ ‡ (åŸºäº Config)
        # æ•°å€¼ç»Ÿè®¡æŒ‡æ ‡
        if is_num:
            # éå† config.stat_metrics åŠ¨æ€ç”Ÿæˆ
            for metric in config.stat_metrics:
                if metric.lower() == "psi":
                    # PSI éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè·³è¿‡
                    continue
                expr = self._get_metric_expr(col, metric)
                if expr is not None:
                    exprs.append(expr.alias(metric))
        # éæ•°å€¼åˆ—ï¼Œç›´æ¥å¡«å…… Null
        else:
            null_lit = pl.lit(None, dtype=pl.Float64)
            for metric in config.stat_metrics:
                if metric.lower() == "psi":
                    # PSI éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè·³è¿‡
                    continue
                exprs.append(null_lit.alias(metric))
        return exprs

    def _get_single_metric_expr(self, col: str, metric_type: str) -> pl.Expr:
        """[Factory] ä¸ºå•ä¸ªåˆ—ç”ŸæˆæŒ‡å®šæŒ‡æ ‡çš„è®¡ç®—è¡¨è¾¾å¼ (ç”¨äº Pivot)ã€‚"""
        return self._get_metric_expr(col, metric_type)
    
    def _get_metric_expr(self, col: str, metric_type: str) -> pl.Expr:
        """
        [Factory] ç”Ÿæˆå•ä¸ªæŒ‡æ ‡çš„è®¡ç®—è¡¨è¾¾å¼ã€‚

        **ç‰¹æ®Šå€¼å¤„ç†é€»è¾‘ (Special Values Handling)**:
        1. **DQ æŒ‡æ ‡ (Missing/Unique/Top1)**: åŸºäºå…¨é‡æ•°æ®è®¡ç®—ã€‚ç‰¹æ®Šå€¼ä¼šè¢«è§†ä¸ºâ€œå€¼â€å‚ä¸ Unique/Top1 ç»Ÿè®¡ï¼Œæˆ–è¢«å½’ä¸º Missingã€‚
        2. **ç»Ÿè®¡æŒ‡æ ‡ (Mean/Std/Quantile)**: åŸºäºå‰”é™¤ç‰¹æ®Šå€¼åçš„â€œå‡€æ•°æ®â€è®¡ç®—ã€‚é˜²æ­¢ -999 æ‹‰ä½å‡å€¼æˆ–æ‰­æ›²åˆ†å¸ƒã€‚

        Parameters
        ----------
        col : str
            ç›®æ ‡åˆ—åã€‚
        metric_type : str
            æŒ‡æ ‡åç§°ã€‚

        Returns
        -------
        pl.Expr
            Polars è¡¨è¾¾å¼å¯¹è±¡ã€‚
        """
        # 1. è·å–è¯¥åˆ—å¯¹åº”çš„ç‰¹æ®Šå€¼/ç¼ºå¤±å€¼åˆ—è¡¨
        valid_missing = self._get_valid_missing(col)
        
        # 2. å®šä¹‰åŸºç¡€åˆ—å¯¹è±¡ (Raw Data)
        raw_col = pl.col(col)
        is_num = self._is_numeric(col)
        col_dtype = self._dtype_map.get(col)

        # ---------------------------------------------------------
        # Group A: æ•°æ®è´¨é‡æŒ‡æ ‡ (åŸºäº Raw Data è®¡ç®—)
        # ---------------------------------------------------------
        if metric_type == "missing":
            # ç¼ºå¤±ç‡ = (åŸç”Ÿ Null + NaN + è‡ªå®šä¹‰ç‰¹æ®Šå€¼) / æ€»è¡Œæ•°
            # [ä¿®æ”¹] å¢åŠ å¯¹ NaN çš„åˆ¤å®šï¼Œå› ä¸º np.nan åœ¨ Polars ä¸­è¢«è¯†åˆ«ä¸º NaN
            missing_cond = raw_col.is_null()
            if is_num and col_dtype in [pl.Float32, pl.Float64]:
                missing_cond |= raw_col.is_nan()
            
            if valid_missing:
                missing_cond |= raw_col.is_in(valid_missing)
                
            return missing_cond.sum() / pl.len()
        
        elif metric_type == "zeros":
            # é›¶å€¼ç‡ (ç‰©ç†æ„ä¹‰ä¸Šçš„ 0)
            return (raw_col == 0).sum() / pl.len() if is_num else pl.lit(0, dtype=pl.UInt32)
        
        elif metric_type == "unique":
            # å”¯ä¸€å€¼æ•°é‡ (åŒ…å«ç‰¹æ®Šå€¼)
            return raw_col.n_unique() / pl.len()
        
        elif metric_type == "top1":
            # ä¼—æ•°å æ¯” (ç‰¹æ®Šå€¼ä¹Ÿå¯èƒ½æˆä¸ºä¼—æ•°ï¼Œéœ€æš´éœ²é£é™©)
            return raw_col.value_counts(sort=True).first().struct.field("count") / pl.len()

        # ---------------------------------------------------------
        # Group B: æ•°å€¼ç»Ÿè®¡æŒ‡æ ‡ (åŸºäº Clean Data è®¡ç®—)
        # ---------------------------------------------------------
            
        if not is_num: 
            return pl.lit(None)

        # æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„å¸ƒå°”æ©ç  (Keep Mask)ï¼Œè€Œä¸æ˜¯åˆ†æ­¥ filter
        keep_mask = pl.lit(True)
        if col_dtype in [pl.Float32, pl.Float64]:
            # æµ®ç‚¹æ•°å¿…é¡»åŒæ—¶æ’é™¤ Null å’Œ NaN
            keep_mask &= raw_col.is_not_nan() & raw_col.is_not_null()
        else:
            keep_mask &= raw_col.is_not_null()

        exclude_vals = self._get_values_to_exclude(col)
        if exclude_vals:
            keep_mask &= ~raw_col.is_in(exclude_vals)

        # ã€å†³ç­–ã€‘è¿™é‡Œä½¿ç”¨ filterï¼Œå› ä¸ºåç»­æ¶‰åŠ quantile/median è®¡ç®—ã€‚
        # filter ä¼šç‰©ç†å‡å°‘æ•°æ®é‡ï¼Œè¿™å¯¹äºæ’åºç±»æ“ä½œ(Sorting-based ops)æ€§èƒ½æ›´ä¼˜ï¼Œ
        # ä¸”åœ¨ generate_profile ä¸­æˆ‘ä»¬åªè¿”å›æ ‡é‡(Scalar)ï¼Œä¸ä¼šå¯¼è‡´åˆ—é•¿åº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
        clean_col = raw_col.filter(keep_mask)

        mapper = {
            # é›†ä¸­åº¦
            "mean": clean_col.mean(),
            "median": clean_col.median(),
            "sum": clean_col.sum(),
            
            # ç¦»æ•£åº¦
            "std": clean_col.std(),
            
            # æå€¼ (æœ€å°å€¼å¦‚æœæ˜¯ -999 å°±æ²¡æ„ä¹‰äº†ï¼Œæ‰€ä»¥è¦ç”¨ clean_col)
            "min": clean_col.min(),
            "max": clean_col.max(),
            
            # åˆ†ä½æ•°
            "p25": clean_col.quantile(0.25),
            "p75": clean_col.quantile(0.75),
            
            # åˆ†å¸ƒå½¢æ€
            "skew": clean_col.skew(),
            "kurtosis": clean_col.kurtosis()
        }
        
        return mapper.get(metric_type, pl.lit(None))

    # ==========================================================================
    # Helpers & Utilities
    # ==========================================================================
    def _get_feature_dtypes(self) -> pl.DataFrame:
        """[Helper] è·å– Schema ä¿¡æ¯è¡¨"""
        schema = {"feature": [], "dtype": []}
        for n, d in self.df.schema.items():
            schema["feature"].append(n)
            schema["dtype"].append(str(d))
        return pl.DataFrame(schema)

    def _is_numeric(self, col: str) -> bool:
        """[Helper] åˆ¤æ–­åˆ—æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹"""
        # å…¼å®¹ Polars è¿™é‡Œçš„ç±»å‹åˆ¤æ–­
        dtype = self._dtype_map.get(col)
        return dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, 
                        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, 
                        pl.Float32, pl.Float64]

    def _get_valid_missing(self, col: str) -> List[Any]:
        """[Helper] ç±»å‹å®‰å…¨çš„ç¼ºå¤±å€¼åŒ¹é… (é˜²æ­¢ç±»å‹ä¸åŒ¹é…æŠ¥é”™)"""
        # Polars å¾ˆä¸¥æ ¼ï¼Œå¦‚æœæ‹¿å­—ç¬¦ä¸² "unknown" å»è¿‡æ»¤æ•´æ•°åˆ—ï¼Œä¼šå´©ã€‚
        # è¿™ä¸ªå‡½æ•°ä¼šæ£€æŸ¥å½“å‰åˆ—çš„ç±»å‹ï¼Œåªè¿”å›ç±»å‹åŒ¹é…çš„è‡ªå®šä¹‰ç¼ºå¤±å€¼ã€‚
        if not self.custom_missing: 
            return []
        is_num = self._is_numeric(col)
        is_str = self.df[col].dtype == pl.String
        return [v for v in self.custom_missing if (is_num and isinstance(v, (int, float))) or (is_str and isinstance(v, str))]
    
    def _get_values_to_exclude(self, col: str) -> List[Any]:
        """
        [Helper] è·å–å½“å‰åˆ—éœ€è¦å‰”é™¤çš„æ‰€æœ‰ç‰¹å®šå€¼ (ç±»å‹å®‰å…¨)ã€‚

        è¯¥æ–¹æ³•åˆå¹¶äº†å®ä¾‹çš„ `custom_missing` (è‡ªå®šä¹‰ç¼ºå¤±å€¼) å’Œ `special_values` (ç‰¹æ®Šå€¼)ï¼Œ
        å¹¶æ ¹æ®ç›®æ ‡åˆ—çš„ç‰©ç†ç±»å‹ (`dtype`) å¯¹å€¼è¿›è¡Œä¸¥æ ¼è¿‡æ»¤ã€‚

        Polars çš„æ¯”è¾ƒç®—å­ (`is_in`, `eq`) æ˜¯å¼ºç±»å‹çš„ã€‚å¦‚æœå°è¯•å°†å­—ç¬¦ä¸²ç±»å‹çš„å€¼ï¼ˆå¦‚ "unknown"ï¼‰
        åº”ç”¨äºæ•°å€¼ç±»å‹çš„åˆ—ï¼ˆå¦‚ `Int64`ï¼‰ï¼Œç¨‹åºä¼šæŠ›å‡ºå¼‚å¸¸ã€‚æœ¬æ–¹æ³•ç¡®ä¿åç»­è¿‡æ»¤æ“ä½œçš„ç±»å‹å®‰å…¨æ€§ã€‚

        Parameters
        ----------
        col : str
            ç›®æ ‡åˆ—çš„åç§°ã€‚

        Returns
        -------
        List[Any]
            å½“å‰åˆ—ä¸­åº”å½“è¢«è§†ä¸ºâ€œéæ­£å¸¸æ•°å€¼â€çš„åˆ—è¡¨ã€‚
            åˆ—è¡¨ä¸­çš„å…ƒç´ ç±»å‹ä¿è¯ä¸ `col` çš„æ•°æ®ç±»å‹å…¼å®¹ (ä¾‹å¦‚æ•°å€¼åˆ—åªè¿”å›æ•°å€¼ï¼Œå­—ç¬¦ä¸²åˆ—åªè¿”å›å­—ç¬¦ä¸²)ã€‚
        """
        # 1. åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨ 
        # å¦‚æœ self.special_values è¿˜æ²¡å®šä¹‰ï¼Œå°±ç”¨ç©ºåˆ—è¡¨ä»£æ›¿
        special_vals = getattr(self, "special_values", [])
        candidates = self.custom_missing + special_vals
        
        if not candidates: 
            return []

        # 2. è·å–åˆ—ç±»å‹
        is_num = self._is_numeric(col)
        is_str = self.df[col].dtype == pl.String

        # 3. ç±»å‹å®‰å…¨è¿‡æ»¤
        valid_values = []
        for v in candidates:
            # åªæœ‰å½“ å€¼ç±»å‹ ä¸ åˆ—ç±»å‹ åŒ¹é…æ—¶ï¼Œæ‰åŠ å…¥åˆ—è¡¨
            if is_num and isinstance(v, (int, float)) and not isinstance(v, bool):
                valid_values.append(v)
            elif is_str and isinstance(v, str):
                valid_values.append(v)
                
        return valid_values

