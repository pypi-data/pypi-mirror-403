import polars as pl
import numpy as np
import pandas as pd
from typing import List, Optional, Dict, Union, Any, Tuple, Literal

from mars.core.base import MarsBaseEstimator
from mars.feature.binner import MarsBinnerBase, MarsNativeBinner, MarsOptimalBinner
from mars.analysis.report import MarsEvaluationReport 
from mars.utils.logger import logger
from mars.utils.decorators import time_it
from mars.utils.date import MarsDate
from mars.utils.plotter import MarsPlotter

class MarsBinEvaluator(MarsBaseEstimator):
    """
    [MarsBinEvaluator] ç‰¹å¾æ•ˆèƒ½ä¸ç¨³å®šæ€§è¯„ä¼°å¼•æ“ (High-Performance Production Edition).

    è¯¥ç±»å®ç°äº†åŸºäº **Map-Reduce æ¶æ„** çš„å¤§è§„æ¨¡ç‰¹å¾è¯„ä¼°ã€‚
    å®ƒè§£å†³äº†ä¼ ç»Ÿ Python é£æ§åº“åœ¨å¤„ç†å®½è¡¨ï¼ˆWide Table, 5000+ Colsï¼‰æ—¶çš„å†…å­˜æº¢å‡ºå’Œ I/O ç“¶é¢ˆé—®é¢˜ã€‚

    æ ¸å¿ƒæ¶æ„ (Architecture)
    -----------------------
    1. **å•æ¬¡æ‰«æ (Single-Pass I/O)**:
        å…¨æµç¨‹ä»…å¯¹åŸå§‹å¤§æ•°æ®æ‰§è¡Œä¸€æ¬¡ `unpivot` å’Œ `agg` æ“ä½œï¼ˆMap é˜¶æ®µï¼‰ã€‚
        åç»­æ‰€æœ‰è®¡ç®—ï¼ˆWOEè¡¥å…¨ã€åŸºå‡†å¯¹æ¯”ã€Totalèšåˆï¼‰å‡åœ¨èšåˆåçš„â€œä¸­é—´ç»Ÿè®¡è¡¨â€ä¸Šå®Œæˆï¼ˆReduce é˜¶æ®µï¼‰ã€‚
    
    2. **å‘é‡åŒ–æŒ‡æ ‡è®¡ç®— (Vectorized Metrics)**:
        PSI, IV, KS, AUC, Lift ç­‰æŒ‡æ ‡å‡é€šè¿‡ Polars è¡¨è¾¾å¼å¼•æ“åœ¨åˆ—å¼å†…å­˜ä¸­è®¡ç®—ï¼Œ
        å®Œå…¨é¿å…äº† Python å¾ªç¯ï¼Œåˆ©ç”¨ SIMD æŒ‡ä»¤é›†åŠ é€Ÿã€‚

    3. **å†…å­˜/è®¡ç®—å‡è¡¡**:
        åˆ©ç”¨ `streaming=True` å¤„ç† Map é˜¶æ®µçš„é‡å‹è®¡ç®—ï¼Œåˆ©ç”¨å†…å­˜è®¡ç®—å¤„ç† Reduce é˜¶æ®µçš„é€»è¾‘ï¼Œ
        åœ¨é€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚

    Attributes
    ----------
    target_col : str
        ç›®æ ‡å˜é‡åˆ—å (0/1)ã€‚
    binner : MarsBinnerBase
        åˆ†ç®±å™¨å®ä¾‹ã€‚å¦‚æœæœªæä¾›ï¼Œevaluate æ—¶ä¼šè‡ªåŠ¨æ‹Ÿåˆã€‚
    binner_kwargs : dict
        ä¼ é€’ç»™è‡ªåŠ¨åˆ†ç®±å™¨çš„é¢å¤–å‚æ•°ã€‚
        
    Examples
    --------
    >>> import polars as pl
    >>> from mars.analysis import MarsBinEvaluator

    >>> # 1. å‡†å¤‡æ•°æ® (æ”¯æŒ Polars/Pandas)
    >>> df = pl.read_parquet("credit_risk_data.parquet")
    >>> target_col = "is_default"

    >>> # 2. åˆå§‹åŒ–è¯„ä¼°å™¨
    >>> evaluator = MarsBinEvaluator(target_col=target_col)

    >>> # 3. [æœ€ç®€æ¨¡å¼] ä¸€é”®è¯„ä¼° + ç»˜å›¾
    >>> # è‡ªåŠ¨æ‹Ÿåˆåˆ†ç®± -> è®¡ç®— IV/PSI -> ç»˜åˆ¶ Top 10 ç‰¹å¾è¶‹åŠ¿å›¾
    >>> report = evaluator.evaluate_and_plot(df)

    >>> # 4. æŸ¥çœ‹ç»“æœ
    >>> print(report.summary_table.head())  # æŸ¥çœ‹å®¡è®¡æ±‡æ€»è¡¨
    >>> report.write_excel("risk_report.xlsx") # å¯¼å‡ºç²¾ç¾ Excel
    """
    

    def __init__(
        self, 
        target_col: str = "target",
        binner: Optional[MarsBinnerBase] = None,
        bining_type: Literal["native", "opt"] = "native",
        **binner_kwargs
    ) -> None:
        """
        åˆå§‹åŒ–è¯„ä¼°å¼•æ“ã€‚

        Parameters
        ----------
        target_col : str, default "target"
            Label åˆ—åï¼Œé€šå¸¸ä¸º 0ï¼ˆå¥½äººï¼‰å’Œ 1ï¼ˆåäººï¼‰ã€‚
        binner : MarsBinnerBase, optional
            é¢„è®­ç»ƒå¥½çš„åˆ†ç®±å™¨ã€‚è‹¥ä¸º Noneï¼Œå°†åœ¨ evaluate å†…éƒ¨è‡ªåŠ¨è®­ç»ƒ MarsNativeBinnerã€‚
        bining_type : Literal["native", "opt"], default "native"
            åˆ†ç®±å™¨ç±»å‹é€‰æ‹©ã€‚å½“ binner ä¸º None æ—¶ç”Ÿæ•ˆã€‚
        **binner_kwargs : dict
            é€ä¼ ç»™è‡ªåŠ¨åˆ†ç®±å™¨çš„å‚æ•° (ä»…åœ¨ binner ä¸º None æ—¶ç”Ÿæ•ˆ)ï¼Œå¦‚ `n_bins`, `strategy` ç­‰ã€‚
        """
        super().__init__()
        self.target_col = target_col
        self.binner = binner
        self.binner_kwargs = binner_kwargs
        self.bining_type = bining_type
        
    @time_it
    def evaluate(
        self,
        df: Union[pl.DataFrame, pd.DataFrame],  # Modified: æ”¯æŒ Pandas è¾“å…¥
        features: Optional[List[str]] = None,
        profile_by: Optional[str] = None,
        dt_col: Optional[str] = None,
        benchmark_df: Union[pl.DataFrame, pd.DataFrame, None] = None, # Modified: æ”¯æŒ Pandas è¾“å…¥
        weights_col: Optional[str] = None,
        batch_size: int = 500
    ) -> "MarsEvaluationReport":
        """
        [Core] æ‰§è¡Œç‰¹å¾è¯„ä¼°çš„ä¸»å…¥å£ã€‚

        è¯¥æ–¹æ³•æ¶µç›–äº†ä»åŸå§‹æ•°æ®åˆ°æœ€ç»ˆè¯„ä¼°æŠ¥å‘Šçš„å…¨æµç¨‹ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åˆ†ç®±ã€æŒ‡æ ‡è®¡ç®—å’Œå•è°ƒæ€§æ£€æµ‹ã€‚

        Parameters
        ----------
        df : Union[pl.DataFrame, pd.DataFrame]
            å¾…è¯„ä¼°çš„æ•°æ®é›†ï¼ˆé€šå¸¸æ˜¯è®­ç»ƒé›†ã€æµ‹è¯•é›†æˆ– OOT æ•°æ®ï¼‰ã€‚æ”¯æŒ Pandas DataFrameã€‚
        features : List[str], optional
            æŒ‡å®šè¯„ä¼°çš„ç‰¹å¾åˆ—è¡¨ã€‚è‹¥ä¸º Noneï¼Œè‡ªåŠ¨è¯†åˆ«é™¤ Target/Group/Weight å¤–çš„æ‰€æœ‰åˆ—ã€‚
        profile_by : str, optional
            åˆ†ç»„ç»´åº¦åï¼ˆå¦‚ 'month', 'vintage'ï¼‰ã€‚è‹¥æä¾›ï¼Œå°†ç”ŸæˆåŸºäºè¯¥ç»´åº¦çš„ Trend è¶‹åŠ¿æŠ¥è¡¨ã€‚
        dt_col : str, optional
            æ—¥æœŸåˆ—åã€‚
            - è‹¥é…åˆ `profile_by='week'`ï¼Œåˆ™æŒ‰å‘¨èšåˆã€‚
            - **[é»˜è®¤]** è‹¥æä¾›äº† `dt_col` ä½†æœªæä¾› `profile_by`ï¼Œé»˜è®¤ä¸º **'month'** (æŒ‰æœˆèšåˆ)ã€‚
        benchmark_df : Union[pl.DataFrame, pd.DataFrame], optional
            å¤–éƒ¨åŸºå‡†æ•°æ®é›†ã€‚ç”¨äºè®¡ç®— PSI çš„ Expected åˆ†å¸ƒã€‚æ”¯æŒ Pandas DataFrameã€‚
            è‹¥ä¸º Noneï¼Œé»˜è®¤ä½¿ç”¨ `df` ä¸­æ—¶é—´/åˆ†ç»„é¡ºåºæœ€æ—©çš„ä¸€ç»„ä½œä¸ºåŸºå‡†ã€‚
        weights_col : str, optional
            æ ·æœ¬æƒé‡åˆ—åã€‚è‹¥æŒ‡å®šï¼Œæ‰€æœ‰æŒ‡æ ‡ï¼ˆBadRate, AUC, PSIç­‰ï¼‰å‡åŸºäºåŠ æƒå€¼è®¡ç®—ã€‚
        batch_size : int, default 500
            æ‰¹å¤„ç†å¤§å°ã€‚ç”¨äºæ§åˆ¶å†…å­˜æ¶ˆè€—ä¸è®¡ç®—é€Ÿåº¦çš„å¹³è¡¡ã€‚

        Returns
        -------
        MarsEvaluationReport
            åŒ…å« Summary (æ±‡æ€»), Trend (è¶‹åŠ¿), Detail (åˆ†ç®±è¯¦æƒ…) ä¸‰å¼ æ ¸å¿ƒè¡¨çš„æŠ¥å‘Šå®¹å™¨ã€‚
        """
        
        # --- 1. ä¸Šä¸‹æ–‡å‡†å¤‡ (Context Setup) ---
        
        # [Pandas å…¼å®¹] æ ¸å¿ƒä¿®æ”¹ï¼šåˆ©ç”¨çˆ¶ç±»æ–¹æ³•ç»Ÿä¸€è½¬æ¢ä¸º Polars å¹¶è®¾ç½®è¾“å‡ºæ ‡å¿—ä½
        # å¦‚æœ df æ˜¯ Pandasï¼Œself._return_pandas ä¼šè¢«è®¾ä¸º True
        working_df = self._ensure_polars_dataframe(df)
        
        # [Pandas å…¼å®¹] åŒæ ·å¤„ç† benchmark_dfï¼Œä½†ä¸éœ€è¦æ”¹å˜ _return_pandas çŠ¶æ€ï¼ˆä»…åšè®¡ç®—ç”¨ï¼‰
        if benchmark_df is not None:
            benchmark_df = self._ensure_polars_dataframe(benchmark_df)

        # æ£€æŸ¥ Target æœ‰æ•ˆæ€§ï¼Œé¿å…åç»­ AUC/KS è®¡ç®—å´©æºƒ
        n_unique = working_df.select(pl.col(self.target_col).n_unique()).item()
        if n_unique < 2:
            logger.warning(f"âš ï¸ Target '{self.target_col}' has < 2 unique values. Metrics (AUC/KS) may be invalid.")

        # å¤„ç†æ—¥æœŸèšåˆé€»è¾‘ï¼šå°†æ—¥æœŸåˆ—è½¬åŒ–ä¸º '2023-01' ç­‰æ ¼å¼
        # æ³¨æ„ï¼šæ­¤å¤„ working_df å·²ç»æ˜¯ Polars æ ¼å¼ï¼Œå¯ä»¥å®‰å…¨è°ƒç”¨ Polars API
        working_df, group_col = self._prepare_context(working_df, profile_by, dt_col)
        
        # è‡ªåŠ¨è¯†åˆ«ç‰¹å¾åˆ—ï¼šæ’é™¤ç›®æ ‡åˆ—ã€åˆ†ç»„åˆ—å’Œæƒé‡åˆ—
        exclude_cols = {self.target_col, group_col}
        if weights_col: exclude_cols.add(weights_col)
        
        target_features = features if features else [
            c for c in working_df.columns if c not in exclude_cols
        ]

        if self.binner is None:
            # 1. å‡†å¤‡å‚æ•°
            # ç¡®ä¿ kwargs æ˜¯å­—å…¸ (è™½ç„¶ __init__ ä¸­çš„ **kwargs ä¿è¯äº†å®ƒæ˜¯ dictï¼Œä½†é˜²å¾¡æ€§ç¼–ç¨‹æ€»æ²¡é”™)
            fit_kwargs = self.binner_kwargs if self.binner_kwargs is not None else {}
            
            # 2. å®šä¹‰å·¥å‚æ˜ å°„
            binner_factory = {
                "native": MarsNativeBinner,
                "opt": MarsOptimalBinner
            }
            
            # 3. è·å–åˆ†ç®±å™¨ç±»
            binner_cls = binner_factory.get(self.bining_type)
            if binner_cls is None:
                # å¦‚æœè¾“å…¥äº†æœªçŸ¥çš„ç±»å‹ï¼Œå›é€€åˆ° native å¹¶è­¦å‘Š
                logger.warning(f"âš ï¸ Unknown bining_type '{self.bining_type}'. Fallback to 'native'.")
                binner_cls = MarsNativeBinner
            
            logger.info(f"âš™ï¸ No binner provided. Auto-fitting {binner_cls.__name__} internally...")
            
            # 4. å®ä¾‹åŒ–
            self.binner = binner_cls(features=target_features, **fit_kwargs)
            
            # 5. æ‹Ÿåˆ
            # æ³¨æ„ï¼šMarsOptimalBinner éœ€è¦ yï¼Œä¸”é€šå¸¸èƒ½å¤„ç† Polars Series
            y_series = working_df.get_column(self.target_col)
            self.binner.fit(working_df, y_series)
        
        # [Transform] æ•°æ®è½¬æ¢ï¼šå°†åŸå§‹è¿ç»­å€¼/ç¦»æ•£å€¼æ˜ å°„ä¸ºåˆ†ç®±ç´¢å¼• (Int16)
        # æ˜ å°„åçš„åˆ—åä¸º {feat}_bin
        logger.debug("ğŸ”„ Transforming features to bin indices...")
        df_binned = self.binner.transform(working_df, return_type="index")
        
        # ==============================================================================
        # ğŸš€ [æ ¸å¿ƒä¼˜åŒ–]: å•æ¬¡æ‰«ææ¶æ„ (Single-Pass Aggregation)
        # ==============================================================================
        
        # 2. [Map Phase] æ‰§è¡Œå…¨é‡æ•°æ®çš„æµå¼æ‰«æ
        # å°†å®½è¡¨ unpivot åèšåˆï¼Œå¾—åˆ°æœ€å°ç²’åº¦ç»Ÿè®¡è¡¨ (Group, Feature, Bin, Count, Bad)
        logger.debug("ğŸ“Š Step 1: Scanning raw data for stats (Single Pass Map)...")
        group_stats_raw = self._agg_basic_stats(
            df_binned, group_col, target_features, self.target_col, weights_col,
            batch_size=batch_size 
        )
        
        # 3. [Reduce Phase A] è¡¥å…¨ WOE ä¿¡æ¯
        # è®¡ç®— KS/AUC ä¾èµ– WOE æ’åºã€‚è‹¥åˆ†ç®±å™¨æ—  WOEï¼Œåˆ©ç”¨ group_stats_raw å†…å­˜è®¡ç®—ï¼Œæ— éœ€æ‰«åŸè¡¨ã€‚
        self._ensure_woe_info_optimized(group_stats_raw)

        # 4. [Reduce Phase B] è·å– PSI åŸºå‡†åˆ†å¸ƒ
        # è·å– Expected Distributionã€‚è‹¥æ— å¤–éƒ¨åŸºå‡†ï¼Œå– group_stats_raw ä¸­æœ€æ—©çš„ä¸€ç»„ã€‚
        expected_dist = self._get_benchmark_dist_optimized(
            group_stats_raw, benchmark_df, group_col, target_features, weights_col
        )

        # 5. [Reduce Phase C] æ±‡æ€» Total ç»Ÿè®¡é‡
        # åœ¨å†…å­˜ä¸­å°†ä¸åŒ Group çš„ç»Ÿè®¡é‡ç´¯åŠ ï¼Œå¾—åˆ°å…¨é‡æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚
        logger.debug("âˆ‘  Step 2: Rolling up stats for Total (Reduce)...")
        total_stats_raw = (
            group_stats_raw
            .group_by(["feature", "bin_index"])
            .agg([
                pl.col("count").sum(),
                pl.col("bad").sum()
            ])
            .with_columns(pl.lit("Total").alias(group_col)) # æ˜¾å¼æ ‡è®°ä¸ºå…¨é‡
        )

        # --- 6. å‘é‡åŒ–æŒ‡æ ‡è®¡ç®— (Vectorized Metrics Calculation) ---
        logger.debug("ğŸ§® Step 3: Calculating metrics (PSI/AUC/KS/IV)...")
        
        # 6.1 è®¡ç®— Trend æ•°æ®ï¼šæ¯ä¸ªåˆ†ç»„ï¼ˆå¦‚æ¯æœˆï¼‰çš„ç‰¹å¾è¡¨ç°
        metrics_groups = self._calc_metrics_from_stats(
            group_stats_raw, expected_dist, group_col
        ).with_columns(pl.col(group_col).cast(pl.String))
        
        # 6.2 è®¡ç®— Total æ•°æ®ï¼šç‰¹å¾åœ¨å…¨é‡æ ·æœ¬ä¸Šçš„è¡¨ç°
        metrics_total = self._calc_metrics_from_stats(
            total_stats_raw, expected_dist, group_col
        )

        # 6.3 åˆå¹¶åˆ†ç»„ä¸æ€»ä½“ç»“æœ
        metrics_total = metrics_total.select(metrics_groups.columns)
        stats_long = pl.concat([metrics_total, metrics_groups])

        # --- 7. å•è°ƒæ€§æ£€æŸ¥ (Monotonicity Check) ---
        # ä½¿ç”¨æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°åˆ¤æ–­åˆ†ç®±ç´¢å¼•ä¸åç‡çš„å…³ç³»æ˜¯å¦å•è°ƒ
        logger.debug("ğŸ“‰ Step 4: Checking monotonicity...")
        monotonicity_df = (
            stats_long
            .filter((pl.col("bin_index") >= 0) & (pl.col(group_col) == "Total"))
            .group_by("feature")
            .agg(
                pl.corr("bin_index", "bad_rate", method="spearman").alias("Monotonicity")
            )
        )

        # 8. æ ¼å¼åŒ–è¾“å‡ºï¼šé‡å¡‘æ•°æ®ä¸ºæœ€ç»ˆ Report å®¹å™¨
        # æ³¨æ„ï¼š_format_report å†…éƒ¨ç°åœ¨ä¼šè°ƒç”¨ _format_output æ¥å¤„ç† Pandas è½¬æ¢
        report = self._format_report(
            stats_long, metrics_groups, metrics_total, group_col, monotonicity_df
        )

        logger.info(f"âœ… Evaluation complete. [Features: {len(target_features)} | Groups: {stats_long[group_col].n_unique() - 1}]")
        return report

    def _agg_basic_stats(
        self,
        df_binned: pl.DataFrame,
        group_col: str,
        features: List[str],
        y_col: str,
        weights_col: Optional[str],
        batch_size: int = 500  # [æ–°å¢] æ¥æ”¶æ‰¹æ¬¡å‚æ•°
    ) -> pl.DataFrame:
        """
        [Map Phase] å…¨é‡æ•°æ®æ‰«æä¸æ ¸å¿ƒèšåˆ (Batched Implementation)ã€‚

        é‡‡ç”¨ "åˆ†æ‰¹-æµå¼-èšåˆ" (Batch-Stream-Agg) ç­–ç•¥ï¼š
        1. å°†æ•°åƒä¸ªç‰¹å¾åˆ‡åˆ†ä¸ºå¤šä¸ªæ‰¹æ¬¡ (Chunk)ã€‚
        2. å¯¹æ¯ä¸ªæ‰¹æ¬¡æ„å»ºç‹¬ç«‹çš„ Lazy Query Planã€‚
        3. åˆ©ç”¨ Streaming å¼•æ“æ‰§è¡Œèšåˆï¼Œå¹¶ç«‹å³é‡Šæ”¾ä¸­é—´ç»“æœã€‚
        4. æœ€åçºµå‘åˆå¹¶ (Vertical Concat) æ‰€æœ‰æ‰¹æ¬¡çš„ç»Ÿè®¡ç»“æœã€‚

        Parameters
        ----------
        df_binned : pl.DataFrame
            å·²ç»è¿‡åˆ†ç®±ç´¢å¼•è½¬æ¢çš„æ•°æ®é›†ã€‚
        group_col : str
            åˆ†ç»„åˆ—ã€‚
        features : List[str]
            ç‰¹å¾ååˆ—è¡¨ã€‚
        y_col : str
            ç›®æ ‡å˜é‡åˆ—ã€‚
        weights_col : Optional[str]
            æƒé‡åˆ—ã€‚
        batch_size : int
            æ¯æ¬¡èšåˆå¤„ç†çš„ç‰¹å¾æ•°é‡ã€‚

        Returns
        -------
        pl.DataFrame
            é•¿è¡¨æ ¼å¼çš„ç»Ÿè®¡æ±‡æ€»è¡¨ï¼ŒåŒ…å« [group_col, feature, bin_index, count, bad]ã€‚
        """
        # 1. æ„é€ ç†è®ºä¸Šåº”è¯¥å­˜åœ¨çš„ bin åˆ—å
        theoretical_bin_cols = [f"{f}_bin" for f in features]
        
        # 2. [ä¿®å¤] è·å–å®é™…å­˜åœ¨çš„åˆ—å
        # ä½¿ç”¨ set è¿ç®—æé€Ÿè¿‡æ»¤ï¼Œé˜²æ­¢ä¼ å…¥äº†æœªè¢«åˆ†ç®±çš„ç‰¹å¾å¯¼è‡´æŠ¥é”™
        existing_cols = set(df_binned.columns)
        actual_bin_cols = [c for c in theoretical_bin_cols if c in existing_cols]
        
        # è®°å½•ä¸¢å¤±çš„åˆ—ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
        missing_cols = set(theoretical_bin_cols) - set(actual_bin_cols)
        if missing_cols:
            logger.warning(f"âš ï¸ {len(missing_cols)} features were not binned and will be skipped in evaluation. Examples: {list(missing_cols)[:3]}")
            
        if not actual_bin_cols:
            raise ValueError("âŒ No valid binned columns found in dataframe. Check your binner fit results.")

        # ä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—è¿›è¡Œåç»­æ“ä½œ
        bin_cols = actual_bin_cols
        
        # ç¡®å®šå¿…é¡»è¦ä¿ç•™çš„ç´¢å¼•åˆ— (Group, Target, Weight)
        index_cols = [group_col, y_col]
        if weights_col:
            index_cols.append(weights_col)

        # é¢„å®šä¹‰èšåˆè¡¨è¾¾å¼ (Lazy Expr)ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤æ„å»º
        # 1. ç»Ÿè®¡æ ·æœ¬æ•° (Count)
        expr_count = pl.col(weights_col).sum() if weights_col else pl.len()
        # 2. ç»Ÿè®¡åæ ·æœ¬æ•° (Bad)
        expr_bad = (pl.col(y_col) * pl.col(weights_col)).sum() if weights_col else pl.col(y_col).sum()
        
        agg_exprs = [
            expr_count.alias("count"),
            expr_bad.alias("bad")
        ]

        result_frames: List[pl.DataFrame] = []
        total_batches = (len(bin_cols) + batch_size - 1) // batch_size

        # æ ¸å¿ƒå¾ªç¯ï¼šåˆ†æ‰¹å¤„ç†ç‰¹å¾
        for i in range(0, len(bin_cols), batch_size):
            # 1. åˆ‡ç‰‡ï¼šè·å–å½“å‰æ‰¹æ¬¡çš„ç‰¹å¾åˆ—
            batch_bins = bin_cols[i : i + batch_size]
            
            # 2. é€‰æ‹©ï¼šä»…é€‰å–å½“å‰æ‰¹æ¬¡éœ€è¦çš„åˆ— + ç´¢å¼•åˆ—
            cols_to_select = index_cols + batch_bins
            
            # 3. æ„é€ æŸ¥è¯¢è®¡åˆ’ (Lazy Plan)
            # è¿™é‡Œçš„ .lazy() å¾ˆå…³é”®ï¼Œå®ƒå…è®¸ Polars ä¼˜åŒ–å™¨ä»…é’ˆå¯¹å½“å‰åˆ‡ç‰‡è¿›è¡Œå†…å­˜è§„åˆ’
            batch_res = (
                df_binned.lazy()
                .select(cols_to_select)
                .unpivot(
                    index=index_cols, 
                    on=batch_bins, 
                    variable_name="feature_bin", 
                    value_name="bin_index"
                )
                # è¿˜åŸåŸå§‹ç‰¹å¾å (å»é™¤ _bin åç¼€)
                .with_columns(
                    pl.col("feature_bin").str.replace("_bin", "").alias("feature")
                )
                # èšåˆè‡³æœ€å°ç²’åº¦ï¼š(Group x Feature x Bin)
                .group_by([group_col, "feature", "bin_index"])
                .agg(agg_exprs)
                # æ‰§è¡Œå¹¶ç‰©åŒ– (Streaming æ¨¡å¼é˜²æ­¢å¤§èšåˆ OOM)
                .collect(streaming=True)
            )
            
            result_frames.append(batch_res)
            # logger.debug(f"   ... Processed batch {i // batch_size + 1}/{total_batches}")

        if not result_frames:
            return pl.DataFrame()

        # 4. åˆå¹¶ç»“æœï¼šå°†æ‰€æœ‰æ‰¹æ¬¡çš„å°è¡¨ (Reduced Tables) çºµå‘åˆå¹¶
        return pl.concat(result_frames)

    def _ensure_woe_info_optimized(self, group_stats_raw: pl.DataFrame):
        """
        [Optimization] å†…å­˜å†… WOE è¡¥å…¨ã€‚

        å¦‚æœè¯„ä¼°å™¨ç¼ºä¹ WOE ä¿¡æ¯ï¼ˆå¦‚ç›´æ¥ä¼ å…¥è¯„ä¼°è€Œéè®­ç»ƒï¼‰ï¼Œåˆ©ç”¨å·²èšåˆçš„å°è¡¨è®¡ç®— WOEã€‚
        
        Formula:
        WOE = ln( (Bad_i / Total_Bad) / (Good_i / Total_Good) )
        """
        features = group_stats_raw["feature"].unique().to_list()
        missing_woe_feats = [
            f for f in features 
            if f not in self.binner.bin_woes_ or not self.binner.bin_woes_[f]
        ]
        
        if not missing_woe_feats:
            return

        logger.debug(f"âš¡ Calculating missing WOEs for {len(missing_woe_feats)} features (Memory Optimized)...")
        
        # ä»…å¯¹ç¼ºå¤±ç‰¹å¾è¿›è¡Œèšåˆ
        target_stats = group_stats_raw.filter(pl.col("feature").is_in(missing_woe_feats))
        
        global_bin_stats = (
            target_stats
            .group_by(["feature", "bin_index"])
            .agg([
                pl.col("count").sum().alias("n"),
                pl.col("bad").sum().alias("b")
            ])
        )
        
        feature_totals = (
            global_bin_stats.group_by("feature")
            .agg([
                pl.col("n").sum().alias("total_n"),
                pl.col("b").sum().alias("total_b")
            ])
        )
        
        epsilon = 1e-9 # é˜²æ­¢åˆ†æ¯ä¸º 0
        woe_df = (
            global_bin_stats
            .join(feature_totals, on="feature", how="left")
            .with_columns([
                (pl.col("n") - pl.col("b")).alias("g"),
                (pl.col("total_n") - pl.col("total_b")).alias("total_g")
            ])
            .with_columns([
                (((pl.col("b") + epsilon) / (pl.col("total_b") + epsilon)) / 
                 ((pl.col("g") + epsilon) / (pl.col("total_g") + epsilon))).log().alias("woe")
            ])
        )
        
        # å°†è®¡ç®—å‡ºçš„ WOE å›å¡«è‡³åˆ†ç®±å™¨å¯¹è±¡ä¸­ä»¥ä¾¿åç»­ä½¿ç”¨
        for (feat,), sub_df in woe_df.partition_by("feature", as_dict=True).items():
            self.binner.bin_woes_[feat] = dict(zip(sub_df["bin_index"].to_list(), sub_df["woe"].to_list()))

    def _get_benchmark_dist_optimized(
        self, 
        group_stats_raw: pl.DataFrame, 
        bench_df: Optional[pl.DataFrame], 
        group_col: str, 
        features: List[str], 
        w_col: str
    ) -> pl.DataFrame:
        """
        [Optimization] è·å–ç”¨äº PSI è®¡ç®—çš„åŸºå‡†åˆ†å¸ƒã€‚

        ç­–ç•¥ï¼š
        - è‹¥ä¼ å…¥å¤–éƒ¨ bench_dfï¼šéœ€å¯¹å…¶æ‰§è¡Œ transform å¹¶èšåˆã€‚
        - è‹¥æœªä¼ å…¥ï¼šå–å½“å‰æ•°æ®ä¸­æœ€æ—©çš„æ—¶é—´åˆ‡ç‰‡ä½œä¸ºåŸºå‡†ã€‚
        """
        if bench_df is not None:
            # Case A: å¤„ç†å¤–éƒ¨åŸºå‡†é›† (æ¶‰åŠ I/O ä¸è®¡ç®—)
            bench_binned = self.binner.transform(bench_df, return_type="index")
            bin_cols = [f"{f}_bin" for f in features]
            agg_expr = pl.col(w_col).sum().alias("N_E") if w_col else pl.len().alias("N_E")
            idx_cols = [w_col] if w_col else []
            
            return (
                bench_binned.select(bin_cols + idx_cols)
                .unpivot(index=idx_cols, on=bin_cols, variable_name="feat_bin", value_name="bin_index")
                .with_columns(pl.col("feat_bin").str.replace("_bin", "").alias("feature"))
                .group_by(["feature", "bin_index"])
                .agg(agg_expr)
                .with_columns((pl.col("N_E") / pl.col("N_E").sum().over("feature")).alias("expected_dist"))
                .select(["feature", "bin_index", "expected_dist"])
            )
        else:
            # Case B: å†…éƒ¨åŸºå‡† (é›¶ I/Oï¼Œç›´æ¥åˆ‡ç‰‡)
            min_group = group_stats_raw.select(pl.col(group_col).min()).item()
            logger.debug(f"ğŸ“… Using earliest group '{min_group}' as baseline (from stats cache).")
            
            return (
                group_stats_raw
                .filter(pl.col(group_col) == min_group)
                .group_by(["feature", "bin_index"])
                .agg(pl.col("count").sum().alias("N_E"))
                .with_columns((pl.col("N_E") / pl.col("N_E").sum().over("feature")).alias("expected_dist"))
                .select(["feature", "bin_index", "expected_dist"])
            )

    def _calc_metrics_from_stats(self, stats_df: pl.DataFrame, expected_dist: pl.DataFrame, group_col: str) -> pl.DataFrame:
        """
        [Math Core] åŸºäºèšåˆç»“æœçš„å‘é‡åŒ–æŒ‡æ ‡è®¡ç®—å¼•æ“ã€‚

        é€šè¿‡ Polars çš„çª—å£å‡½æ•°å’Œç´¯åŠ æ“ä½œï¼Œåœ¨ O(N) æ—¶é—´å†…å®Œæˆæ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡è®¡ç®—ã€‚

        Metrics:
        - PSI: (Actual% - Expected%) * ln(Actual% / Expected%)
        - IV: (Good% - Bad%) * WOE
        - KS: Max(|CumBad% - CumGood%|)
        - AUC: åŸºäºæ¢¯å½¢æ³•åˆ™çš„æ•°å€¼ç§¯åˆ†
        - Lift: åˆ†ç®±åç‡ / æ€»ä½“åç‡
        """
        # 1. æ„å»º WOE æ˜ å°„è¡¨
        woe_data = [
            {"feature": f, "bin_index": i, "woe": w}
            for f, m in self.binner.bin_woes_.items() for i, w in m.items()
        ]
        schema = {"feature": pl.String, "bin_index": pl.Int16, "woe": pl.Float64}
        woe_df = pl.DataFrame(woe_data, schema=schema) if woe_data else pl.DataFrame([], schema=schema)

        # 2. åŸºç¡€å…³è”ï¼šåˆå¹¶ç»Ÿè®¡é‡ã€åŸºå‡†åˆ†å¸ƒä¸ WOE
        base_df = (
            stats_df
            .join(expected_dist, on=["feature", "bin_index"], how="left")
            .join(woe_df, on=["feature", "bin_index"], how="left")
            .with_columns([
                (pl.col("count") - pl.col("bad")).alias("good"), 
                pl.col("expected_dist").fill_null(1e-9), 
                pl.col("woe").fill_null(0)               
            ])
        )

        epsilon = 1e-9
        
        # 3. è®¡ç®—åˆ†ç»„æ±‡æ€»å€¼ (Window Functions)
        base_df = base_df.with_columns([
            pl.col("count").sum().over([group_col, "feature"]).alias("total_count"),
            pl.col("bad").sum().over([group_col, "feature"]).alias("total_bad"),
            pl.col("good").sum().over([group_col, "feature"]).alias("total_good"),
        ])

        # 4. è®¡ç®—å æ¯”æŒ‡æ ‡
        base_df = base_df.with_columns([
            ((pl.col("count") + epsilon) / (pl.col("total_count") + epsilon)).alias("actual_dist"),
            (pl.col("bad") / (pl.col("total_bad") + epsilon)).alias("bad_dist"),    
            (pl.col("good") / (pl.col("total_good") + epsilon)).alias("good_dist"), 
            (pl.col("bad") / (pl.col("count") + epsilon)).alias("bad_rate"),        
        ])

        # 5. è®¡ç®— PSI å’Œ Lift (æ— åºæŒ‡æ ‡)
        base_df = base_df.with_columns([
            ((pl.col("actual_dist") - pl.col("expected_dist")) * (pl.col("actual_dist") / pl.col("expected_dist")).log()).alias("psi_bin"),
            (pl.col("bad_rate") / ((pl.col("total_bad") + epsilon) / (pl.col("total_count") + epsilon))).alias("lift")  
        ])

        # 6. è®¡ç®—æœ‰åºæŒ‡æ ‡ (AUC, KS, IV)ï¼šå…³é”®åœ¨äºå¿…é¡»æŒ‰ WOE é£é™©ç¨‹åº¦æ’åº
        sorted_df = base_df.sort([group_col, "feature", "woe"])

        # ç´¯ç§¯åˆ†å¸ƒç”¨äºè®¡ç®— KS å’Œ AUC
        sorted_df = sorted_df.with_columns([
            pl.col("bad_dist").cum_sum().over([group_col, "feature"]).alias("cum_bad_dist"),
            pl.col("good_dist").cum_sum().over([group_col, "feature"]).alias("cum_good_dist"),
        ])

        sorted_df = sorted_df.with_columns([
            # KS = |CumBad - CumGood|
            (pl.col("cum_bad_dist") - pl.col("cum_good_dist")).abs().alias("ks_bin"),
            
            # AUC æ¢¯å½¢æ³•åˆ™è®¡ç®—é¢ç§¯
            ((pl.col("cum_good_dist") - pl.col("cum_good_dist").shift(1, fill_value=0).over([group_col, "feature"])) * (pl.col("cum_bad_dist") + pl.col("cum_bad_dist").shift(1).over([group_col, "feature"]).fill_null(0)) / 2
            ).alias("auc_bin"),

            # IV å…¬å¼ï¼š(Good% - Bad%) * ln(Good%/Bad%)
            ((pl.col("good_dist") - pl.col("bad_dist")) * ((pl.col("good_dist") + epsilon) / (pl.col("bad_dist") + epsilon)).log()).alias("iv_bin")
        ])

        return sorted_df

    def _prepare_context(self, df: pl.DataFrame, profile_by: Optional[str], dt_col: Optional[str]) -> Tuple[pl.DataFrame, str]:
        """
        [Helper] ä¸Šä¸‹æ–‡å‡†å¤‡ï¼šå¤„ç†æ—¥æœŸæˆªæ–­é€»è¾‘ä¸åˆ†ç»„åˆ—å…œåº•ã€‚
        
        ç­–ç•¥ä¼˜å…ˆçº§ï¼š
        1. [æ™ºèƒ½é»˜è®¤] è‹¥æœ‰ dt_col ä½†æ—  profile_by -> é»˜è®¤æŒ‰ 'month' èšåˆã€‚
        2. [è‡ªåŠ¨èšåˆ] è‹¥æœ‰ dt_col ä¸” profile_by ä¸º day/week/month -> æ‰§è¡Œæ—¥æœŸæˆªæ–­ç”Ÿæˆæ–°åˆ—ã€‚
        3. [å¸¸è§„åˆ†ç»„] è‹¥æœ‰ profile_by -> ç›´æ¥ä½¿ç”¨è¯¥åˆ—ã€‚
        4. [å…œåº•é€»è¾‘] è‹¥å•¥éƒ½æ²¡ -> ç”Ÿæˆ 'Total' å¸¸é‡åˆ—ï¼Œè§†ä¸ºå•ç‚¹è¯„ä¼°ã€‚
        """
        
        # 1. [æ™ºèƒ½é»˜è®¤] æœ‰æ—¶é—´åˆ—æ²¡åˆ†ç»„ -> é»˜è®¤æŒ‰æœˆ
        if dt_col and not profile_by:
            logger.info(f"â„¹ï¸ `dt_col` provided ('{dt_col}') without `profile_by`. Defaulting trend to 'month'.")
            profile_by = "month"

        # 2. [è‡ªåŠ¨èšåˆ] å¤„ç†æ—¶é—´åˆ‡ç‰‡
        if dt_col and profile_by in ["day", "week", "month"]:
            if profile_by == "month":
                date_expr = MarsDate.dt2month(dt_col)
            elif profile_by == "week":
                date_expr = MarsDate.dt2week(dt_col)
            else:
                date_expr = MarsDate.dt2day(dt_col)
            
            temp_group = f"_mars_auto_{profile_by}"
            # è¿™é‡Œç”Ÿæˆä¸€ä¸ªæ–°çš„ä¸´æ—¶åˆ—ä½œä¸ºåˆ†ç»„åˆ—
            return df.with_columns(date_expr.alias(temp_group)), temp_group
        
        # 3. [å¸¸è§„åˆ†ç»„] ç”¨æˆ·æŒ‡å®šäº†ç°æœ‰çš„åˆ— (æ¯”å¦‚ 'city', 'channel')
        if profile_by:
            # ç®€å•çš„æ ¡éªŒï¼Œé˜²æ­¢ç”¨æˆ·æ‹¼å†™é”™è¯¯
            if profile_by not in df.columns:
                # æ­¤æ—¶å¯èƒ½æ˜¯ç”¨æˆ·æƒ³æŒ‰æœˆåˆ†ï¼Œä½†æ²¡ä¼  dt_colï¼Œæˆ–è€…å•çº¯å†™é”™äº†
                # ä¸ºäº†ä¸æŠ¥é”™ï¼Œè¿™é‡Œè¿˜æ˜¯ warn ä¸€ä¸‹æ¯”è¾ƒå¥½ï¼Œæˆ–è€…è®©å®ƒåœ¨åç»­ unpivot æ—¶æŠ¥é”™
                pass 
            return df, profile_by

        # 4. [å…œåº•é€»è¾‘] ç”¨æˆ·å•¥éƒ½æ²¡ä¼  -> è§†ä¸ºå•ç‚¹è¯„ä¼° (Snapshot)
        logger.info("â„¹ï¸ No grouping specified. Evaluating as a single snapshot (Group='Total').")
        fallback_col = "_mars_auto_total"
        return df.with_columns(pl.lit("Total").alias(fallback_col)), fallback_col

    def _format_report(
        self, 
        stats_long: pl.DataFrame, 
        metrics_groups: pl.DataFrame, 
        metrics_total: pl.DataFrame, 
        group_col: str, 
        monotonicity_df: pl.DataFrame
    ) -> "MarsEvaluationReport":
        """
        [Helper] æŠ¥å‘Šæ„é€ ä¸ç‰¹å¾ç¨³å®šæ€§å®¡è®¡å¼•æ“ã€‚

        è¯¥æ–¹æ³•è´Ÿè´£å°† `evaluate` é˜¶æ®µäº§å‡ºçš„å‘é‡åŒ–è®¡ç®—é•¿è¡¨é‡å¡‘ä¸ºå…·å¤‡ä¸šåŠ¡å†³ç­–æ·±åº¦çš„ä¸‰å±‚æŠ¥è¡¨ä½“ç³»ï¼š
        æ˜ç»†å±‚ (Detail)ã€å®¡è®¡å±‚ (Summary) å’Œè¶‹åŠ¿å±‚ (Trend)ã€‚

        Parameters
        ----------
        stats_long : pl.DataFrame
            å…¨é‡åˆ†ç®±ç»Ÿè®¡é•¿è¡¨ã€‚åŒ…å«æ¯ä¸ªç‰¹å¾ã€æ¯ä¸ªåˆ†ç»„ã€æ¯ä¸ªåˆ†ç®±çš„åŸå§‹ç»Ÿè®¡é‡åŠåˆ†ç®±çº§æŒ‡æ ‡ã€‚
            Schema åŒ…å«: [feature, group_col, bin_index, iv_bin, ks_bin, psi_bin, ...]
        metrics_groups : pl.DataFrame
            ä»…åŒ…å«åˆ†ç»„æ•°æ®çš„é•¿è¡¨ã€‚ç”¨äºè®¡ç®—è·¨æœŸç¨³å®šæ€§ã€‚
        metrics_total : pl.DataFrame
            ä»…åŒ…å«å…¨é‡ï¼ˆTotalï¼‰ç»Ÿè®¡çš„æ•°æ®ã€‚ç”¨äºè·å–ç‰¹å¾å…¨å±€åŒºåˆ†åº¦ã€‚
        group_col : str
            åˆ†ç»„ç»´åº¦åˆ—åï¼ˆå¦‚ 'month'ï¼‰ã€‚
        monotonicity_df : pl.DataFrame
            å•è°ƒæ€§æ£€æŸ¥ç»“æœã€‚åŒ…å«ç‰¹å¾åˆ†ç®±ç´¢å¼•ä¸åç‡çš„æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°ã€‚

        Returns
        -------
        MarsEvaluationReport
            æŠ¥å‘Šå®¹å™¨å®ä¾‹ã€‚åŒ…å« Summary, Trend, Detail ä¸‰å¼ é‡å¡‘åçš„æŠ¥è¡¨ã€‚
            **[Pandas å…¼å®¹]**ï¼šå¦‚æœ evaluate è¾“å…¥ä¸º Pandasï¼Œåˆ™è¿”å›çš„ Report å†…éƒ¨å±æ€§ä¹Ÿä¼šè‡ªåŠ¨è½¬ä¸º Pandasã€‚

        Notes
        -----
        **1. æ˜ç»†è¡¨ (Detail Table) é€»è¾‘**
        å°† `stats_long` ä¸åˆ†ç®±å™¨ä¸­çš„ä¸šåŠ¡æ ‡ç­¾æ˜ å°„è¡¨ (`bin_mappings_`) å…³è”ï¼Œå°†ç‰©ç†ç´¢å¼•ï¼ˆå¦‚ 0, 1ï¼‰è½¬æ¢ä¸º
        ä¸šåŠ¡å¯è¯»æ ‡ç­¾ï¼ˆå¦‚ '[20, 30)'ï¼‰ã€‚è¯¥è¡¨ç”¨äºä¸‹é’»åˆ†æç‰¹å¾åœ¨ç‰¹å®šæ—¶é—´ç‚¹çš„å…·ä½“åˆ†å¸ƒã€‚

        **2. å®¡è®¡æ±‡æ€»è¡¨ (Summary Table) æ ¸å¿ƒæŒ‡æ ‡é€»è¾‘**
        è¿™æ˜¯ Mars åº“çš„æ ¸å¿ƒå†³ç­–ä¾æ®ï¼ŒåŒ…å«ä»¥ä¸‹å®¡è®¡æŒ‡æ ‡ï¼š
        
        * **ç¨³å®šæ€§å®¡è®¡ (Stability Audit)**:
            - `IV_cv` (IV å˜å¼‚ç³»æ•°): $\frac{\sigma(IV_{period})}{\mu(IV_{period}) + \epsilon}$ã€‚
              åæ˜ ç‰¹å¾åŒºåˆ†èƒ½åŠ›çš„æ³¢åŠ¨ç¨‹åº¦ã€‚CV > 0.4 é€šå¸¸æ„å‘³ç€ç‰¹å¾åœ¨ä¸åŒæ ·æœ¬æ®µè¡¨ç°æå…¶ä¸ç¨³å®šã€‚
            - `PSI_max`: è·¨åˆ†ç»„ä¸­æœ€å¤§çš„ PSI å€¼ã€‚è¯†åˆ«ç‰¹å¾åˆ†å¸ƒåç§»çš„æœ€åæƒ…å†µã€‚
            - `RC_min` (æœ€å°é£é™©ä¸€è‡´æ€§): è·¨æœŸ `RiskCorr` çš„æœ€å°å€¼ã€‚
              è‹¥ `RC_min < 0.7`ï¼Œè¯´æ˜ç‰¹å¾é€»è¾‘åœ¨æŸäº›æœˆä»½å‘ç”Ÿäº†åè½¬æˆ–å´©æºƒã€‚

        * **æ•ˆèƒ½å®¡è®¡ (Efficiency Audit)**:
            - `IV_total`: ç‰¹å¾åœ¨å…¨é‡æ ·æœ¬ä¸Šçš„æ€»ä¿¡æ¯å€¼ã€‚
            - `Efficiency_Score` (æ•ˆèƒ½å¾—åˆ†): $\frac{IV_{avg} \times RC_{avg}}{1 + IV_{cv}}$ã€‚
              è¿™æ˜¯ Mars ç‹¬æœ‰çš„æ€§ä»·æ¯”æŒ‡æ ‡ã€‚å®ƒå¥–åŠ±â€œé«˜ IVã€é«˜é€»è¾‘ç¨³å®šæ€§ã€ä½æ³¢åŠ¨â€çš„ç‰¹å¾ã€‚
            - `Monotonicity`: ç‰¹å¾åœ¨å…¨å±€ç»´åº¦ä¸‹çš„åç‡å•è°ƒæ€§å¾—åˆ† (-1 ~ 1)ã€‚

        * **è‡ªåŠ¨åŒ–å»ºè®® (Mars_Decision)**:
            - `âŒ Drop: Logical Inversion`: é£é™©ä¸€è‡´æ€§è§¦ç¢°åº•çº¿ (RC < 0.7)ã€‚
            - `âš ï¸ Watch: High Drift`: æ ·æœ¬åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆåç§» (PSI > 0.25)ã€‚
            - `ğŸ—‘ï¸ Drop: Low IV`: ç‰¹å¾åŒºåˆ†åº¦è¿‡ä½ (IV < 0.02)ã€‚
            - `âœ… Keep: Stable & Strong`: æ»¡è¶³æ‰€æœ‰ç¨³å®šæ€§ä¸å¼ºåº¦è¦æ±‚çš„ä¼˜è´¨ç‰¹å¾ã€‚

        **3. é£é™©ä¸€è‡´æ€§ç›¸å…³æ€§ (RiskCorr/RC) è®¡ç®—é€»è¾‘**
        $$RC_{group\_i} = \text{Pearson\_Corr}(\vec{BR}_{baseline}, \vec{BR}_{group\_i})$$
        å…¶ä¸­ $\vec{BR}$ æ˜¯å„åˆ†ç®±åç‡ç»„æˆçš„å‘é‡ã€‚é€‰å–æœ€æ—©çš„ä¸€ä¸ªåˆ†ç»„ä½œä¸ºåŸºå‡†ã€‚
        RC è¡¡é‡äº†ç‰¹å¾çš„â€œé£é™©æ’åºâ€éšæ—¶é—´å˜åŒ–çš„ç¨³å®šæ€§ã€‚RC æ¥è¿‘ 1 è¯´æ˜â€œå¥½äººå§‹ç»ˆæ˜¯å¥½äººï¼Œåäººå§‹ç»ˆæ˜¯åäººâ€ã€‚

        **4. è¶‹åŠ¿é€è§†è¡¨ (Trend Tables) é€»è¾‘**
        å°†å„é¡¹æŒ‡æ ‡ï¼ˆPSI, AUC, KS, IV, BadRate, RiskCorrï¼‰ä»¥ `feature` ä¸ºè¡Œï¼Œ`group_col` ä¸ºåˆ—è¿›è¡Œ `Pivot` è½¬æ¢ï¼Œ
        ç”Ÿæˆå®½è¡¨ï¼Œä¸“é—¨ç”¨äºåœ¨ Jupyter ä¸­æ¸²æŸ“é¢œè‰²çƒ­åŠ›å›¾ã€‚
        """
        # 1. æ˜ å°„åˆ†ç®± Label (ä»æ•°å­—ç´¢å¼•æ˜ å°„å›å¯è¯»çš„èŒƒå›´æè¿°)
        map_rows = []
        feats = set(stats_long["feature"].unique().to_list())
        for f, m in self.binner.bin_mappings_.items():
            if f in feats:
                for i, l in m.items(): map_rows.append({"feature":f, "bin_index":i, "bin_label":l})
        
        map_schema = {"feature": pl.String, "bin_index": pl.Int16, "bin_label": pl.String}
        map_df = pl.DataFrame(map_rows, schema=map_schema) if map_rows else pl.DataFrame([], schema=map_schema)

        # 2. Detail Table: è¯¦ç»†åˆ†ç®±è¡¨å…³è”
        detail_table = (
            stats_long
            .join(map_df, on=["feature", "bin_index"], how="left")
            .with_columns(pl.col("bin_label").fill_null(pl.col("bin_index").cast(pl.Utf8)))
            .select(["feature", group_col, "bin_index", "bin_label", "count", "bad", "bad_rate", "lift", "psi_bin", "ks_bin", "auc_bin", "iv_bin", "total_count"])
            .sort(["feature", group_col, "bin_index"])
        )

        # --- 3. [è®¡ç®—å‰ç½®] è®¡ç®— RiskCorr (RC) è·¨æœŸç¨³å®šæ€§é€»è¾‘ ---
        # ç¡®å®šåŸºå‡†åºåˆ— (é€‰å–æ—¶é—´æœ€æ—©çš„ä¸€ç»„)
        first_group = metrics_groups.select(pl.col(group_col).min()).item()
        baseline_df = (
            metrics_groups
            .filter((pl.col(group_col) == first_group) & (pl.col("bin_index") >= 0))
            .select(["feature", "bin_index", "bad_rate"])
            .rename({"bad_rate": "base_br"})
        )

        # æ„é€ åŒ…å« Group å’Œ Total çš„å…¨é‡æ•°æ®æµç”¨äºè®¡ç®— RC
        all_metrics_for_corr = pl.concat([
            metrics_groups.select(["feature", group_col, "bin_index", "bad_rate"]),
            metrics_total.select(["feature", group_col, "bin_index", "bad_rate"]) 
        ])

        # è®¡ç®— RiskCorr é•¿è¡¨: [feature, group_col, risk_corr]
        risk_corr_long = (
            all_metrics_for_corr
            .filter(pl.col("bin_index") >= 0)
            .join(baseline_df, on=["feature", "bin_index"], how="left")
            .group_by(["feature", group_col])
            .agg(
                pl.corr("bad_rate", "base_br", method="pearson").alias("risk_corr")
            )
        )

        # --- 4. Summary Table: ç¨³å®šæ€§ä¸æ•ˆèƒ½å®¡è®¡æ ¸å¿ƒæ±‡æ€» ---
        
        # 4.1 A. åˆ†ç»„æŒ‡æ ‡èšåˆ [feature, group_col] -> æœˆåº¦æŒ‡æ ‡æ€»å’Œ
        group_level_metrics = (
            metrics_groups
            .group_by(["feature", group_col])
            .agg([
                pl.col("iv_bin").sum().alias("iv"),
                pl.col("auc_bin").sum().alias("auc"),
                pl.col("psi_bin").sum().alias("psi"),
            ])
            # [ä¿®æ­£] ç¡®ä¿åˆ†ç»„çº§åˆ«çš„ AUC å§‹ç»ˆ >= 0.5ï¼Œå¦åˆ™ä¼šå¹²æ‰°åç»­ Summary çš„å¹³å‡å€¼è®¡ç®—
            .with_columns(
                pl.when(pl.col("auc") < 0.5).then(pl.lit(1) - pl.col("auc")).otherwise(pl.col("auc")).alias("auc")
            )
            .join(risk_corr_long, on=["feature", group_col], how="left")
        )

        # 4.2 B. è·¨æœŸç¨³å®šæ€§è®¡ç®— [feature] -> å®¡è®¡æ±‡æ€»
        summary_audit = (
            group_level_metrics
            .group_by("feature")
            .agg([
                pl.col("iv").mean().alias("IV_avg"),
                (pl.col("iv").std() / (pl.col("iv").mean() + 1e-9)).alias("IV_cv"),
                pl.col("auc").mean().alias("AUC_avg"),
                pl.col("auc").std().alias("AUC_std"),
                pl.col("psi").max().alias("PSI_max"), 
                pl.col("psi").mean().alias("PSI_avg"),
                pl.col("risk_corr").min().alias("RC_min"), 
                pl.col("risk_corr").mean().alias("RC_avg")
            ])
        )

        # 4.3 C. å…³è”å…¨é‡æŒ‡æ ‡ä¸å•è°ƒæ€§ï¼Œç”Ÿæˆæœ€ç»ˆå®¡è®¡å†³ç­–
        total_metrics_agg = (
            metrics_total.group_by("feature")
            .agg([
                pl.col("iv_bin").sum().alias("IV_total"),
                pl.col("ks_bin").max().alias("KS_total"),
                pl.col("auc_bin").sum().alias("AUC_total")
            ])
            # [ä¿®å¤ç‚¹] ç¡®ä¿å…¨é‡ AUC ä¹Ÿæ˜¯æ–¹å‘ä¿®æ­£åçš„
            .with_columns(
                pl.when(pl.col("AUC_total") < 0.5).then(pl.lit(1) - pl.col("AUC_total")).otherwise(pl.col("AUC_total")).alias("AUC_total")
            )
        )

        summary_df = (
            summary_audit
            .join(total_metrics_agg, on="feature", how="left")
            .join(monotonicity_df, on="feature", how="left")
            # æ•ˆèƒ½å¾—åˆ†è®¡ç®—
            .with_columns(
                ((pl.col("IV_avg") * pl.col("RC_avg")) / (1 + pl.col("IV_cv"))).alias("Efficiency_Score")
            )
            # æ‰§è¡Œä¸“å®¶å»ºè®®è§„åˆ™
            .with_columns(
                pl.when(pl.col("RC_min") < 0.7).then(pl.lit("âŒ Drop: Logical Inversion"))
                .when(pl.col("PSI_max") > 0.25).then(pl.lit("âš ï¸ Watch: High Drift"))
                .when(pl.col("IV_total") < 0.02).then(pl.lit("ğŸ—‘ï¸ Drop: Low IV"))
                .when(pl.col("IV_cv") > 0.4).then(pl.lit("ğŸ“‰ Review: High Volatility"))
                .otherwise(pl.lit("âœ… Keep: Stable & Strong"))
                .alias("Mars_Decision")
            )
            .sort("Efficiency_Score", descending=True)
            .with_columns(pl.lit("Float64").alias("dtype"))
        )

        # --- 5. Trend Tables: å®½è¡¨çƒ­åŠ›å›¾æ•°æ®æ„é€  ---
        trend_tables = {}
        target_metrics = ["psi", "auc", "ks", "iv", "bad_rate", "risk_corr"] 
        
        for metric in target_metrics:
            if metric == "risk_corr":
                pivot_src = risk_corr_long
            else:
                if metric == "bad_rate": 
                    agg_func = (pl.col("bad").sum() / (pl.col("count").sum() + 1e-9))
                elif metric == "ks": 
                    agg_func = pl.col(f"{metric}_bin").max()
                else: 
                    agg_func = pl.col(f"{metric}_bin").sum()

                pivot_src = stats_long.group_by([group_col, "feature"]).agg(agg_func.alias(metric))
                
                # [ä¿®æ­£] ç¡®ä¿åœ¨ Pivot ä¹‹å‰ï¼Œæ‰€æœ‰æŒ‡æ ‡ï¼ˆåŒ…æ‹¬ Total è¡Œå’Œåˆ†ç»„è¡Œï¼‰éƒ½è¿›è¡Œäº†æ–¹å‘æ ¡æ­£
                if metric == "auc":
         
                    pivot_src = pivot_src.with_columns(
                        pl.when(pl.col(metric) < 0.5).then(pl.lit(1) - pl.col(metric)).otherwise(pl.col(metric)).alias(metric)
                    )

            # æ‰§è¡Œ Pivot é‡å¡‘
            pivot_df = pivot_src.pivot(
                index="feature", on=group_col, values=metric
            ).sort("feature").with_columns(pl.lit("Float64").alias("dtype"))
            
            # æ’åºåˆ—é¡ºåºï¼Œç¡®ä¿ Total åœ¨æœ€å³
            cols = [c for c in pivot_df.columns if c not in ["feature", "dtype"]]
            sorted_cols = sorted([c for c in cols if c != "Total"]) + (["Total"] if "Total" in cols else [])
            
            # [Pandas å…¼å®¹] åœ¨è¿™é‡Œå¯¹ dict ä¸­çš„æ¯ä¸ª df è¿›è¡Œè¾“å‡ºæ ¼å¼åŒ–
            trend_tables[metric] = self._format_output(pivot_df.select(["feature", "dtype"] + sorted_cols))

        # [Pandas å…¼å®¹] åˆ©ç”¨ self._format_output å¤„ç†æœ€ç»ˆè¿”å›çš„ DataFrame
        # MarsEvaluationReport æ¥æ”¶è½¬æ¢åçš„ Pandas DF æˆ– Polars DF
        return MarsEvaluationReport(
            summary_table=self._format_output(summary_df), 
            trend_tables=trend_tables, 
            detail_table=self._format_output(detail_table), 
            group_col=group_col
        )
    
    def plot_feature_binning_risk_trends(
        self,
        report: Optional["MarsEvaluationReport"] = None,
        df_detail: Union[pl.DataFrame, pd.DataFrame, None] = None, # Modified: æ”¯æŒ Pandas è¾“å…¥
        features: Union[str, List[str], None] = None,
        group_col: Optional[str] = None, 
        sort_by: str = "iv",
        ascending: bool = False,
        dpi: int = 150
    ):
        """
        [Visualization] æ‰¹é‡ç»˜åˆ¶ç‰¹å¾åˆ†ç®±é£é™©è¶‹åŠ¿å›¾ã€‚

        è¯¥å›¾è¡¨å±•ç¤ºäº†ç‰¹å¾åˆ†ç®±åœ¨ä¸åŒæ—¶é—´åˆ‡ç‰‡ä¸‹çš„æ ·æœ¬å æ¯”å’Œåç‡è¡¨ç°ï¼Œæ˜¯ç‰¹å¾ç­›é€‰æœ€ç›´è§‚çš„ä¾æ®ã€‚

        Parameters
        ----------
        report : MarsEvaluationReport, optional
            ç”± evaluate ç”Ÿæˆçš„æŠ¥å‘Šå¯¹è±¡ã€‚
        df_detail : Union[pl.DataFrame, pd.DataFrame], optional
            ç›´æ¥ä¼ å…¥æ˜ç»†è¡¨ï¼ˆè‹¥æœªæä¾› reportï¼‰ã€‚æ”¯æŒ Pandas DataFrameã€‚
        features : str or List[str], optional
            è¦ç»˜å›¾çš„ç‰¹å¾åã€‚è‹¥ä¸º Noneï¼Œç»˜åˆ¶æ˜ç»†è¡¨ä¸­æ‰€æœ‰ç‰¹å¾ã€‚
        group_col : str, optional
            åˆ†ç»„åˆ—åã€‚
        sort_by : str, default "iv"
            ç‰¹å¾å±•ç¤ºçš„æ’åºæŒ‡æ ‡ã€‚
        ascending : bool, default False
            æ˜¯å¦å‡åºæ’åˆ—ã€‚
        dpi : int, default 150
            å›¾åƒåˆ†è¾¨ç‡ã€‚
        """
        target_df = None
        target_group_col = None
        
        # 1. å°è¯•ä» Report æå–ç»˜å›¾æ‰€éœ€æ•°æ®
        if report is not None:
            # æ­¤æ—¶ report.detail_table å¯èƒ½æ˜¯ Pandas æˆ– Polarsï¼Œå–å†³äºä¹‹å‰çš„ evaluate
            # ä¸ºäº†ä¿é™©èµ·è§ï¼Œç»˜å›¾å‰å¯ä»¥ç»Ÿä¸€è½¬å› Polars å¤„ç†ï¼Œæˆ–è€… MarsPlotter æ”¯æŒ Pandas
            target_df = report.detail_table
            target_group_col = report.group_col
        # 2. å°è¯•ä» df_detail æå–æ•°æ®
        elif df_detail is not None:
            # [Pandas å…¼å®¹] å¦‚æœä¼ å…¥çš„æ˜¯ Pandasï¼Œå…ˆè½¬ä¸º Polars æ–¹ä¾¿åç»­å¤„ç†
            # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦è®¾ç½® _return_pandasï¼Œå› ä¸ºç»˜å›¾ä¸è¿”å›æ•°æ®
            if isinstance(df_detail, pd.DataFrame):
                target_df = pl.from_pandas(df_detail)
            else:
                target_df = df_detail
                
            if group_col:
                target_group_col = group_col
            else:
                # è‡ªåŠ¨æ¨æ–­åˆ†ç»„åˆ—
                known = {"feature", "bin_index", "bin_label", "count", "bad", "bad_rate", "lift", "psi_bin", "ks_bin", "auc_bin", "iv_bin", "total_count"}
                candidates = [c for c in target_df.columns if c not in known]
                target_group_col = candidates[0] if candidates else "month"
                logger.debug(f"â„¹ï¸ Auto-inferred group_col: '{target_group_col}'")
        else:
            raise ValueError("âŒ Must provide either 'report' or 'df_detail' to plot.")

        if features is None:
            features = target_df["feature"].unique().to_list()
        elif isinstance(features, str):
            features = [features]
            
        # è°ƒç”¨ MarsPlotter ç»˜å›¾ç»„ä»¶è¿›è¡Œæ¸²æŸ“
        MarsPlotter.plot_feature_binning_risk_trend_batch(
            df_detail=target_df,
            features=features,
            group_col=target_group_col,
            target_name=self.target_col,
            sort_by=sort_by,
            ascending=ascending,
            dpi=dpi
        )
        
    def evaluate_and_plot(
        self,
        # --- 1. Evaluate é˜¶æ®µæ ¸å¿ƒå‚æ•° ---
        df: Union[pl.DataFrame, pd.DataFrame],
        features: Optional[List[str]] = None,
        profile_by: Optional[str] = None,
        dt_col: Optional[str] = None,
        target_col: Optional[str] = None, 
        benchmark_df: Union[pl.DataFrame, pd.DataFrame, None] = None,
        weights_col: Optional[str] = None,
        batch_size: int = 500,
        
        # --- 2. Binner ç­–ç•¥å‚æ•° ---
        bining_type: Optional[Literal["native", "opt"]] = None, 
        
        # --- 3. Plot é˜¶æ®µæ ¸å¿ƒå‚æ•° ---
        sort_by: str = "iv",       
        ascending: bool = False,   
        max_plots: int = 10,       
        dpi: int = 120,
        
        # --- 4. Binner ä¸´æ—¶é€ä¼ å‚æ•° ---
        **kwargs
    ) -> "MarsEvaluationReport":
        """
        [One-Stop Shop] ä¸€ç«™å¼è¯„ä¼°ä¸ç»˜å›¾å·¥ä½œæµ (Evaluation & Visualization Pipeline)ã€‚
        
        è¯¥æ–¹æ³•å°è£…äº† "æ‹Ÿåˆ -> è½¬æ¢ -> è¯„ä¼° -> ç»˜å›¾" çš„å®Œæ•´é—­ç¯ã€‚å®ƒé‡‡ç”¨äº† **ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (Context Manager)** çš„è®¾è®¡æ€æƒ³ï¼Œ
        å…è®¸ç”¨æˆ·åœ¨ä¸æ±¡æŸ“å®ä¾‹åŸå§‹çŠ¶æ€çš„å‰æä¸‹ï¼Œä¸´æ—¶è¦†ç›–å‚æ•°è¿›è¡Œå¿«é€Ÿå®éªŒã€‚


        Parameters
        ----------
        df : Union[pl.DataFrame, pd.DataFrame]
            å¾…è¯„ä¼°çš„è¾“å…¥æ•°æ®é›† (Train/Test/OOT)ã€‚
        features : List[str], optional
            æŒ‡å®šè¯„ä¼°çš„ç‰¹å¾åˆ—è¡¨ã€‚è‹¥ä¸º Noneï¼Œè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰å¯ç”¨ç‰¹å¾ã€‚
        profile_by : str, optional
            è¶‹åŠ¿åˆ†æçš„åˆ†ç»„åˆ—å (å¦‚ 'month', 'vintage')ã€‚
            ç”¨äºç”Ÿæˆæ—¶é—´åˆ‡ç‰‡ä¸‹çš„ Risk Trend å›¾è¡¨ã€‚
        dt_col : str, optional
            æ—¥æœŸåˆ—åã€‚
            - è‹¥é…åˆ `profile_by='week'`ï¼Œåˆ™æŒ‰å‘¨èšåˆã€‚
            - **[æ™ºèƒ½é»˜è®¤]** è‹¥æä¾›äº† `dt_col` ä½†æœªæä¾› `profile_by`ï¼Œé»˜è®¤ä¸º **'month'** (æŒ‰æœˆèšåˆ)ã€‚
        target_col : str, optional
            **[ä¸´æ—¶è¦†ç›–]** ç›®æ ‡å˜é‡åˆ—åã€‚
            å…è®¸ä¸´æ—¶æŒ‡å®šä¸åŒçš„ Label (å¦‚ 'is_bad_7d' vs 'is_bad_30d') è¿›è¡Œè¯„ä¼°ï¼Œæ‰§è¡Œåä¼šè‡ªåŠ¨è¿˜åŸã€‚
        bining_type : {'native', 'opt'}, optional
            **[ä¸´æ—¶è¦†ç›–]** åˆ†ç®±ç®—æ³•ç­–ç•¥ã€‚
            - 'native': æé€ŸåŸç”Ÿåˆ†ç®± (Quantile/Uniform)ã€‚
            - 'opt': æœ€ä¼˜åˆ†ç®± (OptBinning)ã€‚
            æŒ‡å®šæ­¤å‚æ•°å°†å¼ºåˆ¶è§¦å‘é‡æ–°æ‹Ÿåˆ (Re-fit)ã€‚
        max_plots : int, default 10
            **[å¯è§†åŒ–ç†”æ–­]**ã€‚
            é™åˆ¶æœ€ç»ˆç”Ÿæˆçš„å›¾è¡¨æ•°é‡ã€‚å³ä½¿è¯„ä¼°äº† 5000 ä¸ªç‰¹å¾ï¼Œä¹Ÿåªç»˜åˆ¶æ’åºæœ€é å‰ (Top-N) çš„ N å¼ å›¾ã€‚
            é˜²æ­¢å› æ¸²æŸ“è¿‡å¤š Canvas å¯¼è‡´ Jupyter Notebook å¡æ­»æˆ–å†…å­˜æº¢å‡ºã€‚
        sort_by : str, default 'iv'
            **[ç»˜å›¾ç­›é€‰å™¨]**ã€‚
            å†³å®šç»˜åˆ¶å“ªäº›ç‰¹å¾çš„ä¾æ®ã€‚æ”¯æŒ 'iv', 'ks', 'auc', 'psi'ã€‚
            é…åˆ `ascending=False` (é»˜è®¤) ä½¿ç”¨ï¼Œä¼˜å…ˆå±•ç¤ºâ€œæœ€é‡è¦â€æˆ–â€œæœ€ä¸ç¨³å®šâ€çš„ç‰¹å¾ã€‚
        ascending : bool, default False
            æ’åºæ–¹å‘ã€‚é»˜è®¤é™åº (Descending)ï¼Œå³æŒ‡æ ‡å€¼å¤§çš„æ’å‰é¢ã€‚
        **kwargs : dict
            **[åˆ†ç®±å™¨é€ä¼ å‚æ•°]**ã€‚
            ç›´æ¥ä¼ é€’ç»™åº•å±‚çš„ `MarsNativeBinner` æˆ– `MarsOptimalBinner`ã€‚
            ä¾‹å¦‚: `n_bins=10`, `min_bin_size=0.05`, `monotonic_trend='ascending'`ã€‚
            æ³¨æ„: ä¼ å…¥ä»»ä½• kwargs éƒ½ä¼šè§¦å‘åˆ†ç®±å™¨çš„é‡æ–°æ‹Ÿåˆã€‚

        Returns
        -------
        MarsEvaluationReport
            åŒ…å«æ±‡æ€»è¡¨ (Summary)ã€è¶‹åŠ¿è¡¨ (Trend) å’Œè¯¦æƒ…è¡¨ (Detail) çš„æŠ¥å‘Šå®¹å™¨å¯¹è±¡ã€‚
            
        Examples
        --------
        **åœºæ™¯ 1: å¿«é€ŸåŸºçº¿è¯„ä¼° (Quick Baseline)**
        ä½¿ç”¨é»˜è®¤çš„ Native åˆ†ç®± (Quantile) å¿«é€ŸæŸ¥çœ‹æ•°æ®æ¦‚è²Œã€‚

        >>> evaluator = MarsBinEvaluator(target_col="bad_0")
        >>> report = evaluator.evaluate_and_plot(
        ...     df=train_df,
        ...     profile_by="month",   # æŒ‰æœˆæŸ¥çœ‹è¶‹åŠ¿
        ...     dt_col="apply_date",
        ...     max_plots=5           # åªç”» IV æœ€é«˜çš„å‰ 5 ä¸ªç‰¹å¾
        ... )

        **åœºæ™¯ 2: ç­–ç•¥ A/B æµ‹è¯• (ç²¾ç»†åŒ–æœ€ä¼˜åˆ†ç®±)**
        è§‰å¾—åŸç”Ÿåˆ†ç®±ä¸å¤Ÿç²¾ç»†ï¼Ÿåˆ‡æ¢åˆ°æœ€ä¼˜åˆ†ç®± (OptBinning) å¹¶æ³¨å…¥ä¸¥æ ¼çš„**é£æ§ä¸šåŠ¡çº¦æŸ**ã€‚
        è¿™é‡Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ `kwargs` é€ä¼ å‚æ•°æ¥æ§åˆ¶æ±‚è§£å™¨è¡Œä¸ºã€‚

        >>> # å°è¯•: æœ€ä¼˜åˆ†ç®± + å¼ºçº¦æŸ
        >>> # bining_type="opt" ä¼šè‡ªåŠ¨è§¦å‘é‡æ–°æ‹Ÿåˆ
        >>> report_opt = evaluator.evaluate_and_plot(
        ...     df, 
        ...     profile_by="month",
        ...     dt_col="apply_date",
        ...     bining_type="opt",               # 1. åˆ‡æ¢ç®—æ³•
        ...     # --- ä»¥ä¸‹å‚æ•°ç›´æ¥é€ä¼ ç»™ MarsOptimalBinner ---
        ...     n_bins=10,                        # æœ€å¤§åˆ†ç®±æ•°
        ...     min_bin_size=0.05,               # æœ€å°ç®±å æ¯” 5% 
        ...     min_bin_n_event=5,               #  æ¯ç®±è‡³å°‘ 5 ä¸ªåäºº
        ...     prebinning_method="cart",        # é¢„åˆ†ç®±æ–¹æ³•
        ...     n_prebins=50,                     # é¢„åˆ†ç®±æ•°
        ...     min_prebin_size=0.01,            # é¢„åˆ†ç®±æœ€å°å æ¯” 1%
        ...     monotonic_trend="auto_asc_desc", 
        ...     time_limit=20                    # å•ç‰¹å¾æ±‚è§£è¶…æ—¶é™åˆ¶ (ç§’)
        ... )

        **åœºæ™¯ 3: ä¸åŒ Label çš„å¿«é€ŸéªŒè¯ (Label Shifting)**
        æ— éœ€é‡æ–°å®ä¾‹åŒ–ï¼Œç›´æ¥æµ‹è¯•ä¸åŒçš„ Y (å¦‚ 7å¤©é€¾æœŸ vs 30å¤©é€¾æœŸ)ã€‚

        >>> # è¯„ä¼° 7å¤©é€¾æœŸ
        >>> evaluator.evaluate_and_plot(df, target_col="is_bad_7d")
        # -> è§¦å‘é‡ç½®ã€‚æ ¹æ® is_bad_7d é‡æ–°è®¡ç®—æœ€ä¼˜åˆ†ç®±åˆ‡ç‚¹ï¼Œè®¡ç®— IVã€‚

        >>> # è¯„ä¼° 30å¤©é€¾æœŸ
        >>> evaluator.evaluate_and_plot(df, target_col="is_bad_30d")
        # -> å†æ¬¡è§¦å‘é‡ç½®ã€‚æ ¹æ® is_bad_30d é‡æ–°è®¡ç®—æœ€ä¼˜åˆ†ç®±åˆ‡ç‚¹ï¼Œè®¡ç®— IVã€‚

        **åœºæ™¯ 4: å†…å­˜å—é™çš„å¤§è§„æ¨¡è®¡ç®—**
        å¤„ç† 5000+ ç»´å®½è¡¨æ—¶ï¼Œå‡å° batch_size é˜²æ­¢ OOMã€‚

        >>> evaluator.evaluate_and_plot(
        ...     df_huge_wide, 
        ...     batch_size=100,  # é™ä½ Map é˜¶æ®µå†…å­˜å‹åŠ›
        ...     max_plots=20     # åªå…³æ³¨ Top 20 æ ¸å¿ƒç‰¹å¾
        ... )
        """
        
        # ==============================================================================
        # 0. Context Setup: çŠ¶æ€æš‚å­˜ä¸ç¯å¢ƒé…ç½®
        # ==============================================================================
        # æš‚å­˜åŸå§‹çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨ finally å—ä¸­è¿˜åŸï¼Œä¿è¯å‡½æ•°æ— å‰¯ä½œç”¨ (Side-effect free)
        original_target = self.target_col
        original_bining_type = self.bining_type 
        original_binner_kwargs = self.binner_kwargs.copy() if self.binner_kwargs else {}
        
        # 1. åº”ç”¨ä¸´æ—¶è¦†ç›–: Target
        if target_col:
            self.target_col = target_col
            
        # 2. åº”ç”¨ä¸´æ—¶è¦†ç›–: Bining Type (ç®—æ³•ç­–ç•¥)
        if bining_type:
            self.bining_type = bining_type

        # 3. å¤„ç†åŠ¨æ€å‚æ•° (kwargs) ä¸å¼ºåˆ¶é‡ç½®é€»è¾‘
        #  å¢åŠ  target_col çš„åˆ¤æ–­
        # åªè¦å‘ç”Ÿä»¥ä¸‹ä»»ä¸€æƒ…å†µï¼Œéƒ½å¿…é¡»æŠ›å¼ƒæ—§çš„åˆ†ç®±å™¨ï¼Œé‡æ–°è®­ç»ƒï¼š
        # 1. ä¼ å…¥äº†æ–°çš„åˆ†ç®±å‚æ•° (kwargs)
        # 2. åˆ‡æ¢äº†ç®—æ³•ç±»å‹ (bining_type)
        # 3. åˆ‡æ¢äº†ç›®æ ‡å˜é‡ (target_col) -> å…³é”®ï¼å› ä¸ºæœ€ä¼˜åˆ†ç®±ä¾èµ– Y
        should_reset_binner = (
            (kwargs is not None and len(kwargs) > 0) or 
            (bining_type is not None) or
            (target_col is not None)  # <--- æ–°å¢è¿™è¡Œ
        )

        if kwargs:
            if self.binner_kwargs is None: 
                self.binner_kwargs = {}
            self.binner_kwargs.update(kwargs)
            
        if should_reset_binner and self.binner is not None:
            # å¢åŠ æ›´è¯¦ç»†çš„æ—¥å¿—ï¼Œå‘Šè¯‰ç”¨æˆ·ä¸ºä»€ä¹ˆé‡ç½®äº†
            reason = []
            if kwargs: 
                reason.append("params_changed")
            if bining_type: 
                reason.append("type_changed")
            if target_col: 
                reason.append("target_changed")
            
            logger.info(f"âš¡ Context changed ({'+'.join(reason)}). Resetting binner to trigger auto-refit.")
            self.binner = None

        try:
            # ==========================================================================
            # 1. Execution: æ‰§è¡Œæ ¸å¿ƒè¯„ä¼°è®¡ç®—
            # ==========================================================================
            # è°ƒç”¨åº•å±‚ evaluate æ–¹æ³•ï¼Œå®ƒä¼šå¤„ç†ï¼š
            # - Auto-Fitting (å¦‚æœ binner ä¸º None)
            # - Transform (åˆ†ç®±æ˜ å°„)
            # - Aggregation (Map-Reduce è®¡ç®—æŒ‡æ ‡)
            report = self.evaluate(
                df=df,
                features=features,
                profile_by=profile_by,
                dt_col=dt_col,
                benchmark_df=benchmark_df,
                weights_col=weights_col,
                batch_size=batch_size
            )
            
            # ==========================================================================
            # 2. Selection: æ™ºèƒ½ç­›é€‰ç»˜å›¾ç‰¹å¾ (Top-N é€»è¾‘)
            # ==========================================================================
            plot_features = features # é»˜è®¤å›é€€ï¼šå…¨éƒ¨ç‰¹å¾
            
            # è·å– Summary è¡¨ç”¨äºæ’åº
            summary = report.summary_table
            # å…¼å®¹æ€§å¤„ç†: ç¡®ä¿ summary æ˜¯ Polars DataFrame ä»¥ä¾¿ä½¿ç”¨é«˜æ€§èƒ½ sort
            if isinstance(summary, pd.DataFrame):
                summary_pl = pl.from_pandas(summary)
            else:
                summary_pl = summary
            
            # æ˜ å°„ç®€å†™æŒ‡ä»¤åˆ°çœŸå®åˆ—å (UX ä¼˜åŒ–)
            sort_map = {
                "iv": "IV_total", 
                "ks": "KS_total", 
                "auc": "AUC_total", 
                "psi": "PSI_max" # æ³¨æ„ PSI é€šå¸¸çœ‹ Max æˆ– Avg
            }
            # get(key, default) -> å…è®¸ç”¨æˆ·ç›´æ¥ä¼  'IV_total' è¿™ç§åŸå§‹åˆ—å
            sort_key = sort_map.get(sort_by.lower(), sort_by)
            
            # æ‰§è¡Œæ’åºä¸æˆªæ–­
            if sort_key in summary_pl.columns:
                sorted_feats = (
                    summary_pl
                    .sort(sort_key, descending=not ascending)
                    .get_column("feature")
                    .to_list()
                )
                
                # [ç†”æ–­æœºåˆ¶] å¦‚æœç‰¹å¾æ•°è¶…è¿‡ max_plotsï¼Œä»…ç»˜åˆ¶ Top N
                if max_plots and max_plots > 0 and len(sorted_feats) > max_plots:
                    logger.info(f"ğŸ“‰ [Visual Protection] Plotting Top {max_plots} features sorted by '{sort_key}' (out of {len(sorted_feats)}).")
                    plot_features = sorted_feats[:max_plots]
                else:
                    plot_features = sorted_feats
            else:
                logger.warning(f"âš ï¸ Sort key '{sort_key}' not found in summary table. Plotting unsorted features.")

            # ==========================================================================
            # 3. Visualization: æ‰¹é‡ç»˜å›¾
            # ==========================================================================
            self.plot_feature_binning_risk_trends(
                report=report,
                features=plot_features, # ä¼ å…¥ç­›é€‰åçš„åˆ—è¡¨
                group_col=report.group_col,
                sort_by=sort_by,
                ascending=ascending,
                dpi=dpi
            )
            
            return report
            
        finally:
            # ==========================================================================
            # 4. Teardown: çŠ¶æ€è¿˜åŸ
            # ==========================================================================
            # æ— è®ºæ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œå¿…é¡»è¿˜åŸå®ä¾‹å±æ€§ï¼Œé˜²æ­¢æœ¬æ¬¡ä¸´æ—¶å‚æ•°æ±¡æŸ“åç»­è°ƒç”¨
            self.target_col = original_target
            self.bining_type = original_bining_type
            # [æ ¸å¿ƒæ”¹è¿›] è¿˜åŸ kwargsï¼Œé˜²æ­¢æ±¡æŸ“
            self.binner_kwargs = original_binner_kwargs