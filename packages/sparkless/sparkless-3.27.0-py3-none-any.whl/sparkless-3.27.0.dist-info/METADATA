Metadata-Version: 2.1
Name: sparkless
Version: 3.27.0
Summary: Lightning-fast PySpark testing without JVM - 10x faster with 100% API compatibility
Author-email: Odos Matthews <odosmatthews@gmail.com>
Maintainer-email: Odos Matthews <odosmatthews@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/eddiethedean/sparkless
Project-URL: Repository, https://github.com/eddiethedean/sparkless
Project-URL: Issues, https://github.com/eddiethedean/sparkless/issues
Keywords: spark,pyspark,testing,development,data-engineering,dataframe,spark-session,unit-testing,type-safe,mypy,error-simulation,performance-testing,data-generation,enterprise
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Testing
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: spark-ddl-parser>=0.1.0
Requires-Dist: polars>=0.20.0
Requires-Dist: psutil>=5.8.0
Requires-Dist: typing-extensions>=4.0.0; python_version < "3.9"
Provides-Extra: analytics
Requires-Dist: pandas<3,>=1.3.0; extra == "analytics"
Requires-Dist: pandas-stubs<3,>=2.0.0; extra == "analytics"
Requires-Dist: numpy>=1.20.0; extra == "analytics"
Requires-Dist: polars[pyarrow]>=0.20.0; extra == "analytics"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-xdist>=3.0.0; extra == "dev"
Requires-Dist: mypy>=1.19.0; extra == "dev"
Requires-Dist: ruff>=0.4.0; extra == "dev"
Requires-Dist: pandas<3,>=1.3.0; extra == "dev"
Requires-Dist: pandas-stubs<3,>=2.0.0; extra == "dev"
Requires-Dist: types-psutil>=6.0.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=5.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Requires-Dist: myst-parser>=1.0.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=1.19.0; extra == "docs"
Provides-Extra: generate-outputs
Requires-Dist: pyspark<3.6.0,>=3.5.0; extra == "generate-outputs"
Requires-Dist: delta-spark<4.0.0,>=3.0.0; extra == "generate-outputs"
Provides-Extra: pandas
Requires-Dist: pandas<3,>=1.3.0; extra == "pandas"
Requires-Dist: pandas-stubs<3,>=2.0.0; extra == "pandas"
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-cov>=4.0.0; extra == "test"
Requires-Dist: pytest-xdist>=3.0.0; extra == "test"
Requires-Dist: hypothesis>=6.0.0; extra == "test"

# Sparkless

<div align="center">

**ğŸš€ Test PySpark code at lightning speedâ€”no JVM required**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PySpark 3.2-3.5](https://img.shields.io/badge/pyspark-3.2--3.5-orange.svg)](https://spark.apache.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/sparkless.svg)](https://badge.fury.io/py/sparkless)
[![Documentation](https://readthedocs.org/projects/sparkless/badge/?version=latest)](https://sparkless.readthedocs.io/)
[![Tests](https://img.shields.io/badge/tests-1309+%20passing%20%7C%200%20failing-brightgreen.svg)](https://github.com/eddiethedean/sparkless)
[![Type Checked](https://img.shields.io/badge/mypy-260%20files%20clean-blue.svg)](https://github.com/python/mypy)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

*âš¡ 10x faster tests â€¢ ğŸ¯ Drop-in PySpark replacement â€¢ ğŸ“¦ Zero JVM overhead â€¢ ğŸ§µ Thread-safe Polars backend*

ğŸ“š **[Full Documentation â†’](https://sparkless.readthedocs.io/)**

</div>

---

## Why Sparkless?

**Tired of waiting 30+ seconds for Spark to initialize in every test?**

Sparkless is a lightweight PySpark replacement that runs your tests **10x faster** by eliminating JVM overhead. Your existing PySpark code works unchangedâ€”just swap the import.

```python
# Before
from pyspark.sql import SparkSession

# After  
from sparkless.sql import SparkSession
```

### Key Benefits

| Feature | Description |
|---------|-------------|
| âš¡ **10x Faster** | No JVM startup (30s â†’ 0.1s) |
| ğŸ¯ **Drop-in Replacement** | Use existing PySpark code unchanged |
| ğŸ“¦ **Zero Java** | Pure Python with Polars backend (thread-safe, no SQL required) |
| ğŸ§ª **100% Compatible** | Full PySpark 3.2-3.5 API support |
| ğŸ”„ **Lazy Evaluation** | Mirrors PySpark's execution model |
| ğŸ­ **Production Ready** | 1309+ passing tests, 100% mypy typed |
| ğŸ§µ **Thread-Safe** | Polars backend designed for parallel execution |
| ğŸ”§ **Modular Design** | DDL parsing via standalone spark-ddl-parser package |
| ğŸ¯ **Type Safe** | Full type checking with `ty`, comprehensive type annotations |

### Perfect For

- **Unit Testing** - Fast, isolated test execution with automatic cleanup
- **CI/CD Pipelines** - Reliable tests without infrastructure or resource leaks
- **Local Development** - Prototype without Spark cluster
- **Documentation** - Runnable examples without setup
- **Learning** - Understand PySpark without complexity
- **Integration Tests** - Configurable memory limits for large dataset testing

---

## Quick Start

### Installation

```bash
pip install sparkless
```

ğŸ“– **Need help?** Check out the [full documentation](https://sparkless.readthedocs.io/) for detailed guides, API reference, and examples.

### Basic Usage

```python
from sparkless.sql import SparkSession, functions as F

# Create session
spark = SparkSession("MyApp")

# Your PySpark code works as-is
data = [{"name": "Alice", "age": 25}, {"name": "Bob", "age": 30}]
df = spark.createDataFrame(data)

# All operations work
result = df.filter(F.col("age") > 25).select("name").collect()
print(result)
# Output: [Row(name='Bob')]

# Show the DataFrame
df.show()
# Output:
# DataFrame[2 rows, 2 columns]
# age name 
# 25    Alice  
# 30    Bob
```

### Testing Example

```python
import pytest
from sparkless.sql import SparkSession, functions as F

def test_data_pipeline():
    """Test PySpark logic without Spark cluster."""
    spark = SparkSession("TestApp")
    
    # Test data
    data = [{"score": 95}, {"score": 87}, {"score": 92}]
    df = spark.createDataFrame(data)
    
    # Business logic
    high_scores = df.filter(F.col("score") > 90)
    
    # Assertions
    assert high_scores.count() == 2
    assert high_scores.agg(F.avg("score")).collect()[0][0] == 93.5
    
    # Always clean up
    spark.stop()
```

---

## Core Features

### ğŸš€ Complete PySpark API Compatibility

Sparkless implements **120+ functions** and **70+ DataFrame methods** across PySpark 3.0-3.5:

| Category | Functions | Examples |
|----------|-----------|----------|
| **String** (40+) | Text manipulation, regex, formatting | `upper`, `concat`, `regexp_extract`, `soundex` |
| **Math** (35+) | Arithmetic, trigonometry, rounding | `abs`, `sqrt`, `sin`, `cos`, `ln` |
| **DateTime** (30+) | Date/time operations, timezones | `date_add`, `hour`, `weekday`, `convert_timezone` |
| **Array** (25+) | Array manipulation, lambdas | `array_distinct`, `transform`, `filter`, `aggregate` |
| **Aggregate** (20+) | Statistical functions | `sum`, `avg`, `median`, `percentile`, `max_by` |
| **Map** (10+) | Dictionary operations | `map_keys`, `map_filter`, `transform_values` |
| **Conditional** (8+) | Logic and null handling | `when`, `coalesce`, `ifnull`, `nullif` |
| **Window** (8+) | Ranking and analytics | `row_number`, `rank`, `lag`, `lead` |
| **XML** (9+) | XML parsing and generation | `from_xml`, `to_xml`, `xpath_*` |
| **Bitwise** (6+) | Bit manipulation | `bit_count`, `bit_and`, `bit_xor` |

ğŸ“– **See complete function list**: [`PYSPARK_FUNCTION_MATRIX.md`](PYSPARK_FUNCTION_MATRIX.md) | [Full API Documentation](https://sparkless.readthedocs.io/)

### DataFrame Operations

- **Transformations**: `select`, `filter`, `withColumn`, `drop`, `distinct`, `orderBy`, `replace`
- **Aggregations**: `groupBy`, `agg`, `count`, `sum`, `avg`, `min`, `max`, `median`, `mode`
- **Joins**: `inner`, `left`, `right`, `outer`, `cross`
- **Advanced**: `union`, `pivot`, `unpivot`, `explode`, `transform`

### Window Functions

```python
from sparkless.sql import Window, functions as F

# Ranking and analytics
df = spark.createDataFrame([
    {"name": "Alice", "dept": "IT", "salary": 50000},
    {"name": "Bob", "dept": "HR", "salary": 60000},
    {"name": "Charlie", "dept": "IT", "salary": 70000},
])

result = df.withColumn("rank", F.row_number().over(
    Window.partitionBy("dept").orderBy("salary")
))

# Show results
for row in result.collect():
    print(row)
# Output:
# Row(dept='HR', name='Bob', salary=60000, rank=1)
# Row(dept='IT', name='Alice', salary=50000, rank=1)
# Row(dept='IT', name='Charlie', salary=70000, rank=2)
```

### SQL Support

```python
df = spark.createDataFrame([
    {"name": "Alice", "salary": 50000},
    {"name": "Bob", "salary": 60000},
    {"name": "Charlie", "salary": 70000},
])

# Create temporary view for SQL queries
df.createOrReplaceTempView("employees")

# Execute SQL queries
result = spark.sql("SELECT name, salary FROM employees WHERE salary > 50000")
result.show()
# SQL support enables querying DataFrames using SQL syntax
```

### Delta Lake Format

Full Delta Lake table format support:

```python
# Write as Delta table
df.write.format("delta").mode("overwrite").saveAsTable("catalog.users")

# Time travel - query historical versions
v0_data = spark.read.format("delta").option("versionAsOf", 0).table("catalog.users")

# Schema evolution
new_df.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .saveAsTable("catalog.users")

# MERGE operations for upserts
spark.sql("""
    MERGE INTO catalog.users AS target
    USING updates AS source
    ON target.id = source.id
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")
```

### Lazy Evaluation

Sparkless mirrors PySpark's lazy execution model:

```python
# Transformations are queued (not executed)
result = df.filter(F.col("age") > 25).select("name")  

# Actions trigger execution
rows = result.collect()  # â† Execution happens here
count = result.count()    # â† Or here
```

### CTE Query Optimization

DataFrame operation chains are automatically optimized using Common Table Expressions:

```python
# Enable lazy evaluation for CTE optimization
data = [
    {"name": "Alice", "age": 25, "salary": 50000},
    {"name": "Bob", "age": 30, "salary": 60000},
    {"name": "Charlie", "age": 35, "salary": 70000},
    {"name": "David", "age": 28, "salary": 55000},
]
df = spark.createDataFrame(data)

# This entire chain executes as ONE optimized query:
result = (
    df.filter(F.col("age") > 25)           # CTE 0: WHERE clause
      .select("name", "age", "salary")     # CTE 1: Column selection
      .withColumn("bonus", F.col("salary") * 0.1)  # CTE 2: New column
      .orderBy(F.desc("salary"))           # CTE 3: ORDER BY
      .limit(2)                            # CTE 4: LIMIT
).collect()  # Single query execution here

# Result:
# [Row(name='Charlie', age=35, salary=70000, bonus=7000.0),
#  Row(name='Bob', age=30, salary=60000, bonus=6000.0)]

# Performance: 5-10x faster than creating 5 intermediate tables
```

---

## Backend Architecture

### Polars Backend (Default)

Sparkless uses **Polars** as the default backend, providing:

- ğŸ§µ **Thread Safety** - Designed for parallel execution
- âš¡ **High Performance** - Optimized DataFrame operations
- ğŸ“Š **Parquet Storage** - Tables persist as Parquet files
- ğŸ”„ **Lazy Evaluation** - Automatic query optimization

```python
# Default backend (Polars) - thread-safe, high-performance
spark = SparkSession("MyApp")

# Explicit backend selection
spark = SparkSession.builder \
    .config("spark.sparkless.backend", "polars") \
    .getOrCreate()
```

### Alternative Backends

```python
# Memory backend for lightweight testing
spark = SparkSession.builder \
    .config("spark.sparkless.backend", "memory") \
    .getOrCreate()

# File backend for persistent storage
spark = SparkSession.builder \
    .config("spark.sparkless.backend", "file") \
    .config("spark.sparkless.backend.basePath", "/tmp/sparkless") \
    .getOrCreate()
```

---

## Advanced Features

### Table Persistence

Tables created with `saveAsTable()` can persist across multiple sessions:

```python
# First session - create table
spark1 = SparkSession("App1", db_path="test.db")
df = spark1.createDataFrame([{"id": 1, "name": "Alice"}])
df.write.mode("overwrite").saveAsTable("schema.my_table")
spark1.stop()

# Second session - table persists
spark2 = SparkSession("App2", db_path="test.db")
assert spark2.catalog.tableExists("schema", "my_table")  # âœ… True
result = spark2.table("schema.my_table").collect()  # âœ… Works!
spark2.stop()
```

**Key Features:**
- **Cross-Session Persistence**: Tables persist when using `db_path` parameter
- **Schema Discovery**: Automatically discovers existing schemas and tables
- **Catalog Synchronization**: Reliable `catalog.tableExists()` checks
- **Data Integrity**: Full support for `append` and `overwrite` modes

### Configurable Memory & Isolation

Control memory usage and test isolation:

```python
# Default: 1GB memory limit, no disk spillover (best for tests)
spark = SparkSession("MyApp")

# Custom memory limit
spark = SparkSession("MyApp", max_memory="4GB")

# Allow disk spillover for large datasets
spark = SparkSession(
    "MyApp",
    max_memory="8GB",
    allow_disk_spillover=True  # Uses unique temp directory per session
)
```

---

## Performance Comparison

Real-world test suite improvements:

| Operation | PySpark | Sparkless | Speedup |
|-----------|---------|------------|---------|
| Session Creation | 30-45s | 0.1s | **300x** |
| Simple Query | 2-5s | 0.01s | **200x** |
| Window Functions | 5-10s | 0.05s | **100x** |
| Full Test Suite | 5-10min | 30-60s | **10x** |

### Performance Tooling

- ğŸ“Š [Hot path profiling guide](https://sparkless.readthedocs.io/en/latest/performance/profiling.html)
- ğŸ“ˆ [Pandas fallback vs native benchmarks](https://sparkless.readthedocs.io/en/latest/performance/pandas_fallback.html)

---

---

## Recent Updates

### Version 3.26.0 - Missing String & JSON Functions (Issue #189)

- ğŸ”¤ **9 New String & JSON Functions** - Implemented missing PySpark functions for improved compatibility
  - **String Functions**: `soundex()` (phonetic matching), `translate()` (character translation), `levenshtein()` (edit distance), `crc32()` (checksum), `xxhash64()` (deterministic hashing), `regexp_extract_all()` (array of regex matches), `substring_index()` (delimiter-based extraction)
  - **JSON Functions**: `get_json_object()` (JSONPath extraction), `json_tuple()` (multi-field extraction to separate columns)
  - All functions match PySpark behavior exactly, including edge cases (nulls, empty strings, invalid JSON)
  - `xxhash64` uses deterministic XXHash64 algorithm (seed=42) matching PySpark output
  - `regexp_extract_all` extracts all regex matches as arrays (PySpark 3.5+ feature)
- ğŸ§ª **Comprehensive Test Coverage** - 18 new tests ensuring full PySpark compatibility
  - 8 parity tests validating exact PySpark behavior
  - 9 robust edge case tests covering nulls, empty strings, invalid JSON, missing paths/fields
  - All tests pass in both Sparkless (mock) and PySpark backends
- ğŸ“š **Documentation Updates** - Updated CHANGELOG and PYSPARK_FUNCTION_MATRIX with implementation details

### Version 3.25.0 - Case-Insensitive Column Names, Tuple DataFrame Creation, & Enhanced Compatibility

- ğŸ”¤ **Case-Insensitive Column Names** - Complete refactoring with centralized `ColumnResolver` system
  - Added `spark.sql.caseSensitive` configuration (default: `false`, matching PySpark)
  - All column resolution respects case sensitivity settings
  - Ambiguity detection for multiple columns differing only by case
  - Comprehensive test coverage: 34 unit tests + 17 integration tests
  - **Issue #264**: Fixed case-insensitive column resolution in `withColumn` with `F.col()`
  - Fixed case-sensitive mode enforcement for all DataFrame operations
- ğŸ“Š **Tuple-Based DataFrame Creation** - Fixed tuple-based data parameter support
  - **Issue #270**: Fixed `createDataFrame` with tuple-based data to convert tuples to dictionaries
  - All operations now work correctly: `.show()`, `.unionByName()`, `.fillna()`, `.replace()`, `.dropna()`, `.join()`, etc.
  - Strict length validation matching PySpark behavior (`LENGTH_SHOULD_BE_THE_SAME` error)
  - Supports tuple, list, and mixed tuple/dict/Row data
  - 23 comprehensive unit tests, all passing in Sparkless and PySpark modes
- ğŸ”§ **PySpark Compatibility Enhancements**
  - **Issue #247**: Added `elementType` keyword argument support to `ArrayType` (PySpark convention)
  - **Issue #260**: Implemented `Column.eqNullSafe` for null-safe equality comparisons (`NULL <=> NULL` returns `True`)
  - **Issue #261**: Implemented full support for `Column.between()` API with inclusive bounds
  - **Issue #262**: Fixed `ArrayType` initialization with positional arguments (e.g., `ArrayType(DoubleType(), True)`)
  - **Issue #263**: Fixed `isnan()` on string columns to match PySpark behavior (returns `False` for strings)
- ğŸ› **Bug Fixes**
  - Fixed `fillna()` to properly materialize lazy DataFrames after joins
  - Fixed all AttributeError issues with tuple-based data (`.keys()`, `.get()`, `.items()`, `.copy()`)
  - Fixed `isnan()` Polars backend errors on string columns


### Version 3.23.0 - Issues 225-231 Fixes & PySpark Compatibility Improvements

- ğŸ› **Issue Fixes** â€“ Fixed 7 critical issues (225-231) improving PySpark compatibility:
  - **Issue #225**: String-to-numeric type coercion for comparison operations
  - **Issue #226**: `isin()` method with `*values` arguments and type coercion
  - **Issue #227**: `getItem()` out-of-bounds handling (returns `None` instead of errors)
  - **Issue #228**: Regex look-ahead/look-behind fallback support
  - **Issue #229**: Pandas DataFrame support with proper recognition
  - **Issue #230**: Case-insensitive column name matching across all operations
  - **Issue #231**: `simpleString()` method for all DataType classes
- ğŸ”§ **SQL JOIN Parsing** â€“ Fixed SQL JOIN condition parsing and validation
- âœ… **select() Validation** â€“ Fixed validation to properly handle ColumnOperation expressions
- ğŸ§ª **Test Coverage** â€“ All 50 tests passing for issues 225-231, including pandas DataFrame support
- ğŸ“¦ **Code Quality** â€“ Applied ruff formatting, fixed linting issues, and resolved mypy type errors

### Version 3.20.0 - Logic Bug Fixes & Code Quality Improvements

- ğŸ› **Exception Handling Fixes** â€“ Fixed critical exception handling issues (issue #183): replaced bare `except:` clause with `except Exception:` and added comprehensive logging to exception handlers for better debuggability.
- ğŸ§ª **Comprehensive Test Coverage** â€“ Added 10 comprehensive test cases for string concatenation cache handling edge cases (issue #188), covering empty strings, None values, nested operations, and numeric vs string operations.
- ğŸ“š **Improved Documentation** â€“ Enhanced documentation for string concatenation cache heuristic, documenting limitations and expected behavior vs PySpark.
- ğŸ” **Code Quality Review** â€“ Systematic review of dictionary.get() usage patterns throughout codebase, confirming all patterns are safe with appropriate default values.
- âœ… **Type Safety** â€“ Fixed mypy errors in CI: improved type narrowing for ColumnOperation.operation and removed redundant casts in writer.py.

### Version 3.7.0 - Full SQL DDL/DML Support

- ğŸ—„ï¸ **Complete SQL DDL/DML** â€“ Full implementation of `CREATE TABLE`, `DROP TABLE`, `INSERT INTO`, `UPDATE`, and `DELETE FROM` statements in the SQL executor.
- ğŸ“ **Enhanced SQL Parser** â€“ Comprehensive support for DDL statements with column definitions, `IF NOT EXISTS`, and `IF EXISTS` clauses.
- ğŸ’¾ **INSERT Operations** â€“ Support for `INSERT INTO ... VALUES (...)` with multiple rows and `INSERT INTO ... SELECT ...` sub-queries.
- ğŸ”„ **UPDATE & DELETE** â€“ Full support for `UPDATE ... SET ... WHERE ...` and `DELETE FROM ... WHERE ...` with Python-based expression evaluation.
- ğŸ› **Bug Fixes** â€“ Fixed recursion errors in schema projection and resolved import shadowing issues in SQL executor.
- âœ¨ **Code Quality** â€“ Improved linting, formatting, and type safety across the codebase.

### Version 3.6.0 - Profiling & Adaptive Execution

- âš¡ **Feature-Flagged Profiling** â€“ Introduced `sparkless.utils.profiling` with opt-in instrumentation for Polars hot paths and expression evaluation, plus a new guide at `docs/performance/profiling.md`.
- ğŸ” **Adaptive Execution Simulation** â€“ Query plans can now inject synthetic `REPARTITION` steps based on skew metrics, configurable via `QueryOptimizer.configure_adaptive_execution` and covered by new regression tests.
- ğŸ¼ **Pandas Backend Choice** â€“ Added an optional native pandas mode (`MOCK_SPARK_PANDAS_MODE`) with benchmarking support (`scripts/benchmark_pandas_fallback.py`) and documentation in `docs/performance/pandas_fallback.md`.

### Version 3.5.0 - Session-Aware Catalog & Safer Fallbacks

- ğŸ§­ **Session-Literal Helpers** â€“ `F.current_catalog`, `F.current_database`, `F.current_schema`, and `F.current_user` return PySpark-compatible literals and understand the active session (with new regression coverage).
- ğŸ—ƒï¸ **Reliable Catalog Context** â€“ The Polars backend and unified storage manager now track the selected schema so `setCurrentDatabase` works end-to-end, and `SparkContext.sparkUser()` mirrors PySpark behaviour.
- ğŸ§® **Pure-Python Stats** â€“ Lightweight `percentile` and `covariance` helpers keep percentile/cov tests green even without NumPy, eliminating native-crash regressions.
- ğŸ› ï¸ **Dynamic Dispatch** â€“ `F.call_function("func_name", ...)` lets wrappers dynamically invoke registered Sparkless functions with PySpark-style error messages.

### Version 3.4.0 - Workflow & CI Refresh

- â™»ï¸ **Unified Commands** â€“ `Makefile`, `install.sh`, and docs now point to `bash tests/run_all_tests.sh`, `ruff`, and `mypy` as the standard dev workflow.
- ğŸ›¡ï¸ **Automated Gates** â€“ New GitHub Actions pipeline runs linting, type-checking, and the full test suite on every push and PR.
- ğŸ—ºï¸ **Forward Roadmap** â€“ Published `plans/typing_delta_roadmap.md` to track mypy debt reduction and Delta feature milestones.
- ğŸ“ **Documentation Sweep** â€“ README and quick-start docs highlight the 3.4.0 tooling changes and contributor expectations.

### Version 3.3.0 - Type Hardening & Clean Type Check

- ğŸ§® **Zero mypy Debt** â€“ `mypy sparkless` now runs clean after migrating the Polars executor,
  expression evaluator, Delta merge helpers, and reader/writer stack to Python 3.8+ compatible type syntax.
- ğŸ§¾ **Accurate DataFrame Interfaces** â€“ `DataFrameReader.load()` and related helpers now return
  `IDataFrame` consistently while keeping type-only imports behind `TYPE_CHECKING`.
- ğŸ§± **Safer Delta & Projection Fallbacks** â€“ Python-evaluated select columns always receive string
  aliases, and Delta merge alias handling no longer leaks `None` keys into evaluation contexts.
- ğŸ“š **Docs & Metadata Updated** â€“ README highlights the new type guarantees and all packaging
  metadata points to v3.3.0.

### Version 3.2.0 - Python 3.8 Baseline & Tooling Refresh

- ğŸ **Python 3.8+ Required** â€“ Packaging metadata, tooling configs, and installation docs now align on Python 3.8 as the minimum supported runtime.
- ğŸ§© **Compatibility Layer** â€“ Uses `typing_extensions` for Python 3.8 compatibility; datetime helpers use native typing with proper fallbacks.
- ğŸª„ **Type Hint Modernisation** â€“ Uses `typing` module generics (`List`, `Dict`, `Tuple`) for Python 3.8 compatibility, with `from __future__ import annotations` for deferred evaluation.
- ğŸ§¼ **Ruff Formatting by Default** â€“ Adopted `ruff format` across the repository, keeping style consistent with the Ruff rule set.

### Version 3.1.0 - Type-Safe Protocols & Tooling

- âœ… **260-File Type Coverage** â€“ DataFrame mixins now implement structural typing protocols (`SupportsDataFrameOps`), giving a clean `mypy` run across the entire project.
- ğŸ§¹ **Zero Ruff Debt** â€“ Repository-wide linting is enabled by default; `ruff check` passes with no warnings thanks to tighter casts, imports, and configuration.
- ğŸ§­ **Backend Selection Docs** â€“ Updated configuration builder and new `docs/backend_selection.md` make it trivial to toggle between Polars, Memory, File, or DuckDB backends.
- ğŸ§ª **Delta Schema Evolution Fixes** â€“ Polars mergeSchema appends now align frames to the on-disk schema, restoring compatibility with evolving Delta tables.
- ğŸ§° **Improved Test Harness** â€“ `tests/run_all_tests.sh` respects virtual environments and ensures documentation examples are executed with the correct interpreter.

### Version 3.0.0+ - Code Quality & Cleanup

**Dependency Cleanup & Type Safety:**

- ğŸ§¹ **Removed Legacy Dependencies** - Removed unused `sqlglot` dependency (legacy DuckDB/SQL backend code)
- ğŸ—‘ï¸ **Code Cleanup** - Removed unused legacy SQL translation modules (`sql_translator.py`, `spark_function_mapper.py`)
- âœ… **Type Safety** - Fixed 177 type errors using `ty` type checker, improved return type annotations
- ğŸ” **Linting** - Fixed all 63 ruff linting errors, codebase fully formatted
- âœ… **All Tests Passing** - Full test suite validated (1309+ tests, all passing)
- ğŸ“¦ **Cleaner Dependencies** - Reduced dependency footprint, faster installation

### Version 3.0.0 - MAJOR UPDATE

**Polars Backend Migration:**

- ğŸš€ **Polars Backend** - Complete migration to Polars for thread-safe, high-performance operations
- ğŸ§µ **Thread Safety** - Polars is thread-safe by design - no more connection locks or threading issues
- ğŸ“Š **Parquet Storage** - Tables now persist as Parquet files
- âš¡ **Performance** - Better performance for DataFrame operations
- âœ… **All tests passing** - Full test suite validated with Polars backend
- ğŸ“¦ **Production-ready** - Stable release with improved architecture

See [Migration Guide](https://sparkless.readthedocs.io/en/latest/migration_from_v2_to_v3.html) for details.

---

## Documentation

ğŸ“š **Full documentation available at [sparkless.readthedocs.io](https://sparkless.readthedocs.io/)**

### Getting Started
- ğŸ“– [Installation & Setup](https://sparkless.readthedocs.io/en/latest/getting_started.html)
- ğŸ¯ [Quick Start Guide](https://sparkless.readthedocs.io/en/latest/getting_started.html#quick-start)
- ğŸ”„ [Migration from PySpark](https://sparkless.readthedocs.io/en/latest/guides/migration.html)

### Related Packages
- ğŸ”§ [spark-ddl-parser](https://github.com/eddiethedean/spark-ddl-parser) - Zero-dependency PySpark DDL schema parser

### Core Concepts
- ğŸ“Š [API Reference](https://sparkless.readthedocs.io/en/latest/api_reference.html)
- ğŸ”„ [Lazy Evaluation](https://sparkless.readthedocs.io/en/latest/guides/lazy_evaluation.html)
- ğŸ—„ï¸ [SQL Operations](https://sparkless.readthedocs.io/en/latest/sql_operations_guide.html)
- ğŸ’¾ [Storage & Persistence](https://sparkless.readthedocs.io/en/latest/storage_serialization_guide.html)

### Advanced Topics
- âš™ï¸ [Configuration](https://sparkless.readthedocs.io/en/latest/guides/configuration.html)
- ğŸ“ˆ [Benchmarking](https://sparkless.readthedocs.io/en/latest/guides/benchmarking.html)
- ğŸ”Œ [Plugins & Hooks](https://sparkless.readthedocs.io/en/latest/guides/plugins.html)
- ğŸ [Pytest Integration](https://sparkless.readthedocs.io/en/latest/guides/pytest_integration.html)
- ğŸ§µ [Threading Guide](https://sparkless.readthedocs.io/en/latest/guides/threading.html)
- ğŸ§  [Memory Management](https://sparkless.readthedocs.io/en/latest/guides/memory_management.html)
- âš¡ [CTE Optimization](https://sparkless.readthedocs.io/en/latest/guides/cte_optimization.html)

---

## Development Setup

```bash
# Install for development
git clone https://github.com/eddiethedean/sparkless.git
cd sparkless
pip install -e ".[dev]"

# Run all tests (with proper isolation)
bash tests/run_all_tests.sh

# Format code
ruff format .
ruff check . --fix

# Type checking
mypy sparkless tests

# Linting
ruff check .
```

---

## Contributing

We welcome contributions! Areas of interest:

- âš¡ **Performance** - Further Polars optimizations
- ğŸ“š **Documentation** - Examples, guides, tutorials
- ğŸ› **Bug Fixes** - Edge cases and compatibility issues
- ğŸ§ª **PySpark API Coverage** - Additional functions and methods
- ğŸ§ª **Tests** - Additional test coverage and scenarios

---

## Known Limitations

While Sparkless provides comprehensive PySpark compatibility, some advanced features are planned for future releases:

- **Error Handling**: Enhanced error messages with recovery strategies
- **Performance**: Advanced query optimization, parallel execution, intelligent caching
- **Enterprise**: Schema evolution, data lineage, audit logging
- **Compatibility**: PySpark 3.6+, Iceberg support

**Want to contribute?** These are great opportunities for community contributions!

---

## License

MIT License - see [LICENSE](LICENSE) file for details.

---

## Links

- **GitHub**: [github.com/eddiethedean/sparkless](https://github.com/eddiethedean/sparkless)
- **PyPI**: [pypi.org/project/sparkless](https://pypi.org/project/sparkless/)
- **Documentation**: [sparkless.readthedocs.io](https://sparkless.readthedocs.io/)
- **Issues**: [github.com/eddiethedean/sparkless/issues](https://github.com/eddiethedean/sparkless/issues)

---

<div align="center">

**Built with â¤ï¸ for the PySpark community**

*Star â­ this repo if Sparkless helps speed up your tests!*

</div>
