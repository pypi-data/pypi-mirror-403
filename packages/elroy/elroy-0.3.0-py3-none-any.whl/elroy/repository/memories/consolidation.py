from dataclasses import dataclass
from functools import cached_property, partial
from typing import List, Optional

import numpy as np
from pydantic import BaseModel
from toolz import pipe
from toolz.curried import map, take

from ...core.ctx import ElroyContext
from ...core.logging import get_logger
from ...core.tracing import tracer
from ...db.db_models import Memory
from ...io.base import ElroyIO
from ...models import MemoryResponse
from .prompts import get_memory_consolidation_prompt

logger = get_logger()


@dataclass
class MemoryCluster:
    memories: List[Memory]
    embeddings: np.ndarray

    def __len__(self):
        return len(self.memories)

    def __str__(self) -> str:
        # Return a string representation of the object
        return pipe(
            self.memories,
            map(lambda x: "\n".join(["## Memory Title:", x.name, x.text])),
            list,
            "\n".join,
            lambda x: "#Memory Cluster:\n" + x,
        )  # type: ignore

    def __lt__(self, other: "MemoryCluster") -> bool:
        """Define default sorting behavior.
        First sort by cluster size (larger clusters first)
        Then by mean distance (tighter clusters first)"""

        return self._sort_key < other._sort_key

    @property
    def _sort_key(self):
        # Sort such that clusters early in a list are those that are most in need of consolidation.
        # Sort by: cluster size and then mean distance (ie tightness of cluster)
        return (-len(self), self.mean_distance)

    def token_count(self, chat_model_name: str):
        from litellm.utils import token_counter

        return token_counter(chat_model_name, text=str(self))

    @cached_property
    def distance_matrix(self) -> np.ndarray:
        """Lazily compute and cache the distance matrix."""
        from scipy.spatial.distance import cosine  # lazy load

        size = len(self)
        _distance_matrix = np.zeros((size, size))
        for i in range(size):
            for j in range(i + 1, size):
                dist = cosine(self.embeddings[i], self.embeddings[j])
                _distance_matrix[i, j] = dist
                _distance_matrix[j, i] = dist
        return _distance_matrix

    @cached_property
    def mean_distance(self) -> float:
        """Calculate the mean intra cluster distance between all pairs of embeddings in the cluster using cosine similarity"""
        if len(self) < 2:
            return 0.0

        dist_matrix = self.distance_matrix
        # Get upper triangle of matrix (excluding diagonal of zeros)
        upper_triangle = dist_matrix[np.triu_indices_from(dist_matrix, k=1)]
        return float(np.mean(upper_triangle))

    def get_densest_n(self, n: int = 2) -> "MemoryCluster":
        """Get a new MemoryCluster containing the n members with lowest mean distance to other cluster members.

        Args:
            n: Number of members to return. Defaults to 2.

        Returns:
            A new MemoryCluster containing the n members with lowest mean distance to other members.
        """
        if len(self) <= n:
            return self

        dist_matrix = self.distance_matrix
        # Calculate mean distance for each member (excluding self-distance on diagonal)
        mean_distances = []
        for i in range(len(self)):
            # Get all distances except the diagonal (which is 0)
            member_distances = np.concatenate([dist_matrix[i, :i], dist_matrix[i, i + 1 :]])
            mean_dist = np.mean(member_distances)
            mean_distances.append((mean_dist, i))

        # Sort by mean distance and take top n indices
        mean_distances.sort(key=lambda x: x[0])
        closest_indices = [idx for _, idx in mean_distances[:n]]

        # Create new cluster with selected memories and embeddings
        return MemoryCluster(memories=[self.memories[i] for i in closest_indices], embeddings=self.embeddings[closest_indices])


@tracer.chain
def consolidate_memories(ctx: ElroyContext, cluster_limit: int = 3, io: Optional[ElroyIO] = None):
    """Consolidate memories by finding clusters of similar memories and consolidating them into a single memory."""
    from .queries import get_active_memories

    clusters = pipe(
        get_active_memories(ctx),
        lambda x: _find_clusters(ctx, x, io),
        take(cluster_limit),
        list,
    )

    logger.info(f"Found {len(clusters)} memory clusters to consolidate")

    if io:
        from rich.progress import track

        items = track(clusters, "Consolidating memory clusters")
    else:
        items = iter(clusters)

    for cluster in items:
        assert isinstance(cluster, MemoryCluster)
        consolidate_memory_cluster(ctx, cluster)


def _find_clusters(ctx: ElroyContext, memories: List[Memory], io: Optional[ElroyIO] = None) -> List[MemoryCluster]:

    import time

    start_time = time.perf_counter()

    embeddings = []
    valid_memories = []

    logger.info(f"Gathering embeddings for {len(memories)} memories")

    # TODO: Optimize this to batch load in single query (currently N+1)
    # Challenge: pgvector deserialization issues with some DB configurations
    for memory in memories:
        embedding = ctx.db.get_embedding(memory)
        if embedding is not None:
            embeddings.append(embedding)
            valid_memories.append(memory)

    duration_ms = (time.perf_counter() - start_time) * 1000
    logger.info(f"Got {len(embeddings)} embeddings in {duration_ms:.0f}ms ({len(memories)} queries)")

    if not embeddings:
        raise ValueError("No embeddings found for memories")

    logger.info(f"Creating np array")
    embeddings_array = np.array(embeddings)

    from sklearn.cluster import DBSCAN  # lazy load

    logger.info("Calculating clusters")
    clustering = DBSCAN(
        eps=ctx.memory_cluster_similarity_threshold,
        metric="cosine",
        min_samples=ctx.min_memory_cluster_size,
    ).fit(embeddings_array)

    # Group memories by cluster
    clusters = {}
    for idx, label in enumerate(clustering.labels_):
        if label == -1:  # Skip noise points
            continue
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(idx)

    # Create MemoryCluster objects
    clusters = pipe(
        [
            MemoryCluster(
                embeddings=embeddings_array[indices],
                memories=[valid_memories[i] for i in indices],
            )
            for indices in clusters.values()
        ],
        map(lambda x: x.get_densest_n(ctx.max_memory_cluster_size)),
        list,
        partial(sorted),
    )

    return clusters


@tracer.chain
def create_consolidated_memory(ctx: ElroyContext, name: str, text: str, sources: List[Memory]):
    from .operations import do_create_memory, mark_inactive

    logger.info(f"Creating consolidated memory {name} for user {ctx.user_id}")
    logger.info(f"source memories are: {', '.join([m.name for m in sources])}")

    memory = do_create_memory(
        ctx,
        name,
        text,
        sources,
        False,
    )

    [mark_inactive(ctx, m) for m in sources]
    assert isinstance(memory, Memory)
    memory_id = memory.id
    assert memory_id
    return memory_id


@tracer.chain
def consolidate_memory_cluster(ctx: ElroyContext, cluster: MemoryCluster):
    """Consolidate memory cluster using fast model for efficiency."""

    class ConsolidationResponse(BaseModel):
        reasoning: str  # noqa F841
        memories: List[MemoryResponse]

    logger.info(f"Consolidating memories {len(cluster)} memories in cluster.")
    for memory in cluster.memories:
        logger.info(f"Will consolidate: {memory.name}")
    response = ctx.fast_llm.query_llm_with_response_format(
        system=get_memory_consolidation_prompt(),
        prompt=str(cluster),
        response_format=ConsolidationResponse,
    )

    for memory in response.memories:
        create_consolidated_memory(ctx, memory.name, memory.text, cluster.memories)
