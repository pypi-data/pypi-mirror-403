import fnmatch
import hashlib
import os
import re
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, Generator, Iterator, List

from ...core.constants import RecoverableToolError, allow_unused
from ...core.ctx import ElroyContext
from ...core.logging import get_logger
from ...core.tracing import tracer
from ...db.db_models import DocumentExcerpt, SourceDocument
from ...llm.client import LlmClient
from ...utils.clock import utc_now
from ..memories.operations import do_create_memory
from ..recall.operations import upsert_embedding_if_needed
from .queries import (
    get_source_doc_by_address,
    get_source_doc_by_content_md5,
    get_source_doc_excerpts,
)

logger = get_logger()


@dataclass
class DocumentChunk:
    address: str
    content: str
    chunk_index: int


@allow_unused
@tracer.chain
def convert_to_text(llm: LlmClient, content: str) -> str:
    return llm.query_llm(
        system="Your task is to convert the following text into plain text. You should NOT summarize content, "
        "but rather convert it into plain text. That is, the information in the output should be the same as the information in the input.",
        prompt=content,
    )


class DocIngestStatus(Enum):
    SUCCESS = "Document has been ingested successfully."
    UPDATED = "Document has been re-ingested successfully."
    UNCHANGED = "Document not ingested as it has not changed."
    TOO_LONG = "Document exceeds the configured max_ingested_doc_lines, and was not ingested."
    PENDING = "Document is queued for ingestion"
    UNSUPPORTED_FORMAT = "Document format is not supported"
    MOVED = "Document existing document that had a different address, so existing doc address was updated."


def should_process_file(path: Path, include: List[str], exclude: List[str]) -> bool:
    """
    Determine if a file or directory should be processed based on include and exclude glob patterns.

    Args:
        path (Path): The path to the file or directory to check
        include (str, optional): Comma-separated glob patterns to include. If specified, path must match at least one pattern.
        exclude (list[str], optional): List of glob patterns to exclude. If specified, path must not match any pattern.

    Returns:
        bool: True if the path should be processed, False otherwise
    """
    # Skip dot files and directories (files/directories starting with .)
    if path.name.startswith("."):
        return False

    path_str = str(path)

    # First check exclude patterns against full path
    if any(fnmatch.fnmatch(path_str, pattern) for pattern in exclude):
        return False

    # Then check include patterns against just filename if includes specified
    if include:
        return any(fnmatch.fnmatch(path.name, pattern) for pattern in include)

    return True


def recursive_file_walk(directory: Path, include: List[str], exclude: List[str]) -> Generator[Path, Any, None]:
    for root, dirnames, files in os.walk(directory):
        root_path = Path(root)

        # Filter out dot directories and those matching exclude patterns
        dirnames[:] = [
            d
            for d in dirnames
            if not d.startswith(".") and not any(fnmatch.fnmatch(str(root_path / d / "**"), pattern) for pattern in exclude)
        ]
        for file in files:
            file_path = Path(os.path.join(root, file))
            if should_process_file(file_path, include, exclude):
                yield file_path


def do_ingest_dir(
    ctx: ElroyContext,
    directory: Path,
    force_refresh: bool,
    recursive: bool,
    include: List[str],
    exclude: List[str],
) -> Generator[Dict[DocIngestStatus, int], None, None]:
    """
    Recursively ingest all files in a directory that match the include/exclude patterns.

    Args:
        ctx (ElroyContext): The Elroy context
        directory (str): The directory to recursively ingest
        force_refresh (bool, optional): If True, will re-ingest documents even if they seem unchanged. Defaults to False.
        include (str, optional): Comma-separated glob patterns to include. If specified, files must match at least one pattern.
        exclude (list[str], optional): List of glob patterns to exclude. If specified, files and directories must not match any pattern.

    Returns:
        int: Number of files successfully processed
    """
    if not os.path.isdir(directory):
        raise RecoverableToolError(f"{directory} is not a directory.")

    if recursive:
        file_paths = list(recursive_file_walk(directory, include, exclude))
    else:
        file_paths = list(
            (Path(os.path.join(directory, f)) for f in os.listdir(directory) if should_process_file(Path(f), include, exclude))
        )

    statuses = {status: 0 for status in DocIngestStatus}

    yield statuses

    for idx, file_path in enumerate(file_paths):
        try:
            result = do_ingest(ctx, file_path, force_refresh)
            statuses[result] = statuses.get(result, 0) + 1

        except Exception as e:
            logger.error(f"Failed to ingest {file_path}: {str(e)}", exc_info=True)
            raise
        statuses[DocIngestStatus.PENDING] = len(file_paths) - (idx + 1)
        yield statuses

    yield statuses


def do_ingest(ctx: ElroyContext, address: Path, force_refresh: bool) -> DocIngestStatus:
    """Downloads the document at the given address, and extracts content into memory.

    Args:
        address (str): The address of the document. Can be a local file, or a url.
        force (bool, optional): If True, will re-ingest the document even if it has already been ingested and seems to be unchanged. Defaults to False.

    Returns:
        str: The content of the document.
    """
    if os.path.isdir(address):
        raise RecoverableToolError(f"{address} is a directory, please specify a file.")
    elif not os.path.isfile(address):
        raise RecoverableToolError(f"Invalid path: {address}")

    if not is_markdown(address):
        logger.info("non-markdown files may not have optimal results")

    if not os.path.isfile(address):
        raise NotImplementedError("Only local files are supported at the moment.")

    if os.path.isfile(address):
        if not Path(address).is_absolute():
            logger.info(f"Converting relative path {address} to absolute path.")
            address = address.resolve()

    try:
        with open(address, "r", encoding="utf-8") as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        logger.warning(f"Cannot decode file {address} as utf-8, skipping")
        return DocIngestStatus.UNSUPPORTED_FORMAT

    if len(lines) > ctx.max_ingested_doc_lines:
        logger.info(f"Document {address} exceeds max_ingested_doc_lines ({ctx.max_ingested_doc_lines}), skipping")
        return DocIngestStatus.TOO_LONG

    content = "\n".join(lines)

    source_doc = get_source_doc_by_address(ctx, address)

    content_md5 = hashlib.md5(content.encode()).hexdigest()

    doc_was_updated = False

    if source_doc:
        if source_doc.content_md5 != content_md5:
            logger.info("Source doc contents changed, re-ingesting")
        elif force_refresh:
            logger.info(f"Force flag set, re-ingesting doc {address}")
        else:
            logger.info(f"Source doc {address} not changed and no force flag set, skipping")
            return DocIngestStatus.UNCHANGED
        logger.info(f"Refreshing source doc {address}")

        source_doc.content = content
        source_doc.extracted_at = utc_now()
        source_doc.content_md5 = content_md5
        mark_source_document_excerpts_inactive(ctx, source_doc)
        doc_was_updated = True
    else:
        # Check for documents with matching content MD5 (moved documents)
        existing_doc_with_same_content = get_source_doc_by_content_md5(ctx, content_md5)
        if existing_doc_with_same_content and existing_doc_with_same_content.address != str(address):
            logger.info(
                f"Found existing document with same content at {existing_doc_with_same_content.address}, updating address to {address}"
            )
            existing_doc_with_same_content.address = str(address)
            existing_doc_with_same_content.name = str(address)
            existing_doc_with_same_content.extracted_at = utc_now()
            ctx.db.persist(existing_doc_with_same_content)
            return DocIngestStatus.MOVED

        logger.info(f"Persisting source document {address}")
        source_doc = SourceDocument(
            user_id=ctx.user_id,
            address=str(address),
            name=str(address),
            content=content,
            content_md5=content_md5,
            extracted_at=utc_now(),
        )

    source_doc = ctx.db.persist(source_doc)
    source_doc_id = source_doc.id
    assert source_doc_id

    logger.info(f"Breaking source document into chunks for storage: {address}")
    for chunk in excerpts_from_doc(address, content):
        title = f"Excerpt {chunk.chunk_index} from doc {address}"
        doc_excerpt = ctx.db.persist(
            DocumentExcerpt(
                source_document_id=source_doc_id,
                chunk_index=chunk.chunk_index,
                content=chunk.content,
                is_active=True,
                user_id=ctx.user_id,
                name=title,
                content_md5=hashlib.md5(chunk.content.encode()).hexdigest(),
            )
        )

        upsert_embedding_if_needed(ctx, doc_excerpt)

        logger.info(f"Creating memory from excerpt of document {address} (chunk {chunk.chunk_index})")

        do_create_memory(
            ctx,
            title,
            chunk.content,
            [doc_excerpt],
            False,
        )
    return DocIngestStatus.SUCCESS if not doc_was_updated else DocIngestStatus.UPDATED


def excerpts_from_doc(address: Path, content: str) -> Generator[DocumentChunk, Any, None]:
    if is_markdown(address):
        yield from chunk_markdown(address, content)
    else:
        yield from chunk_generic(address, content)


def mark_source_document_excerpts_inactive(ctx: ElroyContext, source_document: SourceDocument) -> None:
    for excerpt in get_source_doc_excerpts(ctx, source_document):
        excerpt.is_active = None
        ctx.db.add(excerpt)
    ctx.db.commit()


def chunk_generic(address: Path, content: str, max_chars: int = 3000, overlap: int = 200) -> Iterator[DocumentChunk]:
    """Chunk any text file into overlapping segments of roughly max_chars length.

    Args:
        address: Source file path
        content: Text content to chunk
        max_chars: Target maximum characters per chunk
        overlap: Number of characters to overlap between chunks

    Returns:
        Iterator of DocumentChunk objects
    """

    if not str(address).endswith(".txt"):
        logger.info(f"Chunking file: {address}: Generic file chunker, performance might be suboptimal.")

    # Split on paragraph breaks
    splits = re.split(r"(\n\s*\n)", content)

    last_emitted_chunk = None
    current_chunk = ""

    for split in splits:
        if len(current_chunk) + len(split) < max_chars:
            current_chunk += split
        else:
            if last_emitted_chunk and overlap:
                current_chunk = last_emitted_chunk.content[:-overlap] + current_chunk
            last_emitted_chunk = DocumentChunk(
                str(address),
                current_chunk,
                last_emitted_chunk.chunk_index + 1 if last_emitted_chunk else 0,
            )
            yield last_emitted_chunk
            current_chunk = ""

    if current_chunk:
        if last_emitted_chunk and overlap:
            current_chunk = last_emitted_chunk.content[-overlap:] + current_chunk
        yield DocumentChunk(
            str(address),
            current_chunk,
            last_emitted_chunk.chunk_index + 1 if last_emitted_chunk else 0,
        )


def chunk_markdown(address: Path, content: str, max_tokens: int = 8000, overlap: int = 200) -> Iterator[DocumentChunk]:
    from litellm.utils import token_counter

    # Split on markdown headers or double newlines
    splits = re.split(r"(#{1,6}\s.*?\n|(?:\n\n))", content)

    last_emitted_chunk = None
    current_chunk = ""

    for split in splits:
        if token_counter(text=current_chunk) + token_counter(text=split) < max_tokens:
            current_chunk += split
        else:
            if last_emitted_chunk and overlap:
                current_chunk = last_emitted_chunk.content[:-overlap] + current_chunk
            last_emitted_chunk = DocumentChunk(
                str(address),
                current_chunk,
                last_emitted_chunk.chunk_index + 1 if last_emitted_chunk else 0,
            )
            yield last_emitted_chunk
            current_chunk = ""
    if current_chunk and overlap and last_emitted_chunk:
        current_chunk = last_emitted_chunk.content[-overlap:] + current_chunk
    yield DocumentChunk(
        str(address),
        current_chunk,
        last_emitted_chunk.chunk_index + 1 if last_emitted_chunk else 0,
    )


def is_markdown(address: Path) -> bool:
    return str(address).endswith(".md") or str(address).endswith(".markdown")
