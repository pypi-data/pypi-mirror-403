"""
Functions to parse margin data compiled by Prof. Aswath Damodaran, Stern School of Business, NYU.

Provides a non-parametric estimate of the kernel bandwidth for adding
Gaussian noise when resampling from the empirical distribution in the
source (margin) data.

Data are downloaded or reused from a local copy, on demand.

For terms of use of Prof. Damodaran's data, please see:
https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datahistory.html

Notes
-----
Prof. Damodaran notes that data construction may not be consistent from
iteration to iteration (year to year). He also notes that, "the best use
for my data is in real time corporate financial analysis
and valuation."

Here, gross margin data compiled by Prof. Damodaran are
optionally used to model the distribution of price-cost margin
across firms that antitrust enforcement agencies are likely to review in
merger enforcement investigations over a multi-year span. The resampling
procedure implemented here reflects an assumption that Dr. Damodaran's
*margin* data for US firms (reported under "Cash Flow Estimation") can be
to obtain consistent samples by resampling average margins with replacement,
using the average firm counts as probability weights, and adding Gaussian noise
(scaled by the estimated bandwidth).

Other procedures included in this package allow the researcher to
generate margins for a single firm and impute margins of other firms by
either margin symmetry, cost-symmetry or restrictions from
first-order conditions (FOCs) for profit maximization by firms facing MNL demand.
These restrictions are expected to result in a distribution of margins, over the
data for firms other than the benchmark firm, that does not match
the empirical margin distribution.

In summary, data generated by relying on Prof. Damodaran's margin data are
useful for exploring the implications of merger enforcement policy, but cannot
be considered representative of realized margins for modeling, say,
financial performance or investment decisions.
"""

import datetime
import os
import re
import shutil
from pathlib import Path
from types import MappingProxyType

import certifi
import numpy as np
import urllib3
from bs4 import BeautifulSoup
from KDEpy.bw_selection import improved_sheather_jones  # type: ignore
from python_calamine import CalamineWorkbook

from .. import VERSION, ArrayDouble, this_yaml, zipfile
from .. import WORK_DIR as PKG_WORK_DIR
from .._serialization import _mappingproxy_from_mapping
from . import MGNDATA_ARCHIVE_PATH, EmpiricalMarginData

__version__ = VERSION

WORK_DIR: Path = globals().get("WORK_DIR", PKG_WORK_DIR)
"""Redefined, in case the user defines WORK_DIR between module imports."""


type DamodaranMarginData = MappingProxyType[
    str, MappingProxyType[str, MappingProxyType[str, float | int]]
]

FINANCIAL_SERVICES = {
    _i.upper()
    for _i in (
        "Bank (Money Center)",
        "Banks (Regional)",
        "Brokerage & Investment Banking",
        "Financial Svcs. (Non-bank & Insurance)",
        "Insurance (General)",
        "Insurance (Life)",
        "Insurance (Prop/Cas.)",
        "Investments & Asset Management",
        "R.E.I.T.",
        "Retail (REITs)",
        "Reinsurance",
    )
}


def margin_data_builder(
    _margin_data_dict: DamodaranMarginData | None = None,
) -> EmpiricalMarginData:
    """Derive average firm-counts and gross-margins by industry from source data."""
    _margin_data_dict = (
        margin_data_getter() if _margin_data_dict is None else _margin_data_dict
    )

    # Compile distinct set of industries in source data
    dmd_keys = set()
    for _, _v in _margin_data_dict.items():
        dmd_keys |= set(_v.keys())
    dmd_keys = sorted(dmd_keys)

    _MGN_DATA_BLANK = {"GROSS MARGIN": 0.0, "NUMBER OF FIRMS": 0}
    dist_parms = np.array([np.nan, np.nan], float)
    for _sk in dmd_keys:
        if _sk in FINANCIAL_SERVICES or _sk.startswith("TOTAL"):
            continue

        # Compile gross margin data across years for given industry, _sk from source
        gm, fc = zip(
            *[
                [_v.get(_sk, _MGN_DATA_BLANK)[_f] for _f in _MGN_DATA_BLANK]
                for _v in _margin_data_dict.values()
            ],
            strict=True,
        )

        average_margin, firm_count = np.array(gm, float), np.array(fc, int)
        # print(firm_count, average_margin)

        # Append average gross margin and average firm count for industry, _sk.
        # We filter out negative and zero margins as bad data. For this reason, and
        # also because the data may potentially add new industries, we return the
        # weighted-average firm-count, rather than total firm count, for weighting
        # downstream
        dist_parms = np.vstack((
            dist_parms,
            np.array((
                np.average(
                    average_margin,
                    weights=(average_margin > 0) * (firm_count > 0) * firm_count,
                ),
                np.average(firm_count, weights=(average_margin > 0) * (firm_count > 0)),
            )),
        ))

    dist_parms = dist_parms[1:, :].view(ArrayDouble)

    _obs, _wts = (dist_parms[:, _f] for _f in range(2))

    # np.average retains the array-type of obs and _wts, which throws
    # the type-checker off; the casting below shoudld be costless
    _gm_avg, num_firms = (
        float(_f) for _f in np.average(_obs, weights=_wts, returned=True)
    )

    _gm_std = np.sqrt(
        np.average((_obs - _gm_avg) ** 2, weights=_wts)
        * num_firms
        * len(_obs)
        / ((num_firms - len(_obs)) * (len(_obs) - 1))
    )

    # Specifying data and weights for KDEpy functions is done differently than
    # in numpy and scipy functions, and improved_sheather_jones() is from KDEpy.
    # Please mind the syntax gap.
    _bw = improved_sheather_jones(dist_parms[:, [0]], dist_parms[:, 1])

    return EmpiricalMarginData(
        _obs, _wts, [_gm_avg, _gm_std, _obs.min(), _obs.max()], _bw
    )


def margin_data_getter(
    *, data_archive_path: Path = MGNDATA_ARCHIVE_PATH, data_download_flag: bool = False
) -> DamodaranMarginData:
    """Download and parse Prof.Damodaran's margin data."""
    if data_archive_path.is_file() and not data_download_flag:
        with zipfile.ZipFile(data_archive_path) as _yzp:
            _margin_data: DamodaranMarginData = this_yaml.load(
                _yzp.read(data_archive_path.with_suffix(".yaml").name)
            )
        return _margin_data

    # Get workbooks from source
    if data_download_flag or not list(data_archive_path.glob("margin*.xls")):
        margin_data_downloader()

    #  Parse workbooks and save margin data dictionary
    margin_dict: dict[str, dict[str, MappingProxyType[str, float]]] = {}
    for _p in (WORK_DIR / "damodaran_margin_data_archive").iterdir():
        xl_wbk = CalamineWorkbook.from_path(_p)
        xl_wks = xl_wbk.get_sheet_by_index(
            0
            if (_p.stem.startswith("margin") and _p.stem[-2:] in {"17", "18", "19"})
            else 1
        ).to_python()
        if xl_wks[8][2] != "Gross Margin":
            raise ValueError("Worksheet does not match expected layout.")
        row_keys: list[str] = [_c.upper() for _c in xl_wks[8][1:]]  # type: ignore

        _u = xl_wks[0][1]
        if not isinstance(_u, datetime.date):
            print(_u)
            print(xl_wks[:8])
            raise ValueError("Worksheet does not match expected layout.")
        update: str = _u.isoformat()[:10]

        # Below, margin_dict_annual is a pointer to margin_dict[update]
        # Updating margin_dict_annual updates margin_dict
        margin_dict_annual = margin_dict.setdefault(update, {})
        for xl_row in xl_wks[9:]:
            row_key = _s.upper() if isinstance((_s := xl_row[0]), str) else ""

            if not row_key or row_key.startswith("TOTAL"):
                continue
            else:
                xl_row[1] = int(xl_row[1])  # type: ignore
                margin_dict_annual |= MappingProxyType({
                    row_key: MappingProxyType(
                        dict(zip(row_keys, xl_row[1:], strict=True))  # type: ignore
                    )
                })

    _margin_data: DamodaranMarginData = _mappingproxy_from_mapping(margin_dict)  # type: ignore[no-redef]
    with (
        zipfile.ZipFile(data_archive_path, "w", compression=93) as _yzp,
        _yzp.open(f"{data_archive_path.stem}.yaml", "w") as _yfh,
    ):
        this_yaml.dump(_margin_data, _yfh)

    shutil.copy2(
        data_archive_path, Path(__file__).parents[1] / "data" / data_archive_path.name
    )

    return _margin_data


def margin_data_downloader() -> None:
    """Download Prof.Damodaran's margin data."""
    _u3pm = urllib3.PoolManager(ca_certs=certifi.where())
    _data_source_url = "https://pages.stern.nyu.edu/~adamodar/pc/datasets/"
    _archive_source_url = "https://pages.stern.nyu.edu/~adamodar/pc/archives/"

    dest_dir = WORK_DIR / "damodaran_margin_data_archive"
    if not dest_dir.is_dir():
        dest_dir.mkdir()

    # Get current-year margin data
    workbook_name = "margin.xls"
    workbook_path = dest_dir / workbook_name
    if workbook_path.is_file():
        workbook_path.unlink()

    u3pm = urllib3.PoolManager(ca_certs=certifi.where())
    download_file(u3pm, f"{_data_source_url}{workbook_name}", workbook_path)

    # Get archived margin data
    workbook_re = re.compile(r"margin(?P<YY>\d{2}).xls")
    archive_html = _u3pm.request("GET", _archive_source_url).data.decode("utf-8")
    archive_tree = BeautifulSoup(archive_html, "lxml")
    for _tag in archive_tree.find_all("a"):
        if (
            (_r := workbook_re.fullmatch(_w := _tag.get("href", "")))
            and not (_ryy := _r["YY"]).startswith("9")
            and int(_ryy) > 16
        ):
            _url, _path = f"{_archive_source_url}{_w}", dest_dir / _w
            if _path.is_file():
                _path.unlink()

            download_file(_u3pm, _url, _path)


def download_file(_u3pm: urllib3.PoolManager, _url: str, _path: Path) -> None:
    """Download a a binary file from URL to filesystem path."""
    chunk_size_ = 1024 * 1024
    with (
        _u3pm.request("GET", _url, preload_content=False) as _uh,
        _path.open("wb") as _fh,
    ):
        while True:
            data_ = _uh.read(chunk_size_)
            if not data_:
                break
            _fh.write(data_)
    os.utime(
        _path,
        times=(
            (
                _t := datetime.datetime
                .strptime(_uh.headers["Last-Modified"], "%a, %d %b %Y %H:%M:%S %Z")
                .astimezone(datetime.UTC)
                .timestamp()
            ),
            _t,
        ),
    )

    print(f"Downloaded {_url} to {_path}.")
