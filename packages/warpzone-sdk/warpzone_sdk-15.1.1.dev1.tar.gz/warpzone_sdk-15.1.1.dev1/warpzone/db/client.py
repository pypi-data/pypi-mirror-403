from typing import Optional

import datamazing.pandas as pdz
import pandas as pd
import pyarrow.compute as pc
from azure.core.credentials import (
    AzureNamedKeyCredential,
    AzureSasCredential,
    TokenCredential,
)
from azure.identity import DefaultAzureCredential

from warpzone.deltastorage.slicing import HyperSlice
from warpzone.deltastorage.store import Store


def camel_case_to_snake_case(name: str) -> str:
    """Convert CamelCase to snake_case

    Args:
        name (str): Name in CamelCase
    """
    import re

    name = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
    name = re.sub("([a-z0-9])([A-Z])", r"\1_\2", name)
    return name.lower()


class WarpzoneDatabaseClient:
    def __init__(
        self,
        path: str,
        storage_options: dict[str, str] | None = None,
        table_prefix: str = "",
    ):
        self.store = Store(
            path=path,
            storage_options=storage_options,
        )
        self.table_prefix = table_prefix

    @classmethod
    def from_resource_name(
        cls,
        storage_account: str,
        container_name: str = "datasets",
        sub_path: str = "",
        table_prefix: str = "",
        credential: (
            AzureNamedKeyCredential | AzureSasCredential | TokenCredential
        ) = DefaultAzureCredential(),
    ):
        """Create a WarpzoneDatabaseClient from resource name (storage account).
        This assumes the path of the delta lake is of the form:
        abfss://{container_name}@{storage_account}.dfs.core.windows.net/{sub_path}

        Args:
            storage_account (str): Storage account name.
            container_name (str, optional): Container name. Defaults to "datasets".
            sub_path (str, optional): Sub-path within the container. Defaults to "".
            table_prefix (str, optional): Table prefix to use (e.g. `mz_` for archive).
            Defaults to "".
            credential (optional): Azure credential to use.
            Defaults to DefaultAzureCredential().
        """
        path = f"abfss://{container_name}@{storage_account}.dfs.core.windows.net"
        if sub_path:
            path += f"/{sub_path}"

        token = credential.get_token("https://storage.azure.com/.default")
        storage_options = {
            "account_name": storage_account,
            "token": token.token,
        }

        return cls(
            path=path, storage_options=storage_options, table_prefix=table_prefix
        )

    def get_unit_and_multiple(self, timedelta: pd.Timedelta) -> tuple[str | None, int]:
        """
        Get unit and multiple of a timedelta. E.g. for a timedelta of "PT5M" then
        unit = "minute" and multiple = 5.
        NOTE: Timedelta must have one and only one non-zero component,
        i.e. "PT0S" doesnt work, and neither does "PT5M10S".

        Args:
            timedelta (pd.Timedelta): Timedelta

        Returns:
            tuple[str, int]: Unit and multiple
        """
        components = timedelta.components._asdict()

        # remove plural ending from unit, since
        # this is the standard pyarrow uses
        components = {k[:-1]: v for k, v in components.items()}

        non_zero_components = {
            unit: multiple for unit, multiple in components.items() if multiple != 0
        }

        if len(non_zero_components) == 0:
            return None, 0

        if len(non_zero_components) != 1:
            raise ValueError("Timedelta must have one and only one non-zero multiple.")

        return next(iter(non_zero_components.items()))

    def relative_time_travel_version(
        self, time_column: str, block: pd.Timedelta, horizon: pd.Timedelta
    ) -> pc.Expression:
        """
        Get value to use for filtering a relative time travel
        (i.e. the interval [valid-from, valid-to] must contain
        this value)
        """
        unit, multiple = self.get_unit_and_multiple(block)

        if multiple == 0:
            # `pc.floor_temporal` fails with multiple=0,
            # but in this case we don't need to floor
            # the time anyway
            start_of_block = pc.field("time_utc")
        else:
            start_of_block = pc.floor_temporal(
                pc.field(time_column),
                multiple=multiple,
                unit=unit,
            )

        return start_of_block - horizon.to_pytimedelta()

    def time_travel_filter(
        self,
        time_travel: pdz.TimeTravel,
        time_column: str,
        valid_from_column: str,
        valid_to_column: str,
    ) -> list[HyperSlice]:
        """Filter delta table on a time travel

        Args:
            time_travel (pdz.TimeTravel): Time travel
            time_column (str): Time column name
            valid_from_column (str): Valid-from column name
            valid_to_column (str): Valid-to column name
        """
        match time_travel.tense:
            case "absolute":
                # If the time travel is absolute, we filter
                # to entries where [valid-from, valid-to]
                # contains `as_of_time`
                version = time_travel.as_of_time.to_pydatetime()
            case "relative":
                version = self.relative_time_travel_version(
                    time_column, time_travel.block, time_travel.horizon
                )

        return [
            HyperSlice((valid_from_column, "<=", version)),
            HyperSlice((valid_to_column, ">", version)),
        ]

    def query(
        self,
        table_name: str,
        time_interval: Optional[pdz.TimeInterval] = None,
        time_travel: Optional[pdz.TimeTravel] = None,
        filters: Optional[dict[str, object]] = None,
        columns: Optional[list[str]] = None,
        include_validity_period_columns: bool = False,
        include_generated_columns: bool = False,
    ) -> pd.DataFrame:
        """Query table.
        Query defaults are set to match old Table Storage client behavior.
        Time travel defaults to "as of now"
        Validity period columns are dropped by default.
        Generated columns are dropped by default.

        Args:
            table_name (str): Name of the table
            time_interval (Optional[pdz.TimeInterval], optional): Time interval for the
            query. Defaults to None.
            time_travel (Optional[pdz.TimeTravel], optional): Time travel information.
            Defaults to None.
            filters (Optional[dict[str, object]], optional): Filters to apply to the
            query.
            Defaults to None.
            columns (Optional[list[str]], optional): Columns to return.
            Selecting columns can significantly improve query performance.
            Defaults to None, meaning all columns will be returned.
            include_validity_period_columns (bool, optional): Whether to include
            validity period columns in the result;
            (`valid_from_time_utc`, `valid_to_time_utc`).
            Defaults to False. If set to True while using `columns`-argument, make sure
            to include these columns in the `columns`-list.
            include_generated_columns (bool, optional): Whether to include generated
            columns in the result; (e.g. `valid_from_time_utc`, `valid_to_time_utc`).
            Defaults to False. If set to True while using `columns`-argument, make sure
            to include these columns in the `columns`-list.

        Returns:
            pd.DataFrame: The result of the query.
        """
        # We do 'camelCaseToSnake_case' conversion here because the old
        # naming convention used in WarpZone was CamelCase, while the new
        # naming convention is snake_case. The goal is to remove this
        #  when the CamelCase naming convention is no longer used.
        table_name = camel_case_to_snake_case(table_name)

        table = self.store.get_table(table_name)
        hyper_slice = []

        if filters:
            for key, value in filters.items():
                if isinstance(value, (list, tuple, set)):
                    hyper_slice.append((key, "in", value))
                else:
                    hyper_slice.append((key, "=", value))
        if time_interval:
            hyper_slice.append(("time_utc", ">=", time_interval.left))
            hyper_slice.append(("time_utc", "<=", time_interval.right))

        if time_travel is None:
            time_travel = pdz.TimeTravel(
                as_of_time=pd.Timestamp.utcnow(),
            )

        tt_filter = self.time_travel_filter(
            time_travel,
            time_column="time_utc",
            valid_from_column="valid_from_time_utc",
            valid_to_column="valid_to_time_utc",
        )

        hyper_slice.extend(tt_filter)
        pl_df = table.read(hyper_slice=HyperSlice(hyper_slice), columns=columns)

        pd_df = pl_df.to_pandas()

        # We truncate to second, and change to nanosecond
        # precision because this was used by the old solution (Azure Table Storage)
        for col in pd_df.select_dtypes(include=["datetime", "datetimetz"]).columns:
            pd_df[col] = pd_df[col].dt.floor("s").dt.as_unit("ns")

        # Drop generated columns
        if not include_generated_columns:
            generated_cols = []
            for field in table.schema().fields:
                if field.generated_as is not None:
                    generated_cols.append(field.column_name)
            pd_df = pd_df.drop(columns=generated_cols)

        # Drop valid-from/to columns
        if not include_validity_period_columns:
            pd_df = pd_df.drop(columns=["valid_from_time_utc", "valid_to_time_utc"])

        return pd_df
