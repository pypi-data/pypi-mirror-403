# EvalVault Analysis Report

**Run ID:** `1d91a667-4288-4742-be3a-a8f5310c5140`
**Generated:** 2026-01-04 00:53:05

## Executive Summary

**Overall Status:** âŒ FAIL
**Pass Rate:** 0.0%

### Metric Pass Rates
| Metric | Pass Rate |
|--------|-----------|
| answer_relevancy | 0.0% |
| faithfulness | 80.0% |

## Statistical Analysis

### Metric Statistics
| Metric | Mean | Std | Min | Max |
|--------|------|-----|-----|-----|
| answer_relevancy | 0.000 | 0.000 | 0.000 | 0.000 |
| faithfulness | 0.800 | 0.400 | 0.000 | 1.000 |

### Low Performing Cases
| Test Case | Question | Metric | Score |
|-----------|----------|--------|-------|
| kr-001 | ì´ ë³´í—˜ì˜ ì‚¬ë§ë³´í—˜ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”? | answer_relevancy | 0.00 |
| kr-002 | ë³´í—˜ë£Œ ë‚©ì… ê¸°ê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”? | faithfulness | 0.00 |
| kr-002 | ë³´í—˜ë£Œ ë‚©ì… ê¸°ê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”? | answer_relevancy | 0.00 |
| kr-003 | ì•” ì§„ë‹¨ë¹„ëŠ” ì–¼ë§ˆë¥¼ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”? | answer_relevancy | 0.00 |
| kr-004 | ë³´í—˜ í•´ì§€ ì‹œ í™˜ê¸‰ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”? | answer_relevancy | 0.00 |

### Insights
- Low pass rate requires attention: 0.0%
- Best performing metric: faithfulness (mean: 0.800)
- Worst performing metric: answer_relevancy (mean: 0.000)
- High variance metrics (std > 0.2): faithfulness
- Found 6 low-performing test cases (score < threshold)
- Most problematic metric: answer_relevancy (5 low performers)

## Recommendations

> **Critical:** Overall pass rate is below 50%. Review the evaluation pipeline and data quality.

> **Improve answer_relevancy:** Pass rate is 0.0%. Consider reviewing context quality and answer generation.

> **Review low performers:** 6 cases are underperforming. Consider retraining or adjusting prompts.

---

*Report generated by EvalVault on 2026-01-04 00:53:05*

## Stage Metrics Summary

- Total metrics: 13
- Evaluated: 7
- Passed: 7 / Failed: 0
- Pass rate: 100.0%


# RAG ê°œì„  ê°€ì´ë“œ ë¦¬í¬íŠ¸

## ìš”ì•½

- **í‰ê°€ ID**: `1d91a667-4288-4742-be3a-a8f5310c5140`
- **ìƒì„± ì‹œê°„**: 2026-01-04 00:53:05
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 5ê°œ
- **í†µê³¼ìœ¨**: 0.0%

### ë©”íŠ¸ë¦­ë³„ í˜„í™©

| ë©”íŠ¸ë¦­ | ì ìˆ˜ | ëª©í‘œ | ê°­ | ìƒíƒœ |
|--------|------|------|-----|------|
| faithfulness | 0.800 | 0.70 | -0.100 | âœ… |
| answer_relevancy | 0.000 | 0.70 | +0.700 | âŒ |

### ë‹¨ê³„ ë©”íŠ¸ë¦­ ìš”ì•½

- ì´ ë©”íŠ¸ë¦­: 13ê°œ
- í‰ê°€ ëŒ€ìƒ(ì„ê³„ê°’ ìˆìŒ): 7ê°œ
- í†µê³¼: 7ê°œ / ì‹¤íŒ¨: 0ê°œ
- í†µê³¼ìœ¨: 100.0%

---

## 1. Generator ê°œì„  ğŸŸ 

**ìš°ì„ ìˆœìœ„**: p1_high
**ëŒ€ìƒ ë©”íŠ¸ë¦­**: answer_relevancy

### ë¬¸ì œ íŒ¨í„´

- **off_topic_response**: 5/5ê±´ (100.0%)

### ê°œì„  ì•¡ì…˜

#### 1. ì§ˆë¬¸ ì¬ê°•ì¡° í”„ë¡¬í”„íŠ¸ (ì˜ˆìƒ ê°œì„ : +12%) ğŸŸ¢

í”„ë¡¬í”„íŠ¸ì—ì„œ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì–¸ê¸‰

```
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
"""
ì§ˆë¬¸: {question}

ì»¨í…ìŠ¤íŠ¸: {context}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
ë‹¤ë¥¸ ì£¼ì œë¡œ ë²—ì–´ë‚˜ì§€ ë§ˆì„¸ìš”.

ì§ˆë¬¸ ë‹¤ì‹œ í™•ì¸: {question}

ë‹µë³€:
"""

```

#### 2. Few-shot ì˜ˆì œ (ì˜ˆìƒ ê°œì„ : +15%) ğŸŸ¢

ì¢‹ì€ ë‹µë³€ ì˜ˆì‹œ ì œê³µ

```
few_shot_examples = [
    {"question": "ë³´í—˜ë£Œ ë‚©ì… ë°©ë²•ì€?",
     "answer": "ë³´í—˜ë£ŒëŠ” ì›”ë‚©, ì—°ë‚©, ì¼ì‹œë‚© ì¤‘ ì„ íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤."},
    {"question": "í•´ì§€í™˜ê¸‰ê¸ˆì€ ì–¸ì œ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
     "answer": "í•´ì§€ ì‹ ì²­ í›„ 3ì˜ì—…ì¼ ì´ë‚´ ì§€ê¸‰ë©ë‹ˆë‹¤."}
]

# í”„ë¡¬í”„íŠ¸ì— ì˜ˆì œ í¬í•¨
prompt = f"""
ë‹¤ìŒ ì˜ˆì‹œì²˜ëŸ¼ ë‹µë³€í•˜ì„¸ìš”:
{format_examples(few_shot_examples)}

ì§ˆë¬¸: {question}
ë‹µë³€:
"""

```

### ê²€ì¦ ë°©ë²•

```bash
# ê°œì„  ì „/í›„ ë¹„êµ
evalvault run dataset.json --metrics answer_relevancy --tag baseline
# ... ê°œì„  ì ìš© ...
evalvault run dataset.json --metrics answer_relevancy --tag after_fix
evalvault compare baseline after_fix --metrics answer_relevancy
```

---

*Generated by EvalVault*
