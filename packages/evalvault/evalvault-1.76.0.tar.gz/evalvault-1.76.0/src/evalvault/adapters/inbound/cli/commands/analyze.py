"""EvalVault CLIì˜ ë¶„ì„ ê´€ë ¨ ëª…ë ¹."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

import typer
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from evalvault.adapters.outbound.analysis import (
    CausalAnalysisAdapter,
    HypothesisGeneratorModule,
    NetworkAnalyzerModule,
    NLPAnalysisAdapter,
    StatisticalAnalysisAdapter,
    TimeSeriesAdvancedModule,
)
from evalvault.adapters.outbound.analysis.pipeline_factory import (
    build_analysis_pipeline_service,
)
from evalvault.adapters.outbound.analysis.pipeline_helpers import to_serializable
from evalvault.adapters.outbound.cache import MemoryCacheAdapter
from evalvault.adapters.outbound.llm import get_llm_adapter
from evalvault.adapters.outbound.report import DashboardGenerator, MarkdownReportAdapter
from evalvault.adapters.outbound.storage.factory import build_storage_adapter
from evalvault.adapters.outbound.storage.postgres_adapter import PostgreSQLStorageAdapter
from evalvault.config.phoenix_support import get_phoenix_trace_url
from evalvault.config.settings import Settings, apply_profile
from evalvault.domain.entities import EvaluationRun
from evalvault.domain.entities.analysis_pipeline import AnalysisIntent
from evalvault.domain.services.analysis_service import AnalysisService

from ..utils.analysis_io import (
    build_comparison_scorecard,
    extract_markdown_report,
    get_node_output,
    resolve_artifact_dir,
    resolve_output_paths,
    serialize_pipeline_result,
    write_json,
    write_pipeline_artifacts,
)
from ..utils.options import db_option, profile_option
from ..utils.validators import parse_csv_option

_console = Console()


def register_analyze_commands(app: typer.Typer, console: Console) -> None:
    """Attach analyze/analyze-compare commands to the root Typer app."""

    global _console
    _console = console

    @app.command()
    def analyze(  # noqa: PLR0913 - CLI ì˜µì…˜ ë‹¤ì–‘ì„±ì„ ìœ„í•œ ê¸¸ì´ í—ˆìš©
        run_id: str = typer.Argument(..., help="ë¶„ì„í•  Run ID"),
        nlp: bool = typer.Option(False, "--nlp", "-N", help="NLP ë¶„ì„ í¬í•¨"),
        causal: bool = typer.Option(False, "--causal", "-c", help="ì¸ê³¼ ë¶„ì„ í¬í•¨"),
        playbook: bool = typer.Option(
            False, "--playbook", "-B", help="í”Œë ˆì´ë¶ ê¸°ë°˜ ê°œì„  ë¶„ì„ í¬í•¨"
        ),
        enable_llm: bool = typer.Option(
            False,
            "--enable-llm",
            "-L",
            help="í”Œë ˆì´ë¶ ë¶„ì„ì—ì„œ LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±",
        ),
        dashboard: bool = typer.Option(False, "--dashboard", help="ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±"),
        dashboard_format: str = typer.Option(
            "png", "--dashboard-format", help="ëŒ€ì‹œë³´ë“œ ì¶œë ¥ í˜•ì‹ (png, svg, pdf)"
        ),
        anomaly_detect: bool = typer.Option(
            False, "--anomaly-detect", "-A", help="ì´ìƒì¹˜ íƒì§€ ì‹¤í–‰ (Phase 2)"
        ),
        window_size: int = typer.Option(
            200, "--window-size", "-w", help="ì´ìƒì¹˜ íƒì§€ ìœˆë„ í¬ê¸°", min=50, max=500
        ),
        forecast: bool = typer.Option(False, "--forecast", "-F", help="ì„±ëŠ¥ ì˜ˆì¸¡ ì‹¤í–‰ (Phase 2)"),
        forecast_horizon: int = typer.Option(
            3, "--forecast-horizon", help="ì˜ˆì¸¡ ë²”ìœ„(ëŸ° ê°œìˆ˜)", min=1, max=10
        ),
        network: bool = typer.Option(
            False, "--network", help="ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ìƒì„± (Phase 3)"
        ),
        min_correlation: float = typer.Option(
            0.5, "--min-correlation", help="ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ìƒê´€ê³„ìˆ˜", min=0, max=1
        ),
        generate_hypothesis: bool = typer.Option(
            False, "--generate-hypothesis", "-H", help="ê°€ì„¤ ìë™ ìƒì„± (Phase 4)"
        ),
        hypothesis_method: str = typer.Option(
            "heuristic",
            "--hypothesis-method",
            help="ê°€ì„¤ ìƒì„± ë°©ì‹ (heuristic, hyporefine, union)",
        ),
        num_hypotheses: int = typer.Option(
            5, "--num-hypotheses", help="ìƒì„±í•  ê°€ì„¤ ìˆ˜", min=1, max=20
        ),
        output: Path | None = typer.Option(None, "--output", "-o", help="JSON ì¶œë ¥ íŒŒì¼"),
        report: Path | None = typer.Option(
            None, "--report", "-r", help="ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼ (*.md ë˜ëŠ” *.html)"
        ),
        excel_output: Path | None = typer.Option(
            None, "--excel-output", help="ë¶„ì„ ê²°ê³¼ Excel ì¶œë ¥ ê²½ë¡œ"
        ),
        save: bool = typer.Option(False, "--save", "-S", help="ë¶„ì„ ê²°ê³¼ DB ì €ì¥"),
        db_path: Path | None = db_option(help_text="DB ê²½ë¡œ"),
        profile: str | None = profile_option(
            help_text="NLP ì„ë² ë”©ìš© ëª¨ë¸ í”„ë¡œí•„ (dev, prod, openai)",
        ),
    ) -> None:
        """í‰ê°€ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µê³„ ì¸ì‚¬ì´íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""

        storage = build_storage_adapter(settings=Settings(), db_path=db_path)

        try:
            run = storage.get_run(run_id)
        except KeyError:
            _console.print(f"[red]ì˜¤ë¥˜: Runì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {run_id}[/red]")
            raise typer.Exit(1)

        if not run.results:
            _console.print("[yellow]ê²½ê³ : ë¶„ì„í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            raise typer.Exit(0)
        trace_url = get_phoenix_trace_url(getattr(run, "tracker_metadata", None))

        analysis_adapter = StatisticalAnalysisAdapter()
        cache_adapter = MemoryCacheAdapter()

        # Create NLP adapter if requested
        nlp_adapter = None
        if nlp:
            settings = Settings()
            profile_name = profile or settings.evalvault_profile
            if profile_name:
                settings = apply_profile(settings, profile_name)

            llm_adapter = get_llm_adapter(settings)
            nlp_adapter = NLPAnalysisAdapter(
                llm_adapter=llm_adapter,
                use_embeddings=True,
            )

        causal_adapter = None
        if causal:
            causal_adapter = CausalAnalysisAdapter()

        service = AnalysisService(
            analysis_adapter=analysis_adapter,
            nlp_adapter=nlp_adapter,
            causal_adapter=causal_adapter,
            cache_adapter=cache_adapter,
        )

        _console.print(f"\n[bold]ë¶„ì„ ì‹œì‘: {run_id}[/bold]")
        if trace_url:
            _console.print(f"[dim]Phoenix íŠ¸ë ˆì´ìŠ¤: {trace_url}[/dim]")
        _console.print()
        bundle = service.analyze_run(run, include_nlp=nlp, include_causal=causal)

        if not bundle.statistical:
            _console.print("[yellow]í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            raise typer.Exit(0)

        analysis = bundle.statistical
        _display_analysis_summary(analysis)
        _display_metric_stats(analysis)
        _display_correlations(analysis)
        _display_low_performers(analysis)
        _display_insights(analysis)

        if bundle.has_nlp and bundle.nlp:
            _display_nlp_analysis(bundle.nlp)

        if bundle.has_causal and bundle.causal:
            _display_causal_analysis(bundle.causal)

        improvement_report = None
        if playbook:
            stage_metrics = storage.list_stage_metrics(run_id)
            if not stage_metrics:
                _console.print(
                    "[yellow]ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤. "
                    "`evalvault stage compute-metrics <run_id>` ì‹¤í–‰ í›„ ê°€ì´ë“œë¥¼ í¬í•¨í•˜ì„¸ìš”."
                    "[/yellow]"
                )
            improvement_report = _perform_playbook_analysis(
                run,
                enable_llm,
                profile,
                stage_metrics=stage_metrics,
            )

        def _save_analysis_payload(payload: Any, analysis_type: str) -> None:
            serialized = to_serializable(payload)
            if not isinstance(serialized, dict):
                serialized = {"value": serialized}
            storage.save_analysis_result(
                run_id=run_id,
                analysis_type=analysis_type,
                result_data=serialized,
            )

        if save or excel_output:
            storage.save_analysis(analysis)
            if bundle.nlp is not None:
                storage.save_nlp_analysis(bundle.nlp)
            if bundle.causal is not None:
                _save_analysis_payload(bundle.causal, "causal")
            if improvement_report is not None:
                _save_analysis_payload(improvement_report, "playbook")
            storage_label = (
                "PostgreSQL"
                if isinstance(storage, PostgreSQLStorageAdapter)
                else f"SQLite ({db_path})"
            )
            _console.print(f"\n[green]ë¶„ì„ ê²°ê³¼ DB ì €ì¥: {storage_label}[/green]")

        if dashboard:
            dashboard_gen = DashboardGenerator()
            _console.print("\n[bold cyan]Generating visualization dashboard...[/bold cyan]")

            fig = dashboard_gen.generate_evaluation_dashboard(run_id)

            output_dir = Path("reports/dashboard")
            output_dir.mkdir(parents=True, exist_ok=True)

            output_path = output_dir / f"dashboard_{run_id[:8]}.{dashboard_format}"
            fig.savefig(output_path, dpi=300, bbox_inches="tight")
            _console.print(f"\n[green]Dashboard saved to: {output_path}[/green]")

        anomaly_result = None
        forecast_result = None
        if anomaly_detect or forecast:
            ts_analyzer = TimeSeriesAdvancedModule(window_size=window_size)
            run_history = storage.list_runs(limit=50)

            if not run_history or len(run_history) < 5:
                _console.print("[yellow]Need at least 5 runs for time series analysis.[/yellow]")
            else:
                if anomaly_detect:
                    _console.print("\n[bold cyan]Running anomaly detection...[/bold cyan]")
                    history_data = [
                        {
                            "run_id": r.run_id,
                            "pass_rate": r.pass_rate,
                            "timestamp": r.started_at,
                        }
                        for r in run_history
                    ]
                    anomaly_result = ts_analyzer.detect_anomalies(history_data)
                    _display_anomaly_detection(anomaly_result)

                if forecast:
                    _console.print("\n[bold cyan]Running performance forecasting...[/bold cyan]")
                    history_data = [
                        {"run_id": r.run_id, "pass_rate": r.pass_rate} for r in run_history
                    ]
                    forecast_result = ts_analyzer.forecast_performance(
                        history_data, horizon=forecast_horizon
                    )
                    _display_forecast_result(forecast_result)

        net_result = None
        if network:
            _console.print("\n[bold cyan]Building metric correlation network...[/bold cyan]")
            net_analyzer = NetworkAnalyzerModule()

            if not bundle.statistical or not bundle.statistical.significant_correlations:
                _console.print("[yellow]No significant correlations for network analysis.[/yellow]")
            else:
                correlations_data = [
                    {
                        "variable1": corr.variable1,
                        "variable2": corr.variable2,
                        "correlation": corr.correlation,
                        "p_value": corr.p_value,
                        "is_significant": corr.is_significant,
                    }
                    for corr in bundle.statistical.significant_correlations
                ]
                graph = net_analyzer.build_correlation_network(
                    correlations_data, min_correlation=min_correlation
                )
                net_result = net_analyzer.analyze_metric_network(graph)
                _display_network_analysis(net_result)

        hypotheses = None
        if generate_hypothesis:
            _console.print(
                f"\n[bold cyan]Generating hypotheses ({hypothesis_method})...[/bold cyan]"
            )
            hyp_gen = HypothesisGeneratorModule(
                method=hypothesis_method, num_hypotheses=num_hypotheses
            )

            metric_scores = {}
            for metric_name, stats in analysis.metrics_summary.items():
                metric_scores[metric_name] = stats.mean

            low_performers_data = [
                {
                    "question": lp.test_case_id,
                    "metric_name": lp.metric_name,
                }
                for lp in (analysis.low_performers or [])
            ]

            hypotheses = hyp_gen.generate_simple_hypotheses(
                run_id, metric_scores, low_performers_data
            )
            _display_hypothesis_generation(hypotheses, hypothesis_method)

        if save or excel_output:
            if anomaly_result is not None:
                _save_analysis_payload(anomaly_result, "time_series_anomaly")
            if forecast_result is not None:
                _save_analysis_payload(forecast_result, "time_series_forecast")
            if net_result is not None:
                _save_analysis_payload(net_result, "network")
            if hypotheses is not None:
                _save_analysis_payload(hypotheses, "hypotheses")

        if output:
            _export_analysis_json(analysis, output, bundle.nlp if nlp else None, improvement_report)
            _console.print(f"\n[green]ë¶„ì„ ê²°ê³¼ ë‚´ë³´ëƒ„: {output}[/green]")

        if report:
            _generate_report(bundle, report, include_nlp=nlp, improvement_report=improvement_report)
            _console.print(f"\n[green]ë¦¬í¬íŠ¸ ìƒì„±: {report}[/green]")

        if excel_output:
            exported = storage.export_analysis_results_to_excel(run_id, excel_output)
            _console.print(f"\n[green]Excel ìƒì„±: {exported}[/green]")

    @app.command(name="analyze-compare")
    @app.command(name="compare-analysis")
    def analyze_compare(
        run_id1: str = typer.Argument(..., help="ì²« ë²ˆì§¸ Run ID"),
        run_id2: str = typer.Argument(..., help="ë‘ ë²ˆì§¸ Run ID"),
        metrics: str | None = typer.Option(
            None, "--metrics", "-m", help="ë¹„êµí•  ë©”íŠ¸ë¦­(ì‰¼í‘œ êµ¬ë¶„)"
        ),
        test: str = typer.Option("t-test", "--test", "-t", help="í†µê³„ ê²€ì • (t-test, mann-whitney)"),
        output: Path | None = typer.Option(None, "--output", "-o", help="JSON ì¶œë ¥ íŒŒì¼"),
        report: Path | None = typer.Option(None, "--report", "-r", help="ë¦¬í¬íŠ¸ ì¶œë ¥ íŒŒì¼ (*.md)"),
        output_dir: Path | None = typer.Option(
            None,
            "--output-dir",
            help="ë¹„êµ ì‚°ì¶œë¬¼ ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: reports/comparison)",
        ),
        db_path: Path | None = db_option(help_text="DB ê²½ë¡œ"),
        profile: str | None = profile_option(
            help_text="ë¹„êµ ë¦¬í¬íŠ¸ìš© LLM í”„ë¡œí•„ (dev, prod, openai)",
        ),
    ) -> None:
        """ë‘ ì‹¤í–‰ì„ í†µê³„ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤."""

        storage = build_storage_adapter(settings=Settings(), db_path=db_path)

        try:
            run_a = storage.get_run(run_id1)
            run_b = storage.get_run(run_id2)
        except KeyError as exc:
            _console.print(f"[red]ì˜¤ë¥˜: {exc}[/red]")
            raise typer.Exit(1) from exc

        metric_list = parse_csv_option(metrics)
        if not metric_list:
            metric_list = None

        analysis_adapter = StatisticalAnalysisAdapter()
        service = AnalysisService(analysis_adapter)

        trace_a = get_phoenix_trace_url(getattr(run_a, "tracker_metadata", None))
        trace_b = get_phoenix_trace_url(getattr(run_b, "tracker_metadata", None))

        _console.print("\n[bold]ì‹¤í–‰ ë¹„êµ:[/bold]")
        _console.print(f"  ì‹¤í–‰ A: {run_id1}")
        if trace_a:
            _console.print(f"    Phoenix íŠ¸ë ˆì´ìŠ¤: {trace_a}")
        _console.print(f"  ì‹¤í–‰ B: {run_id2}")
        if trace_b:
            _console.print(f"    Phoenix íŠ¸ë ˆì´ìŠ¤: {trace_b}")
        _console.print(f"  ê²€ì •: {test}\n")

        if test == "t-test":
            test_type = "t-test"
        elif test == "mann-whitney":
            test_type = "mann-whitney"
        else:
            _console.print(f"[red]Error: Unsupported test type: {test}[/red]")
            raise typer.Exit(1)

        comparisons = service.compare_runs(run_a, run_b, metrics=metric_list, test_type=test_type)

        if not comparisons:
            _console.print("[yellow]ë¹„êµí•  ê³µí†µ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            raise typer.Exit(0)

        table = Table(title="í†µê³„ ë¹„êµ", show_header=True, header_style="bold cyan")
        table.add_column("ë©”íŠ¸ë¦­")
        table.add_column("ì‹¤í–‰ A (í‰ê· )", justify="right")
        table.add_column("ì‹¤í–‰ B (í‰ê· )", justify="right")
        table.add_column("ë³€í™” (%)", justify="right")
        table.add_column("p-ê°’", justify="right")
        table.add_column("íš¨ê³¼ í¬ê¸°", justify="right")
        table.add_column("ìœ ì˜")
        table.add_column("ìŠ¹ì")

        for comparison in comparisons:
            sig_style = "green" if comparison.is_significant else "dim"
            winner = comparison.winner[:8] if comparison.winner else "-"
            table.add_row(
                comparison.metric,
                f"{comparison.mean_a:.3f}",
                f"{comparison.mean_b:.3f}",
                f"{comparison.diff_percent:+.1f}%",
                f"{comparison.p_value:.4f}",
                f"{comparison.effect_size:.2f} ({comparison.effect_level.value})",
                f"[{sig_style}]{'ì˜ˆ' if comparison.is_significant else 'ì•„ë‹ˆì˜¤'}[/{sig_style}]",
                winner,
            )

        _console.print(table)
        _console.print()

        comparison_prefix = f"comparison_{run_id1[:8]}_{run_id2[:8]}"
        base_dir = output_dir or Path("reports/comparison")
        output_path, report_path = resolve_output_paths(
            base_dir=base_dir,
            output_path=output,
            report_path=report,
            prefix=comparison_prefix,
        )

        settings = Settings()
        profile_name = profile or settings.evalvault_profile
        if profile_name:
            settings = apply_profile(settings, profile_name)
        llm_adapter = None
        try:
            llm_adapter = get_llm_adapter(settings)
        except Exception as exc:
            _console.print(f"[yellow]ê²½ê³ : LLM ì–´ëŒ‘í„° ì´ˆê¸°í™” ì‹¤íŒ¨ ({exc})[/yellow]")

        pipeline_service = build_analysis_pipeline_service(
            storage=storage,
            llm_adapter=llm_adapter,
        )
        with _console.status("[bold green]ë¹„êµ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
            pipeline_result = pipeline_service.analyze_intent(
                AnalysisIntent.GENERATE_COMPARISON,
                run_id=run_id1,
                run_ids=[run_id1, run_id2],
                compare_metrics=metric_list,
                test_type=test,
                report_type="comparison",
                use_llm_report=True,
            )

        artifacts_dir = resolve_artifact_dir(
            base_dir=output_dir,
            output_path=output_path,
            report_path=report_path,
            prefix=comparison_prefix,
        )
        artifact_index = write_pipeline_artifacts(
            pipeline_result,
            artifacts_dir=artifacts_dir,
        )
        payload = serialize_pipeline_result(pipeline_result)
        payload["run_ids"] = [run_id1, run_id2]
        payload["artifacts"] = artifact_index
        write_json(output_path, payload)

        report_text = extract_markdown_report(pipeline_result.final_output)
        if not report_text:
            report_text = "# ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ\n\në³´ê³ ì„œ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
        report_path.write_text(report_text, encoding="utf-8")

        _display_pipeline_comparison_summary(pipeline_result, run_id1, run_id2)

        _console.print(f"[green]ë¹„êµ ë¶„ì„ ê²°ê³¼ ì €ì¥:[/green] {output_path}")
        _console.print(f"[green]ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥:[/green] {report_path}\n")
        _console.print(
            "[green]ë¹„êµ ë¶„ì„ ìƒì„¸ ê²°ê³¼ ì €ì¥:[/green] "
            f"{artifact_index['dir']} (index: {artifact_index['index']})\n"
        )


def _display_analysis_summary(analysis) -> None:
    """Display analysis summary panel."""

    panel = Panel(
        f"""[bold]ë¶„ì„ ìš”ì•½[/bold]
ì‹¤í–‰ ID: {analysis.run_id}
ë¶„ì„ ìœ í˜•: {analysis.analysis_type.value}
ìƒì„± ì‹œê°: {analysis.created_at.strftime("%Y-%m-%d %H:%M:%S")}

ì „ì²´ í†µê³¼ìœ¨: [{"green" if analysis.overall_pass_rate >= 0.7 else "yellow" if analysis.overall_pass_rate >= 0.5 else "red"}]{analysis.overall_pass_rate:.1%}[/]
ë¶„ì„ ë©”íŠ¸ë¦­ ìˆ˜: {len(analysis.metrics_summary)}
ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„: {len(analysis.significant_correlations)}
ì €ì„±ëŠ¥ ì¼€ì´ìŠ¤: {len(analysis.low_performers)}""",
        title="[bold cyan]í†µê³„ ë¶„ì„[/bold cyan]",
        border_style="cyan",
    )
    _console.print(panel)


def _display_metric_stats(analysis) -> None:
    """Display metric statistics table."""

    if not analysis.metrics_summary:
        return

    table = Table(title="ë©”íŠ¸ë¦­ í†µê³„", show_header=True, header_style="bold cyan")
    table.add_column("ë©”íŠ¸ë¦­")
    table.add_column("í‰ê· ", justify="right")
    table.add_column("í‘œì¤€í¸ì°¨", justify="right")
    table.add_column("ìµœì†Œ", justify="right")
    table.add_column("ìµœëŒ€", justify="right")
    table.add_column("ì¤‘ì•™ê°’", justify="right")
    table.add_column("í†µê³¼ìœ¨", justify="right")

    for metric_name, stats in analysis.metrics_summary.items():
        pass_rate = analysis.metric_pass_rates.get(metric_name, 0)
        pass_style = "green" if pass_rate >= 0.7 else "yellow" if pass_rate >= 0.5 else "red"

        table.add_row(
            metric_name,
            f"{stats.mean:.3f}",
            f"{stats.std:.3f}",
            f"{stats.min:.3f}",
            f"{stats.max:.3f}",
            f"{stats.median:.3f}",
            f"[{pass_style}]{pass_rate:.1%}[/{pass_style}]",
        )

    _console.print(table)


def _display_pipeline_comparison_summary(pipeline_result, run_id1: str, run_id2: str) -> None:
    """Display a concise comparison summary for pipeline reports."""

    comparison_output = get_node_output(pipeline_result, "run_metric_comparison")
    change_output = get_node_output(pipeline_result, "run_change_detection")
    run_output = get_node_output(pipeline_result, "load_runs")

    runs = run_output.get("runs", []) if isinstance(run_output, dict) else []
    run_a = runs[0] if len(runs) > 0 else None
    run_b = runs[1] if len(runs) > 1 else None

    model_a = run_a.model_name if isinstance(run_a, EvaluationRun) else "-"
    model_b = run_b.model_name if isinstance(run_b, EvaluationRun) else "-"
    dataset_a = run_a.dataset_name if isinstance(run_a, EvaluationRun) else "-"
    dataset_b = run_b.dataset_name if isinstance(run_b, EvaluationRun) else "-"

    summary = comparison_output.get("summary", {}) if isinstance(comparison_output, dict) else {}
    pass_rate_diff = summary.get("pass_rate_diff")
    avg_score_diff = summary.get("avg_score_diff")

    dataset_changes = change_output.get("dataset_changes", [])
    config_changes = change_output.get("config_changes", [])
    prompt_changes = change_output.get("prompt_changes", {})
    prompt_summary = prompt_changes.get("summary", {}) if isinstance(prompt_changes, dict) else {}

    _console.print("\n[bold]ë¹„êµ ë¶„ì„ ìš”ì•½[/bold]")
    _console.print(f"- ì‹¤í–‰ A: {run_id1} ({model_a}, {dataset_a})")
    _console.print(f"- ì‹¤í–‰ B: {run_id2} ({model_b}, {dataset_b})")
    _console.print(f"- í†µê³¼ìœ¨ ë³€í™”: {_format_percent(pass_rate_diff, signed=True)}")
    _console.print(f"- í‰ê·  ì ìˆ˜ ë³€í™”: {_format_float(avg_score_diff, signed=True)}")
    _console.print(
        f"- ë°ì´í„°ì…‹ ë³€ê²½: {len(dataset_changes) if isinstance(dataset_changes, list) else 0}ê±´"
    )
    _console.print(
        f"- ì„¤ì • ë³€ê²½: {len(config_changes) if isinstance(config_changes, list) else 0}ê±´"
    )
    _console.print(
        "- í”„ë¡¬í”„íŠ¸ ë³€ê²½: "
        f"{prompt_summary.get('changed', 0)}ê±´ (ìƒíƒœ: {prompt_changes.get('status', 'ì•Œ ìˆ˜ ì—†ìŒ')})"
    )

    scorecard = build_comparison_scorecard(comparison_output)
    if not scorecard:
        _console.print("[yellow]ë¹„êµ ìŠ¤ì½”ì–´ì¹´ë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]\n")
        return

    table = Table(title="ë¹„êµ ìŠ¤ì½”ì–´ì¹´ë“œ", show_header=True, header_style="bold cyan")
    table.add_column("ë©”íŠ¸ë¦­")
    table.add_column("A", justify="right")
    table.add_column("B", justify="right")
    table.add_column("ì°¨ì´", justify="right")
    table.add_column("p-ê°’", justify="right")
    table.add_column("íš¨ê³¼ í¬ê¸°", justify="right")
    table.add_column("ìœ ì˜ ì—¬ë¶€")

    for row in scorecard:
        effect_size = _format_float(row.get("effect_size"), precision=2)
        effect_level = row.get("effect_level")
        effect_text = f"{effect_size} ({effect_level})" if effect_level else f"{effect_size}"
        significant = "ì˜ˆ" if row.get("is_significant") else "ì•„ë‹ˆì˜¤"
        table.add_row(
            str(row.get("metric") or "-"),
            _format_float(row.get("mean_a")),
            _format_float(row.get("mean_b")),
            _format_float(row.get("diff"), signed=True),
            _format_float(row.get("p_value")),
            effect_text,
            significant,
        )

    _console.print(table)


def _format_float(value: float | None, precision: int = 3, *, signed: bool = False) -> str:
    if value is None:
        return "-"
    try:
        if signed:
            return f"{float(value):+.{precision}f}"
        return f"{float(value):.{precision}f}"
    except (TypeError, ValueError):
        return "-"


def _format_percent(value: float | None, precision: int = 1, *, signed: bool = False) -> str:
    if value is None:
        return "-"
    try:
        if signed:
            return f"{float(value):+.{precision}%}"
        return f"{float(value):.{precision}%}"
    except (TypeError, ValueError):
        return "-"
    _console.print()


def _display_correlations(analysis) -> None:
    """Display significant correlations."""

    if not analysis.significant_correlations:
        return

    _console.print("[bold]ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„:[/bold]")
    for corr in analysis.significant_correlations[:5]:
        direction = "[green]+" if corr.correlation > 0 else "[red]-"
        _console.print(
            f"  {direction}{abs(corr.correlation):.2f}[/] "
            f"{corr.variable1} â†” {corr.variable2} "
            f"(p={corr.p_value:.4f}, {corr.interpretation})"
        )
    _console.print()


def _display_low_performers(analysis) -> None:
    """Display low performing test cases."""

    if not analysis.low_performers:
        return

    _console.print(f"[bold]ì €ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ({len(analysis.low_performers)}):[/bold]")

    table = Table(show_header=True, header_style="bold yellow")
    table.add_column("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
    table.add_column("ë©”íŠ¸ë¦­")
    table.add_column("ì ìˆ˜", justify="right")
    table.add_column("ì„ê³„ê°’", justify="right")
    table.add_column("ê°€ëŠ¥í•œ ì›ì¸")

    for low_perf in analysis.low_performers[:10]:
        causes = ", ".join(low_perf.potential_causes[:2]) if low_perf.potential_causes else "-"
        table.add_row(
            low_perf.test_case_id[:12] + "..."
            if len(low_perf.test_case_id) > 15
            else low_perf.test_case_id,
            low_perf.metric_name,
            f"[red]{low_perf.score:.3f}[/red]",
            f"{low_perf.threshold:.2f}",
            causes[:40] + "..." if len(causes) > 40 else causes,
        )

    _console.print(table)
    _console.print()


def _display_insights(analysis) -> None:
    """Display analysis insights."""

    if not analysis.insights:
        return

    _console.print("[bold]ì¸ì‚¬ì´íŠ¸:[/bold]")
    for insight in analysis.insights:
        _console.print(f"  â€¢ {insight}")
    _console.print()


def _display_nlp_analysis(nlp_analysis) -> None:
    """Display NLP analysis results."""

    _console.print("\n[bold cyan]NLP ë¶„ì„[/bold cyan]\n")

    if nlp_analysis.question_stats:
        _console.print("[bold]í…ìŠ¤íŠ¸ í†µê³„(ì§ˆë¬¸):[/bold]")
        stats = nlp_analysis.question_stats
        table = Table(show_header=False, box=None, padding=(0, 2))
        table.add_column("ì§€í‘œ", style="bold")
        table.add_column("ê°’", justify="right")

        table.add_row("ì „ì²´ ë¬¸ì ìˆ˜", str(stats.char_count))
        table.add_row("ì „ì²´ ë‹¨ì–´ ìˆ˜", str(stats.word_count))
        table.add_row("ì „ì²´ ë¬¸ì¥ ìˆ˜", str(stats.sentence_count))
        table.add_row("í‰ê·  ë‹¨ì–´ ê¸¸ì´", f"{stats.avg_word_length:.2f}")
        table.add_row("ì–´íœ˜ ë‹¤ì–‘ì„±", f"{stats.unique_word_ratio:.1%}")
        table.add_row("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{stats.avg_sentence_length:.1f} ë‹¨ì–´")

        _console.print(table)
        _console.print()

    if nlp_analysis.question_types:
        _console.print("[bold]ì§ˆë¬¸ ìœ í˜• ë¶„í¬:[/bold]")
        table = Table(show_header=True, header_style="bold")
        table.add_column("ìœ í˜•")
        table.add_column("ê°œìˆ˜", justify="right")
        table.add_column("ë¹„ìœ¨", justify="right")
        table.add_column("í‰ê·  ì ìˆ˜")

        for question_type in nlp_analysis.question_types:
            avg_scores_str = ", ".join(
                f"{name}: {score:.2f}" for name, score in (question_type.avg_scores or {}).items()
            )
            table.add_row(
                question_type.question_type.value.capitalize(),
                str(question_type.count),
                f"{question_type.percentage:.1%}",
                avg_scores_str or "-",
            )

        _console.print(table)
        _console.print()

    if nlp_analysis.top_keywords:
        _console.print("[bold]ìƒìœ„ í‚¤ì›Œë“œ:[/bold]")
        table = Table(show_header=True, header_style="bold")
        table.add_column("í‚¤ì›Œë“œ")
        table.add_column("ë¹ˆë„", justify="right")
        table.add_column("TF-IDF ì ìˆ˜", justify="right")

        for keyword in nlp_analysis.top_keywords[:10]:
            table.add_row(keyword.keyword, str(keyword.frequency), f"{keyword.tfidf_score:.3f}")

        _console.print(table)
        _console.print()

    if nlp_analysis.insights:
        _console.print("[bold]NLP ì¸ì‚¬ì´íŠ¸:[/bold]")
        for insight in nlp_analysis.insights:
            _console.print(f"  â€¢ {insight}")
        _console.print()


def _display_causal_analysis(causal_analysis) -> None:
    """Display causal analysis results."""

    _console.print("\n[bold magenta]ì¸ê³¼ ë¶„ì„[/bold magenta]\n")

    significant_impacts = causal_analysis.significant_impacts
    if significant_impacts:
        _console.print("[bold]ìœ ì˜ë¯¸í•œ ìš”ì¸-ë©”íŠ¸ë¦­ ê´€ê³„:[/bold]")
        table = Table(show_header=True, header_style="bold")
        table.add_column("ìš”ì¸")
        table.add_column("ë©”íŠ¸ë¦­")
        table.add_column("ë°©í–¥")
        table.add_column("ê°•ë„")
        table.add_column("ìƒê´€ê³„ìˆ˜", justify="right")
        table.add_column("p-ê°’", justify="right")

        for impact in significant_impacts[:10]:
            direction_style = "green" if impact.direction.value == "positive" else "red"
            table.add_row(
                impact.factor_type.value,
                impact.metric_name,
                f"[{direction_style}]{impact.direction.value}[/{direction_style}]",
                impact.strength.value,
                f"{impact.correlation:.3f}",
                f"{impact.p_value:.4f}",
            )

        _console.print(table)
        _console.print()

    strong_relationships = causal_analysis.strong_relationships
    if strong_relationships:
        _console.print("[bold]ê°•í•œ ì¸ê³¼ ê´€ê³„ (ì‹ ë¢°ë„ > 0.7):[/bold]")
        for rel in strong_relationships[:5]:
            direction_arrow = "â†‘" if rel.direction.value == "positive" else "â†“"
            _console.print(
                f"  â€¢ {rel.cause.value} â†’ {rel.effect_metric} {direction_arrow} "
                f"(ì‹ ë¢°ë„: {rel.confidence:.2f})"
            )
        _console.print()

    if causal_analysis.root_causes:
        _console.print("[bold]ê·¼ë³¸ ì›ì¸ ë¶„ì„:[/bold]")
        for rc in causal_analysis.root_causes:
            primary_str = ", ".join(f.value for f in rc.primary_causes)
            _console.print(f"  [bold]{rc.metric_name}:[/bold]")
            _console.print(f"    ì£¼ìš” ì›ì¸: {primary_str}")
            if rc.contributing_factors:
                contributing_str = ", ".join(f.value for f in rc.contributing_factors)
                _console.print(f"    ê¸°ì—¬ ìš”ì¸: {contributing_str}")
            if rc.explanation:
                _console.print(f"    ì„¤ëª…: {rc.explanation}")
        _console.print()

    if causal_analysis.interventions:
        _console.print("[bold]ê¶Œì¥ ê°œì…:[/bold]")
        for intervention in causal_analysis.interventions[:5]:
            priority_str = {1: "ğŸ”´ ë†’ìŒ", 2: "ğŸŸ¡ ì¤‘ê°„", 3: "ğŸŸ¢ ë‚®ìŒ"}.get(
                intervention.priority, f"ìš°ì„ ìˆœìœ„ {intervention.priority}"
            )
            _console.print(f"  [{priority_str}] {intervention.intervention}")
            _console.print(f"      ëŒ€ìƒ: {intervention.target_metric}")
            _console.print(f"      ê¸°ëŒ€ íš¨ê³¼: {intervention.expected_impact}")
        _console.print()

    if causal_analysis.insights:
        _console.print("[bold]ì¸ê³¼ ì¸ì‚¬ì´íŠ¸:[/bold]")
        for insight in causal_analysis.insights:
            _console.print(f"  â€¢ {insight}")
        _console.print()


def _export_analysis_json(
    analysis, output_path: Path, nlp_analysis=None, improvement_report=None
) -> None:
    """Export analysis to JSON file."""

    from dataclasses import asdict

    data = {
        "analysis_id": analysis.analysis_id,
        "run_id": analysis.run_id,
        "analysis_type": analysis.analysis_type.value,
        "created_at": analysis.created_at.isoformat(),
        "overall_pass_rate": analysis.overall_pass_rate,
        "metric_pass_rates": analysis.metric_pass_rates,
        "metrics_summary": {
            name: asdict(stats) for name, stats in analysis.metrics_summary.items()
        },
        "correlation_matrix": analysis.correlation_matrix,
        "correlation_metrics": analysis.correlation_metrics,
        "significant_correlations": [asdict(c) for c in analysis.significant_correlations],
        "low_performers": [asdict(lp) for lp in analysis.low_performers],
        "insights": analysis.insights,
    }

    if nlp_analysis:
        data["nlp_analysis"] = {
            "run_id": nlp_analysis.run_id,
            "question_stats": asdict(nlp_analysis.question_stats)
            if nlp_analysis.question_stats
            else None,
            "answer_stats": asdict(nlp_analysis.answer_stats)
            if nlp_analysis.answer_stats
            else None,
            "context_stats": asdict(nlp_analysis.context_stats)
            if nlp_analysis.context_stats
            else None,
            "question_types": [
                {
                    "question_type": qt.question_type.value,
                    "count": qt.count,
                    "percentage": qt.percentage,
                    "avg_scores": qt.avg_scores,
                }
                for qt in nlp_analysis.question_types
            ],
            "top_keywords": [asdict(kw) for kw in nlp_analysis.top_keywords],
            "insights": nlp_analysis.insights,
        }

    if improvement_report:
        data["improvement_report"] = improvement_report.to_dict()

    with open(output_path, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=2)


def _perform_playbook_analysis(
    run,
    enable_llm: bool,
    profile: str | None,
    *,
    stage_metrics=None,
):
    """Perform playbook-based improvement analysis."""

    from evalvault.adapters.outbound.improvement.insight_generator import InsightGenerator
    from evalvault.adapters.outbound.improvement.pattern_detector import PatternDetector
    from evalvault.adapters.outbound.improvement.playbook_loader import get_default_playbook
    from evalvault.adapters.outbound.improvement.stage_metric_playbook_loader import (
        StageMetricPlaybookLoader,
    )
    from evalvault.domain.services.improvement_guide_service import ImprovementGuideService

    _console.print("\n[bold cyan]í”Œë ˆì´ë¶ ê¸°ë°˜ ê°œì„  ë¶„ì„[/bold cyan]\n")

    playbook = get_default_playbook()
    detector = PatternDetector(playbook=playbook)

    insight_generator = None
    if enable_llm:
        settings = Settings()
        profile_name = profile or settings.evalvault_profile
        if profile_name:
            settings = apply_profile(settings, profile_name)

        llm_adapter = get_llm_adapter(settings)
        insight_generator = InsightGenerator(llm_adapter=llm_adapter)
        _console.print("[dim]LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„± í™œì„±í™”[/dim]")

    stage_metric_playbook = StageMetricPlaybookLoader().load()

    service = ImprovementGuideService(
        pattern_detector=detector,
        insight_generator=insight_generator,
        playbook=playbook,
        stage_metric_playbook=stage_metric_playbook,
        enable_llm_enrichment=enable_llm,
    )

    with _console.status("[bold green]íŒ¨í„´ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘..."):
        report = service.generate_report(
            run,
            include_llm_analysis=enable_llm,
            stage_metrics=stage_metrics,
        )

    _display_improvement_report(report)
    return report


def _display_improvement_report(report) -> None:
    """Display improvement report in console."""

    from evalvault.domain.entities.improvement import ImprovementPriority

    summary = f"""[bold]ê°œì„  ë¶„ì„ ìš”ì•½[/bold]
ì‹¤í–‰ ID: {report.run_id}
ì „ì²´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {report.total_test_cases}
ìƒì„±ëœ ê°€ì´ë“œ: {len(report.guides)}
ë¶„ì„ ë°©ë²•: {", ".join(m.value for m in report.analysis_methods_used)}

[bold]ë©”íŠ¸ë¦­ ì„±ëŠ¥ vs ì„ê³„ê°’[/bold]"""

    for metric, score in report.metric_scores.items():
        gap = report.metric_gaps.get(metric, 0)
        status = "[red]ì„ê³„ê°’ ë¯¸ë‹¬[/red]" if gap > 0 else "[green]ì„ê³„ê°’ ì¶©ì¡±[/green]"
        summary += f"\n  {metric}: {score:.3f} ({status})"
        if gap > 0:
            summary += f" [dim](ê²©ì°¨: -{gap:.3f})[/dim]"

    _console.print(Panel(summary, title="[bold cyan]ê°œì„  ë¶„ì„[/bold cyan]", border_style="cyan"))

    stage_summary = report.metadata.get("stage_metrics_summary")
    if stage_summary:
        pass_rate = stage_summary.get("pass_rate")
        pass_rate_text = f"{pass_rate:.1%}" if pass_rate is not None else "-"
        _console.print(
            "\n[bold]ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ìš”ì•½[/bold] "
            f"(í‰ê°€ë¨: {stage_summary.get('evaluated', 0)}, "
            f"í†µê³¼: {stage_summary.get('passed', 0)}, "
            f"ì‹¤íŒ¨: {stage_summary.get('failed', 0)}, "
            f"í†µê³¼ìœ¨: {pass_rate_text})"
        )
        top_failures = stage_summary.get("top_failures", [])
        if top_failures:
            table = Table(show_header=True, header_style="bold cyan")
            table.add_column("ë©”íŠ¸ë¦­")
            table.add_column("ì‹¤íŒ¨ ê±´ìˆ˜", justify="right")
            table.add_column("í‰ê·  ì ìˆ˜", justify="right")
            table.add_column("ì„ê³„ê°’", justify="right")
            for item in top_failures:
                threshold = item.get("threshold")
                threshold_text = f"{threshold:.3f}" if threshold is not None else "-"
                table.add_row(
                    str(item.get("metric_name", "-")),
                    str(item.get("count", 0)),
                    f"{item.get('avg_score', 0.0):.3f}",
                    threshold_text,
                )
            _console.print(table)
        else:
            _console.print("[green]ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ì‹¤íŒ¨ê°€ ì—†ìŠµë‹ˆë‹¤.[/green]")

    if not report.guides:
        _console.print("[yellow]ê°œì„  ê°€ì´ë“œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/yellow]")
        return

    critical_guides = report.get_critical_guides()
    if critical_guides:
        _console.print("\n[bold red]ì¹˜ëª…ì  ì´ìŠˆ (P0)[/bold red]")
        for guide in critical_guides:
            _display_guide(guide)

    high_priority = [g for g in report.guides if g.priority == ImprovementPriority.P1_HIGH]
    if high_priority:
        _console.print("\n[bold yellow]ë†’ì€ ìš°ì„ ìˆœìœ„ (P1)[/bold yellow]")
        for guide in high_priority[:3]:
            _display_guide(guide)

    medium_priority = [g for g in report.guides if g.priority == ImprovementPriority.P2_MEDIUM]
    if medium_priority:
        _console.print("\n[bold blue]ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (P2)[/bold blue]")
        for guide in medium_priority[:2]:
            _display_guide(guide)


def _display_guide(guide) -> None:
    """Display a single improvement guide."""

    component_icons = {
        "retriever": "ğŸ”",
        "reranker": "ğŸ“Š",
        "generator": "ğŸ¤–",
        "chunker": "ğŸ“„",
        "embedder": "ğŸ“",
        "query_processor": "ğŸ”§",
        "prompt": "ğŸ’¬",
    }

    icon = component_icons.get(guide.component.value, "ğŸ“Œ")
    _console.print(
        f"\n  {icon} [bold]{guide.component.value.upper()}[/bold] - {', '.join(guide.target_metrics)}"
    )

    if guide.evidence:
        primary = guide.evidence.primary_pattern
        if primary:
            _console.print(f"     íŒ¨í„´: {primary.pattern_type.value}")
            _console.print(
                f"     ì˜í–¥: {primary.affected_count}/{primary.total_count} í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ "
                f"({primary.affected_ratio:.1%})"
            )
        elif guide.evidence.total_failures > 0:
            _console.print(f"     ì‹¤íŒ¨: {guide.evidence.total_failures} í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
            _console.print(f"     ì‹¤íŒ¨ í‰ê·  ì ìˆ˜: {guide.evidence.avg_score_failures:.3f}")

    if guide.actions:
        _console.print("     [bold]ê¶Œì¥ ì¡°ì¹˜:[/bold]")
        for action in guide.actions[:3]:
            effort_color = {"low": "green", "medium": "yellow", "high": "red"}.get(
                action.effort, "white"
            )
            effort_label = {"low": "ë‚®ìŒ", "medium": "ì¤‘ê°„", "high": "ë†’ìŒ"}.get(
                action.effort, action.effort
            )
            _console.print(f"       â€¢ {action.title}")
            if action.description:
                if len(action.description) > 60:
                    _console.print(f"         [dim]{action.description[:60]}...[/dim]")
                else:
                    _console.print(f"         [dim]{action.description}[/dim]")
            _console.print(
                f"         ê¸°ëŒ€ ê°œì„ : +{action.expected_improvement:.1%} | ë…¸ë ¥ë„: "
                f"[{effort_color}]{effort_label}[/{effort_color}]"
            )

    if guide.verification_command:
        _console.print(f"     [dim]ê²€ì¦: {guide.verification_command}[/dim]")


def _generate_report(
    bundle, output_path: Path, include_nlp: bool = True, improvement_report=None
) -> None:
    """Generate analysis report (Markdown or HTML)."""

    adapter = MarkdownReportAdapter()
    suffix = output_path.suffix.lower()
    if suffix == ".html":
        content = adapter.generate_html(bundle, include_nlp=include_nlp)
    else:
        content = adapter.generate_markdown(bundle, include_nlp=include_nlp)

    if improvement_report:
        stage_summary = improvement_report.metadata.get("stage_metrics_summary")
        if stage_summary:
            pass_rate = stage_summary.get("pass_rate")
            pass_rate_text = f"{pass_rate:.1%}" if pass_rate is not None else "í•´ë‹¹ ì—†ìŒ"
            content += "\n\n## ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ìš”ì•½\n"
            content += f"\n- ì „ì²´ ë©”íŠ¸ë¦­: {stage_summary.get('total', 0)}"
            content += f"\n- í‰ê°€ë¨: {stage_summary.get('evaluated', 0)}"
            content += (
                f"\n- í†µê³¼: {stage_summary.get('passed', 0)} / "
                f"ì‹¤íŒ¨: {stage_summary.get('failed', 0)}"
            )
            content += f"\n- í†µê³¼ìœ¨: {pass_rate_text}\n"
            top_failures = stage_summary.get("top_failures", [])
            if top_failures:
                content += "\n| ë©”íŠ¸ë¦­ | ì‹¤íŒ¨ ê±´ìˆ˜ | í‰ê·  ì ìˆ˜ | ì„ê³„ê°’ |\n"
                content += "|--------|----------|-----------|--------|\n"
                for item in top_failures:
                    threshold = item.get("threshold")
                    threshold_text = f"{threshold:.3f}" if threshold is not None else "-"
                    content += (
                        f"| {item.get('metric_name')} | {item.get('count', 0)} | "
                        f"{item.get('avg_score', 0.0):.3f} | {threshold_text} |\n"
                    )
        content += "\n\n" + improvement_report.to_markdown()

    with open(output_path, "w", encoding="utf-8") as file:
        file.write(content)


def _display_anomaly_detection(anomaly_result) -> None:
    _console.print("\n[bold]Anomaly Detection Results[/bold]")
    _console.print(f"Detection method: {anomaly_result.detection_method}")
    _console.print(f"Threshold: {anomaly_result.threshold:.2f}")
    _console.print(f"Total runs: {anomaly_result.total_runs}")

    if anomaly_result.anomalies:
        detected = [a for a in anomaly_result.anomalies if a.is_anomaly]
        if detected:
            _console.print(f"\n[red]Detected {len(detected)} anomalies:[/red]")
            table = Table(show_header=True, header_style="bold cyan")
            table.add_column("Run ID")
            table.add_column("Score", justify="right")
            table.add_column("Pass Rate", justify="right")
            table.add_column("Severity")

            for anomaly in detected[:10]:
                severity_color = (
                    "red"
                    if anomaly.severity == "high"
                    else "yellow"
                    if anomaly.severity == "medium"
                    else "green"
                )
                table.add_row(
                    anomaly.run_id[:12] + "...",
                    f"{anomaly.anomaly_score:.2f}",
                    f"{anomaly.pass_rate:.1%}",
                    f"[{severity_color}]{anomaly.severity}[/{severity_color}]",
                )
            _console.print(table)
        else:
            _console.print("[green]No anomalies detected.[/green]")

    if anomaly_result.insights:
        _console.print("\n[bold]Insights:[/bold]")
        for insight in anomaly_result.insights:
            _console.print(f"  â€¢ {insight}")


def _display_forecast_result(forecast_result) -> None:
    _console.print("\n[bold]Forecast Results[/bold]")
    _console.print(f"Method: {forecast_result.method}")
    _console.print(f"Horizon: {forecast_result.horizon} runs")

    if forecast_result.predicted_values:
        _console.print("\n[bold]Predicted Pass Rates:[/bold]")
        table = Table(show_header=True, header_style="bold cyan")
        table.add_column("Run")
        table.add_column("Predicted", justify="right")

        for i, value in enumerate(forecast_result.predicted_values, 1):
            table.add_row(f"+{i}", f"{value:.1%}")
        _console.print(table)

        avg_forecast = sum(forecast_result.predicted_values) / len(forecast_result.predicted_values)
        _console.print(f"\nAverage forecast: {avg_forecast:.1%}")


def _display_network_analysis(net_result) -> None:
    _console.print("\n[bold]Network Analysis Results[/bold]")
    _console.print(f"Nodes (metrics): {net_result.node_count}")
    _console.print(f"Edges (correlations): {net_result.edge_count}")
    _console.print(f"Density: {net_result.density:.3f}")
    _console.print(f"Avg clustering: {net_result.avg_clustering:.3f}")

    if net_result.communities:
        _console.print(f"\n[bold]Communities ({len(net_result.communities)}):[/bold]")
        for i, community in enumerate(net_result.communities):
            if len(community) > 1:
                _console.print(f"  Community {i + 1}: {', '.join(community)}")

    if net_result.hub_metrics:
        _console.print("\n[bold]Hub Metrics:[/bold]")
        for metric in net_result.hub_metrics:
            _console.print(f"  â€¢ {metric}")

    if net_result.insights:
        _console.print("\n[bold]Insights:[/bold]")
        for insight in net_result.insights:
            _console.print(f"  â€¢ {insight}")


def _display_hypothesis_generation(hypotheses, method: str) -> None:
    _console.print("\n[bold]Hypothesis Generation Results[/bold]")
    _console.print(f"Method: {method}")
    _console.print(f"Total hypotheses: {len(hypotheses)}")

    if hypotheses:
        _console.print("\n[bold]Generated Hypotheses:[/bold]")
        table = Table(show_header=True, header_style="bold cyan")
        table.add_column("#")
        table.add_column("Hypothesis")
        table.add_column("Metric")
        table.add_column("Confidence", justify="right")
        table.add_column("Evidence")

        for i, hyp in enumerate(hypotheses[:10], 1):
            confidence_color = (
                "green" if hyp.confidence >= 0.8 else "yellow" if hyp.confidence >= 0.6 else "red"
            )
            table.add_row(
                str(i),
                hyp.text[:60] + "..." if len(hyp.text) > 60 else hyp.text,
                hyp.metric_name or "-",
                f"[{confidence_color}]{hyp.confidence:.2f}[/{confidence_color}]",
                hyp.evidence[:30] + "..." if len(hyp.evidence) > 30 else hyp.evidence,
            )
        _console.print(table)

        high_conf = [h for h in hypotheses if h.confidence >= 0.8]
        if high_conf:
            _console.print(
                f"\n[green]High confidence hypotheses: {len(high_conf)}/{len(hypotheses)}[/green]"
            )


__all__ = [
    "register_analyze_commands",
    "_perform_playbook_analysis",
    "_display_improvement_report",
]
