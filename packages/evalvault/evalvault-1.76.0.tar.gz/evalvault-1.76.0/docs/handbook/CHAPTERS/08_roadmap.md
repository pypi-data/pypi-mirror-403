# 08. Roadmap (우선순위, 성공 기준, 의사결정 규칙)

이 챕터는 “다음에 무엇을 할지”를 기능 목록이 아니라 **가치/리스크/검증 가능성** 관점에서 정리한다.
로드맵은 약속이 아니라, 우선순위 결정 규칙과 성공 기준을 포함한 운영 문서다.

## TL;DR

- P0는 안정성/재현성: 실험이 흔들리면 어떤 개선도 의미가 없다.
- P1은 자동화된 품질 차단(회귀 게이트): 사람이 매번 판단하면 언젠가 실수한다.
- P2는 멀티턴/대화 평가: 제품이 대화형으로 갈수록 단일 QA 평가는 충분하지 않다.
- P3는 GraphRAG/고급 리트리벌 비교: “좋아졌다”를 실험 프레임워크로 증명해야 한다.
- P4는 Web UI 기반 Judge 캘리브레이션: 운영과 공유의 효율을 높인다.

## 목표

- 팀이 같은 기준으로 우선순위를 정할 수 있게 한다.
- 각 우선순위 항목이 “무엇이 완료인지(DoD)”를 정의한다.
- 리스크(시간/복잡도/외부 의존)를 명시해 계획이 현실적으로 유지되게 한다.

---

## SSoT(문서 역할 분담): 무엇을 어디에 적을까

로드맵은 문서가 여러 개라서, “어느 문서가 정답인가”를 먼저 고정해야 한다.

- 외부 공유용 요약(개념/방향): `docs/handbook/EXTERNAL.md`
- 내부 운영/제약/현상태(코드 근거 포함): `docs/handbook/CHAPTERS/00_overview.md`, `docs/handbook/CHAPTERS/04_operations.md`

호환성 노트:

- `docs/ROADMAP.md`, `docs/STATUS.md`는 과거 링크 호환을 위한 deprecated 스텁이며, 최신 내용은 handbook을 따른다.

이 챕터(`docs/handbook/CHAPTERS/08_roadmap.md`)는 내부 독자용으로,
각 우선순위가 실제 코드/테스트/워크플로와 어떻게 연결되는지까지 포함한다.

주의:

- `docs/ROADMAP.md`의 P0~P3는 “외부 공유용 큰 방향”이며, 이 챕터의 P0~P4(내부 실행 프레임)와 1:1로 같지 않을 수 있다.

---

## 우선순위 정의

### P0: 안정성/재현성(Foundation)

핵심 질문: “같은 조건이면 같은 결과가 나오나?”

대표 작업:

- 프로필/환경 검증(잘못된 설정을 초기에 실패 처리)
- DB/아티팩트/리포트의 저장 규약 고정
- 기본 테스트/린트 안정화

완료 정의(DoD):

- 동일 조건(데이터셋/프로필/메트릭)에서 결과가 재현된다.
- `run_id`로 결과를 항상 조회할 수 있다.

### P1: 자동 회귀 게이트(CI/CD)

핵심 질문: “회귀가 PR 단계에서 자동으로 차단되나?”

대표 작업:

- baseline 대비 current 비교 자동화
- 게이트 기준(임계값, 허용 회귀율) 고정
- PR 코멘트에 근거(표/요약) 제공

완료 정의(DoD):

- PR에서 회귀가 나면 병합이 자동으로 막힌다.
- 실패 시 ‘왜 실패했는지’가 PR 코멘트만으로 파악 가능하다.

근거(코드/워크플로):

- GitHub Actions 워크플로: `.github/workflows/regression-gate.yml`
- 회귀 판단 로직(서비스): `src/evalvault/domain/services/regression_gate_service.py`
- CI 통합 커맨드(`ci-gate`, `regress`, `regress-baseline`): `src/evalvault/adapters/inbound/cli/commands/regress.py`

운영 주의:

- “병합 자동 차단”은 GitHub 브랜치 보호(required status checks) 설정까지 포함해야 완성된다.
  레포 내부 파일만으로는 강제 여부를 증명할 수 없으므로, 운영에서 명시적으로 설정해야 한다.

### P2: 멀티턴 RAG 평가

핵심 질문: “대화가 길어질수록 품질이 어떻게 변하는지 측정하나?”

대표 작업:

- 멀티턴 데이터셋 스키마
- 턴별/전체 대화 메트릭(일관성, 드리프트 등)
- 벤치마크 데이터셋(3~10턴, 드리프트 케이스 포함)

완료 정의(DoD):

- 멀티턴 데이터셋을 로드하고 평가 결과를 턴 단위로 저장/리포트할 수 있다.
- 드리프트 감지 결과가 재현 가능하다.

근거(데이터/코드):

- 멀티턴 벤치마크 픽스처: `tests/fixtures/e2e/multiturn_benchmark.json`
- 멀티턴 엔티티/스키마: `src/evalvault/domain/entities/multiturn.py`

### P3: GraphRAG 실험 프레임워크

핵심 질문: “top-k vs GraphRAG 비교를 동일 조건에서 반복 실행할 수 있나?”

대표 작업:

- GraphRAG retriever/컨텍스트 생성
- A/B 비교 실험(동일 데이터셋, 동일 메트릭)
- 아티팩트 저장(서브그래프, 엔티티 추출 결과)
- 하이브리드 검색(BM25 + Dense) 기준 파라미터/평가 루틴 확정
- 벡터 스토리지 로드맵: PostgreSQL pgvector 우선, 규모 확장 시 Milvus/Weaviate

완료 정의(DoD):

- 동일 데이터셋에서 top-k와 GraphRAG를 비교 실행할 수 있다.
- 결과 차이를 근거(아티팩트)로 설명할 수 있다.
- 하이브리드 검색과 BM25 단독의 성능 차이를 반복 측정할 수 있다.
- pgvector 기반 임베딩 저장/검색이 실험 규모에서 재현 가능하다.

근거(코드):

- GraphRAG 실험 서비스: `src/evalvault/domain/services/graph_rag_experiment.py`

### P4: Judge 캘리브레이션 Web UI

핵심 질문: “캘리브레이션 결과를 UI에서 이해/공유/반복 실행할 수 있나?”

대표 작업:

- 설정/실행/히스토리/시각화
- 아티팩트 링크(근거) 제공

완료 정의(DoD):

- UI에서 캘리브레이션 결과를 탐색 가능하며, 핵심 지표를 이해할 수 있다.

근거(UI/서비스):

- UI 페이지: `frontend/src/pages/JudgeCalibration.tsx`
- 관련 서비스(보정/통계): `src/evalvault/domain/services/judge_calibration_service.py`

---

## 우선순위 결정 규칙(팀 합의용)

새 작업을 넣기 전에 아래 질문으로 필터링한다.

1) 이것이 없으면 다른 작업의 결과가 신뢰되지 않는가? (P0)
2) 이것이 없으면 사람이 매번 판단해야 해서 실수가 발생하는가? (P1)
3) 사용자/제품이 실제로 겪는 문제를 측정하지 못하고 있는가? (P2)
4) 성능 개선을 주장하려면 실험 프레임워크가 필요한가? (P3)
5) 공유/운영 효율을 크게 올리는가? (P4)

---

## 리스크와 완화

- 외부 LLM 의존: 비용/레이트리밋/데이터 유출 위험 → 프로필 분리, 샘플 최소화, 레덕션
- 그래프 기반 기능 복잡도(P3): 범위 폭발 → v0(휴리스틱) → v1(LLM) 단계화
- 멀티턴 메트릭 정의(P2): 해석 난이도 → 메트릭 정의/한계/예시를 문서로 고정

---

## 체크리스트

- [ ] 각 P 항목의 DoD가 측정 가능하게 정의되어 있는가?
- [ ] P0/P1 없이 P2/P3를 하는 것이 “의미 있는가”를 설명할 수 있는가?
- [ ] 작업 범위가 폭발할 가능성을 미리 차단했는가?

## 자기 점검 질문

1) 왜 P0가 먼저인가?
2) 회귀 게이트는 왜 사람 대신 자동화되어야 하나?
3) GraphRAG 개선을 주장하려면 어떤 근거가 필요한가?
