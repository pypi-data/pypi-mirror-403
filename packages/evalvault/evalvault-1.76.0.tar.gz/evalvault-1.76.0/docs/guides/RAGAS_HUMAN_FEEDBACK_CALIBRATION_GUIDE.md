# RAGAS 평가를 인간 피드백으로 보정하는 방법론

> **대상**: RAG 평가/분석을 운영하는 ML/데이터/플랫폼 엔지니어
>
> **목적**: RAGAS 점수의 신뢰도와 사용자 만족도 정합성을 높이기 위해 대표 샘플링 + 인간 평가 + 학습 기반 보정 모델을 설계한다.
>
> **상태**: Draft v0.1

## 1. 서론

RAGAS(Retrieval-Augmented Generation Assessment Suite)는 RAG 기반 질의응답 시스템을 평가하기 위한 오픈소스 프레임워크다. Faithfulness, Answer Relevance, Context Precision/Recall 같은 지표로 답변이 컨텍스트에 근거해 생성됐는지와 질문에 제대로 답했는지를 자동 평가한다. 그러나 실무에서는 RAGAS 점수가 실제 사용자 만족도와 잘 맞지 않는 사례가 반복적으로 보고된다. 따라서 자동 지표만으로는 서비스 품질을 판단하기 어렵고, 일부 인간 평가를 포함한 보정이 필요하다.

## 2. 사용자 상황과 목표

- 이미 RAGAS 평가/분석을 위한 Web UI 또는 배치 평가가 운영 중이다.
- 이해관계자는 RAGAS 점수를 단독 KPI로 신뢰하지 않는다.
- 전체 데이터셋에 대한 인간 평가는 불가능하므로 대표 샘플만 평가해야 한다.
- 인간 평가를 학습하여 전체 데이터셋에 대해 사용자 관점의 보정 점수를 추정해야 한다.

## 3. RAGAS의 한계와 인간 피드백 필요성

### 3.1 점수 불안정성과 재현성 문제
LLM judge 기반 평가는 동일 입력에서도 점수 변동이 발생할 수 있다. 일부 케이스는 0점 또는 NaN 같은 비정상 점수가 나오며 신뢰도를 떨어뜨린다.

### 3.2 사용자 판단과의 상관 부족
RAGAS가 사용자 만족도와 높은 상관을 가진다는 실증이 충분하지 않다. 자동 지표만으로는 “사용자가 느끼는 품질”을 대변하기 어렵다.

### 3.3 지표 범위의 한계
RAGAS는 근거 충실성/관련성 중심이다. 사용자 만족은 다음 요소에도 크게 좌우된다.
- 답변의 완결성, 실행 가능성
- 명료성, 가독성
- 톤/형식의 적절성
- 유용성(정확하지만 쓸모없는 답변 문제)

### 3.4 임계치 해석의 어려움
점수 0.8이 만족을 의미하는지 같은 해석 기준이 불명확하다. 결과적으로 이해관계자는 RAGAS를 단독 KPI로 인정하지 않기도 한다.

## 4. 제안 솔루션 개요

핵심 아이디어는 대표 문항을 선택해 인간 평가를 수집하고, 이를 기반으로 보정 모델을 학습하여 전체 데이터셋에 적용하는 것이다.

1. 대표 문항 선정(클러스터링/이산화)
2. 대표 문항에 대한 인간 평가 수집
3. RAGAS + 언어/행동 특성 기반 피처 구성
4. 만족도 예측 모델 학습
5. 전체 데이터셋에 보정 점수 적용 및 UI 통합
6. 반복 개선(액티브 러닝)

### 4.1 운영 관점: 노이즈 저감이 보정 품질을 결정한다

인간 피드백 기반 보정은 “RAGAS 점수 → 만족도 라벨”의 관계를 학습한다. 따라서 입력/점수의 노이즈(흔들림)가 큰 상태에서는 다음 문제가 쉽게 발생한다.

- 대표 샘플이 실제 분포를 대표하지 못한다(샘플링 편향)
- 인간 평가자가 일관되게 판단하기 어려워 라벨 노이즈가 커진다
- 보정 모델이 “품질”이 아니라 “노이즈 패턴”을 학습해 과적합한다

운영 적용 전에는 최소한 아래를 선행 점검하고, 필요하면 `docs/guides/RAG_NOISE_REDUCTION_GUIDE.md`의 데이터/모델 노이즈 저감 항목을 먼저 적용한다.

#### 4.1.1 보정 전 노이즈 점검 체크리스트(권장)

- 데이터 품질(데이터 노이즈)
  - 빈 질문/답변/컨텍스트가 존재하는지(존재하면 사전 제거/보정)
  - 컨텍스트 중복/과다(동일 문서 반복, 너무 긴 컨텍스트)가 많은지
  - 레퍼런스가 필요한 메트릭에서 결측이 많은지
- 점수 안정성(모델 노이즈)
  - NaN/비숫자 점수가 발생하는지(발생하면 원인 분석 후 재실행 정책 확정)
  - 동일 케이스를 반복 실행했을 때 점수가 과도하게 흔들리는지(재현성 문제)
  - 한국어 데이터에서 프롬프트 언어가 맞는지(언어 불일치로 오판/변동 발생 가능)
- 원인 분리(가능하다면)
  - 점수 하락이 retrieval 문제인지 generation 문제인지 최소한 구분 가능한지(stage/로그 기반)

## 5. 단계별 방법론

### 5.1 1단계: 대표 문항 선정

**목표**: 적은 라벨로 데이터 분포를 넓게 커버한다.

- **데이터 표현**
  - 질문/답변 임베딩(질문+답변 결합 또는 각각 결합)
  - RAGAS 점수(예: faithfulness, answer relevance, context precision/recall) 결합
  - 임베딩과 점수는 스케일링을 맞춰 결합

- **알고리즘 선택**
  - K-Means(기본)
  - 대안: 계층적 클러스터링, K-Medoids, K-Center Greedy

- **대표 샘플 선택 규칙**
  - 중심(Centroid) 근접 샘플(기본)
  - 필요 시 극단값(최저/최고 점수) 소수 추가(경계 케이스 커버)
  - 클러스터 내부 분산이 큰 군집은 샘플을 1~2개 추가(다양성 확보)

- **운영 Pitfall(자주 터지는 함정)**
  - 극단값 위주로 샘플을 구성하면 “평균적 품질”을 대표하지 못한다.
  - 반대로 centroid만 뽑으면 문제 케이스(저품질/고위험)를 놓친다.
  - 해결책: centroid 기반 대표 + 극단값 소량 + 분산 큰 군집 추가 샘플링을 조합한다.

### 5.2 2단계: 인간 평가 수집

**목표**: 사용자 만족도를 직접 라벨로 수집한다.

- **UI/엑셀 입력 설계**
  - 질문, 답변, (선택)컨텍스트
  - 만족도 점수(1~5 등)
  - (선택)코멘트

- **평가 기준(가이드)**
  - 질문에 제대로 답했는가(관련성)
  - 사실/규정/근거가 정확한가(정확성)
  - 답변이 충분히 구체적이고 실행 가능한가(유용성)
  - 표현이 명확하고 읽기 쉬운가(명료성)
  - 톤/형식이 서비스 목적에 맞는가(커뮤니케이션 품질)

- **라벨 노이즈 저감(운영 필수 가드레일)**
  - 동일 문항을 평가자가 다르게 해석하는 경우가 가장 큰 노이즈 원인이다.
  - 아래 규칙을 최소 기준으로 고정한다.
    - “근거 부족”과 “답변 품질 부족”을 구분한다(근거가 없으면 낮은 점수).
    - 길거나 그럴듯한 문장에 가점을 주지 않는다(verbosity bias 주의).
    - 컨텍스트가 과다한 케이스는 컨텍스트 품질 문제로 분류하고 코멘트에 기록한다.

- **다중 평가자 권장**
  - 가능하면 2명 이상 평가 후 평균 또는 합의
  - 불일치가 큰 케이스는 루브릭을 업데이트하거나 합의 절차를 운영한다(샘플 수는 적어도 효과가 큼).

### 5.3 3단계: 피처 구성

**목표**: RAGAS만으로 부족한 정보를 보완한다.

- **RAGAS 기반 피처**
  - faithfulness
  - answer relevance
  - context precision/recall
  - (가능하면) 통합 지표/추가 메트릭

- **언어적 피처**
  - 답변 길이(토큰/문장 수)
  - 질문-답변 임베딩 유사도
  - 답변 구조(목록/단계/근거 인용 여부)
  - 가독성 지표, 톤/감성 플래그

- **행동/멀티턴 피처(있다면 강력)**
  - 재질문 여부
  - 후속 질문/목표 달성까지 턴 수
  - thumbs up/down 등 암묵 피드백

### 5.4 4단계: 만족도 보정 모델 학습

- **문제 정의**
  - 만족도 1~5: 회귀
  - Good/Bad/Okay: 분류

- **추천 모델**
  - XGBoost/LightGBM/RandomForest: 소량 라벨에서도 안정적
  - 선형 회귀/로지스틱: 해석 가능한 베이스라인

- **노이즈 대응 학습 전략(권장)**
  - 라벨이 적을수록 “모델 선택”보다 “검증 설계”가 중요하다.
  - 최소 권장 세트:
    - K-fold cross validation(예: 5-fold)
    - 피처 수 제한(너무 많은 피처는 과적합을 유발)
    - 이상치(라벨/점수) 존재를 가정하고 로버스트한 손실/규제를 고려

- **검증**
  - 라벨 적으면 K-fold cross validation
  - 회귀: RMSE/MAE, Spearman/Pearson
  - 분류: Accuracy, F1

- **베이스라인 비교**
  - RAGAS 단일 지표 vs 보정 모델 성능 비교로 개선 폭 제시

- **운영 Pitfall(자주 터지는 함정)**
  - 라벨이 너무 “좋은 케이스”로만 구성되면 모델이 낮은 품질 구간을 구분하지 못한다.
  - 해결책: 샘플링 단계에서 극단값(최저/최고)과 경계 케이스를 일부 포함한다.

### 5.5 5단계: 전체 데이터셋 적용 및 UI 통합

- 전체 문항에 동일 피처 생성 후 모델 점수 산출
- RAGAS 점수와 보정 점수를 나란히 표시
- RAGAS와 보정 점수 불일치 케이스는 리뷰 대상으로 표시
- 낮은 보정 점수만 모아 QA 개선 우선순위로 활용

### 5.6 6단계: 반복 개선(액티브 러닝)

- 불확실성 높은 케이스, 불일치 큰 케이스 우선 라벨링
- 서비스 변화에 따라 대표 문항/라벨 세트 주기적 갱신
- 주기적 재학습으로 사용자 만족 정합성 개선

## 6. 실행 가능성과 효과

### 6.1 구현 가능성
- 클러스터링/샘플 선택은 표준 라이브러리로 구현 가능
- 라벨 수가 적어도 트리/회귀 모델로 충분히 학습 가능
- UI 통합은 기존 평가 UI에 점수 컬럼/필터 추가 수준

### 6.2 효과 기대 근거
- 자동 지표가 사용자 만족을 완전히 대변하지 못한다는 문제는 LLM 평가 전반의 공통 이슈
- 소량 라벨을 활용한 보정은 액티브 러닝/휴먼-인-더-루프의 대표적 접근
- 인간 피드백 기반 학습은 사용자 선호 정합성을 높이는 데 효과적이라는 사례가 많음

## 7. 리스크와 대응

- **라벨 품질/일관성**: 평가 가이드 제공, 다중 평가자, 불일치 검토
- **과적합**: 단순 모델 우선, 피처 수 제한, CV 적용
- **희귀 케이스 미포착**: 불확실/불일치 샘플 추가 라벨링
- **운영 변화**: 대표 세트/모델 주기적 갱신

### 7.1 운영 절차 체크리스트(노이즈 저감 포함)

아래 체크리스트는 “한 번 해보고 끝”이 아니라, 보정 리포트를 주기적으로 갱신할 때마다 반복 실행하는 것을 전제로 한다.

- 실행 전(입력/점수 안정화)
  - 데이터 노이즈(빈 필드, 컨텍스트 중복/과다, 레퍼런스 부족)가 관리 가능한 수준인지 점검한다.
  - 모델 노이즈(NaN/비정상 점수, 프롬프트 언어 불일치, 재현성 저하)가 발생하면 먼저 원인을 제거한다.
- 샘플링 검증(대표성)
  - 클러스터별로 최소 1개 이상이 포함되어 분포를 넓게 커버하는지 확인한다.
  - 극단값/경계 케이스가 “소량” 포함되어 문제 구간을 놓치지 않는지 확인한다.
- 라벨 품질 검증(라벨 노이즈)
  - 다중 평가자의 불일치 케이스를 샘플링해, 루브릭이 실제로 같은 해석을 유도하는지 확인한다.
  - 길이/어조에 대한 무의식적 편향(verbosity/agreeableness)을 경고하고 코멘트로 근거를 남긴다.
- 모델 검증(과적합/일반화)
  - CV 성능이 fold마다 크게 흔들리지 않는지 확인한다.
  - RAGAS 단일 지표 대비 개선 폭(상관/오차)을 리포트에 명시한다.
- 결과 해석(불일치 triage)
  - RAGAS 점수와 보정 점수가 불일치하는 케이스를 분리해 “데이터 노이즈 vs 모델 노이즈 vs 진짜 품질 저하”로 분류한다.

### 7.2 검증 방법(최소 재현 가능한 확인)

문서만으로는 노이즈 저감이 “적용되었다”를 보장할 수 없다. 따라서 최소한 아래 검증을 수행한다.

- 대표 샘플 10~20개를 골라, 동일 설정으로 평가를 2~3회 반복 실행해 점수 변동이 과도하지 않은지 확인한다.
- NaN/비숫자 점수가 발생하면 “0으로 치환해서 넘어간다”로 끝내지 말고, 해당 케이스의 입력 품질/모델 호출 실패 여부를 함께 기록한다.
- 인간 라벨이 있는 구간에서, 보정 모델의 예측이 1~5 범위를 벗어나지 않으며(클리핑 포함), 분포가 한쪽으로 붕괴하지 않는지 확인한다.

## 8. 결론

대표 문항 선정 → 사용자 평가 → 학습 기반 보정 점수 확장은 실무적으로 구현 가능하며, RAGAS 단독 지표의 한계를 보완하는 유효한 방법이다. 특히 이해관계자에게 “우리 사용자 평가로 학습한 점수”라는 근거를 제공함으로써 신뢰도를 높일 수 있다. 반복 개선 루프를 적용하면 시간이 지날수록 사용자 만족 정합성이 강화된다.

## 9. 참고 문헌

- Srinivas Bommena, “A Practitioner’s Guide to Evaluating GenAI Applications with RAGAS,” Medium, 2025
- Reddit r/LangChain, “Why is everyone using RAGAS for RAG evaluation? It looks very unreliable,” 2024
- Ethan M. Rudd et al., “A Practical Guide for Evaluating LLMs and LLM-Reliant Systems,” arXiv 2506.13023, 2025
- Nguyen & Smeulders, “Active Learning Using Pre-clustering,” ICML 2004
- Ouyang et al., “Training Language Models to Follow Instructions with Human Feedback,” NeurIPS 2022

## 변경 이력
- 2026-01-13 v0.1: 두 문서 통합 초안 작성
