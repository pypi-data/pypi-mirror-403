#!/usr/bin/env python3
"""
EvalVault ê°œë°œ ë°±ì„œ ìƒì„± ë„êµ¬

ëª¨ë“  ì„¹ì…˜ íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ ì™„ì „í•œ ë°±ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from pathlib import Path

# ë°±ì„œ ì„¹ì…˜ íŒŒì¼ ìˆœì„œ
SECTIONS = [
    "00-frontmatter.md",
    "01-project-overview.md",
    "02-architecture.md",
    "03-data-flow.md",
    # ì¶”ê°€ ì„¹ì…˜ë“¤ì´ ê³„ì† ì¶”ê°€ë  ê²ƒì…ë‹ˆë‹¤
]

WHITEPAPER_DIR = Path(__file__).parent.parent / "whitepaper"
WHITEPAPER_OUTPUT = Path(__file__).parent.parent.parent / "WHITEPAPER.md"


def generate_whitepaper():
    """ì„¹ì…˜ íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ ì™„ì „í•œ ë°±ì„œ ìƒì„±"""

    # ì„¹ì…˜ íŒŒì¼ë“¤ì„ ì½ê¸°
    sections = []
    for section_file in SECTIONS:
        section_path = WHITEPAPER_DIR / section_file

        if not section_path.exists():
            print(f"âš ï¸  ì„¹ì…˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {section_path}")
            continue

        with open(section_path, encoding="utf-8") as f:
            content = f.read()
            sections.append(content)

        print(f"âœ… ì„¹ì…˜ ë¡œë“œ ì™„ë£Œ: {section_file}")

    # ì„¹ì…˜ë“¤ì„ í•©ì¹˜ê¸°
    full_paper = "\n\n".join(sections)

    # ì™„ì „í•œ ë°±ì„œ ìƒì„±
    WHITEPAPER_OUTPUT.parent.mkdir(parents=True, exist_ok=True)

    with open(WHITEPAPER_OUTPUT, "w", encoding="utf-8") as f:
        f.write(full_paper)

    print(f"\nâœ… ë°±ì„œ ìƒì„± ì™„ë£Œ: {WHITEPAPER_OUTPUT}")
    print(f"   ì´ ë¼ì¸ ìˆ˜: {len(full_paper.splitlines())}")
    print(f"   ì´ ë‹¨ì–´ ìˆ˜: {len(full_paper.split())}")


def generate_stats():
    """ë°±ì„œ í†µê³„ ìƒì„±"""

    if not WHITEPAPER_OUTPUT.exists():
        print(f"âš ï¸  ë°±ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {WHITEPAPER_OUTPUT}")
        print("ë¨¼ì € ë°±ì„œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return

    with open(WHITEPAPER_OUTPUT, encoding="utf-8") as f:
        content = f.read()

    import re

    stats = {
        "ì´ ë¼ì¸ ìˆ˜": len(content.splitlines()),
        "ì´ ë‹¨ì–´ ìˆ˜": len(content.split()),
        "ì´ ë¬¸ì ìˆ˜": len(content),
        "ì„¹ì…˜ ìˆ˜": len(re.findall(r"^##\s+", content, re.MULTILINE)),
        "ì½”ë“œ ë¸”ë¡ ìˆ˜": len(re.findall(r"```", content)) // 2,
        "í‘œ ìˆ˜": len(re.findall(r"\|.*\|", content)),
    }

    print("\nğŸ“Š ë°±ì„œ í†µê³„:")
    for key, value in stats.items():
        print(f"  {key}: {value}")


def validate_links():
    """ë°±ì„œ ë§í¬ ìœ íš¨ì„± ê²€ì¦"""

    if not WHITEPAPER_OUTPUT.exists():
        print(f"âš ï¸  ë°±ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {WHITEPAPER_OUTPUT}")
        return

    with open(WHITEPAPER_OUTPUT, encoding="utf-8") as f:
        content = f.read()

    import re

    # ë§ˆí¬ë‹¤ìš´ ë§í¬ ì¶”ì¶œ
    link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
    links = re.findall(link_pattern, content)

    print(f"\nğŸ” ë°œê²¬ëœ ë§í¬: {len(links)}ê°œ")

    # ì™¸ë¶€ ë§í¬ì™€ ë‚´ë¶€ ë§í¬ êµ¬ë¶„
    external_links = []
    internal_links = []

    for text, url in links:
        if url.startswith("http"):
            external_links.append((text, url))
        else:
            internal_links.append((text, url))

    print(f"  ì™¸ë¶€ ë§í¬: {len(external_links)}ê°œ")
    print(f"  ë‚´ë¶€ ë§í¬: {len(internal_links)}ê°œ")

    # ì ì¬ì ì¸ ë¬¸ì œ ë§í¬
    problematic_links = []
    for text, url in internal_links:
        # ì„¹ì…˜ ì°¸ì¡° í˜•ì‹ í™•ì¸ (ì˜ˆ: ì„¹ì…˜ X.Y)
        if not re.match(r"section\s+\d+\.\d+", url, re.IGNORECASE):
            problematic_links.append((text, url))

    if problematic_links:
        print(f"\nâš ï¸  ì ì¬ì ì¸ ë¬¸ì œ ë§í¬ ({len(problematic_links)}ê°œ):")
        for text, url in problematic_links:
            print(f"  - [{text}]({url})")


def check_sections():
    """ì„¹ì…˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""

    print("ğŸ“‹ ì„¹ì…˜ íŒŒì¼ í™•ì¸:")

    missing_sections = []
    existing_sections = []

    for section_file in SECTIONS:
        section_path = WHITEPAPER_DIR / section_file

        if section_path.exists():
            existing_sections.append(section_file)
            print(f"  âœ… {section_file}")
        else:
            missing_sections.append(section_file)
            print(f"  âŒ {section_file}")

    print(f"\nì¡´ì¬í•˜ëŠ” ì„¹ì…˜: {len(existing_sections)}ê°œ")
    print(f"ëˆ„ë½ëœ ì„¹ì…˜: {len(missing_sections)}ê°œ")

    if missing_sections:
        print("\nâš ï¸  ëˆ„ë½ëœ ì„¹ì…˜ì´ ìˆìŠµë‹ˆë‹¤:")
        for section_file in missing_sections:
            print(f"  - {section_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    import argparse

    parser = argparse.ArgumentParser(description="EvalVault ê°œë°œ ë°±ì„œ ìƒì„± ë„êµ¬")
    parser.add_argument(
        "--stats",
        action="store_true",
        help="ë°±ì„œ í†µê³„ ìƒì„±",
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="ë°±ì„œ ë§í¬ ìœ íš¨ì„± ê²€ì¦",
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="ì„¹ì…˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸",
    )

    args = parser.parse_args()

    # ì„¹ì…˜ íŒŒì¼ í™•ì¸
    if args.check or not any([args.stats, args.validate]):
        check_sections()

    # ë°±ì„œ ìƒì„±
    if not any([args.stats, args.validate]):
        generate_whitepaper()

    # í†µê³„ ìƒì„±
    if args.stats:
        generate_stats()

    # ë§í¬ ê²€ì¦
    if args.validate:
        validate_links()


if __name__ == "__main__":
    main()
