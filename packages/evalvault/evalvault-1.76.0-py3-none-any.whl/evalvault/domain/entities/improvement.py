"""RAG Improvement Guide entities.

í‰ê°€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œ ê°œì„ ì„ ìœ„í•œ ì—”í‹°í‹°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
Rule-based íŒ¨í„´ íƒì§€ì™€ LLM-based ì¸ì‚¬ì´íŠ¸ ìƒì„±ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any
from uuid import uuid4


class RAGComponent(str, Enum):
    """RAG íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸."""

    RETRIEVER = "retriever"
    RERANKER = "reranker"
    GENERATOR = "generator"
    CHUNKER = "chunker"
    EMBEDDER = "embedder"
    QUERY_PROCESSOR = "query_processor"
    PROMPT = "prompt"


class ImprovementPriority(str, Enum):
    """ê°œì„  ìš°ì„ ìˆœìœ„."""

    P0_CRITICAL = "p0_critical"  # ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
    P1_HIGH = "p1_high"  # ë†’ì€ ìš°ì„ ìˆœìœ„
    P2_MEDIUM = "p2_medium"  # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    P3_LOW = "p3_low"  # ë‚®ì€ ìš°ì„ ìˆœìœ„


class PatternType(str, Enum):
    """ë¬¸ì œ íŒ¨í„´ ìœ í˜•."""

    # Retrieval ê´€ë ¨
    LONG_QUERY_LOW_PRECISION = "long_query_low_precision"
    LOW_KEYWORD_OVERLAP = "low_keyword_overlap"
    MISSING_CONTEXT = "missing_context"
    IRRELEVANT_CONTEXT = "irrelevant_context"
    CONTEXT_BOUNDARY_ISSUE = "context_boundary_issue"

    # Generation ê´€ë ¨
    HALLUCINATION = "hallucination"
    INCOMPLETE_ANSWER = "incomplete_answer"
    OFF_TOPIC_RESPONSE = "off_topic_response"
    VERBOSE_RESPONSE = "verbose_response"

    # ë³µí•© ë¬¸ì œ
    MULTI_HOP_FAILURE = "multi_hop_failure"
    REASONING_FAILURE = "reasoning_failure"

    # ê¸°íƒ€
    UNKNOWN = "unknown"
    STAGE_METRIC_BELOW_THRESHOLD = "stage_metric_below_threshold"


class EffortLevel(str, Enum):
    """ê°œì„  ë…¸ë ¥ ìˆ˜ì¤€."""

    LOW = "low"  # ì„¤ì • ë³€ê²½, íŒŒë¼ë¯¸í„° ì¡°ì •
    MEDIUM = "medium"  # ì½”ë“œ ìˆ˜ì •, ìƒˆ ì»´í¬ë„ŒíŠ¸ ì¶”ê°€
    HIGH = "high"  # ì•„í‚¤í…ì²˜ ë³€ê²½, ëª¨ë¸ ì¬í•™ìŠµ


class EvidenceSource(str, Enum):
    """ì¦ê±° ì¶œì²˜."""

    RULE_BASED = "rule_based"  # ê·œì¹™ ê¸°ë°˜ íƒì§€
    STATISTICAL = "statistical"  # í†µê³„ ë¶„ì„
    LLM_ANALYSIS = "llm_analysis"  # LLM ë¶„ì„
    HYBRID = "hybrid"  # ë³µí•© ë¶„ì„


@dataclass
class FailureSample:
    """ì‹¤íŒ¨ ì‚¬ë¡€ ìƒ˜í”Œ.

    ê°œì„ ì´ í•„ìš”í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” êµ¬ì²´ì ì¸ ì‹¤íŒ¨ ì‚¬ë¡€ì…ë‹ˆë‹¤.
    """

    test_case_id: str
    question: str
    answer: str
    contexts: list[str]
    ground_truth: str | None = None

    # ë©”íŠ¸ë¦­ ì ìˆ˜
    metric_scores: dict[str, float] = field(default_factory=dict)

    # ì‹¤íŒ¨ ë¶„ì„
    failure_reason: str = ""
    detected_patterns: list[PatternType] = field(default_factory=list)

    # ê°œì„  ë°©í–¥ (LLM ìƒì„± ë˜ëŠ” ê·œì¹™ ê¸°ë°˜)
    suggested_context: str | None = None  # ì´ëŸ° ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì—ˆë‹¤ë©´...
    suggested_answer: str | None = None  # ì´ë ‡ê²Œ ë‹µí–ˆì–´ì•¼...

    # ë©”íƒ€ë°ì´í„°
    analysis_source: EvidenceSource = EvidenceSource.RULE_BASED

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "test_case_id": self.test_case_id,
            "question": self.question,
            "answer": self.answer,
            "contexts": self.contexts,
            "ground_truth": self.ground_truth,
            "metric_scores": self.metric_scores,
            "failure_reason": self.failure_reason,
            "detected_patterns": [p.value for p in self.detected_patterns],
            "suggested_context": self.suggested_context,
            "suggested_answer": self.suggested_answer,
            "analysis_source": self.analysis_source.value,
        }


@dataclass
class PatternEvidence:
    """íŒ¨í„´ íƒì§€ ì¦ê±°.

    íŠ¹ì • ë¬¸ì œ íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŒì„ ì¦ëª…í•˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.
    """

    pattern_type: PatternType

    # í•´ë‹¹ ì¼€ì´ìŠ¤ í†µê³„
    affected_count: int
    total_count: int

    # í†µê³„ì  ê·¼ê±°
    correlation: float | None = None  # ë©”íŠ¸ë¦­ê³¼ì˜ ìƒê´€ê³„ìˆ˜
    p_value: float | None = None
    mean_score_affected: float | None = None  # í•´ë‹¹ íŒ¨í„´ ì¼€ì´ìŠ¤ì˜ í‰ê·  ì ìˆ˜
    mean_score_unaffected: float | None = None  # ë¹„í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ í‰ê·  ì ìˆ˜

    # ì„ê³„ê°’ (ê·œì¹™ ê¸°ë°˜ íƒì§€ì— ì‚¬ìš©ëœ ê°’)
    threshold_used: dict[str, Any] = field(default_factory=dict)

    # ëŒ€í‘œ ì‹¤íŒ¨ ì‚¬ë¡€
    representative_failures: list[FailureSample] = field(default_factory=list)

    # ë¶„ì„ ì¶œì²˜
    source: EvidenceSource = EvidenceSource.RULE_BASED

    @property
    def affected_ratio(self) -> float:
        """ì˜í–¥ë°›ì€ ì¼€ì´ìŠ¤ ë¹„ìœ¨."""
        if self.total_count == 0:
            return 0.0
        return self.affected_count / self.total_count

    @property
    def is_statistically_significant(self) -> bool:
        """í†µê³„ì  ìœ ì˜ì„± ì—¬ë¶€ (p < 0.05)."""
        return self.p_value is not None and self.p_value < 0.05

    @property
    def score_gap(self) -> float | None:
        """ì˜í–¥/ë¹„ì˜í–¥ ê·¸ë£¹ ê°„ ì ìˆ˜ ì°¨ì´."""
        if self.mean_score_affected is None or self.mean_score_unaffected is None:
            return None
        return self.mean_score_unaffected - self.mean_score_affected

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "pattern_type": self.pattern_type.value,
            "affected_count": self.affected_count,
            "total_count": self.total_count,
            "affected_ratio": self.affected_ratio,
            "correlation": self.correlation,
            "p_value": self.p_value,
            "mean_score_affected": self.mean_score_affected,
            "mean_score_unaffected": self.mean_score_unaffected,
            "score_gap": self.score_gap,
            "is_significant": self.is_statistically_significant,
            "threshold_used": self.threshold_used,
            "source": self.source.value,
            "representative_failures": [f.to_dict() for f in self.representative_failures],
        }


@dataclass
class ImprovementEvidence:
    """ê°œì„  ì œì•ˆì˜ ì¦ê±° ë°ì´í„°.

    ì™œ ì´ ê°œì„ ì´ í•„ìš”í•œì§€ë¥¼ ì¦ëª…í•˜ëŠ” ì¢…í•© ì¦ê±°ì…ë‹ˆë‹¤.
    """

    evidence_id: str = field(default_factory=lambda: str(uuid4()))

    # ê´€ë ¨ ë©”íŠ¸ë¦­
    target_metric: str = ""

    # íƒì§€ëœ íŒ¨í„´ë“¤
    detected_patterns: list[PatternEvidence] = field(default_factory=list)

    # ì¢…í•© í†µê³„
    total_failures: int = 0
    avg_score_failures: float = 0.0
    avg_score_passes: float = 0.0

    # ë¶„ì„ ë°©ë²•
    analysis_methods: list[EvidenceSource] = field(default_factory=list)

    # LLM ë¶„ì„ ê²°ê³¼ (ì„ íƒì )
    llm_analysis: str | None = None
    llm_confidence: float | None = None

    # ìœ ì‚¬ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ ì°¸ì¡° (ì„ íƒì )
    benchmark_reference: str | None = None

    @property
    def primary_pattern(self) -> PatternEvidence | None:
        """ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” íŒ¨í„´."""
        if not self.detected_patterns:
            return None
        # affected_count ê¸°ì¤€ ì •ë ¬
        return max(self.detected_patterns, key=lambda p: p.affected_count)

    @property
    def has_statistical_evidence(self) -> bool:
        """í†µê³„ì  ì¦ê±° ì¡´ì¬ ì—¬ë¶€."""
        return any(p.is_statistically_significant for p in self.detected_patterns)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "evidence_id": self.evidence_id,
            "target_metric": self.target_metric,
            "detected_patterns": [p.to_dict() for p in self.detected_patterns],
            "total_failures": self.total_failures,
            "avg_score_failures": self.avg_score_failures,
            "avg_score_passes": self.avg_score_passes,
            "analysis_methods": [m.value for m in self.analysis_methods],
            "llm_analysis": self.llm_analysis,
            "llm_confidence": self.llm_confidence,
            "benchmark_reference": self.benchmark_reference,
            "has_statistical_evidence": self.has_statistical_evidence,
        }


@dataclass
class ImprovementAction:
    """êµ¬ì²´ì ì¸ ê°œì„  ì•¡ì…˜.

    RAG ì‹œìŠ¤í…œì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‹¨ì¼ ì•¡ì…˜ì…ë‹ˆë‹¤.
    """

    action_id: str = field(default_factory=lambda: str(uuid4()))

    # ì•¡ì…˜ ì„¤ëª…
    title: str = ""  # "top_k ì¦ê°€", "Reranker ë„ì…"
    description: str = ""  # ìƒì„¸ ì„¤ëª…
    implementation_hint: str = ""  # êµ¬í˜„ íŒíŠ¸ (ì½”ë“œ ì˜ˆì‹œ ë“±)

    # ì˜ˆìƒ íš¨ê³¼
    expected_improvement: float = 0.0  # 0.05 = 5% ê°œì„  ì˜ˆìƒ
    expected_improvement_range: tuple[float, float] = (0.0, 0.0)  # ë²”ìœ„

    # ë…¸ë ¥ ìˆ˜ì¤€
    effort: EffortLevel = EffortLevel.LOW

    # ìš°ì„ ìˆœìœ„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    priority_score: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "action_id": self.action_id,
            "title": self.title,
            "description": self.description,
            "implementation_hint": self.implementation_hint,
            "expected_improvement": self.expected_improvement,
            "expected_improvement_range": list(self.expected_improvement_range),
            "effort": self.effort.value,
            "priority_score": self.priority_score,
        }


@dataclass
class RAGImprovementGuide:
    """RAG íŒŒì´í”„ë¼ì¸ ê°œì„  ê°€ì´ë“œ.

    íŠ¹ì • ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•œ ê°œì„  ê°€ì´ë“œì…ë‹ˆë‹¤.
    """

    guide_id: str = field(default_factory=lambda: str(uuid4()))
    created_at: datetime = field(default_factory=datetime.now)

    # ëŒ€ìƒ ì»´í¬ë„ŒíŠ¸
    component: RAGComponent = RAGComponent.RETRIEVER

    # ê´€ë ¨ ë©”íŠ¸ë¦­
    target_metrics: list[str] = field(default_factory=list)

    # ìš°ì„ ìˆœìœ„
    priority: ImprovementPriority = ImprovementPriority.P2_MEDIUM

    # ê°œì„  ì•¡ì…˜ ëª©ë¡
    actions: list[ImprovementAction] = field(default_factory=list)

    # ì¦ê±° ë°ì´í„°
    evidence: ImprovementEvidence | None = None

    # ì˜í–¥ë°›ëŠ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    affected_test_case_ids: list[str] = field(default_factory=list)

    # ê²€ì¦ ë°©ë²•
    verification_command: str = ""  # evalvault compare baseline after_fix

    # ë©”íƒ€ë°ì´í„°
    metadata: dict[str, Any] = field(default_factory=dict)

    @property
    def total_expected_improvement(self) -> float:
        """ëª¨ë“  ì•¡ì…˜ì˜ ì˜ˆìƒ ê°œì„  í•©ê³„ (ì¤‘ë³µ ê³ ë ¤í•˜ì§€ ì•Šì€ ë‹¨ìˆœ í•©)."""
        return sum(a.expected_improvement for a in self.actions)

    @property
    def top_action(self) -> ImprovementAction | None:
        """ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ì•¡ì…˜."""
        if not self.actions:
            return None
        return max(self.actions, key=lambda a: a.priority_score)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "guide_id": self.guide_id,
            "created_at": self.created_at.isoformat(),
            "component": self.component.value,
            "target_metrics": self.target_metrics,
            "priority": self.priority.value,
            "actions": [a.to_dict() for a in self.actions],
            "evidence": self.evidence.to_dict() if self.evidence else None,
            "affected_test_case_ids": self.affected_test_case_ids,
            "verification_command": self.verification_command,
            "total_expected_improvement": self.total_expected_improvement,
            "metadata": self.metadata,
        }


@dataclass
class ImprovementReport:
    """ì¢…í•© ê°œì„  ë¦¬í¬íŠ¸.

    í‰ê°€ ê²°ê³¼ì— ëŒ€í•œ ì „ì²´ ê°œì„  ê°€ì´ë“œ ëª¨ìŒì…ë‹ˆë‹¤.
    """

    report_id: str = field(default_factory=lambda: str(uuid4()))
    run_id: str = ""
    created_at: datetime = field(default_factory=datetime.now)

    # ìš”ì•½ ì •ë³´
    total_test_cases: int = 0
    failed_test_cases: int = 0
    pass_rate: float = 0.0

    # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜
    metric_scores: dict[str, float] = field(default_factory=dict)
    metric_thresholds: dict[str, float] = field(default_factory=dict)
    metric_gaps: dict[str, float] = field(default_factory=dict)  # threshold - score

    # ê°œì„  ê°€ì´ë“œ ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
    guides: list[RAGImprovementGuide] = field(default_factory=list)

    # ì „ì²´ ì˜ˆìƒ ê°œì„ í­
    total_expected_improvement: dict[str, float] = field(default_factory=dict)

    # ë¶„ì„ ë°©ë²•
    analysis_methods_used: list[EvidenceSource] = field(default_factory=list)

    # ë©”íƒ€ë°ì´í„°
    metadata: dict[str, Any] = field(default_factory=dict)

    def get_guides_by_metric(self, metric: str) -> list[RAGImprovementGuide]:
        """íŠ¹ì • ë©”íŠ¸ë¦­ ê´€ë ¨ ê°€ì´ë“œ ì¡°íšŒ."""
        return [g for g in self.guides if metric in g.target_metrics]

    def get_guides_by_priority(self, priority: ImprovementPriority) -> list[RAGImprovementGuide]:
        """íŠ¹ì • ìš°ì„ ìˆœìœ„ ê°€ì´ë“œ ì¡°íšŒ."""
        return [g for g in self.guides if g.priority == priority]

    def get_critical_guides(self) -> list[RAGImprovementGuide]:
        """P0 (Critical) ê°€ì´ë“œë§Œ ì¡°íšŒ."""
        return self.get_guides_by_priority(ImprovementPriority.P0_CRITICAL)

    def to_dict(self) -> dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜."""
        return {
            "report_id": self.report_id,
            "run_id": self.run_id,
            "created_at": self.created_at.isoformat(),
            "total_test_cases": self.total_test_cases,
            "failed_test_cases": self.failed_test_cases,
            "pass_rate": self.pass_rate,
            "metric_scores": self.metric_scores,
            "metric_thresholds": self.metric_thresholds,
            "metric_gaps": self.metric_gaps,
            "guides": [g.to_dict() for g in self.guides],
            "total_expected_improvement": self.total_expected_improvement,
            "analysis_methods_used": [m.value for m in self.analysis_methods_used],
            "metadata": self.metadata,
        }

    def to_markdown(self) -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±."""
        lines = [
            "# RAG ê°œì„  ê°€ì´ë“œ ë¦¬í¬íŠ¸",
            "",
            "## ìš”ì•½",
            "",
            f"- **í‰ê°€ ID**: `{self.run_id}`",
            f"- **ìƒì„± ì‹œê°„**: {self.created_at.strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {self.total_test_cases}ê°œ",
            f"- **í†µê³¼ìœ¨**: {self.pass_rate:.1%}",
            "",
            "### ë©”íŠ¸ë¦­ë³„ í˜„í™©",
            "",
            "| ë©”íŠ¸ë¦­ | ì ìˆ˜ | ëª©í‘œ | ê°­ | ìƒíƒœ |",
            "|--------|------|------|-----|------|",
        ]

        for metric, score in self.metric_scores.items():
            threshold = self.metric_thresholds.get(metric, 0.7)
            gap = self.metric_gaps.get(metric, threshold - score)
            status = "âœ…" if score >= threshold else "âŒ"
            lines.append(f"| {metric} | {score:.3f} | {threshold:.2f} | {gap:+.3f} | {status} |")

        stage_summary = self.metadata.get("stage_metrics_summary")
        if stage_summary:
            pass_rate = stage_summary.get("pass_rate")
            pass_rate_text = f"{pass_rate:.1%}" if pass_rate is not None else "n/a"
            lines.extend(
                [
                    "",
                    "### ë‹¨ê³„ ë©”íŠ¸ë¦­ ìš”ì•½",
                    "",
                    f"- ì´ ë©”íŠ¸ë¦­: {stage_summary.get('total', 0)}ê°œ",
                    f"- í‰ê°€ ëŒ€ìƒ(ì„ê³„ê°’ ìˆìŒ): {stage_summary.get('evaluated', 0)}ê°œ",
                    f"- í†µê³¼: {stage_summary.get('passed', 0)}ê°œ / ì‹¤íŒ¨: {stage_summary.get('failed', 0)}ê°œ",
                    f"- í†µê³¼ìœ¨: {pass_rate_text}",
                ]
            )
            top_failures = stage_summary.get("top_failures", [])
            if top_failures:
                lines.extend(
                    [
                        "",
                        "| ë©”íŠ¸ë¦­ | ì‹¤íŒ¨ ê±´ìˆ˜ | í‰ê·  ì ìˆ˜ | ì„ê³„ê°’ |",
                        "|--------|----------|-----------|--------|",
                    ]
                )
                for item in top_failures:
                    threshold = item.get("threshold")
                    threshold_text = f"{threshold:.3f}" if threshold is not None else "-"
                    lines.append(
                        f"| {item.get('metric_name')} | {item.get('count', 0)} | "
                        f"{item.get('avg_score', 0.0):.3f} | {threshold_text} |"
                    )

        lines.extend(["", "---", ""])

        # ê°€ì´ë“œë³„ ìƒì„¸
        for i, guide in enumerate(self.guides, 1):
            priority_emoji = {
                ImprovementPriority.P0_CRITICAL: "ğŸ”´",
                ImprovementPriority.P1_HIGH: "ğŸŸ ",
                ImprovementPriority.P2_MEDIUM: "ğŸŸ¡",
                ImprovementPriority.P3_LOW: "ğŸŸ¢",
            }
            emoji = priority_emoji.get(guide.priority, "âšª")

            lines.extend(
                [
                    f"## {i}. {guide.component.value.title()} ê°œì„  {emoji}",
                    "",
                    f"**ìš°ì„ ìˆœìœ„**: {guide.priority.value}",
                    f"**ëŒ€ìƒ ë©”íŠ¸ë¦­**: {', '.join(guide.target_metrics)}",
                    "",
                ]
            )

            # ì¦ê±° ìš”ì•½
            if guide.evidence and guide.evidence.detected_patterns:
                lines.append("### ë¬¸ì œ íŒ¨í„´")
                lines.append("")
                for pattern in guide.evidence.detected_patterns:
                    lines.append(
                        f"- **{pattern.pattern_type.value}**: "
                        f"{pattern.affected_count}/{pattern.total_count}ê±´ "
                        f"({pattern.affected_ratio:.1%})"
                    )
                    if pattern.score_gap:
                        lines.append(f"  - ì ìˆ˜ ì°¨ì´: {pattern.score_gap:+.3f}")
                lines.append("")

            # ê°œì„  ì•¡ì…˜
            if guide.actions:
                lines.append("### ê°œì„  ì•¡ì…˜")
                lines.append("")
                for j, action in enumerate(guide.actions, 1):
                    effort_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´"}
                    e_emoji = effort_emoji.get(action.effort.value, "âšª")
                    lines.append(
                        f"#### {j}. {action.title} (ì˜ˆìƒ ê°œì„ : +{action.expected_improvement:.0%}) {e_emoji}"
                    )
                    lines.append("")
                    if action.description:
                        lines.append(action.description)
                        lines.append("")
                    if action.implementation_hint:
                        lines.append("```")
                        lines.append(action.implementation_hint)
                        lines.append("```")
                        lines.append("")

            # ê²€ì¦ ë°©ë²•
            if guide.verification_command:
                lines.extend(
                    [
                        "### ê²€ì¦ ë°©ë²•",
                        "",
                        "```bash",
                        guide.verification_command,
                        "```",
                        "",
                    ]
                )

            lines.extend(["---", ""])

        lines.append("*Generated by EvalVault*")

        return "\n".join(lines)
