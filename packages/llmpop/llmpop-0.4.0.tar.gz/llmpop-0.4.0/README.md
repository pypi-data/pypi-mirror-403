# [LLMPop](https://pypi.org/project/llmpop/)
The Python library that lets you spin up any LLM with a single function.  
#### Why did we need this library:    
1. Needed a single simple command for any LLM, including the free local LLMs that Ollama offers.  
2. Needed a better way for introducing a code library to a LLM that helps you build code. The `llmpop` library comes with a machine-readable file that is minimal and sufficent, see [**`LLM_READABLE_GUIDE.md`**](https://raw.githubusercontent.com/LiorGazit/llmpop/main/LLM_READABLE_GUIDE.md). 
   Add it to your conversation with the coding LLM and it will learn how to build code with `llmpop`. From a security aspect, this approach is safer then directing your LLM to read someone's entire codebase.  

### Devs: [Lior Gazit](https://github.com/LiorGazit), and GPT5  
Total hours spent in total on this project so far: `28 hours`   

### Quick run of LLMPop:  
Quickest on Colab:  
<a target="_blank" href="https://colab.research.google.com/github/LiorGazit/llmpop/blob/main/examples/quick_run_llmpop.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>  
Or if you want to set it up yourself, pick the free `T4 GPU`, and copy code over:    
**Setup:**  
```python
%pip -q install llmpop 
from llmpop import init_llm, install_ollama_deps
install_ollama_deps()
```  
**Run:**  
```python
# Start with Meta's Llama. If you want a stronger (and bigger) model, try OpenAI's free "gpt-oss:20b":
model = init_llm(model="llama3.2:1b", provider="ollama")
user_prompt = "What OS is better for deploying high scale programs in production? Linux, or Windows?"
print(model.invoke(user_prompt).content)
```


## Examples and tools built with LLMPop  
- `../notebooks/`  
- `../examples/`  

Specifically check out the Colab demo UI notebook under `notebooks/multi_llm_webapp.ipynb`.  
That notebook is a minimal â€œclick, pick, and promptâ€ UI for LLMPop that lets you select up to four models and compare their replies side by side.  
It runs entirely free in your free Google Colab session and auto-handles local models via Ollama (no local installs on your machine).  

## Features
- Plug-and-play local LLMs via Ollamaâ€”no cloud or API costs required.  
- Easy remote API support (OpenAI, extendable).  
- Unified interface: Seamlessly switch between local and remote models in your code.  
- Resource monitoring: Track CPU, memory, and (optionally) GPU usage while your agents run.  

## Using LLMPop while coding with an LLM/chatbot  
A dedicated, machine readable guide file, is designed to be the one single necessary file for a bot to get to know LLMPop and to build your code with it.  
This guide file is [**`LLM_READABLE_GUIDE.md`**](https://raw.githubusercontent.com/LiorGazit/llmpop/main/LLM_READABLE_GUIDE.md)     
So, either upload this file to your bot's conversation, or copy the file's content to paste for the bot's context, and it would allow your bot to leverage LLMPop as it builds code.  
Note that this machine readable file is super useful in cases that your bot doesn't have access to the internet and can't learn about code libraries it wasn't trained on.  
More on this guide file in `docs/index.md`  

## Quick start via Colab
Start by running `run_ollama_in_colab.ipynb` in [Colab](https://colab.research.google.com/github/LiorGazit/llmpop/blob/main/examples/run_ollama_in_colab.ipynb).  

ğŸ“– **Quick Guides**
- **Library usage (human-readable):** See [`LLM_READABLE_GUIDE.md`](./LLM_READABLE_GUIDE.md)  
- **Full docs homepage:** See [`docs/index.md`](./docs/index.md)  


## Codebase Structure  
llmpop/  
â”œâ”€ .github/  
â”‚  â””â”€ workflows/  
â”‚     â””â”€ ci.yml  
â”œâ”€ docs/  
â”‚  â””â”€ index.md  
â”œâ”€ notebooks/  
â”‚  â””â”€ multi_llm_webapp.ipynb  
â”œâ”€ examples/  
â”‚  â”œâ”€ quick_run_llmpop.ipynb  
â”‚  â”œâ”€ quick_run_llmpop.py  
â”‚  â””â”€ run_ollama_in_colab.ipynb  
â”œâ”€ src/  
â”‚  â””â”€ llmpop/  
â”‚     â”œâ”€ \_\_init\_\_.py  
â”‚     â”œâ”€ init_llm.py   
â”‚     â”œâ”€ monitor_resources.py  
â”‚     â”œâ”€ py.typed  
â”‚     â””â”€ version.py   
â”œâ”€ tests/  
â”‚  â”œâ”€ test_init_llm.py  
â”‚  â”œâ”€ test_llm_readable_guide.py  
â”‚  â””â”€ test_monitor_resources.py  
â”œâ”€ .gitignore  
â”œâ”€ .pre-commit-config.yaml  
â”œâ”€ CHANGELOG.md  
â”œâ”€ CODE_OF_CONDUCT.md  
â”œâ”€ CONTRIBUTING.md  
â”œâ”€ DEVLOG.md  
â”œâ”€ LICENSE  
â”œâ”€ LLM_READABLE_GUIDE.md   
â”œâ”€ Makefile            
â”œâ”€ MANIFEST.in            
â”œâ”€ pyproject.toml  
â”œâ”€ README.md  
â”œâ”€ requirements-dev.txt      
â””â”€ requirements.txt   

Where:  
â€¢ `src/` layout is the modern standard for packaging.  
â€¢ `tests/` use pytest; weâ€™ll mock shell/network so CI doesnâ€™t try to actually install/run Ollama.  
â€¢ `examples/` contains notebooks users can run locally/Colab.  
â€¢ `docs/` is optional now; you can add mkdocs later.  
â€¢ `CI` runs lint + unit tests on pushes and PRs.  
â€¢ `CHANGELOG` follows Keep a Changelog; DEVLOG is your running engineering journal.  

## Quick setting up  
1. Install from GitHub    
`pip -q install llmpop`  

2. Try it  
    ```python
    from llmpop import init_llm, start_resource_monitoring
    from langchain_core.prompts import ChatPromptTemplate

    model = init_llm(model="gemma3:1b", provider="ollama")
    # Or:
    # os.environ["OPENAI_API_KEY"] = "sk-..."
    # model = init_llm(chosen_llm="gpt-4o", provider="openai")

    prompt = ChatPromptTemplate.from_template("Q: {q}\nA:")
    print((prompt | model).invoke({"q":"What is an agent?"}).content)
    ```

 3. Optional - Resource Monitoring
    ```python
    monitor_thread = start_resource_monitoring(duration=600, interval=10)
    ```

Note: LLMPop wraps and depends on third-party open-source libraries (LangChain, Ollama, etc.).
These are licensed separately and are not included in LLMPopâ€™s distribution.  

Enjoy!
