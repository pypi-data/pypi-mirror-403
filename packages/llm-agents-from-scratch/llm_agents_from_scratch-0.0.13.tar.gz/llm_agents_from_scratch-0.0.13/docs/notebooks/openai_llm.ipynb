{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be1b6cd-6b32-47b3-ab9d-37ea9f2d9e23",
   "metadata": {},
   "source": [
    "# Using `OpenAILLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa644c83-a2df-41c3-9722-c699d89dbb50",
   "metadata": {},
   "source": [
    "In this notebook, we show how to use the `OpenAILLM` class.\n",
    "\n",
    "__Requirements:__\n",
    "\n",
    "1. Have an OpenAI API key configured (for example, via the `OPENAI_API_KEY` environment variable). You can create and manage keys in the [OpenAI dashboard](https://platform.openai.com/account/api-keys).\n",
    "2. Ensure this environment has network access to the OpenAI API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cb19f-7dd3-4aaa-b6af-638bd443cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install \"llm-agents-from-scratch[openai]\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c7d8e-d523-419a-9aad-32ffc07554f4",
   "metadata": {},
   "source": [
    "## Instantiating an `OpenAILLM` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1993b5be-ead6-4ffd-96ec-fbf254064eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.llms.openai import OpenAILLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e300e09-e273-439e-af79-3b74d0423f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILLM(\n",
    "    model=\"gpt-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411926b-c4c9-4562-8b7b-8a45961d29e5",
   "metadata": {},
   "source": [
    "## Complete\n",
    "\n",
    "We use the `.complete()` method to perform text completion with our `OpenAILLM` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1826738b-06f6-406f-a292-f19b05798299",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await llm.complete(prompt=\"Tell me a joke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a7f221-6ac2-4d86-b805-0a8ba1034647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_agents_from_scratch.data_structures.llm.CompleteResult"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2764770f-4505-4fe6-8e4d-d374e68e8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow get promoted? He was outstanding in his field.\n",
      "\n",
      "Want another?\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296fe5c-7eb3-47c6-a652-fcc6bd686773",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "We use the `.chat()` method to chat with `OpenAILLM` object, using the chat API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f20c99e-ff7b-4718-aae2-a6aa7d0ab9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message, response_message = await llm.chat(\"Tell me a joke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed2b650-8a63-4c59-b465-e6e2474ee0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_agents_from_scratch.data_structures.llm.ChatMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5150aafa-f563-4c4d-b5b8-4d2f66ad795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role=<ChatRole.USER: 'user'> content='Tell me a joke.' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "print(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787e522c-5697-439b-816c-8bf8fa89ee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_agents_from_scratch.data_structures.llm.ChatMessage"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a3193e-321c-4b1d-b184-7a007e3dd32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don’t scientists trust atoms? Because they make up everything.\n",
      "\n",
      "Want another—punny, techy, or dad-joke style?\n"
     ]
    }
   ],
   "source": [
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af19a4d-2a5b-424b-b0fd-d7664535914f",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "We use the `.structured_output()` method to produce structured responses, represented as `~pydantic.BaseModel`, with our `OpenAILLM` object. Here, we'll ask it to produce a structured data class `Joke` that contains a `topic` and a `content` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88c8149-a3a2-43e4-9bb7-0ca0e65fcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"A structured representation of a joke.\"\"\"\n",
    "\n",
    "    topic: str = Field(description=\"Topic of the joke.\")\n",
    "    content: str = Field(description=\"Joke content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051ad64d-d257-49ff-abb9-e83ac6102db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke = await llm.structured_output(\n",
    "    prompt=\"Tell me a new joke about any topic you like.\",\n",
    "    mdl=Joke,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb6db9e-5738-4d7f-893b-1d38da76dea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(topic='Smart fridges', content='My smart fridge just sent me a push notification that said, \"We need to talk.\" I opened the door and it goes, \"This relationship isn\\'t healthy—mostly because of what you keep putting inside me. Also, I can\\'t chill until we address your leftover commitment issues.\"')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73083ffe-c3d0-4954-a0aa-f708c80a3f16",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "In this section of the notebook, we demonstrate how to perform tool calls with an `OpenAILLM`. To do so, first we'll need to create some tools using the `llm-agents-from-scratch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1390fd69-79c5-4fbf-9945-118d7f7b0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from llm_agents_from_scratch import (\n",
    "    PydanticFunctionTool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a342458-bd6a-4d71-90d9-3dad8a23d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOneParams(BaseModel):\n",
    "    \"\"\"Parameters for `add_one` tool.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    x: float\n",
    "\n",
    "\n",
    "def add_one(params: AddOneParams) -> int:\n",
    "    \"\"\"Adds one to a given number.\"\"\"\n",
    "    return params.x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b103ad2a-e56d-4e48-9972-1fa6cf9cb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one_tool = PydanticFunctionTool(func=add_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922cb8e-50eb-450b-b27e-8ec76c09cafd",
   "metadata": {},
   "source": [
    "The `parameters_json_schema` of a `BaseTool` object, shows how the tool's parameters will be passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9523f0e3-6c70-417f-80c9-dd80a4b344b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additionalProperties': False,\n",
       " 'description': 'Parameters for `add_one` tool.',\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'}},\n",
       " 'required': ['x'],\n",
       " 'title': 'AddOneParams',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_one_tool.parameters_json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc72e74-0da2-4442-b7bd-7cf6498babe6",
   "metadata": {},
   "source": [
    "### Testing the tool\n",
    "\n",
    "NOTE: this is a direct invocation of the tool, without any LLM invocation. A `ToolCall` is how we bundle the parameters that should be passed to the tool. The `ToolCall` object is also the parameter used in the `__call__` method for all `BaseTool` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb5173f4-c014-4d9c-91b6-85dd8a4cb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.data_structures.tool import ToolCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a45a3c-4406-4957-905a-d013e3c89f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolCallResult(tool_call_id='e7568a27-5e4e-410f-8984-64be30e48061', content='4.15', error=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add one\n",
    "tool_call = ToolCall(\n",
    "    tool_name=add_one_tool.name,\n",
    "    arguments={\"x\": 3.15},\n",
    ")\n",
    "\n",
    "# invoke the __call__ method\n",
    "add_one_tool(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "888c7c53-ac86-4fac-bd8f-0b9fd8bbd1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolCallResult(tool_call_id='f66dd44a-19a2-4994-9b54-81ec8e5aaadf', content='4.15', error=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add one\n",
    "tool_call = ToolCall(\n",
    "    tool_name=add_one_tool.name,\n",
    "    arguments={\"x\": 3.15},\n",
    ")\n",
    "add_one_tool(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05bcfe-5a4c-4d01-b15b-61a5fcb5e1de",
   "metadata": {},
   "source": [
    "### Get `OpenAILLM` to use the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef99d735-dfc9-410e-a5c3-ccaba87f1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message, response_message = await llm.chat(\n",
    "    input=\"Add one to fifty-five point three. Use only the appropriate tools!\",\n",
    "    tools=[add_one_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88343975-2250-4772-b656-07528cac095e",
   "metadata": {},
   "source": [
    "We can see that the LLM is requesting for a tool call for `add_one`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86553462-ed10-4879-b9d4-db9fa6971b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<ChatRole.ASSISTANT: 'assistant'>, content='', tool_calls=[ToolCall(id_='call_RP9rdXx3BncacO5CogKqKhs1', tool_name='add_one', arguments={'x': 55.3})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938aeb3-4cb7-42dc-87f4-3ab1718082c4",
   "metadata": {},
   "source": [
    "In the LLM agent processing cycle, we would next perform the tool call, and pass the result back to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "137ce3aa-47e6-45f0-b526-b60944ec67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'message', 'content': 'Add one to fifty-five point three. Use only the appropriate tools!', 'role': 'user'}, {'type': 'function_call', 'arguments': '{\"x\": 55.3}', 'call_id': 'call_RP9rdXx3BncacO5CogKqKhs1', 'name': 'add_one'}, {'type': 'function_call_output', 'call_id': 'call_RP9rdXx3BncacO5CogKqKhs1', 'output': '{\\n  \"content\": \"56.3\",\\n  \"error\": false\\n}'}]\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    tool_messages,\n",
    "    response_message,\n",
    ") = await llm.continue_chat_with_tool_results(\n",
    "    tool_call_results=[\n",
    "        add_one_tool(\n",
    "            response_message.tool_calls[0],\n",
    "        ),  # returns a ToolCallResult\n",
    "    ],\n",
    "    chat_history=[\n",
    "        user_message,\n",
    "        response_message,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0754eb48-3dc6-4d70-a97a-c2c96edfd8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<ChatRole.TOOL: 'tool'>, content='{\\n    \"tool_call_id\": \"call_RP9rdXx3BncacO5CogKqKhs1\",\\n    \"content\": \"56.3\",\\n    \"error\": false\\n}', tool_calls=None)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1ec8f38-c2cf-4c2b-8afd-c76facf752b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tool_call_id\": \"call_RP9rdXx3BncacO5CogKqKhs1\",\n",
      "    \"content\": \"56.3\",\n",
      "    \"error\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tool_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b57fde4-ac15-41cc-bc8d-16082ba3ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<ChatRole.ASSISTANT: 'assistant'>, content='56.3', tool_calls=[])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fa0f3a8-d15f-4f90-97b1-4ab576ea29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.3\n"
     ]
    }
   ],
   "source": [
    "print(response_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
