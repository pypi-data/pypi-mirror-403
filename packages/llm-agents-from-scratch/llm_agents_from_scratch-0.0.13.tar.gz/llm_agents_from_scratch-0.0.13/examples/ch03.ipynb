{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77fd63b8-688a-49c8-8e40-a2287db46be9",
   "metadata": {},
   "source": [
    "# Examples from Chapter 3 — Working with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d77e6-4704-48c5-87a3-930a6dddc60d",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "To ensure you have the required dependencies to run this notebook, you'll need to have our `llm-agents-from-scratch` framework installed on the running Jupyter kernel. To do this, you can launch this notebook with the following command while within the project's root directory:\n",
    "\n",
    "```sh\n",
    "uv run --with jupyter jupyter lab\n",
    "```\n",
    "\n",
    "Alternatively, if you just want to use the published version of `llm-agents-from-scratch` without local development, you can install it from PyPi by uncommenting the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5eaefa5-2c00-43e8-99fe-31e0322739ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install `llm-agents-from-scratch` from PyPi\n",
    "# !pip install llm-agents-from-scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aebcb08d-3576-4e1b-8815-ee7d10815e3e",
   "metadata": {},
   "source": [
    "## Running an Ollama service\n",
    "\n",
    "To execute the code provided in this section, you’ll need to have Ollama installed on your local machine and have its LLM hosting service running. To download Ollama, follow the instructions found on this page: https://ollama.com/download. After downloading and installing Ollama, you can start a service by opening a terminal and running the command `ollama serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0630f5-6625-49e6-8b3e-880ced011218",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d2302-3dde-4a1d-9312-e8d24129edf2",
   "metadata": {},
   "source": [
    "The code in the book uses `asyncio.run()` to execute coroutines. To align the book code with this Jupyter notebook, we'll use the `nest_asyncio` library, which allows for nested async event loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83338bf-7a71-4143-8a9c-049ba1b9aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow us to execute `asyncio.run()` calls\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c1cf3-85d6-4cc6-a29c-f71f5a8790bf",
   "metadata": {},
   "source": [
    "### Example 1: Joke model for `structured_output()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb422999-9dbd-43bc-87bd-bfc3b57811dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"A structured output model for Jokes.\"\"\"\n",
    "\n",
    "    subject: Literal[\"math\", \"physics\", \"biology\"]\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e2c4c-d8f3-450b-af8f-cdb1e257002d",
   "metadata": {},
   "source": [
    "### Example 2: Instantiating an `OllamaLLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de1ad6-ea64-4ef6-91c7-721a1878141e",
   "metadata": {},
   "source": [
    "Note: the below command requies the `qwen2.5:3b` model to have been pulled. To do so, in terminal run: `ollama pull qwen2.5:3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf19a60-8501-454a-8159-0af1dd8d7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.llms.ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476060f4-86da-4847-924b-5afc5d78ada6",
   "metadata": {},
   "source": [
    "### Example 3: Using `complete()` to have an `OllamaLLM` tell a joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce2224a-356c-49fd-8452-d6904d7db2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response='Sure! Here\\'s one for you:\\n\\nWhy don\\'t scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nThis is a play on the phrase \"they make up matter,\" and in this case, it makes no sense because we can\\'t blame or rely on atoms to do things as humans would.' prompt='Tell me a joke.'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llm_agents_from_scratch.llms.ollama import OllamaLLM\n",
    "\n",
    "async def main():\n",
    "    llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "    response = await llm.complete(\"Tell me a joke.\")\n",
    "    print(response)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a2b88-e862-41f2-865d-6d5651e3b119",
   "metadata": {},
   "source": [
    "### Example 4: Using `structured_output()` to have an `OllamaLLM` tell a (structured) joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b222ee3d-c346-4cfa-ac90-896a46ae06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke\n",
      "subject='math' joke='Why was the math book sad? Because it had too many problems.'\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "    prompt = (\"Tell me a joke.\")\n",
    "    joke = await llm.structured_output(prompt=prompt, mdl=Joke)\n",
    "    print(joke.__class__.__name__)\n",
    "    print(joke)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ded84f-7238-48b9-9969-06fb0d7be882",
   "metadata": {},
   "source": [
    "### Example 5: Hailstone tool call with `OllamaLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110df095-04a3-4d97-a1ff-1a8f19e18be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hailstone_step_func(x: int) -> int:\n",
    "    \"\"\"Performs a single step of the Hailstone sequence.\"\"\"\n",
    "    if x % 2 == 0:\n",
    "        return x // 2\n",
    "    return 3 * x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca97cc9-7002-484d-bfc4-59619c1247ee",
   "metadata": {},
   "source": [
    "#### Eliciting a tool call request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fe7930-1369-4871-9f90-18e464fc13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolCall(id_='96efe041-50ff-45f0-867b-8e493901a28f', tool_name='hailstone_step_func', arguments={'x': 3})]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llm_agents_from_scratch.llms.ollama import OllamaLLM\n",
    "from llm_agents_from_scratch.data_structures.llm import ChatMessage\n",
    "from llm_agents_from_scratch.tools import SimpleFunctionTool\n",
    "\n",
    "hailstone_tool = SimpleFunctionTool(hailstone_step_func)\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "async def main():\n",
    "    user_input = (\n",
    "        \"What is the result of taking the next step of the \"\n",
    "        \"Hailstone sequence on the number 3?\\n\\n\"\n",
    "        \"Be very succinct in your response.\"\n",
    "    )\n",
    "    return await llm.chat(\n",
    "        user_input,\n",
    "        tools=[hailstone_tool],\n",
    "    )\n",
    "\n",
    "user_msg, response_msg = asyncio.run(main())\n",
    "print(response_msg.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700664f-8725-4e73-ba91-c55b6f1c0aaf",
   "metadata": {},
   "source": [
    "#### Executing the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf129e2-119d-42ca-a9d5-72ae00607c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_call_id='96efe041-50ff-45f0-867b-8e493901a28f' content='10' error=False\n"
     ]
    }
   ],
   "source": [
    "tool_call = response_msg.tool_calls[0]\n",
    "tool_call_result = hailstone_tool(tool_call) # a ToolCallResult\n",
    "print(tool_call_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581dfdc-99a3-43e3-91d8-5741d1e5b475",
   "metadata": {},
   "source": [
    "#### Sending tool results back to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29ab3da-bb88-446d-946d-39132ce2131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role=<ChatRole.ASSISTANT: 'assistant'> content='The result of taking the next step in the Hailstone sequence on the number 3 is 10.' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    return await llm.continue_chat_with_tool_results(\n",
    "        tool_call_results=[tool_call_result],\n",
    "        chat_history=[user_msg, response_msg],\n",
    "    )\n",
    "    \n",
    "tools_msg, final_response = asyncio.run(main())\n",
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
