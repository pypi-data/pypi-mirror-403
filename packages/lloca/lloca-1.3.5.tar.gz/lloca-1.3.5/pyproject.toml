[build-system]
requires = ["setuptools>=68", "setuptools-scm>=8", "wheel>=0.42"]
build-backend = "setuptools.build_meta"

[project]
name = "lloca"
dynamic = ["version"] # comes from setuptools-scm
description = "Lorentz Local Canonicalization"
requires-python = ">=3.10"
readme = "README.md"
authors = [
  { name = "Jonas Spinner", email = "j.spinner@thphys.uni-heidelberg.de" },
  { name = "Luigi Favaro", email = "luigi.favaro@uclouvain.be" },
]
dependencies = [
    "torch>=2.1",
    "torch-geometric>=2.3.0",
    "lgatr",
    "pelican-lite",
]

[project.optional-dependencies]
varlen-attention = ["torch>=2.10"]
xformers-attention = ["xformers"]
flex-attention = ["torch>=2.7"]  # experimental in torch 2.5, 2.6
flash-attention = ["flash-attn"]  # non-trivial, check https://deepwiki.com/Dao-AILab/flash-attention/1.1-installation-and-setup for build instructions
dev = [
    "pre-commit",
    # unit tests
    "pytest",
    "pytest-cov",
    # docs
    "sphinx<9.0",
    "sphinx-autodoc-typehints",
    "sphinx-rtd-theme",
]

[project.entry-points."lloca.backbone.attention_backends"]
native = "lloca.backbone.attention_backends.native"
varlen = "lloca.backbone.attention_backends.varlen"
xformers = "lloca.backbone.attention_backends.xformers"
flex = "lloca.backbone.attention_backends.flex"
flash = "lloca.backbone.attention_backends.flash"

[project.urls]
homepage = "https://github.com/heidelberg-hepml/lloca"
repository = "https://github.com/heidelberg-hepml/lloca"

[tool.setuptools]
packages = { find =  { include = ["lloca", "lloca.*"] } }

[tool.setuptools_scm]
version_scheme = "post-release"
local_scheme = "no-local-version"

[tool.ruff]
line-length = 100
target-version = "py310"
extend-exclude = ["data/", "runs/", "notebooks/"]

[tool.ruff.lint]
select = ["E","F","I","B","UP"]
ignore = [
    "E501", # no complaints from line length
    "B006", # allow mutable default function arguments
]
[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
