# This file was generated by Nuitka

# Stubs included by default


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import os
import typing
import gllm_core
import gllm_core.utils
import gllm_inference.em_invoker.AzureOpenAIEMInvoker
import gllm_inference.em_invoker.BedrockEMInvoker
import gllm_inference.em_invoker.CohereEMInvoker
import gllm_inference.em_invoker.GoogleEMInvoker
import gllm_inference.em_invoker.JinaEMInvoker
import gllm_inference.em_invoker.LangChainEMInvoker
import gllm_inference.em_invoker.OpenAICompatibleEMInvoker
import gllm_inference.em_invoker.OpenAIEMInvoker
import gllm_inference.em_invoker.TwelveLabsEMInvoker
import gllm_inference.em_invoker.VoyageEMInvoker
import gllm_inference.lm_invoker.AnthropicLMInvoker
import gllm_inference.lm_invoker.AzureOpenAILMInvoker
import gllm_inference.lm_invoker.BedrockLMInvoker
import gllm_inference.lm_invoker.DatasaurLMInvoker
import gllm_inference.lm_invoker.GoogleLMInvoker
import gllm_inference.lm_invoker.LangChainLMInvoker
import gllm_inference.lm_invoker.LiteLLMLMInvoker
import gllm_inference.lm_invoker.OpenAIChatCompletionsLMInvoker
import gllm_inference.lm_invoker.OpenAICompatibleLMInvoker
import gllm_inference.lm_invoker.OpenAILMInvoker
import gllm_inference.lm_invoker.PortkeyLMInvoker
import gllm_inference.lm_invoker.SeaLionLMInvoker
import gllm_inference.lm_invoker.XAILMInvoker
import gllm_inference.prompt_builder.PromptBuilder
import gllm_inference.output_parser.JSONOutputParser
import json
import abc
import pandas
import pydantic
import re
import gllm_core.utils.retry
import gllm_inference.request_processor.LMRequestProcessor
import gllm_core.utils.imports
import gllm_inference.schema.ModelId
import gllm_inference.schema.ModelProvider
import gllm_inference.schema.TruncationConfig
import gllm_inference.schema.VectorFuserType
import asyncio
import base64
import enum
import gllm_inference.exceptions.BaseInvokerError
import gllm_inference.schema.Attachment
import gllm_inference.schema.AttachmentType
import gllm_inference.schema.EMContent
import gllm_inference.schema.Vector
import aioboto3
import boto3
import cohere
import asyncio.CancelledError
import gllm_inference.exceptions.convert_to_base_invoker_error
import gllm_inference.exceptions.get_error_extractor
import gllm_inference.schema.TruncateSide
import gllm_inference.schema.URLAttachment
import google
import google.auth
import google.genai
import google.genai.types
import httpx
import gllm_inference.exceptions.ProviderInternalError
import gllm_core.utils.concurrency
import langchain_core
import langchain_core.embeddings
import gllm_inference.utils.load_langchain_model
import gllm_inference.utils.parse_model_data
import openai
import gllm_core.utils.logger_manager
import __future__
import io
import twelvelabs
import numpy
import sys
import voyageai
import voyageai.client_async
import gllm_inference.exceptions.error_extractor.EXTRACTOR_MAP
import gllm_inference.exceptions.error_extractor.get_all_provider_extractors
import gllm_inference.exceptions.error_extractor.get_error_extractor
import asyncio.TimeoutError
import http
import http.HTTPStatus
import uuid
import gllm_core.schema
import gllm_inference.schema.BatchStatus
import gllm_inference.schema.CodeEvent
import gllm_inference.schema.CodeExecResult
import gllm_inference.schema.LMInput
import gllm_inference.schema.LMOutput
import gllm_inference.schema.LMOutputType
import gllm_inference.schema.LMTool
import gllm_inference.schema.Message
import gllm_inference.schema.NativeTool
import gllm_inference.schema.NativeToolType
import gllm_inference.schema.OutputTransformerType
import gllm_inference.schema.Reasoning
import gllm_inference.schema.ResponseSchema
import gllm_inference.schema.ThinkingConfig
import gllm_inference.schema.ThinkingEvent
import gllm_inference.schema.TokenUsage
import gllm_inference.schema.ToolCall
import gllm_inference.schema.ToolResult
import anthropic
import anthropic.types
import anthropic.types.message_create_params
import anthropic.types.messages
import anthropic.types.messages.batch_create_params
import gllm_inference.schema.StreamBuffer
import gllm_inference.schema.StreamBufferType
import jsonref
import gllm_inference.schema.AttachmentStore
import gllm_inference.schema.MessageContent
import gllm_inference.schema.MessageRole
import gllm_inference.schema.UploadedAttachment
import gllm_inference.utils.SizeUnit
import gllm_inference.utils.get_size
import langchain_core.language_models
import langchain_core.messages
import langchain_core.tools
import litellm
import functools
import time
import jsonschema
import gllm_core.constants
import gllm_core.event
import gllm_inference.lm_invoker.operations.BatchOperations
import gllm_inference.lm_invoker.operations.DataStoreOperations
import gllm_inference.lm_invoker.operations.FileOperations
import gllm_inference.schema.OperationType
import gllm_inference.lm_invoker.mixin.StreamingBufferMixin
import gllm_inference.exceptions.FileOperationError
import gllm_inference.schema.ActivityEvent
import gllm_inference.schema.MCPCall
import gllm_inference.schema.MCPCallActivity
import gllm_inference.schema.MCPListToolsActivity
import gllm_inference.schema.MCPServer
import gllm_inference.schema.WebSearchActivity
import gllm_inference.exceptions.InvokerRuntimeError
import logging
import portkey_ai
import gllm_inference.exceptions.build_debug_info
import xai_sdk
import xai_sdk.chat
import xai_sdk.search
import xai_sdk.proto
import xai_sdk.proto.v5
import xai_sdk.proto.v5.chat_pb2
import gllm_inference.schema.LMOutputItem
import jinja2
import jinja2.sandbox
import gllm_inference.schema.JinjaEnvType
import gllm_inference.prompt_builder.format_strategy.JinjaFormatStrategy
import gllm_inference.prompt_builder.format_strategy.StringFormatStrategy
import gllm_inference.schema.HistoryFormatter
import transformers
import gllm_inference.prompt_formatter.HuggingFacePromptFormatter
import gllm_inference.realtime_session.input_streamer.KeyboardInputStreamer
import gllm_inference.realtime_session.input_streamer.LinuxMicInputStreamer
import gllm_inference.realtime_session.output_streamer.ConsoleOutputStreamer
import gllm_inference.realtime_session.output_streamer.LinuxSpeakerOutputStreamer
import gllm_inference.realtime_session.schema.RealtimeActivityType
import gllm_inference.realtime_session.schema.RealtimeDataType
import gllm_inference.realtime_session.schema.RealtimeEvent
import gllm_inference.realtime_session.schema.RealtimeEventType
import gllm_inference.realtime_session.schema.RealtimeState
import prompt_toolkit
import gllm_inference.realtime_session.constants.InputAudioConfig
import gllm_inference.realtime_session.constants.NoiseThreshold
import gllm_inference.realtime_session.constants.OutputAudioConfig
import livekit
import soxr
import gllm_inference.schema.Activity
import mimetypes
import pathlib
import pathlib.Path
import filetype
import magic
import requests
import gllm_inference.utils.get_value_repr
import binascii
import fnmatch
import importlib
import gllm_core.schema.chunk
import pickle