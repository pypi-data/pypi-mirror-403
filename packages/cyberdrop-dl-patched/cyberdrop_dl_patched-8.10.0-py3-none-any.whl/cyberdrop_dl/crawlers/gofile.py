from __future__ import annotations

import asyncio
import itertools
import re
from hashlib import sha256
from typing import TYPE_CHECKING, ClassVar, Literal, NotRequired, TypedDict, TypeGuard

from cyberdrop_dl.crawlers.crawler import Crawler, RateLimit, SupportedPaths
from cyberdrop_dl.data_structures.url_objects import FILE_HOST_ALBUM, AbsoluteHttpURL, ScrapeItem
from cyberdrop_dl.exceptions import PasswordProtectedError, ScrapeError
from cyberdrop_dl.utils.utilities import error_handling_wrapper

if TYPE_CHECKING:
    from collections.abc import AsyncGenerator, Iterable

    from typing_extensions import ReadOnly


_API_ENTRYPOINT = AbsoluteHttpURL("https://api.gofile.io")
_GLOBAL_JS_URL = AbsoluteHttpURL("https://gofile.io/dist/js/config.js")
_PRIMARY_URL = AbsoluteHttpURL("https://gofile.io")
_PER_PAGE: int = 1000


class Response(TypedDict):
    status: str


class Node(TypedDict):
    canAccess: ReadOnly[bool]
    id: str
    type: ReadOnly[Literal["folder", "file"]]
    name: str
    createTime: int


class UnlockedNode(Node):
    canAccess: Literal[True]


class File(UnlockedNode):
    type: Literal["file"]
    link: str
    directLink: NotRequired[str]  # Only present in overloaded files (imported)
    isFrozen: NotRequired[bool]  # Only present in files uploaded by free accounts and older than 30 days
    viruses: NotRequired[bool]
    md5: str

    # parentFolder: str
    # size: int
    # downloadCount: int
    # thumbnail: str


class Folder(UnlockedNode):
    type: Literal["folder"]
    code: str
    childrenCount: int
    children: dict[str, Node]
    password: NotRequired[str]
    passwordStatus: NotRequired[str]

    # isRoot: NotRequired[bool]


class FolderMetadata(TypedDict):
    hasNextPage: bool

    # totalCount: int
    # totalPages: int
    # page: int
    # pageSize: int


class FolderResponse(Response):
    data: Folder
    metadata: FolderMetadata


class GoFileCrawler(Crawler):
    SUPPORTED_PATHS: ClassVar[SupportedPaths] = {
        "Folder / File": "/d/<content_id>",
        "Direct link": (
            "/download/<content_id>/<filename>",
            "/download/web/<content_id>/<filename>",
        ),
        "**NOTE**": (
            "Use `password` as a query param to download password protected folders",
            "ex: https://gofile.io/d/ABC654?password=1234",
        ),
    }
    PRIMARY_URL: ClassVar[AbsoluteHttpURL] = _PRIMARY_URL
    DOMAIN: ClassVar[str] = "gofile"
    FOLDER_DOMAIN: ClassVar[str] = "GoFile"
    _RATE_LIMIT: ClassVar[RateLimit] = 4, 10

    def __post_init__(self) -> None:
        self.headers: dict[str, str] = {}

    @classmethod
    def _json_response_check(cls, json_resp: Response) -> None:
        if not isinstance(json_resp, dict):
            return
        if "notFound" in json_resp["status"]:
            raise ScrapeError(404)

    async def async_startup(self) -> None:
        await self._get_credentials(_API_ENTRYPOINT)

    async def fetch(self, scrape_item: ScrapeItem) -> None:
        match scrape_item.url.parts[1:]:
            case ["d", content_id]:
                return await self.folder(scrape_item, content_id)
            case ["download", "web", file_id, _]:
                return await self.single_file(scrape_item, file_id)
            case ["download", file_id, _]:
                return await self.single_file(scrape_item, file_id)
            case _:
                raise ValueError

    @error_handling_wrapper
    async def single_file(self, scrape_item: ScrapeItem, file_id: str) -> None:
        url = await self._get_redirect_url(scrape_item.url)
        scrape_item.url = url.with_fragment(file_id)
        assert "d" in url.parts
        return await self.folder(scrape_item, url.name, file_id)

    @error_handling_wrapper
    async def folder(self, scrape_item: ScrapeItem, content_id: str, single_file_id: str | None = None) -> None:
        is_first_page: bool = True

        async for folder in self._folder_pager(content_id, scrape_item.password):
            if is_first_page:
                if _has_single_not_nested_file(scrape_item, folder):
                    # Consider this file a loose file (autogenerated folder name)
                    title = ""
                    part_of_album = False
                else:
                    title = self.create_title(folder["name"], content_id)
                    part_of_album = True

                scrape_item.setup_as_album(title, album_id=content_id)
                scrape_item.part_of_album = part_of_album
                scrape_item.url = scrape_item.url.with_query(None)
                is_first_page = False

            children = folder["children"]
            if single_file_id:
                file = children.get(single_file_id)
                if not file:
                    continue

                self._handle_children(scrape_item, [file])
                return

            self._handle_children(scrape_item, children.values())

    def _handle_children(self, scrape_item: ScrapeItem, children: Iterable[Node]) -> None:
        def get_website_url(node: Node) -> AbsoluteHttpURL:
            node_id = node["id"]
            if node["type"] == "folder":
                return _PRIMARY_URL / "d" / (node.get("code") or node_id)
            return scrape_item.url.with_fragment(node_id)

        for node in children:
            web_url = get_website_url(node)
            new_scrape_item = scrape_item.create_new(web_url, add_parent=True)
            self._handle_node(new_scrape_item, node)
            scrape_item.add_children()

    @error_handling_wrapper
    def _handle_node(self, scrape_item: ScrapeItem, node: Node) -> None:
        if not _check_node_is_accessible(node):
            return

        coro = self.run(scrape_item) if node["type"] == "folder" else self._file(scrape_item, node)
        self.create_task(coro)

    async def _folder_pager(self, content_id: str, password: str | None = None) -> AsyncGenerator[Folder]:
        api_url = (_API_ENTRYPOINT / "contents" / content_id).with_query(pageSize=_PER_PAGE)

        if password:
            sha256_password = sha256(password.encode()).hexdigest()
            api_url = api_url.update_query(password=sha256_password)

        for page in itertools.count(1):
            resp = await self.request_json(api_url.update_query(page=page), headers=self.headers)
            self._json_response_check(resp)
            folder = resp["data"]
            _check_node_is_accessible(folder)
            yield folder
            if not resp["metadata"]["hasNextPage"]:
                break

    @error_handling_wrapper
    async def _file(self, scrape_item: ScrapeItem, file: File) -> None:
        link_str: str = file["link"]
        if (not link_str or link_str == "overloaded") and "directLink" in file:
            link_str = file["directLink"]

        assert link_str
        link = self.parse_url(link_str)

        if await self.check_complete_by_hash(link, "md5", file["md5"]):
            return

        if file.get("isFrozen"):
            self.log(f"{link} is marked as frozen, download may fail", 30)

        name = file["name"]
        filename, ext = self.get_filename_and_ext(name, assume_ext=".mp4")
        scrape_item.possible_datetime = file["createTime"]
        await self.handle_file(link, scrape_item, name, ext, custom_filename=filename, metadata=file)

    @error_handling_wrapper
    async def _get_credentials(self, _) -> None:
        """Gets the token for the API."""
        with self.disable_on_error("Unable to get website token"):
            api_key, token = await asyncio.gather(self._get_api_key(), self._get_website_token())
            self.headers = {"Authorization": f"Bearer {api_key}", "X-Website-Token": token}
            self.update_cookies({"accountToken": api_key})

    async def _get_api_key(self) -> str:
        if key := self.manager.auth_config.gofile.api_key:
            return key

        api_url = _API_ENTRYPOINT / "accounts"
        json_resp = await self.request_json(api_url, method="POST", data={})
        if json_resp["status"] != "ok":
            raise ScrapeError(401, "Couldn't generate GoFile API token", origin=api_url)

        return json_resp["data"]["token"]

    async def _get_website_token(self) -> str:
        text = await self.request_text(_GLOBAL_JS_URL)
        if match := re.search(r'appdata\.wt\s=\s"([^"]+)"', text):
            return match.group(1)

        raise ScrapeError(401, "Couldn't generate GoFile websiteToken", origin=_GLOBAL_JS_URL)


def _check_node_is_accessible(node: Node) -> TypeGuard[File | Folder]:
    if (type_ := node["type"]) not in ("file", "folder"):
        raise ScrapeError(f"Unknown node type: {type_}")

    if node.get("viruses"):
        raise ScrapeError("Dangerous File")

    if node["canAccess"]:
        return True

    if node.get("password"):
        status = node.get("passwordStatus", "")
        error_msg = {
            "passwordRequired": "Folder is password protected",
            "passwordWrong": "Wrong folder password",
        }.get(status)
        raise PasswordProtectedError(error_msg)

    raise ScrapeError(403, "Folder is private")


def _has_single_not_nested_file(scrape_item: ScrapeItem, folder: Folder) -> bool:
    return folder["childrenCount"] == 1 and folder["name"] == folder["code"] and scrape_item.type != FILE_HOST_ALBUM
