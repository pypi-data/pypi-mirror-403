LICENSE
README.md
pyproject.toml
requirements.txt
src/__init__.py
src/arguments.py
src/logging_utils.py
src/loss.py
src/trainer.py
src/constant/__init__.py
src/constant/dataset_hf_path.py
src/constant/dataset_hflocal_path.py
src/data/__init__.py
src/data/collator/__init__.py
src/data/collator/eval_collator.py
src/data/collator/train_collator.py
src/data/dataset/__init__.py
src/data/dataset/base_pair_dataset.py
src/data/dataset/beir_dataset.py
src/data/dataset/berri.py
src/data/dataset/cls_dataset.py
src/data/dataset/didemo_dataset.py
src/data/dataset/docmatix_dataset.py
src/data/dataset/gui_dataset.py
src/data/dataset/hf_datasets.py
src/data/dataset/llavahound_dataset_caption.py
src/data/dataset/llavahound_dataset_qa.py
src/data/dataset/mmeb_dataset.py
src/data/dataset/msrvtt_dataset.py
src/data/dataset/msvd_dataset.py
src/data/dataset/mteb_cluster.py
src/data/dataset/mteb_training.py
src/data/dataset/pixmo.py
src/data/dataset/ssv2_dataset.py
src/data/dataset/text_dataset.py
src/data/dataset/videotext_dataset.py
src/data/dataset/vidore_dataset.py
src/data/dataset/visrag_dataset.py
src/data/eval_dataset/__init__.py
src/data/eval_dataset/activitynetqa_dataset.py
src/data/eval_dataset/base_eval_dataset.py
src/data/eval_dataset/didemo_dataset.py
src/data/eval_dataset/egoschema_dataset.py
src/data/eval_dataset/gui_dataset.py
src/data/eval_dataset/image_cls_dataset.py
src/data/eval_dataset/image_i2i_vg_dataset.py
src/data/eval_dataset/image_i2t_eval.py
src/data/eval_dataset/image_qa_dataset.py
src/data/eval_dataset/image_t2i_eval.py
src/data/eval_dataset/moment_retrieval_datasets.py
src/data/eval_dataset/momentseeker_dataset.py
src/data/eval_dataset/msrvtt_dataset.py
src/data/eval_dataset/msvd_dataset.py
src/data/eval_dataset/mvbench_dataset.py
src/data/eval_dataset/nextqa_dataset.py
src/data/eval_dataset/ssv2_dataset.py
src/data/eval_dataset/vatex_dataset.py
src/data/eval_dataset/video_classification_datasets.py
src/data/eval_dataset/video_classification_utils.py
src/data/eval_dataset/videomme_dataset.py
src/data/eval_dataset/videommmu_dataset.py
src/data/eval_dataset/vidore_dataset.py
src/data/eval_dataset/visrag_dataset.py
src/data/eval_dataset/youcook2_dataset.py
src/data/loader/__init__.py
src/data/loader/mixed_dataset.py
src/grad_cache/__init__.py
src/grad_cache/context_managers.py
src/grad_cache/functional.py
src/grad_cache/grad_cache.py
src/grad_cache/loss.py
src/grad_cache/cachex/__init__.py
src/grad_cache/cachex/functional.py
src/grad_cache/cachex/training.py
src/grad_cache/cachex/tree_utils.py
src/model/__init__.py
src/model/biencoder_gc.py
src/model/model.py
src/model/processor.py
src/model/utils.py
src/model/baseline_backbone/__init__.py
src/model/baseline_backbone/colpali/__init__.py
src/model/baseline_backbone/colpali/configuration_paligemma.py
src/model/baseline_backbone/colpali/modeling_colpali.py
src/model/baseline_backbone/colpali/modeling_paligemma.py
src/model/baseline_backbone/colpali/processing_colpali.py
src/model/baseline_backbone/colpali/processing_paligemma.py
src/model/baseline_backbone/colpali/processing_utils.py
src/model/baseline_backbone/colpali/torch_utils.py
src/model/baseline_backbone/gme/__init__.py
src/model/baseline_backbone/gme/gme_demo.py
src/model/baseline_backbone/gme/gme_inference.py
src/model/baseline_backbone/internvideo2/__init__.py
src/model/baseline_backbone/internvideo2/config.json
src/model/baseline_backbone/internvideo2/config_bert_large.json
src/model/baseline_backbone/internvideo2/modeling_internvideo2.py
src/model/baseline_backbone/lamra/__init__.py
src/model/baseline_backbone/lamra/lamra_demo.py
src/model/baseline_backbone/lamra/lamra_inference.py
src/model/baseline_backbone/lamra/lamra_qwen25_inference.py
src/model/baseline_backbone/llava_next/__init__.py
src/model/baseline_backbone/llava_next/modeling_llava_next.py
src/model/baseline_backbone/llava_next/processing_llava_next.py
src/model/baseline_backbone/phi3_v/__init__.py
src/model/baseline_backbone/phi3_v/configuration_phi3_v.py
src/model/baseline_backbone/phi3_v/image_embedding_phi3_v.py
src/model/baseline_backbone/phi3_v/image_processing_phi3_v.py
src/model/baseline_backbone/phi3_v/modeling_phi3_v.py
src/model/baseline_backbone/phi3_v/processing_phi3_v.py
src/model/vlm_backbone/__init__.py
src/model/vlm_backbone/qwen2_5_vl/__init__.py
src/model/vlm_backbone/qwen2_5_vl/configuration_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl/image_processing_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl/modeling_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl/modular_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl/processing_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/__init__.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/configuration_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/image_processing_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/modeling_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/modular_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_5_vl_tokenselection/processing_qwen2_5_vl.py
src/model/vlm_backbone/qwen2_vl/__init__.py
src/model/vlm_backbone/qwen2_vl/configuration_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl/demo_hfqwen.py
src/model/vlm_backbone/qwen2_vl/demo_localqwen.py
src/model/vlm_backbone/qwen2_vl/image_processing_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl/modeling_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl/processing_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl/qwen_vl_utils.py
src/model/vlm_backbone/qwen2_vl/tokenization_qwen2.py
src/model/vlm_backbone/qwen2_vl/tokenization_qwen2_fast.py
src/model/vlm_backbone/qwen2_vl_tokenselection/__init__.py
src/model/vlm_backbone/qwen2_vl_tokenselection/configuration_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl_tokenselection/demo_hfqwen.py
src/model/vlm_backbone/qwen2_vl_tokenselection/demo_localqwen.py
src/model/vlm_backbone/qwen2_vl_tokenselection/image_processing_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl_tokenselection/modeling_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl_tokenselection/processing_qwen2_vl.py
src/model/vlm_backbone/qwen2_vl_tokenselection/tokenization_qwen2.py
src/model/vlm_backbone/qwen2_vl_tokenselection/tokenization_qwen2_fast.py
src/prompt/__init__.py
src/prompt/base_prompt.py
src/prompt/e5mistral.py
src/prompt/e5mistral_multilingual.py
src/prompt/e5mistral_public.py
src/prompt/instructor.py
src/prompt/sfr.py
src/prompt/simple_prompts.py
src/prompt/tart.py
src/pyserini_integration/__init__.py
src/pyserini_integration/aggregate_results.py
src/pyserini_integration/download_visdoc.py
src/pyserini_integration/evaluate_results.py
src/pyserini_integration/mmeb_base_encoder.py
src/pyserini_integration/mmeb_corpus_encoder.py
src/pyserini_integration/mmeb_query_encoder.py
src/pyserini_integration/quick_start_demo.py
src/pyserini_integration/save_pyserini_data.py
src/pyserini_integration/visdoc.yaml
src/pyserini_integration/visdoc_dataset.py
src/utils/__init__.py
src/utils/basic_utils.py
src/utils/dataset_utils.py
src/utils/dist_utils.py
src/utils/eval_utils/__init__.py
src/utils/eval_utils/index.py
src/utils/eval_utils/metrics.py
src/utils/text_utils/__init__.py
src/utils/text_utils/basic_utils.py
src/utils/text_utils/dist_utils.py
src/utils/text_utils/ds_utils.py
src/utils/text_utils/infer_utils.py
src/utils/text_utils/logging.py
src/utils/text_utils/lr_utils.py
src/utils/text_utils/minhash_dedup.py
src/utils/text_utils/normalize_text.py
src/utils/text_utils/openaimodel.py
src/utils/vision_utils/__init__.py
src/utils/vision_utils/video_transforms.py
src/utils/vision_utils/video_transforms_videochat2.py
src/utils/vision_utils/vision_utils.py
vlm2vec_for_pyserini.egg-info/PKG-INFO
vlm2vec_for_pyserini.egg-info/SOURCES.txt
vlm2vec_for_pyserini.egg-info/dependency_links.txt
vlm2vec_for_pyserini.egg-info/requires.txt
vlm2vec_for_pyserini.egg-info/top_level.txt