accelerate
datasets
decord
einops
flash-attn  # check out orginal repo README for accelerating installation: https://github.com/Dao-AILab/flash-attention
hjson
hnswlib
huggingface-hub
ninja
numpy
opencv-contrib-python
opencv-python
peft
pillow
py-cpuinfo
pytrec-eval
qwen-vl-utils[decord]==0.0.8  # https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/discussions/30
ray
scikit-image
scikit-learn
scipy
sentencepiece
timm
torch==2.9.1 # pinning this makes flash-attn installation easier, compatible with flash-attn 2.8.3
torchvision
tqdm
transformers==4.51.3 # gme models only work with this version.
wandb
wrapt
