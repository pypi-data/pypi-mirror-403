---
icon: lucide/download
---

# Installation Guide

Choose the best installation method for your platform and performance needs.

## Quick Platform Guide

| Platform         | Recommended Method             | GPU Support   | Performance |
| ---------------- | ------------------------------ | ------------- | ----------- |
| **macOS**        | [Native Setup](macos.md)       | âœ… Metal GPU  | Best        |
| **Linux**        | [Native Setup](linux.md)       | âœ… NVIDIA GPU | Best        |
| **NixOS**        | [System Integration](nixos.md) | âœ… NVIDIA GPU | Best        |
| **Any Platform** | [Docker Setup](docker.md)      | âš ï¸ Limited\*  | Good        |

> [!WARNING]
> Docker on macOS does not support GPU acceleration. For best performance on Mac, use the [native setup](macos.md).

## Installation Methods

### ðŸŽ macOS Native (Recommended)

**Best performance with Metal GPU acceleration**

- Full GPU acceleration for Ollama
- Optimized for Apple Silicon
- Native macOS integrations

ðŸ‘‰ [Follow macOS Setup Guide](macos.md)

### ðŸ§ Linux Native (Recommended)

**Best performance with NVIDIA GPU acceleration**

- NVIDIA GPU support
- Full system integration
- Optimal resource usage

ðŸ‘‰ [Follow Linux Setup Guide](linux.md)

### â„ï¸ NixOS System Integration

**Declarative system configuration with GPU support**

- System-level service integration
- Declarative configuration
- Automatic service management

ðŸ‘‰ [Follow NixOS Setup Guide](nixos.md)

### ðŸ³ Docker (Cross-platform)

**Universal solution, some limitations**

- Works on any platform
- Consistent environment
- âš ï¸ No GPU acceleration on macOS
- âš ï¸ Limited GPU support on other platforms

ðŸ‘‰ [Follow Docker Setup Guide](docker.md)

## What Gets Installed

All installation methods set up these services:

- **ðŸ§  Ollama** - LLM server (gemma3:4b model)
- **ðŸŽ¤ Wyoming Whisper** - Speech-to-text (faster-whisper on Linux/Intel, MLX Whisper on Apple Silicon)
- **ðŸ—£ï¸ Wyoming Piper** - Text-to-speech
- **ðŸ‘‚ Wyoming OpenWakeWord** - Wake word detection

## Service Ports

All methods use the same ports:

- Ollama (LLM): `11434`
- Whisper (ASR): `10300`
- Piper (TTS): `10200`
- OpenWakeWord: `10400`

## After Installation

Once services are running, install the agent-cli package:

```bash
# Using uv (recommended)
uv tool install agent-cli -p 3.13

# Using pip
pip install agent-cli
```

> [!NOTE]
> The `-p 3.13` flag is required because some dependencies don't support Python 3.14 yet.
> See [uv issue #8206](https://github.com/astral-sh/uv/issues/8206) for details.

Then test with:

```bash
agent-cli autocorrect --help
```

## Need Help?

- Check the troubleshooting section in your chosen installation guide
- Open an issue on [GitHub](https://github.com/basnijholt/agent-cli/issues)
