Metadata-Version: 2.4
Name: llmteam-ai
Version: 5.3.0
Summary: Enterprise AI Workflow Runtime - Multi-agent LLM pipelines with security, orchestration, and workflow capabilities
Project-URL: Homepage, https://github.com/KirilinVS/llmteam
Project-URL: Documentation, https://github.com/KirilinVS/llmteam#readme
Project-URL: Repository, https://github.com/KirilinVS/llmteam
Project-URL: Changelog, https://github.com/KirilinVS/llmteam/blob/main/CHANGELOG.md
Project-URL: Issues, https://github.com/KirilinVS/llmteam/issues
Author-email: KirilinVS <LLMTeamai@gmail.com>
License-Expression: Apache-2.0
License-File: LICENSE
License-File: NOTICE
Keywords: agents,ai,enterprise,llm,multi-agent,pipeline,workflow
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Typing :: Typed
Requires-Python: >=3.10
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: click>=8.1.0
Provides-Extra: all
Requires-Dist: aiokafka>=0.10.0; extra == 'all'
Requires-Dist: anthropic>=0.8.0; extra == 'all'
Requires-Dist: asyncpg>=0.28.0; extra == 'all'
Requires-Dist: azure-identity>=1.15.0; extra == 'all'
Requires-Dist: azure-keyvault-secrets>=4.7.0; extra == 'all'
Requires-Dist: boto3>=1.28.0; extra == 'all'
Requires-Dist: cryptography>=41.0.0; extra == 'all'
Requires-Dist: fastapi>=0.109.0; extra == 'all'
Requires-Dist: google-cloud-aiplatform>=1.40.0; extra == 'all'
Requires-Dist: gql[aiohttp]>=3.5.0; extra == 'all'
Requires-Dist: grpcio>=1.60.0; extra == 'all'
Requires-Dist: hvac>=2.1.0; extra == 'all'
Requires-Dist: jsonpath-ng>=1.6.0; extra == 'all'
Requires-Dist: jsonschema>=4.20.0; extra == 'all'
Requires-Dist: litellm>=1.20.0; extra == 'all'
Requires-Dist: openai>=1.0.0; extra == 'all'
Requires-Dist: opentelemetry-api>=1.20.0; extra == 'all'
Requires-Dist: opentelemetry-exporter-otlp>=1.20.0; extra == 'all'
Requires-Dist: opentelemetry-sdk>=1.20.0; extra == 'all'
Requires-Dist: pydantic>=2.5.0; extra == 'all'
Requires-Dist: pyjwt>=2.8.0; extra == 'all'
Requires-Dist: redis>=5.0.0; extra == 'all'
Requires-Dist: slowapi>=0.1.9; extra == 'all'
Requires-Dist: structlog>=24.1.0; extra == 'all'
Requires-Dist: uvicorn[standard]>=0.27.0; extra == 'all'
Requires-Dist: websockets>=12.0; extra == 'all'
Provides-Extra: api
Requires-Dist: fastapi>=0.109.0; extra == 'api'
Requires-Dist: pydantic>=2.5.0; extra == 'api'
Requires-Dist: slowapi>=0.1.9; extra == 'api'
Requires-Dist: uvicorn[standard]>=0.27.0; extra == 'api'
Provides-Extra: auth
Requires-Dist: cryptography>=41.0.0; extra == 'auth'
Requires-Dist: pyjwt>=2.8.0; extra == 'auth'
Provides-Extra: aws
Requires-Dist: boto3>=1.28.0; extra == 'aws'
Provides-Extra: azure-secrets
Requires-Dist: azure-identity>=1.15.0; extra == 'azure-secrets'
Requires-Dist: azure-keyvault-secrets>=4.7.0; extra == 'azure-secrets'
Provides-Extra: dev
Requires-Dist: black>=23.9.0; extra == 'dev'
Requires-Dist: mkdocs-material>=9.5.0; extra == 'dev'
Requires-Dist: mypy>=1.5.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.1.0; extra == 'dev'
Requires-Dist: pytest-timeout>=2.2.0; extra == 'dev'
Requires-Dist: pytest-xdist>=3.5.0; extra == 'dev'
Requires-Dist: pytest>=7.4.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: engine
Provides-Extra: graphql
Requires-Dist: gql[aiohttp]>=3.5.0; extra == 'graphql'
Provides-Extra: grpc
Requires-Dist: grpcio-tools>=1.60.0; extra == 'grpc'
Requires-Dist: grpcio>=1.60.0; extra == 'grpc'
Provides-Extra: jsonpath
Requires-Dist: jsonpath-ng>=1.6.0; extra == 'jsonpath'
Provides-Extra: kafka
Requires-Dist: aiokafka>=0.10.0; extra == 'kafka'
Provides-Extra: litellm
Requires-Dist: litellm>=1.20.0; extra == 'litellm'
Provides-Extra: logging
Requires-Dist: structlog>=24.1.0; extra == 'logging'
Provides-Extra: postgres
Requires-Dist: asyncpg>=0.28.0; extra == 'postgres'
Provides-Extra: providers
Requires-Dist: anthropic>=0.8.0; extra == 'providers'
Requires-Dist: openai>=1.0.0; extra == 'providers'
Provides-Extra: redis
Requires-Dist: redis>=5.0.0; extra == 'redis'
Provides-Extra: tracing
Requires-Dist: opentelemetry-api>=1.20.0; extra == 'tracing'
Requires-Dist: opentelemetry-exporter-otlp>=1.20.0; extra == 'tracing'
Requires-Dist: opentelemetry-sdk>=1.20.0; extra == 'tracing'
Provides-Extra: validation
Requires-Dist: jsonschema>=4.20.0; extra == 'validation'
Provides-Extra: vault
Requires-Dist: hvac>=2.1.0; extra == 'vault'
Provides-Extra: vertex
Requires-Dist: google-cloud-aiplatform>=1.40.0; extra == 'vertex'
Provides-Extra: websockets
Requires-Dist: websockets>=12.0; extra == 'websockets'
Description-Content-Type: text/markdown

# llmteam-ai

Enterprise AI Workflow Runtime for building multi-agent LLM pipelines.

[![PyPI version](https://badge.fury.io/py/llmteam-ai.svg)](https://pypi.org/project/llmteam-ai/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

## Current Version: v4.0.0 — Typed Agent Architecture

### Key Features

- **Three Agent Types** — LLM, RAG, KAG (config-driven, no custom agent classes)
- **Simple API** — Create agents via dict, no boilerplate
- **SegmentRunner Integration** — LLMTeam uses Canvas runtime internally
- **LLMGroup** — Multi-team coordination with automatic routing
- **Presets** — Ready-to-use orchestrator, summarizer, reviewer configs

## Installation

```bash
pip install llmteam-ai

# With optional dependencies
pip install llmteam-ai[api]       # FastAPI server
pip install llmteam-ai[postgres]  # PostgreSQL stores
pip install llmteam-ai[all]       # Everything
```

## Quick Start

### Create a Team with Agents

```python
from llmteam import LLMTeam

# Simple: dict-based config
team = LLMTeam(
    team_id="content",
    agents=[
        {"type": "rag", "role": "retriever", "collection": "docs", "top_k": 5},
        {"type": "llm", "role": "writer", "prompt": "Based on context, write about: {query}"},
    ]
)

# Run
result = await team.run({"query": "AI trends in 2026"})
print(result.output)
```

### Add Agents Dynamically

```python
team = LLMTeam(team_id="support")

# Method 1: Dict
team.add_agent({
    "type": "llm",
    "role": "triage",
    "prompt": "Classify this query: {query}",
    "model": "gpt-4o-mini",
})

# Method 2: Shortcut
team.add_llm_agent(
    role="resolver",
    prompt="Resolve the issue: {issue}",
    temperature=0.3,
)

# Method 3: RAG/KAG
team.add_rag_agent(role="knowledge", collection="faq", top_k=3)
team.add_kag_agent(role="graph", max_hops=2)
```

### Use Presets

```python
from llmteam.agents import create_orchestrator_config, create_summarizer_config

# Orchestrator for adaptive flow
team.add_agent(create_orchestrator_config(
    available_agents=["writer", "editor", "reviewer"],
    model="gpt-4o-mini",
))

# Summarizer preset
team.add_agent(create_summarizer_config(role="summarizer"))
```

### Multi-Team Groups

```python
from llmteam import LLMTeam

research_team = LLMTeam(team_id="research", agents=[...])
writing_team = LLMTeam(team_id="writing", agents=[...])

# Create group with leader
group = research_team.create_group(
    group_id="content_pipeline",
    teams=[writing_team],
)

result = await group.run({"topic": "Quantum Computing"})
```

### Execution Control

```python
# Start
result = await team.run({"query": "..."})

# Pause and resume
snapshot = await team.pause()
# ... later ...
result = await team.resume(snapshot)

# Cancel
await team.cancel()
```

## Agent Types

| Type | Purpose | Key Config |
|------|---------|------------|
| `llm` | Text generation | `prompt`, `model`, `temperature`, `max_tokens` |
| `rag` | Vector retrieval | `collection`, `top_k`, `score_threshold` |
| `kag` | Knowledge graph | `max_hops`, `max_entities` |

### LLM Agent Config

```python
{
    "type": "llm",
    "role": "writer",              # Required: unique ID
    "prompt": "Write: {topic}",    # Required: prompt template
    "model": "gpt-4o-mini",        # Default: gpt-4o-mini
    "temperature": 0.7,            # Default: 0.7
    "max_tokens": 1000,            # Default: 1000
    "system_prompt": "You are...", # Optional
    "use_context": True,           # Use RAG/KAG context
    "output_format": "text",       # "text" | "json"
}
```

### RAG Agent Config

```python
{
    "type": "rag",
    "role": "retriever",
    "collection": "documents",     # Vector store collection
    "top_k": 5,                    # Number of results
    "score_threshold": 0.7,        # Minimum similarity
    "mode": "native",              # "native" | "proxy"
}
```

### KAG Agent Config

```python
{
    "type": "kag",
    "role": "knowledge",
    "max_hops": 2,                 # Graph traversal depth
    "max_entities": 10,            # Max entities to return
    "include_relations": True,     # Include relationships
}
```

## Flow Definition

```python
# Sequential (default)
team = LLMTeam(team_id="seq", flow="sequential")

# String syntax
team = LLMTeam(team_id="pipe", flow="retriever -> writer -> editor")

# Parallel
team = LLMTeam(team_id="par", flow="a, b -> c")  # a and b run parallel, then c

# DAG with conditions
team = LLMTeam(team_id="dag", flow={
    "edges": [
        {"from": "retriever", "to": "writer"},
        {"from": "writer", "to": "reviewer"},
        {"from": "reviewer", "to": "writer", "condition": "rejected"},
        {"from": "reviewer", "to": "publisher", "condition": "approved"},
    ]
})

# Adaptive (with orchestrator)
team = LLMTeam(team_id="adaptive", orchestration=True)
```

## Context Modes

```python
from llmteam import LLMTeam, ContextMode

# Shared context (default) - all agents see all results
team = LLMTeam(team_id="shared", context_mode=ContextMode.SHARED)

# Not shared - each agent gets only explicitly delivered context
team = LLMTeam(team_id="isolated", context_mode=ContextMode.NOT_SHARED)
```

## License Tiers

| Feature | Community | Professional | Enterprise |
|---------|-----------|--------------|------------|
| LLM/RAG/KAG agents | ✅ | ✅ | ✅ |
| Memory stores | ✅ | ✅ | ✅ |
| Canvas runner | ✅ | ✅ | ✅ |
| Process mining | ❌ | ✅ | ✅ |
| PostgreSQL stores | ❌ | ✅ | ✅ |
| Human-in-the-loop | ❌ | ✅ | ✅ |
| Multi-tenant | ❌ | ❌ | ✅ |
| Audit trail | ❌ | ❌ | ✅ |
| SSO/SAML | ❌ | ❌ | ✅ |

## Migration from v3.x

v4.0.0 is a **breaking change**. Key differences:

| v3.x | v4.x |
|------|------|
| `class Agent` with `process()` | Dict config |
| `team.register_agent(agent)` | `team.add_agent(config)` |
| `TeamOrchestrator` class | `flow` parameter or `orchestration=True` |
| Custom agent classes | External logic pattern |
| `result.agents_invoked` | `result.agents_called` |

### Migration Example

```python
# ═══════════════════════════════════════════
# v3.x (old) - Custom agent class
# ═══════════════════════════════════════════
from llmteam import Agent, AgentState, AgentResult

class WriterAgent(Agent):
    async def process(self, state: AgentState) -> AgentResult:
        query = state.data.get("query", "")
        # Custom logic here
        return AgentResult(output={"text": f"Article about {query}"})

team = LLMTeam(team_id="content")
team.register_agent(WriterAgent("writer"))

# ═══════════════════════════════════════════
# v4.x (new) - Dict config
# ═══════════════════════════════════════════
from llmteam import LLMTeam

team = LLMTeam(
    team_id="content",
    agents=[
        {"type": "llm", "role": "writer", "prompt": "Write article about: {query}"}
    ]
)

# For custom logic, use external pattern:
result = await team.run({"query": "AI"})
processed = my_custom_function(result.output)
```

## API Reference

### LLMTeam

```python
class LLMTeam:
    def __init__(
        self,
        team_id: str,
        agents: List[Dict] = None,
        flow: Union[str, Dict] = "sequential",
        model: str = "gpt-4o-mini",
        context_mode: ContextMode = ContextMode.SHARED,
        orchestration: bool = False,
        timeout: int = None,
    ): ...

    def add_agent(self, config: Dict) -> BaseAgent: ...
    def add_llm_agent(self, role: str, prompt: str, **kwargs) -> BaseAgent: ...
    def add_rag_agent(self, role: str = "rag", **kwargs) -> BaseAgent: ...
    def add_kag_agent(self, role: str = "kag", **kwargs) -> BaseAgent: ...
    def get_agent(self, agent_id: str) -> Optional[BaseAgent]: ...
    def list_agents(self) -> List[BaseAgent]: ...

    async def run(self, input_data: Dict, run_id: str = None) -> RunResult: ...
    async def pause(self) -> TeamSnapshot: ...
    async def resume(self, snapshot: TeamSnapshot) -> RunResult: ...
    async def cancel(self) -> bool: ...

    def create_group(self, group_id: str, teams: List[LLMTeam]) -> LLMGroup: ...
    def to_config(self) -> Dict: ...
    @classmethod
    def from_config(cls, config: Dict) -> LLMTeam: ...
```

### RunResult

```python
@dataclass
class RunResult:
    success: bool
    status: RunStatus  # COMPLETED, FAILED, PAUSED, CANCELLED, TIMEOUT
    output: Dict[str, Any]
    final_output: Any
    agents_called: List[str]
    iterations: int
    duration_ms: int
    error: Optional[str]
    started_at: datetime
    completed_at: datetime
```

## Documentation

- [Full Documentation](https://docs.llmteam.ai)
- [API Reference](https://docs.llmteam.ai/api)
- [Examples](https://github.com/llmteamai/llmteam/tree/main/examples)
- [Changelog](https://github.com/llmteamai/llmteam/blob/main/CHANGELOG.md)

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

Apache 2.0 — see [LICENSE](LICENSE) for details.
