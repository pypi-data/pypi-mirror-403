[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "llmteam-ai"
version = "5.3.0"
description = "Enterprise AI Workflow Runtime - Multi-agent LLM pipelines with security, orchestration, and workflow capabilities"
readme = "README.md"
license = "Apache-2.0"
requires-python = ">=3.10"
authors = [
    { name = "KirilinVS", email = "LLMTeamai@gmail.com" }
]
keywords = [
    "llm",
    "ai",
    "agents",
    "pipeline",
    "workflow",
    "multi-agent",
    "enterprise",
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Typing :: Typed",
]

dependencies = [
    "aiohttp>=3.9.0",
    "click>=8.1.0",
]

[project.scripts]
llmteam = "llmteam.cli.main:main"

[project.optional-dependencies]
postgres = [
    "asyncpg>=0.28.0",
]
redis = [
    "redis>=5.0.0",
]
api = [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.5.0",
    "slowapi>=0.1.9",
]
validation = [
    "jsonschema>=4.20.0",
]
engine = [
    # Engine module is pure Python with no additional dependencies
    # This extra exists to support conditional installation: pip install llmteam-ai[engine]
]
logging = [
    "structlog>=24.1.0",
]
providers = [
    "openai>=1.0.0",
    "anthropic>=0.8.0",
]
aws = [
    "boto3>=1.28.0",
]
websockets = [
    "websockets>=12.0",
]
tracing = [
    "opentelemetry-api>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
    "opentelemetry-exporter-otlp>=1.20.0",
]
auth = [
    "PyJWT>=2.8.0",
    "cryptography>=41.0.0",
]
vault = [
    "hvac>=2.1.0",
]
azure-secrets = [
    "azure-keyvault-secrets>=4.7.0",
    "azure-identity>=1.15.0",
]
vertex = [
    "google-cloud-aiplatform>=1.40.0",
]
litellm = [
    "litellm>=1.20.0",
]
graphql = [
    "gql[aiohttp]>=3.5.0",
]
grpc = [
    "grpcio>=1.60.0",
    "grpcio-tools>=1.60.0",
]
jsonpath = [
    "jsonpath-ng>=1.6.0",
]
kafka = [
    "aiokafka>=0.10.0",
]
all = [
    "asyncpg>=0.28.0",
    "redis>=5.0.0",
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "pydantic>=2.5.0",
    "jsonschema>=4.20.0",
    "structlog>=24.1.0",
    "openai>=1.0.0",
    "anthropic>=0.8.0",
    "boto3>=1.28.0",
    "websockets>=12.0",
    "opentelemetry-api>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
    "opentelemetry-exporter-otlp>=1.20.0",
    "PyJWT>=2.8.0",
    "cryptography>=41.0.0",
    "hvac>=2.1.0",
    "azure-keyvault-secrets>=4.7.0",
    "azure-identity>=1.15.0",
    "google-cloud-aiplatform>=1.40.0",
    "litellm>=1.20.0",
    "gql[aiohttp]>=3.5.0",
    "grpcio>=1.60.0",
    "jsonpath-ng>=1.6.0",
    "slowapi>=0.1.9",
    "aiokafka>=0.10.0",
]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "pytest-timeout>=2.2.0",
    "pytest-xdist>=3.5.0",
    "mypy>=1.5.0",
    "ruff>=0.1.0",
    "black>=23.9.0",
    "mkdocs-material>=9.5.0",
]

[project.urls]
Homepage = "https://github.com/KirilinVS/llmteam"
Documentation = "https://github.com/KirilinVS/llmteam#readme"
Repository = "https://github.com/KirilinVS/llmteam"
Changelog = "https://github.com/KirilinVS/llmteam/blob/main/CHANGELOG.md"
Issues = "https://github.com/KirilinVS/llmteam/issues"

[tool.hatch.build.targets.wheel]
packages = ["src/llmteam"]

[tool.hatch.build.targets.sdist]
include = [
    "/src",
    "/tests",
    "/README.md",
    "/LICENSE",
]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
strict_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
show_error_codes = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false

[tool.ruff]
target-version = "py310"
line-length = 100
select = [
    "E",
    "W",
    "F",
    "I",
    "B",
    "C4",
    "UP",
]
ignore = [
    "E501",
    "B008",
]

[tool.ruff.isort]
known-first-party = ["llmteam"]

[tool.black]
target-version = ["py310"]
line-length = 100

[tool.coverage.run]
source = ["src/llmteam"]
branch = true

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if TYPE_CHECKING:",
    "if __name__ == .__main__.:",
]
