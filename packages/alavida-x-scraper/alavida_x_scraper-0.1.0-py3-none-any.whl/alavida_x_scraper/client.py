# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations

import typing

import httpx
from .core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from .environment import XScraperClientEnvironment

if typing.TYPE_CHECKING:
    from .discovery.client import AsyncDiscoveryClient, DiscoveryClient
    from .scraping.client import AsyncScrapingClient, ScrapingClient


class XScraperClient:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : XScraperClientEnvironment
        The environment to use for requests from the client. from .environment import XScraperClientEnvironment



        Defaults to XScraperClientEnvironment.PRODUCTION



    token : typing.Union[str, typing.Callable[[], str]]
    headers : typing.Optional[typing.Dict[str, str]]
        Additional headers to send with every request.

    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.Client]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from alavida_x_scraper import XScraperClient

    client = XScraperClient(
        token="YOUR_TOKEN",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: XScraperClientEnvironment = XScraperClientEnvironment.PRODUCTION,
        token: typing.Union[str, typing.Callable[[], str]],
        headers: typing.Optional[typing.Dict[str, str]] = None,
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.Client] = None,
    ):
        _defaulted_timeout = (
            timeout if timeout is not None else 60 if httpx_client is None else httpx_client.timeout.read
        )
        self._client_wrapper = SyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            token=token,
            headers=headers,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.Client(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self._scraping: typing.Optional[ScrapingClient] = None
        self._discovery: typing.Optional[DiscoveryClient] = None

    @property
    def scraping(self):
        if self._scraping is None:
            from .scraping.client import ScrapingClient  # noqa: E402

            self._scraping = ScrapingClient(client_wrapper=self._client_wrapper)
        return self._scraping

    @property
    def discovery(self):
        if self._discovery is None:
            from .discovery.client import DiscoveryClient  # noqa: E402

            self._discovery = DiscoveryClient(client_wrapper=self._client_wrapper)
        return self._discovery


class AsyncXScraperClient:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : XScraperClientEnvironment
        The environment to use for requests from the client. from .environment import XScraperClientEnvironment



        Defaults to XScraperClientEnvironment.PRODUCTION



    token : typing.Union[str, typing.Callable[[], str]]
    headers : typing.Optional[typing.Dict[str, str]]
        Additional headers to send with every request.

    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.AsyncClient]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from alavida_x_scraper import AsyncXScraperClient

    client = AsyncXScraperClient(
        token="YOUR_TOKEN",
    )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: XScraperClientEnvironment = XScraperClientEnvironment.PRODUCTION,
        token: typing.Union[str, typing.Callable[[], str]],
        headers: typing.Optional[typing.Dict[str, str]] = None,
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.AsyncClient] = None,
    ):
        _defaulted_timeout = (
            timeout if timeout is not None else 60 if httpx_client is None else httpx_client.timeout.read
        )
        self._client_wrapper = AsyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            token=token,
            headers=headers,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self._scraping: typing.Optional[AsyncScrapingClient] = None
        self._discovery: typing.Optional[AsyncDiscoveryClient] = None

    @property
    def scraping(self):
        if self._scraping is None:
            from .scraping.client import AsyncScrapingClient  # noqa: E402

            self._scraping = AsyncScrapingClient(client_wrapper=self._client_wrapper)
        return self._scraping

    @property
    def discovery(self):
        if self._discovery is None:
            from .discovery.client import AsyncDiscoveryClient  # noqa: E402

            self._discovery = AsyncDiscoveryClient(client_wrapper=self._client_wrapper)
        return self._discovery


def _get_base_url(*, base_url: typing.Optional[str] = None, environment: XScraperClientEnvironment) -> str:
    if base_url is not None:
        return base_url
    elif environment is not None:
        return environment.value
    else:
        raise Exception("Please pass in either base_url or environment to construct the client")
