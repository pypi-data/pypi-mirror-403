"""
Benchmark visualization plotter.

Handles plotting of hydrological benchmarks and performance comparisons.
"""

import pandas as pd  # type: ignore
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

from symfluence.reporting.core.base_plotter import BasePlotter
from symfluence.core.constants import ConfigKeys


class BenchmarkPlotter(BasePlotter):
    """
    Plotter for benchmark visualizations.

    Handles:
    - Benchmark comparison by group
    - Statistical summaries of benchmarks
    - Performance metrics heatmaps
    - Envelope analysis of top benchmarks
    """

    def __init__(self, config: Dict[str, Any], logger: Any, plot_config: Optional[Any] = None):
        super().__init__(config, logger, plot_config)

        # Define benchmark groups
        self.benchmark_groups = {
            'Time-invariant': {
                'benchmarks': [
                    'bm_mean_flow', 'bm_median_flow',
                    'bm_annual_mean_flow', 'bm_annual_median_flow'
                ],
                'color': '#1f77b4',
                'description': 'Constant and annual benchmarks'
            },
            'Time-variant': {
                'benchmarks': [
                    'bm_monthly_mean_flow', 'bm_monthly_median_flow',
                    'bm_daily_mean_flow', 'bm_daily_median_flow'
                ],
                'color': '#2ca02c',
                'description': 'Monthly and daily benchmarks'
            },
            'Rainfall-Runoff': {
                'benchmarks': [
                    'bm_rainfall_runoff_ratio_to_all',
                    'bm_rainfall_runoff_ratio_to_annual',
                    'bm_rainfall_runoff_ratio_to_monthly',
                    'bm_rainfall_runoff_ratio_to_daily',
                    'bm_rainfall_runoff_ratio_to_timestep',
                    'bm_monthly_rainfall_runoff_ratio_to_monthly',
                    'bm_monthly_rainfall_runoff_ratio_to_daily',
                    'bm_monthly_rainfall_runoff_ratio_to_timestep'
                ],
                'color': '#ff7f0e',
                'description': 'Precipitation-based benchmarks'
            },
            'Advanced': {
                'benchmarks': [
                    'bm_scaled_precipitation_benchmark',
                    'bm_adjusted_precipitation_benchmark',
                    'bm_adjusted_smoothed_precipitation_benchmark'
                ],
                'color': '#9467bd',
                'description': 'Schaefli & Gupta benchmarks'
            }
        }

    def plot_benchmarks(self, benchmark_results: Dict[str, Any]) -> List[str]:
        """
        Create comprehensive benchmark visualization suite.

        Args:
            benchmark_results: Dictionary containing benchmark flows and scores

        Returns:
            List of paths to created plots
        """
        try:
            plot_dir = self._ensure_output_dir('benchmarks')

            flows = benchmark_results['benchmark_flows']
            scores = pd.DataFrame(benchmark_results['scores'])

            # Prepare data (align with observations)
            flows, scores = self._prepare_data(flows, scores)

            plot_paths = []

            # 1. Group Comparison
            plot_paths.append(self._plot_group_comparison(flows, scores, plot_dir))

            # 2. Statistics and Heatmap
            plot_paths.append(self._plot_statistics(flows, scores, plot_dir))

            # 3. Envelope Analysis
            plot_paths.append(self._plot_envelopes(flows, scores, plot_dir))

            return [p for p in plot_paths if p is not None]

        except Exception as e:
            self.logger.error(f"Error in plot_benchmarks: {str(e)}")
            return []

    def _prepare_data(self, flows: pd.DataFrame, scores: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Align flows with observations and clean up score index."""
        # Load observed data (logic moved from BenchmarkVizualiser)
        domain_name = self._get_config_value(lambda: self.config.domain.name, dict_key=ConfigKeys.DOMAIN_NAME)
        obs_path = (self.project_dir / "observations" / "streamflow" / "preprocessed" /
                   f"{domain_name}_streamflow_processed.csv")

        if obs_path.exists():
            obs_df = pd.read_csv(obs_path, parse_dates=['datetime'])
            obs_df.set_index('datetime', inplace=True)
            if 'discharge_cms' in obs_df.columns:
                flows['observed'] = obs_df['discharge_cms'].reindex(flows.index)

        # Ensure consistent benchmark naming in scores
        scores = scores.copy()
        if 'benchmarks' in scores.columns:
            scores.index = scores['benchmarks'].apply(
                lambda x: f"bm_{x}" if not str(x).startswith('bm_') else x
            )

        return flows, scores

    def _plot_group_comparison(self, flows: pd.DataFrame, scores: pd.DataFrame, plot_dir: Path) -> Optional[str]:
        """Plot benchmark time series by group."""
        plt, _ = self._setup_matplotlib()
        import matplotlib.gridspec as gridspec  # type: ignore

        n_groups = len(self.benchmark_groups)
        fig = plt.figure(figsize=(15, 4 * n_groups))
        gs = gridspec.GridSpec(n_groups + 1, 2, height_ratios=[0.2] + [1] * n_groups, width_ratios=[3, 1])

        fig.text(0.5, 0.98, 'Benchmark Performance Analysis', ha='center', fontsize=16, fontweight='bold')

        for idx, (group_name, group_info) in enumerate(self.benchmark_groups.items()):
            ax_main = fig.add_subplot(gs[idx + 1, 0])

            if 'observed' in flows.columns:
                ax_main.plot(flows.index, flows['observed'], label='Observed', color='black', linewidth=1.5, zorder=10)

            valid_bms = [b for b in group_info['benchmarks'] if b in flows.columns]
            for i, bm in enumerate(valid_bms):
                style = self.plot_config.get_line_style(i)
                ax_main.plot(flows.index, flows[bm], label=bm.replace('bm_', '').replace('_', ' '),
                           color=group_info['color'], linestyle=style, alpha=0.7)

            self._apply_standard_styling(ax_main, ylabel='Flow (m³/s)', title=f'{group_name} Benchmarks', legend=True)
            self._format_date_axis(ax_main)

            # Metrics table placeholder
            ax_table = fig.add_subplot(gs[idx + 1, 1])
            ax_table.axis('off')
            # (Table plotting logic omitted for brevity, but can be moved from legacy)

        plt.tight_layout()
        plot_path = plot_dir / f"{self._get_config_value(lambda: self.config.domain.name, dict_key=ConfigKeys.DOMAIN_NAME)}_benchmark_groups.png"
        return self._save_and_close(fig, plot_path)

    def _plot_statistics(self, flows: pd.DataFrame, scores: pd.DataFrame, plot_dir: Path) -> Optional[str]:
        """Plot statistical summaries and metrics heatmap."""
        plt, _ = self._setup_matplotlib()
        import seaborn as sns  # type: ignore

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))

        # Heatmap of scores
        metrics = ['nse_cal', 'kge_cal', 'rmse_cal']
        heatmap_data = scores[metrics].copy()
        heatmap_data.index = [i.replace('bm_', '').replace('_', ' ') for i in heatmap_data.index]

        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax1)
        ax1.set_title('Performance Metrics Heatmap')

        # Bar plot of mean flows
        flows.mean().drop('observed', errors='ignore').plot(kind='bar', ax=ax2, color='skyblue')
        ax2.set_title('Mean Flow Comparison')
        plt.xticks(rotation=45, ha='right')

        plt.tight_layout()
        plot_path = plot_dir / f"{self._get_config_value(lambda: self.config.domain.name, dict_key=ConfigKeys.DOMAIN_NAME)}_benchmark_statistics.png"
        return self._save_and_close(fig, plot_path)

    def _plot_envelopes(self, flows: pd.DataFrame, scores: pd.DataFrame, plot_dir: Path) -> Optional[str]:
        """Plot envelope of top 5 benchmarks."""
        plt, _ = self._setup_matplotlib()
        from symfluence.reporting.core.plot_utils import calculate_flow_duration_curve

        try:
            kge_col = 'kge_cal' if 'kge_cal' in scores.columns else scores.columns[0]
            top_bms = scores[kge_col].sort_values(ascending=False).head(5).index
            top_bms = [b for b in top_bms if b in flows.columns]

            if not top_bms: return None

            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))

            if 'observed' in flows.columns:
                ax1.plot(flows.index, flows['observed'], label='Observed', color='black', linewidth=1.5, zorder=10)

            envelope_data = flows[top_bms]
            ax1.fill_between(flows.index, envelope_data.min(axis=1), envelope_data.max(axis=1),
                            color='#2196f3', alpha=0.3, label='Top 5 Benchmark Envelope')

            self._apply_standard_styling(ax1, ylabel='Flow (m³/s)', title='Benchmark Envelope (Top 5 by KGE)', legend=True)
            self._format_date_axis(ax1)

            # FDC Envelope
            if 'observed' in flows.columns:
                exc_obs, flows_obs = calculate_flow_duration_curve(flows['observed'].values)
                ax2.plot(exc_obs, flows_obs, label='Observed', color='black', linewidth=1.5)

            # Simplified FDC envelope (min/max of each exceedance point would be better but requires interp)
            for bm in top_bms:
                exc, f = calculate_flow_duration_curve(flows[bm].values)
                ax2.plot(exc, f, alpha=0.3, color='#2196f3')

            ax2.set_xscale('log')
            ax2.set_yscale('log')
            self._apply_standard_styling(ax2, xlabel='Exceedance Prob', ylabel='Flow (m³/s)', title='FDC Envelope', legend=True)

            plt.tight_layout()
            plot_path = plot_dir / f"{self._get_config_value(lambda: self.config.domain.name, dict_key=ConfigKeys.DOMAIN_NAME)}_benchmark_envelopes.png"
            return self._save_and_close(fig, plot_path)

        except Exception as e:
            self.logger.error(f"Error in plot_envelopes: {str(e)}")
            return None

    def plot(self, *args, **kwargs) -> Optional[str]:
        """Main plot method."""
        if 'benchmark_results' in kwargs:
            paths = self.plot_benchmarks(kwargs['benchmark_results'])
            return paths[0] if paths else None
        return None
