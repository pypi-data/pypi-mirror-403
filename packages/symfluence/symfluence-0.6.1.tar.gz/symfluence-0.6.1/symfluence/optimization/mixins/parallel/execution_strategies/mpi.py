"""
MPI Execution Strategy

Executes tasks using MPI (mpirun) for distributed computing.
"""

import logging
import os
import pickle  # nosec B403 - Used for trusted internal MPI task serialization
import subprocess
import sys
import uuid
from pathlib import Path
from typing import List, Dict, Any, Callable

from .base import ExecutionStrategy
from ..worker_environment import WorkerEnvironmentConfig


class MPIExecutionStrategy(ExecutionStrategy):
    """
    MPI execution strategy.

    Uses mpirun to distribute tasks across multiple processes.
    Generates a temporary worker script and communicates via pickle files.
    """

    def __init__(
        self,
        project_dir: Path,
        num_processes: int,
        logger: logging.Logger = None
    ):
        """
        Initialize MPI execution strategy.

        Args:
            project_dir: Project directory for temporary files
            num_processes: Number of MPI processes to use
            logger: Optional logger instance
        """
        self.project_dir = project_dir
        self.num_processes = num_processes
        self.logger = logger or logging.getLogger(__name__)
        self.worker_env = WorkerEnvironmentConfig()

    @property
    def name(self) -> str:
        """Strategy identifier for logging and selection."""
        return "mpi"

    def execute(
        self,
        tasks: List[Dict[str, Any]],
        worker_func: Callable,
        max_workers: int
    ) -> List[Dict[str, Any]]:
        """
        Execute tasks using MPI.

        Args:
            tasks: List of task dictionaries
            worker_func: Function to call for each task
            max_workers: Maximum number of worker processes

        Returns:
            List of results from task execution

        Raises:
            RuntimeError: If MPI execution fails
        """
        work_dir = self.project_dir / "temp_mpi"
        work_dir.mkdir(exist_ok=True)

        unique_id = uuid.uuid4().hex[:8]
        tasks_file = work_dir / f'mpi_tasks_{unique_id}.pkl'
        results_file = work_dir / f'mpi_results_{unique_id}.pkl'
        worker_script = work_dir / f'mpi_worker_{unique_id}.py'

        # Get worker module and function name
        worker_module, worker_function = self._get_worker_info(worker_func)

        cleanup_files = True
        try:
            self.logger.debug(
                f"MPI batch: {len(tasks)} tasks, worker={worker_module}.{worker_function}"
            )

            # Save tasks to file
            with open(tasks_file, 'wb') as f:
                pickle.dump(tasks, f)

            # Create worker script
            self._create_worker_script(
                worker_script, tasks_file, results_file,
                worker_module, worker_function
            )
            worker_script.chmod(0o755)

            # Determine number of processes
            num_procs = min(max_workers, self.num_processes, len(tasks))

            # Find Python executable
            python_exe = self._find_python_executable()

            # Build MPI command
            mpi_cmd = self._build_mpi_command(
                python_exe, num_procs, worker_script, tasks_file, results_file
            )

            self.logger.debug(f"MPI command: {' '.join(mpi_cmd)}")

            # Setup environment
            mpi_env = self._build_mpi_environment()

            # Run MPI command
            result = subprocess.run(mpi_cmd, capture_output=True, text=True, env=mpi_env)

            # Log output
            self._log_mpi_output(result)

            if result.returncode != 0:
                cleanup_files = False
                raise RuntimeError(
                    f"MPI execution failed with returncode {result.returncode}"
                )

            if results_file.exists():
                with open(results_file, 'rb') as f:
                    results = pickle.load(f)  # nosec B301 - Loading trusted MPI results
                self.logger.debug(f"MPI completed: {len(results)} results")
                return results
            else:
                cleanup_files = False
                raise RuntimeError("MPI results file not created")

        finally:
            if cleanup_files:
                self._cleanup_files([tasks_file, results_file, worker_script])

    def _get_worker_info(self, worker_func: Callable) -> tuple:
        """Get module and function name from callable."""
        if hasattr(worker_func, '__module__'):
            return worker_func.__module__, worker_func.__name__
        return (
            "symfluence.optimization.workers.summa_parallel_workers",
            "_evaluate_parameters_worker_safe"
        )

    def _find_python_executable(self) -> str:
        """Find the Python executable to use for MPI workers."""
        python_exe = sys.executable
        self.logger.debug(f"sys.executable: {python_exe}, exists: {Path(python_exe).exists()}")

        # Check for venv Python
        venv_paths = [
            Path(__file__).parent.parent.parent.parent.parent.parent / "venv" / "bin" / "python",
            Path(__file__).parent.parent.parent.parent.parent.parent / "venv" / "bin" / "python3",
            Path(__file__).parent.parent.parent.parent.parent.parent / "venv" / "bin" / "python3.11",
            Path.home() / "venv" / "bin" / "python",
            Path.home() / "venv" / "bin" / "python3",
        ]

        for venv_path in venv_paths:
            if venv_path.exists():
                python_exe = str(venv_path)
                self.logger.info(f"Using venv Python: {python_exe}")
                break

        return python_exe

    def _build_mpi_command(
        self,
        python_exe: str,
        num_processes: int,
        worker_script: Path,
        tasks_file: Path,
        results_file: Path
    ) -> list:
        """Build the mpirun command."""
        return [
            'mpirun',
            '-x', 'OMP_NUM_THREADS',
            '-x', 'HDF5_USE_FILE_LOCKING',
            '-x', 'MKL_NUM_THREADS',
            '-n', str(num_processes),
            python_exe,
            str(worker_script),
            str(tasks_file),
            str(results_file)
        ]

    def _build_mpi_environment(self) -> Dict[str, str]:
        """Build environment for MPI execution."""
        mpi_env = os.environ.copy()

        # Add src to PYTHONPATH
        src_path = str(Path(__file__).parent.parent.parent.parent.parent)
        current_pythonpath = mpi_env.get('PYTHONPATH', '')
        if current_pythonpath:
            mpi_env['PYTHONPATH'] = f"{src_path}:{current_pythonpath}"
        else:
            mpi_env['PYTHONPATH'] = src_path

        # Add worker environment variables
        mpi_env.update(self.worker_env.get_environment())

        # OpenMPI settings
        if 'OMPI_MCA_' not in mpi_env:
            mpi_env['OMPI_MCA_pls_rsh_agent'] = 'ssh'

        self.logger.debug(f"MPI environment - PYTHONPATH: {mpi_env.get('PYTHONPATH')}")

        return mpi_env

    def _log_mpi_output(self, result: subprocess.CompletedProcess) -> None:
        """Log MPI execution output."""
        self.logger.debug(f"MPI returncode: {result.returncode}")

        if result.stdout:
            self.logger.debug(f"MPI stdout: {result.stdout[:1000]}")
        if result.stderr:
            self.logger.debug(f"MPI stderr: {result.stderr[:1000]}")

        if result.returncode != 0:
            self.logger.error(f"MPI execution failed (returncode={result.returncode})")
            self.logger.error(
                f"MPI stdout: {result.stdout[:2000] if result.stdout else 'empty'}"
            )
            self.logger.error(
                f"MPI stderr: {result.stderr[:2000] if result.stderr else 'empty'}"
            )

    def _cleanup_files(self, files: List[Path]) -> None:
        """Clean up temporary files."""
        for file_path in files:
            if file_path.exists():
                try:
                    file_path.unlink()
                except OSError:
                    pass

    def _create_worker_script(
        self,
        script_path: Path,
        tasks_file: Path,
        results_file: Path,
        worker_module: str,
        worker_function: str
    ) -> None:
        """Create the MPI worker script file."""
        # Calculate path to src directory
        src_path = Path(__file__).parent.parent.parent.parent.parent

        script_content = f'''#!/usr/bin/env python3
import sys
import pickle
import os
from pathlib import Path
from mpi4py import MPI
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s')
logger = logging.getLogger(__name__)

# Silence noisy libraries
for noisy_logger in ['rasterio', 'fiona', 'boto3', 'botocore', 'matplotlib', 'urllib3', 's3transfer']:
    logging.getLogger(noisy_logger).setLevel(logging.WARNING)

# Add symfluence src to path
sys.path.insert(0, r"{str(src_path)}")

try:
    from {worker_module} import {worker_function}
except ImportError as e:
    logger.error(f"Failed to import worker function: {{e}}")
    logger.error(f"sys.path = {{sys.path}}")
    sys.exit(1)

def main():
    """MPI worker main function."""
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()

    tasks_file = Path(sys.argv[1])
    results_file = Path(sys.argv[2])

    if rank == 0:
        # Master process - load all tasks
        try:
            with open(tasks_file, 'rb') as f:
                all_tasks = pickle.load(f)
        except (ValueError, RuntimeError, IOError) as e:
            logger.error(f"Master failed to load tasks: {{e}}")
            all_tasks = []

        logger.info(f"Rank 0: Loaded {{len(all_tasks)}} tasks")

        # Distribute tasks by proc_id to avoid race conditions
        from collections import defaultdict
        tasks_by_proc = defaultdict(list)
        for task in all_tasks:
            proc_id = task.get('proc_id', 0)
            assigned_rank = proc_id % size
            tasks_by_proc[assigned_rank].append(task)

        logger.info(f"Rank 0: Distributed tasks by proc_id - {{{{r: len(tasks_by_proc[r]) for r in range(size)}}}}")
        all_results = []

        for worker_rank in range(size):
            worker_tasks = tasks_by_proc[worker_rank]

            if worker_rank == 0:
                my_tasks = worker_tasks
                logger.info(f"Rank 0: Processing {{len(my_tasks)}} tasks locally")
            else:
                logger.info(f"Rank 0: Sending {{len(worker_tasks)}} tasks to rank {{worker_rank}}")
                comm.send(worker_tasks, dest=worker_rank, tag=1)

        # Process rank 0 tasks
        for i, task in enumerate(my_tasks):
            try:
                worker_result = {worker_function}(task)
                all_results.append(worker_result)
            except (ValueError, RuntimeError, IOError) as e:
                logger.error(f"Rank 0: Task {{i}} failed: {{e}}")
                error_result = {{
                    'individual_id': task.get('individual_id', -1),
                    'params': task.get('params', {{}}),
                    'score': None,
                    'error': f'Rank 0 error: {{str(e)}}'
                }}
                all_results.append(error_result)

        # Collect results from workers
        for worker_rank in range(1, size):
            try:
                logger.info(f"Rank 0: Waiting for results from rank {{worker_rank}}")
                worker_results = comm.recv(source=worker_rank, tag=2)
                logger.info(f"Rank 0: Received {{len(worker_results)}} results from rank {{worker_rank}}")
                all_results.extend(worker_results)
            except (ValueError, RuntimeError, IOError) as e:
                logger.error(f"Error receiving from worker {{worker_rank}}: {{e}}")

        # Save results
        logger.info(f"Rank 0: Saving {{len(all_results)}} results to {{results_file}}")
        with open(results_file, 'wb') as f:
            pickle.dump(all_results, f)
        logger.info(f"Rank 0: Results saved successfully")

    else:
        # Worker process
        logger.info(f"Rank {{rank}}: Waiting for tasks from rank 0")
        try:
            my_tasks = comm.recv(source=0, tag=1)
            logger.info(f"Rank {{rank}}: Received {{len(my_tasks)}} tasks")

            my_results = []

            for i, task in enumerate(my_tasks):
                logger.info(f"Rank {{rank}}: Processing task {{i+1}}/{{len(my_tasks)}}")
                try:
                    worker_result = {worker_function}(task)
                    my_results.append(worker_result)
                except (ValueError, RuntimeError, IOError) as e:
                    logger.error(f"Rank {{rank}}: Task {{i}} failed: {{e}}")
                    error_result = {{
                        'individual_id': task.get('individual_id', -1),
                        'params': task.get('params', {{}}),
                        'score': None,
                        'error': f'Rank {{rank}} error: {{str(e)}}'
                    }}
                    my_results.append(error_result)

            logger.info(f"Rank {{rank}}: Sending {{len(my_results)}} results back to rank 0")
            comm.send(my_results, dest=0, tag=2)
            logger.info(f"Rank {{rank}}: Results sent successfully")

        except (ValueError, RuntimeError, IOError) as e:
            logger.error(f"Worker {{rank}} failed: {{e}}")

if __name__ == "__main__":
    main()
'''
        with open(script_path, 'w') as f:
            f.write(script_content)
