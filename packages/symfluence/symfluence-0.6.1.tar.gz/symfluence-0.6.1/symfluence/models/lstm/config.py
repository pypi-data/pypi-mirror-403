"""
LSTM Model Configuration.

Provides configuration schema, defaults, transformers, and validation
for the LSTM machine learning model.

This module registers LSTM-specific configuration components with the
ModelRegistry, enabling the core config system to remain model-agnostic.

PHASE 2 REFACTORING: This adapter now uses AutoGeneratedConfigAdapter
to auto-generate defaults and transformers from Pydantic Field declarations.
All defaults are maintained in LSTMConfig (model_configs.py) for single
source of truth.
"""

from typing import Dict, Any
from symfluence.models.base import AutoGeneratedConfigAdapter
from symfluence.core.config.models.model_configs import LSTMConfig


class LSTMConfigAdapter(AutoGeneratedConfigAdapter):
    """
    Configuration adapter for LSTM model with auto-generated defaults and transformers.

    Defaults and transformers are automatically extracted from LSTMConfig
    Pydantic model. Only custom validation logic is implemented here.
    """

    def __init__(self, model_name: str = 'LSTM'):
        super().__init__(model_name)

    def get_config_schema(self):
        """Return LSTM Pydantic configuration schema."""
        return LSTMConfig

    def validate(self, config: Dict[str, Any]) -> None:
        """
        Validate LSTM-specific configuration parameters.

        Performs custom validation beyond what Pydantic enforces, such as
        checking for valid hyperparameter ranges, feature compatibility,
        and model architecture consistency.

        Args:
            config: Dictionary containing LSTM configuration values to validate.

        Raises:
            ValueError: If configuration values are invalid or inconsistent.

        Note:
            Currently a placeholder. Add custom validation logic for LSTM
            hyperparameters (hidden_size, num_layers, dropout_rate, etc.)
            that cannot be expressed in Pydantic Field constraints.
        """
        pass  # Add custom validation logic if needed
