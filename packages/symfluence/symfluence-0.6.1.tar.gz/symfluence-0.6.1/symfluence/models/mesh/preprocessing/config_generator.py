"""
MESH Configuration Generator

Generates auxiliary configuration files for MESH model.
Core config files (CLASS, hydrology, run_options) are generated by meshflow.
"""

import logging
from datetime import timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd
import xarray as xr

from symfluence.core.config.models import SymfluenceConfig
from symfluence.core.mixins import ConfigMixin


class MESHConfigGenerator(ConfigMixin):
    """
    Generates auxiliary MESH configuration files.

    Meshflow generates core files (CLASS, hydrology, run_options).
    This generator creates observation-dependent files:
    - MESH_input_streamflow.txt
    """

    def __init__(
        self,
        forcing_dir: Path,
        project_dir: Path,
        config: Dict[str, Any],
        logger: logging.Logger = None,
        time_window_func=None
    ):
        """
        Initialize config generator.

        Args:
            forcing_dir: Directory for MESH files
            project_dir: Project directory
            config: Configuration dictionary
            logger: Optional logger instance
            time_window_func: Function to get simulation time window
        """
        self.forcing_dir = forcing_dir
        self.project_dir = project_dir
        if isinstance(config, dict):
            try:
                self._config = SymfluenceConfig(**config)
            except (TypeError, ValueError, KeyError, AttributeError):
                self._config = config
        else:
            self._config = config
        self.logger = logger or logging.getLogger(__name__)
        self.get_simulation_time_window = time_window_func
        self.domain_name = config.get('DOMAIN_NAME', 'domain')

    def create_streamflow_input(self) -> None:
        """Create MESH_input_streamflow.txt with gauge locations and observed data."""
        streamflow_path = self.forcing_dir / "MESH_input_streamflow.txt"

        time_window = self.get_simulation_time_window() if self.get_simulation_time_window else None
        spinup_days = int(self._get_config_value(
            lambda: self.config.model.mesh.spinup_days, default=365, dict_key='MESH_SPINUP_DAYS'
        ))

        if time_window:
            analysis_start, end_date = time_window
            sim_start = analysis_start - timedelta(days=spinup_days)
            start_year = sim_start.year
            start_month = sim_start.month
            start_day = sim_start.day
        else:
            start_year = 2001
            start_month = 1
            start_day = 1
            end_date = None

        outlet_rank, outlet_da = self._find_outlet_subbasin()
        self.logger.info(f"Setting gauge at Rank {outlet_rank} (DA={outlet_da:.1f} kmÂ²)")

        obs_data = self._load_observed_streamflow()

        gauge_id = self._get_config_value(
            lambda: self.config.data.streamflow_station_id, default='gauge1', dict_key='STREAMFLOW_STATION_ID'
        )
        if gauge_id == 'default':
            gauge_id = self.domain_name

        with open(streamflow_path, 'w') as f:
            f.write(f"#{self.domain_name} streamflow gauge\n")
            f.write(f"1 0 0 24 {start_year} {start_month} {start_day}\n")
            f.write(f"1 {outlet_rank} {gauge_id}\n")

            if obs_data is not None and len(obs_data) > 0:
                for q in obs_data:
                    if np.isnan(q) or q < 0:
                        f.write("-1\t-1\n")
                    else:
                        f.write(f"{q:.3f}\t-1\n")
            else:
                if time_window:
                    n_days = (end_date - sim_start).days + 1
                else:
                    n_days = 365

                for _ in range(n_days):
                    f.write("-1\t-1\n")

        self.logger.info(f"Created streamflow input: {streamflow_path}")

    def _find_outlet_subbasin(self) -> Tuple[int, float]:
        """Find outlet subbasin rank and drainage area from DDB."""
        ddb_path = self.forcing_dir / "MESH_drainage_database.nc"

        if ddb_path.exists():
            with xr.open_dataset(ddb_path) as ds:
                next_arr = ds['Next'].values
                rank_arr = ds['Rank'].values
                da_arr = ds['DA'].values if 'DA' in ds else ds['GridArea'].values

                inside_mask = next_arr > 0
                if inside_mask.any():
                    inside_indices = np.where(inside_mask)[0]
                    max_da_idx = inside_indices[np.argmax(da_arr[inside_indices])]
                    outlet_rank = int(rank_arr[max_da_idx])
                    outlet_da = da_arr[max_da_idx] / 1e6
                else:
                    outlet_idx = np.argmax(da_arr)
                    outlet_rank = int(rank_arr[outlet_idx])
                    outlet_da = da_arr[outlet_idx] / 1e6

                return outlet_rank, outlet_da

        return 1, 0

    def _load_observed_streamflow(self) -> Optional[np.ndarray]:
        """Load observed streamflow data if available."""
        obs_dir = self.project_dir / 'observations' / 'streamflow' / 'preprocessed'
        if not obs_dir.exists():
            obs_dir = self.project_dir / 'observations' / 'streamflow' / 'raw_data'

        if not obs_dir.exists():
            return None

        csv_files = list(obs_dir.glob('*.csv'))
        if not csv_files:
            return None

        try:
            df = pd.read_csv(csv_files[0])

            q_col = None
            for col in ['discharge', 'streamflow', 'Q', 'flow', 'FLOW', 'Value']:
                if col in df.columns:
                    q_col = col
                    break

            if q_col is None:
                return None

            return np.asarray(df[q_col].values)

        except Exception as e:
            self.logger.warning(f"Failed to load observed streamflow: {e}")
            return None
