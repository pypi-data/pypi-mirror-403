#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Total Water Storage (TWS) Evaluator.

Evaluates simulated total water storage from SUMMA against GRACE satellite
observations of water storage anomalies.

GRACE Satellites:
    - GRACE (Gravity Recovery and Climate Experiment): Twin satellites measuring
      Earth's gravity field to infer water storage changes
    - GRACE Follow-On: Continuation mission (2018-present)
    - Temporal resolution: Monthly anomalies
    - Spatial resolution: ~300 km × 300 km footprint
    - Sensitivity: 1-2 cm equivalent water height

GRACE Data Products:
    - Multiple processing centers: JPL (Jet Propulsion Lab), CSR (U.Texas), GSFC (NASA)
    - Released as time-variable gravity fields → converted to water storage anomalies
    - Units: mm equivalent water thickness (relative to 2004-2009 baseline)

Water Storage Components (SUMMA):
    - Snow Water Equivalent (SWE): scalarSWE (kg/m² → mm)
    - Canopy water: scalarCanopyWat (mm)
    - Soil water: scalarTotalSoilWat (mm)
    - Groundwater: scalarAquiferStorage (m → mm via ×1000)
    - Glacier mass (optional): glacMass4AreaChange (kg/m² → mm)

Configuration:
    TWS_GRACE_COLUMN: GRACE data column name (default: 'grace_jpl_anomaly')
    TWS_ANOMALY_BASELINE: Baseline period for anomaly calc ('overlap' or year range)
    TWS_UNIT_CONVERSION: Scaling factor for model storage (default: 1.0)
    TWS_STORAGE_COMPONENTS: Comma-separated storage variables to sum
    TWS_DETREND: Remove linear trend (critical for glacierized basins)
    TWS_SCALE_TO_OBS: Scale model variability to match observations
    TWS_OBS_PATH: Direct path override for GRACE data
"""

import logging
import pandas as pd
import numpy as np
import xarray as xr
from pathlib import Path
from typing import Dict, List, Optional, Any, TYPE_CHECKING

from symfluence.evaluation.registry import EvaluationRegistry
from symfluence.evaluation.output_file_locator import OutputFileLocator
from .base import ModelEvaluator

if TYPE_CHECKING:
    from symfluence.core.config.models import SymfluenceConfig


@EvaluationRegistry.register('TWS')
class TWSEvaluator(ModelEvaluator):
    """Total Water Storage evaluator comparing SUMMA to GRACE satellites.

    Compares time series of simulated water storage (SWE + canopy + soil + groundwater)
    from SUMMA land-surface model against monthly water storage anomalies from GRACE
    satellite gravity measurements.

    Key Features:
        - Multi-component storage summation: SWE + canopy + soil + aquifer
        - Flexible storage component selection via configuration
        - GRACE data support from multiple processing centers (JPL, CSR, GSFC)
        - Anomaly computation with configurable baselines
        - Advanced signal processing: detrending, variability scaling
        - Diagnostic exports for visualization and analysis

    Physical Basis:
        GRACE measures vertically-integrated water column changes via:
        - Temporal gravity variations → water storage changes
        - Signal includes: surface water, soil moisture, groundwater, snow/ice
        - Monthly resolution averages diurnal/seasonal noise
        - Spatial footprint ~300 km (cannot resolve sub-grid variability)

    Critical for Glacierized Basins:
        Glacier mass loss creates long-term trend that dominates GRACE signal and
        obscures seasonal patterns (r ≈ 0.3 with trend, r ≈ 0.8 after detrending).
        TWS_DETREND option removes this trend for proper seasonal comparison.

    Storage Component Units (SUMMA):
        - scalarSWE: kg/m² → mm (multiply by 1.0, density=1000 kg/m³)
        - scalarCanopyWat: mm (water depth)
        - scalarTotalSoilWat: mm (water depth)
        - scalarAquiferStorage: m → mm (multiply by 1000.0)
        - glacMass4AreaChange: kg/m² → mm (glacier mass only in timestep files)

    File Search Strategy:
        1. Prefer daily NetCDF files (*_day.nc) for most storage variables
        2. Check original file from get_simulation_files()
        3. Search for timestep files (*_timestep.nc) for glacier mass
        4. Select file with maximum matching variables

    Metrics Calculation:
        1. Load absolute storage from SUMMA (daily or timestep data)
        2. Aggregate to monthly means (match GRACE temporal resolution)
        3. Calculate anomaly relative to overlap period mean
        4. Optional detrending: Remove linear trend (glacierized basins)
        5. Optional scaling: Match model variability to observations
        6. Calculate performance metrics (KGE, RMSE, NSE, etc.)

    Configuration:
        TWS_GRACE_COLUMN: Column name in GRACE CSV (default: 'grace_jpl_anomaly')
        TWS_ANOMALY_BASELINE: Baseline for anomaly ('overlap' or year-range)
        TWS_UNIT_CONVERSION: Multiplicative scaling for model storage
        TWS_STORAGE_COMPONENTS: Comma-separated variable names (default: SWE, canopy, soil, aquifer)
        TWS_DETREND: Remove linear trend from both series (False by default)
        TWS_SCALE_TO_OBS: Scale model std dev to match observations (False)
        TWS_OBS_PATH: Direct path override for GRACE observation file

    Attributes:
        grace_column: GRACE data column name
        anomaly_baseline: Baseline period specification
        unit_conversion: Scaling factor for model storage values
        detrend: Whether to remove linear trends
        scale_to_obs: Whether to scale model variability to observations
        storage_vars: List of SUMMA storage variable names to sum
    """

    DEFAULT_STORAGE_VARS = [
        'scalarSWE', 'scalarCanopyWat', 'scalarTotalSoilWat', 'scalarAquiferStorage'
    ]

    def __init__(self, config: 'SymfluenceConfig', project_dir: Path, logger: logging.Logger):
        """Initialize TWS evaluator with storage component and signal processing options.

        Configures water storage components to sum (SWE, canopy, soil, aquifer),
        GRACE data column selection, and optional signal processing (detrending,
        variability scaling) to handle model-observation mismatches.

        Configuration Parameters:
            TWS_GRACE_COLUMN: GRACE variable column name (default: 'grace_jpl_anomaly')
                - Supports multiple GRACE processing centers:
                  'grace_jpl_anomaly' (JPL, most widely used)
                  'grace_csr_anomaly' (CSR, U.Texas)
                  'grace_gsfc_anomaly' (GSFC, NASA)
            TWS_ANOMALY_BASELINE: Baseline period for anomaly ('overlap' by default)
            TWS_UNIT_CONVERSION: Scaling factor for model storage (default: 1.0)
            TWS_STORAGE_COMPONENTS: Comma-separated SUMMA storage variables
                - Default: 'scalarSWE, scalarCanopyWat, scalarTotalSoilWat, scalarAquiferStorage'
                - Can include glacier mass: 'glacMass4AreaChange'
            TWS_DETREND: Remove linear trend from both sim and obs (default: False)
                - Critical for glacierized basins (glacier mass loss trend obscures seasonal signal)
                - Improves correlation from ~0.3 to ~0.8 for glacier domains
            TWS_SCALE_TO_OBS: Scale model variability to match observations (default: False)
                - Centers both series on zero-mean, rescales model std dev to match obs
                - Useful when pattern is correct but amplitude is wrong

        Storage Component Units:
            - scalarSWE: kg/m² (converted to mm)
            - scalarCanopyWat: mm (water depth, no conversion needed)
            - scalarTotalSoilWat: mm (water depth, no conversion needed)
            - scalarAquiferStorage: m (converted to mm via ×1000)

        Args:
            config: Typed configuration object (SymfluenceConfig)
            project_dir: Project root directory
            logger: Logger instance

        Raises:
            None (uses defaults for missing config values)
        """
        super().__init__(config, project_dir, logger)

        self.grace_column = self.config_dict.get('TWS_GRACE_COLUMN', 'grace_jpl_anomaly')
        self.anomaly_baseline = self.config_dict.get('TWS_ANOMALY_BASELINE', 'overlap')
        self.unit_conversion = self.config_dict.get('TWS_UNIT_CONVERSION', 1.0)

        # Detrending option: removes linear trend from both series before comparison
        # This is critical for glacierized basins where long-term glacier mass loss
        # creates a trend mismatch that obscures the seasonal signal we want to calibrate
        self.detrend = self.config_dict.get('TWS_DETREND', False)

        # Scaling option: scale model variability to match observed
        # Useful when model has correct pattern but wrong amplitude
        self.scale_to_obs = self.config_dict.get('TWS_SCALE_TO_OBS', False)

        storage_str = self.config_dict.get('TWS_STORAGE_COMPONENTS', '')
        if storage_str:
            self.storage_vars = [v.strip() for v in storage_str.split(',') if v.strip()]
        else:
            self.storage_vars = self.DEFAULT_STORAGE_VARS.copy()

    def get_simulation_files(self, sim_dir: Path) -> List[Path]:
        """Locate SUMMA output files containing water storage variables.

        Searches for NetCDF files containing any of the configured storage
        components (SWE, canopy water, soil water, aquifer storage, glacier mass).

        Args:
            sim_dir: Directory containing SUMMA simulation output files

        Returns:
            List[Path]: Paths to storage variable files (typically most recent daily output)
        """
        locator = OutputFileLocator(self.logger)
        # TWS needs the most recent file with storage components
        most_recent = locator.get_most_recent(sim_dir, 'tws')
        return [most_recent] if most_recent else []

    def extract_simulated_data(self, sim_files: List[Path], **kwargs) -> pd.Series:
        """Extract and sum water storage components from SUMMA output files.

        Implements intelligent file selection to find all storage variables across
        multiple SUMMA output files (daily, timestep, etc.), then sums components
        to compute total water storage.

        File Search Strategy:
            1. Start with provided sim_files[0] (typically daily output)
            2. Search output directory for alternative files:
               - Daily files (*_day.nc): Preferred for most storage variables
               - Timestep files (*_timestep.nc): Often contain glacier mass
            3. Test each file for availability of storage variables
            4. Select file(s) with maximum matching variables
            5. If no single file has all vars, try combining vars from multiple files

        Storage Component Summation:
            Iterates through self.storage_vars (e.g., SWE, canopy, soil, aquifer):
            1. Check variable availability in selected file
            2. Extract values as float array
            3. Replace fill values (-9999 from SUMMA) with NaN
            4. Unit conversions:
               - Aquifer storage: m → mm (multiply by 1000)
               - Others: Already in mm or compatible
            5. Collapse spatial dimensions (HRU/GRU) via nanmean
            6. Accumulate: total_tws += component (handles NaN values properly)

        Spatial Aggregation (HRU/GRU):
            Single HRU/GRU: Mean reduces single value (identity operation)
            Multiple HRU/GRU: Averages across dimension via nanmean()
            Result: Scalar time series (1D array of length = n_timesteps)

        Unit Conversions:
            - scalarSWE: kg/m² → mm (density ≈ 1, so 1 kg/m² ≈ 1 mm)
            - scalarAquiferStorage: m → mm (×1000)
            - Others: Already mm (canopy, soil water depth)

        Args:
            sim_files: List of SUMMA output files (typically daily NetCDF)
            **kwargs: Additional parameters (unused)

        Returns:
            pd.Series: Time series of total water storage (mm) with datetime index

        Raises:
            ValueError: If no storage variables found in any candidate file
            Exception: If time coordinate cannot be extracted

        Notes:
            - Handles NaN intelligently: NaN + value = value (not NaN)
            - Fill values (-9999) replaced with NaN to avoid data corruption
            - Logs which file used and how many variables found for debugging
        """
        output_file = sim_files[0]

        # Try to find a file with storage variables
        # For glacier mode, glacMass4AreaChange is often only in timestep files
        output_dir = output_file.parent
        potential_files = [
            output_file,  # Original file
        ]
        # Add daily file alternatives (storage vars are often in daily output)
        for f in output_dir.glob('*_day.nc'):
            if f not in potential_files:
                potential_files.insert(0, f)  # Prefer daily files
        # Also check timestep files for glacier variables like glacMass4AreaChange
        for f in output_dir.glob('*timestep.nc'):
            if f not in potential_files:
                potential_files.append(f)

        # Find a file that has ALL requested storage variables (if possible)
        # or at least the most critical ones including glacier mass
        best_file = None
        best_var_count = 0
        for candidate_file in potential_files:
            try:
                with xr.open_dataset(candidate_file) as test_ds:
                    matching_vars = [v for v in self.storage_vars if v in test_ds.data_vars]
                    if len(matching_vars) > best_var_count:
                        best_var_count = len(matching_vars)
                        best_file = candidate_file
                        self.logger.debug(f"Found {len(matching_vars)}/{len(self.storage_vars)} vars in {candidate_file.name}")
                    # If we find all variables, use this file
                    if len(matching_vars) == len(self.storage_vars):
                        break
            except (OSError, IOError, KeyError):
                continue

        if best_file:
            output_file = best_file
            self.logger.debug(f"Using {output_file.name} for TWS extraction ({best_var_count} storage vars)")

        try:
            ds = xr.open_dataset(output_file)
            total_tws = None

            for var in self.storage_vars:
                if var in ds.data_vars:
                    data = ds[var].values.astype(float)

                    # Replace fill values (-9999) with NaN
                    # SUMMA uses -9999 for missing/invalid data in some domains
                    data = np.where(data < -999, np.nan, data)

                    # Unit conversion: aquifer storage is in meters, others usually in mm
                    if 'aquifer' in var.lower():
                        self.logger.debug(f"Converting {var} from meters to mm")
                        data = data * 1000.0

                    if data.ndim > 1:
                        axes_to_sum = tuple(range(1, data.ndim))
                        data = np.nanmean(data, axis=axes_to_sum)

                    if total_tws is None:
                        total_tws = data.copy()
                    else:
                        # Use nansum behavior: NaN + value = value
                        total_tws = np.where(np.isnan(total_tws), data,
                                            np.where(np.isnan(data), total_tws, total_tws + data))
                else:
                    self.logger.warning(f"Storage variable {var} not found in {output_file}")

            if total_tws is None:
                raise ValueError(f"No storage variables {self.storage_vars} found in {output_file}")

            # Find time coordinate
            time_coord = None
            for var in ['time', 'Time', 'datetime']:
                if var in ds.coords or var in ds.dims:
                    time_coord = pd.to_datetime(ds[var].values)
                    break

            if time_coord is None:
                raise ValueError("Could not extract time coordinate")

            tws_series = pd.Series(total_tws.flatten(), index=time_coord, name='simulated_tws')
            tws_series = tws_series * self.unit_conversion
            ds.close()
            return tws_series
        except Exception as e:
            self.logger.error(f"Error loading SUMMA output: {e}")
            raise

    def get_observed_data_path(self) -> Path:
        """Resolve path to GRACE water storage anomaly observations.

        Implements flexible path resolution supporting multiple GRACE data organization
        patterns and locations. Checks multiple possible locations and file naming
        conventions commonly used in glacier and hydrology projects.

        Path Search Priority:
            1. TWS_OBS_PATH config override (if specified, return immediately)
            2. observations/storage/grace/ (glacier domain organization)
            3. observations/grace/ (standard organization)

        File Naming Conventions Searched:
            - Domain-specific HRU format: {domain}_HRUs_GRUs_grace_tws_anomaly.csv
            - Elevation-based HRU format: {domain}_HRUs_elevation_grace_tws_anomaly_by_hru.csv
            - Processed format: {domain}_grace_tws_processed.csv
            - Generic format: {domain}_grace_tws_anomaly.csv
            - Generic without domain: grace_tws_anomaly.csv, tws_anomaly.csv

        Directory Hierarchy:
            - Preprocessed subdirectory (preferred for processed data)
            - Root directory (fallback for raw/alternative formats)

        Returns Path even if file doesn't exist (may trigger acquisition later).

        Args:
            None (uses self.project_dir, self.domain_name, self.grace_column)

        Returns:
            Path: Absolute path to GRACE observation file
        """
        tws_obs_path = self.config_dict.get('TWS_OBS_PATH')
        if tws_obs_path:
            return Path(tws_obs_path)

        # Search in multiple possible locations for GRACE data
        # Support both observations/grace and observations/storage/grace paths
        obs_base = self.project_dir / 'observations'
        search_dirs = [
            obs_base / 'storage' / 'grace',  # Ashley's glacier domain setup
            obs_base / 'grace',               # Standard location
        ]

        potential_patterns = [
            f'{self.domain_name}_HRUs_GRUs_grace_tws_anomaly.csv',  # HRU-specific format
            f'{self.domain_name}_HRUs_elevation_grace_tws_anomaly_by_hru.csv',  # Elevation-based HRU
            f'{self.domain_name}_grace_tws_processed.csv',
            f'{self.domain_name}_grace_tws_anomaly.csv',
            'grace_tws_anomaly.csv',
            'tws_anomaly.csv',
        ]

        for obs_dir in search_dirs:
            if not obs_dir.exists():
                continue
            preprocessed_dir = obs_dir / 'preprocessed'
            for pattern in potential_patterns:
                for search_dir in [preprocessed_dir, obs_dir]:
                    path = search_dir / pattern
                    if path.exists():
                        return path

        # Fallback to default
        return obs_base / 'grace' / 'preprocessed' / f'{self.domain_name}_grace_tws_processed.csv'

    def _get_observed_data_column(self, columns: List[str]) -> Optional[str]:
        """Identify GRACE data column from multiple processing center options.

        GRACE water storage anomalies are provided by multiple processing centers
        (JPL, CSR, GSFC) which produce similar but slightly different results due
        to different processing algorithms and filtering approaches.

        Column Priority:
            1. Configured column (TWS_GRACE_COLUMN from config, default: 'grace_jpl_anomaly')
            2. Fallback GRACE centers: 'grace_csr_anomaly', 'grace_gsfc_anomaly'
            3. Generic pattern matching: any column containing 'grace' and 'anomaly'

        GRACE Processing Centers:
            - JPL (Jet Propulsion Lab): Most widely used, published since 2002
            - CSR (Center for Space Research, U.Texas): Alternative processing
            - GSFC (NASA Goddard Space Flight Center): Alternative processing

        Args:
            columns: List of column names from GRACE CSV file

        Returns:
            Optional[str]: Name of GRACE anomaly column, or None if not found
        """
        if self.grace_column in columns:
            return self.grace_column
        # Fallback to known columns
        for fallback in ['grace_jpl_anomaly', 'grace_csr_anomaly', 'grace_gsfc_anomaly']:
            if fallback in columns:
                return fallback
        return next((c for c in columns if 'grace' in c.lower() and 'anomaly' in c.lower()), None)

    def needs_routing(self) -> bool:
        """Determine if water storage evaluation requires routing model.

        Water storage (measured by GRACE or simulated by SUMMA) is integrated
        over the basin and does NOT require streamflow routing. Storage changes
        are evaluated directly without propagation.

        Returns:
            bool: False (TWS evaluator never requires routing)
        """
        return False

    def _detrend_series(self, series: pd.Series) -> pd.Series:
        """Remove linear trend from a time series via least-squares regression.

        Detrending isolates anomalies and seasonal patterns by removing the
        underlying long-term trend. Critical for glacierized basins where
        glacier mass loss creates a dominant trend (>80% of variance) that
        obscures seasonal signals.

        Mathematical Approach:
            1. Fit linear model: y = a*t + b using valid (non-NaN) data points
            2. Compute fitted trend: trend = a*t + b for all time points
            3. Detrended series: y_detrended = y - trend
            4. Result: seasonal/anomaly signal with trend removed

        NaN Handling:
            - Ignores NaN values during linear regression
            - Returns original series if < 2 valid data points
            - Preserves NaN positions in detrended output

        Impact on Correlation:
            For glacierized basins: r ≈ 0.30 (with trend) → r ≈ 0.78 (detrended)
            The long-term trend completely dominates model-obs mismatch, preventing
            proper calibration of seasonal storage changes.

        Args:
            series: Time series (pd.Series with datetime index and float values)

        Returns:
            pd.Series: Detrended series (same index and length as input)

        Notes:
            Uses numpy.polyfit (degree 1) for robust linear regression with NaN handling
        """
        # Convert datetime index to numeric for linear regression
        x = np.arange(len(series))
        y = series.values

        # Handle NaN values
        valid_mask = ~np.isnan(y)
        if valid_mask.sum() < 2:
            return series  # Not enough data to detrend

        # Fit linear trend using valid points only
        coeffs = np.polyfit(x[valid_mask], y[valid_mask], 1)
        trend = np.polyval(coeffs, x)

        # Remove trend
        detrended = y - trend
        return pd.Series(detrended, index=series.index, name=series.name)

    def _scale_to_obs_variability(self, sim_series: pd.Series, obs_series: pd.Series) -> pd.Series:
        """Scale model variability to match observed variability amplitude.

        Addresses systematic amplitude differences where model captures the correct
        temporal pattern (seasonal cycle, event timing) but with wrong magnitude.
        Commonly occurs when:
        - SWE accumulation too high or too low
        - Soil water variability underestimated
        - Parameter uncertainty affects storage amplitude

        Rescaling Strategy:
            1. Remove means from both series: center on zero
            2. Compute standard deviations: σ_sim, σ_obs
            3. Scale simulated: sim_scaled = (sim_centered / σ_sim) × σ_obs
            4. Result: both series have same mean and std dev

        Advantage vs Standard Normalization:
            - Preserves zero-mean anomalies (natural time series origin)
            - Corrects amplitude while maintaining temporal structure
            - Allows model with correct pattern but wrong magnitude to achieve high KGE

        Use Cases:
            - Model SWE pattern correct, but 20% too high
            - Soil water storage correct seasonal shape, wrong amplitude
            - When detrending is insufficient for matching observations

        Args:
            sim_series: Simulated water storage time series (zero-mean anomalies)
            obs_series: Observed storage time series (zero-mean anomalies)

        Returns:
            pd.Series: Rescaled simulated series with same std dev as observations

        Notes:
            - Handles zero variability (returns original if sim_std = 0)
            - Both input series should be anomalies (zero-mean) before calling
        """
        obs_std = obs_series.std()
        sim_std = sim_series.std()

        if sim_std == 0 or np.isnan(sim_std):
            return sim_series

        # Scale sim to have same std as obs, keeping zero-mean
        sim_centered = sim_series - sim_series.mean()
        scaled = (sim_centered / sim_std) * obs_std

        return pd.Series(scaled.values, index=sim_series.index, name=sim_series.name)

    def calculate_metrics(self, sim: Any, obs: Optional[pd.Series] = None,
                         mizuroute_dir: Optional[Path] = None,
                         calibration_only: bool = True) -> Optional[Dict[str, float]]:
        """Calculate TWS performance metrics comparing SUMMA storage to GRACE anomalies.

        Overrides base class to implement TWS-specific metric calculation pipeline:
        1. Load absolute storage from SUMMA daily/timestep output
        2. Aggregate to monthly means (match GRACE temporal resolution)
        3. Compute anomalies relative to overlap period baseline
        4. Apply optional signal processing: detrending, variability scaling
        5. Calculate performance metrics (KGE, RMSE, NSE, correlation, bias)

        Args:
            sim: Path to simulation directory or pre-loaded pd.Series
            obs: Optional pre-loaded pd.Series of observations. If None, loads from file.
            mizuroute_dir: mizuRoute simulation directory (unused for TWS)
            calibration_only: Whether to calculate only calibration metrics
        """
        sim_dir = Path(sim) if isinstance(sim, (str, Path)) else None
        if sim_dir is None:
            raise ValueError("TWS evaluation requires simulation directory Path")
        try:
            self.logger.debug(f"[TWS] Starting metrics calculation for {sim_dir}")

            # Load simulated data (absolute storage)
            sim_files = self.get_simulation_files(sim_dir)
            if not sim_files:
                self.logger.error(f"[TWS] No simulation files found in {sim_dir}")
                return None

            sim_tws = self.extract_simulated_data(sim_files)
            self.logger.debug(f"[TWS] Extracted simulated TWS: {len(sim_tws)} points")

            # Load observed data (anomalies)
            obs_path = self.get_observed_data_path()
            if not obs_path.exists():
                self.logger.error(f"[TWS] Observed data path does not exist: {obs_path}")
                return None

            obs_df = pd.read_csv(obs_path, index_col=0, parse_dates=True)
            col = self._get_observed_data_column(obs_df.columns)
            if not col:
                self.logger.error(f"[TWS] Could not find GRACE column in {obs_path}. Available: {list(obs_df.columns)}")
                return None

            obs_tws = obs_df[col].dropna()

            # Aggregate simulated to monthly means to match GRACE
            sim_monthly = sim_tws.resample('MS').mean()

            # Check if we have any simulated data
            if len(sim_monthly) == 0:
                self.logger.error("[TWS] No simulated TWS data available for metrics calculation")
                return None

            # Find common period
            common_idx = sim_monthly.index.intersection(obs_tws.index)
            if len(common_idx) == 0:
                sim_start, sim_end = sim_monthly.index[0], sim_monthly.index[-1]
                obs_start, obs_end = obs_tws.index[0], obs_tws.index[-1]
                self.logger.error(f"[TWS] No overlapping period between simulation ({sim_start} to {sim_end}) and observations ({obs_start} to {obs_end})")
                return None

            self.logger.debug(f"[TWS] Found common period with {len(common_idx)} points")
            sim_matched = sim_monthly.loc[common_idx]
            obs_matched = obs_tws.loc[common_idx]

            # Calculate anomaly for simulated data
            baseline_mean = sim_matched.mean()
            sim_anomaly = sim_matched - baseline_mean

            # Apply detrending if configured
            # This is critical for glacierized basins where glacier mass loss trend
            # obscures the seasonal signal (improves correlation ~0.46 -> ~0.78)
            if self.detrend:
                self.logger.info("[TWS] Applying linear detrending to both series")
                sim_anomaly = self._detrend_series(sim_anomaly)
                obs_matched = self._detrend_series(obs_matched)

            # Apply variability scaling if configured
            # This forces model variability to match observed (removes alpha penalty in KGE)
            if self.scale_to_obs:
                self.logger.info("[TWS] Scaling model variability to match observed")
                sim_anomaly = self._scale_to_obs_variability(sim_anomaly, obs_matched)

            # Calculate metrics
            metrics = self._calculate_performance_metrics(obs_matched, sim_anomaly)
            self.logger.debug(f"[TWS] Calculated metrics: {metrics}")
            return metrics

        except Exception as e:
            self.logger.error(f"[TWS] Error calculating TWS metrics: {str(e)}")
            import traceback
            self.logger.debug(f"[TWS] Traceback: {traceback.format_exc()}")
            return None

    def get_diagnostic_data(self, sim_dir: Path) -> Dict[str, Any]:
        """Export diagnostic data for visualization and detailed analysis.

        Extracts matched time series and intermediate results from TWS evaluation
        for use in plotting, trend analysis, component breakdown, and debugging.

        Diagnostic Data Exported:
            time: Common time index (monthly, for matched data)
            sim_tws: Simulated total storage (absolute, mm)
            sim_anomaly: Storage anomalies (detrended/processed version)
            sim_anomaly_raw: Storage anomalies (before signal processing)
            obs_anomaly: GRACE water storage anomalies (detrended/processed version)
            obs_anomaly_raw: GRACE anomalies (before signal processing)
            grace_column: Selected GRACE data column name
            grace_all_columns: All GRACE columns in overlap period (for comparison)
            detrend_applied: Boolean indicating if detrending was applied
            scale_applied: Boolean indicating if variability scaling was applied

        Use Cases:
            1. Visualization: Plot sim vs obs with/without signal processing
            2. Component analysis: Compare raw vs processed anomalies
            3. Trend analysis: Examine detrended vs raw series
            4. Data QC: Verify time alignment and overlap period
            5. Comparison: Compare multiple GRACE processing centers

        Processing Applied:
            - Temporal aggregation: Daily SUMMA to monthly means
            - Anomaly calculation: Relative to overlap period mean
            - Optional detrending: Remove linear trends
            - Optional scaling: Match model variability to observations

        Args:
            sim_dir: Directory containing SUMMA simulation output

        Returns:
            Dict[str, Any]: Diagnostic dictionary with keys:
            - time: np.datetime64 array of monthly times
            - sim_tws, sim_anomaly, etc.: np.ndarray of values
            - grace_all_columns: pd.DataFrame of GRACE data for overlap
            - detrend_applied, scale_applied: bool flags

        Notes:
            - Returns empty dict if simulation files or observations not found
            - Handles NaN values by creating valid_mask for clean overlap period
            - Preserves raw anomalies before processing for comparison
        """
        sim_files = self.get_simulation_files(sim_dir)
        if not sim_files:
            return {}

        sim_tws = self.extract_simulated_data(sim_files)

        obs_path = self.get_observed_data_path()
        if not obs_path.exists():
            return {}

        obs_df = pd.read_csv(obs_path, index_col=0, parse_dates=True)
        col = self._get_observed_data_column(obs_df.columns)
        if not col:
            return {}

        sim_monthly = sim_tws.resample('MS').mean()
        obs_monthly = obs_df[col].copy()

        common_index = sim_monthly.index.intersection(obs_monthly.index)
        valid_mask = ~(sim_monthly.loc[common_index].isna() | obs_monthly.loc[common_index].isna())

        sim_matched = sim_monthly.loc[common_index][valid_mask]
        obs_matched = obs_monthly.loc[common_index][valid_mask]

        baseline_mean = sim_matched.mean()
        sim_anomaly = sim_matched - baseline_mean

        # Store raw values for diagnostics
        sim_anomaly_raw = sim_anomaly.copy()
        obs_matched_raw = obs_matched.copy()

        # Apply detrending if configured
        if self.detrend:
            sim_anomaly = self._detrend_series(sim_anomaly)
            obs_matched = self._detrend_series(obs_matched)

        # Apply scaling if configured
        if self.scale_to_obs:
            sim_anomaly = self._scale_to_obs_variability(sim_anomaly, obs_matched)

        return {
            'time': sim_matched.index,
            'sim_tws': sim_matched.values,
            'sim_anomaly': sim_anomaly.values,
            'sim_anomaly_raw': sim_anomaly_raw.values,
            'obs_anomaly': obs_matched.values,
            'obs_anomaly_raw': obs_matched_raw.values,
            'grace_column': self.grace_column,
            'grace_all_columns': obs_df.loc[common_index][valid_mask],
            'detrend_applied': self.detrend,
            'scale_applied': self.scale_to_obs
        }
