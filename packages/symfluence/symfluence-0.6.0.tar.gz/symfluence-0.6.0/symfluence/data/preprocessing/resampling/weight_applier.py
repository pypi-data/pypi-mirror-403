"""
Remapping Weight Applier

Applies pre-computed EASYMORE weights to forcing files.
"""

import gc
import logging
import shutil
import time
import uuid
import netCDF4 as nc4
from pathlib import Path
from typing import Optional

from .weight_generator import _create_easymore_instance, _run_easmore_with_suppressed_output
from .file_validator import FileValidator

from symfluence.core.mixins import ConfigMixin


class RemappingWeightApplier(ConfigMixin):
    """
    Applies pre-computed EASYMORE remapping weights to forcing files.

    This is the fast operation that reads weights and applies them,
    unlike weight generation which is an expensive GIS operation.
    """

    def __init__(
        self,
        config: dict,
        project_dir: Path,
        output_dir: Path,
        dataset_handler,
        logger: logging.Logger = None
    ):
        """
        Initialize weight applier.

        Args:
            config: Configuration dictionary
            project_dir: Project directory path
            output_dir: Output directory for remapped files
            dataset_handler: Dataset-specific handler
            logger: Optional logger instance
        """
        # Import here to avoid circular imports

        from symfluence.core.config.models import SymfluenceConfig



        # Auto-convert dict to typed config for backward compatibility

        if isinstance(config, dict):

            try:

                self._config = SymfluenceConfig(**config)

            except Exception:

                # Fallback for partial configs (e.g., in tests)

                self._config = config

        else:

            self._config = config
        self.project_dir = project_dir
        self.output_dir = output_dir
        self.dataset_handler = dataset_handler
        self.logger = logger or logging.getLogger(__name__)
        self.file_validator = FileValidator(self.logger)

        # Cache for shapefile paths (set by weight generator)
        self.cached_target_shp_wgs84: Optional[Path] = None
        self.cached_hru_field: Optional[str] = None

    def set_shapefile_cache(
        self,
        target_shp_wgs84: Path,
        hru_field: str
    ) -> None:
        """
        Set cached shapefile paths from weight generation phase.

        Args:
            target_shp_wgs84: Path to WGS84 target shapefile
            hru_field: HRU ID field name
        """
        self.cached_target_shp_wgs84 = target_shp_wgs84
        self.cached_hru_field = hru_field
        self.logger.debug(f"Cached shapefile: {target_shp_wgs84}, HRU field: {hru_field}")

    def apply_weights(
        self,
        file: Path,
        remap_file: Path,
        output_file: Path,
        worker_id: Optional[int] = None
    ) -> bool:
        """
        Apply pre-computed remapping weights to a forcing file.

        Args:
            file: Path to forcing file to process
            remap_file: Path to pre-computed remapping weights CSV
            output_file: Path for output file
            worker_id: Optional worker ID for logging

        Returns:
            bool: True if successful, False otherwise
        """
        start_time = time.time()
        worker_str = f"Worker {worker_id}: " if worker_id is not None else ""

        try:
            # Check if output already exists
            if output_file.exists():
                file_size = output_file.stat().st_size
                if file_size > 1000:
                    self.logger.debug(f"{worker_str}Output already exists: {file.name}")
                    return True

            # Create unique temp directory
            unique_id = str(uuid.uuid4())[:8]
            temp_dir = self.project_dir / 'forcing' / f'temp_apply_{unique_id}'
            temp_dir.mkdir(parents=True, exist_ok=True)

            try:
                # Configure EASYMORE
                esmr = self._configure_easymore(file, remap_file, temp_dir, worker_str)
                if esmr is None:
                    return False

                # Apply the remapping
                self.logger.debug(f"{worker_str}Applying remapping weights to {file.name}")
                self.logger.debug(f"{worker_str}EASYMORE configured to remap variables: {esmr.var_names}")

                success, stdout, stderr = _run_easmore_with_suppressed_output(esmr, self.logger)

                # Log concerning patterns
                if 'no data' in stdout.lower() or 'no data' in stderr.lower():
                    self.logger.warning(f"{worker_str}EASYMORE reported 'no data' for {file.name}")
                if 'empty' in stdout.lower() or 'empty' in stderr.lower():
                    self.logger.warning(f"{worker_str}EASYMORE reported 'empty' for {file.name}")

                # Find and move output file
                success = self._move_output_file(temp_dir, output_file, file, worker_str)
                if not success:
                    return False

            finally:
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)

            # Verify output
            if output_file.exists():
                file_size = output_file.stat().st_size
                if file_size > 1000:
                    elapsed_time = time.time() - start_time
                    self.logger.debug(
                        f"{worker_str}Successfully processed {file.name} in {elapsed_time:.2f} seconds"
                    )
                    return True
                else:
                    self.logger.error(f"{worker_str}Output file corrupted (size: {file_size})")
                    return False
            else:
                self.logger.error(f"{worker_str}Output file not created: {output_file}")
                return False

        except Exception as e:
            self.logger.error(f"{worker_str}Error processing {file.name}: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

    def _configure_easymore(
        self,
        file: Path,
        remap_file: Path,
        temp_dir: Path,
        worker_str: str
    ):
        """Configure EASYMORE for weight application."""
        esmr = _create_easymore_instance()

        esmr.author_name = 'SUMMA public workflow scripts'
        esmr.case_name = f"{self._get_config_value(lambda: self.config.domain.name, dict_key='DOMAIN_NAME')}_{self._get_config_value(lambda: self.config.forcing.dataset, dict_key='FORCING_DATASET')}"
        esmr.correction_shp_lon = False

        # Use cached shapefile path
        if self.cached_target_shp_wgs84 is None or self.cached_hru_field is None:
            self.logger.error(f"{worker_str}Shapefile cache not available")
            return None

        esmr.target_shp = str(self.cached_target_shp_wgs84)
        esmr.target_shp_ID = self.cached_hru_field
        esmr.target_shp_lat = self._get_config_value(lambda: self.config.paths.catchment_lat, dict_key='CATCHMENT_SHP_LAT')
        esmr.target_shp_lon = self._get_config_value(lambda: self.config.paths.catchment_lon, dict_key='CATCHMENT_SHP_LON')

        # Coordinate variables
        var_lat, var_lon = self.dataset_handler.get_coordinate_names()

        # NetCDF file configuration
        esmr.source_nc = str(file)

        # Detect variables in this specific file
        available_vars = self._detect_file_variables(file, worker_str)
        if not available_vars:
            return None

        esmr.var_names = available_vars
        esmr.var_lat = var_lat
        esmr.var_lon = var_lon
        esmr.var_time = 'time'

        # Directories
        esmr.temp_dir = str(temp_dir) + '/'
        esmr.output_dir = str(temp_dir) + '/'

        # Output configuration
        esmr.remapped_dim_id = 'hru'
        esmr.remapped_var_id = 'hruId'
        esmr.format_list = ['f4']
        esmr.fill_value_list = ['-9999']

        # Point to pre-computed weights
        esmr.remap_csv = str(remap_file)

        # EASYMORE 2.0: Provide NetCDF version if available
        remap_nc = remap_file.with_suffix('.nc')
        case_name = f"{self._get_config_value(lambda: self.config.domain.name, dict_key='DOMAIN_NAME')}_{self._get_config_value(lambda: self.config.forcing.dataset, dict_key='FORCING_DATASET')}"
        attr_nc = remap_file.parent / f"{case_name}_attributes.nc"

        if remap_nc.exists():
            esmr.remap_nc = str(remap_nc)
            self.logger.debug(f"{worker_str}Using NetCDF remapping file: {remap_nc}")

        if attr_nc.exists():
            esmr.attr_nc = str(attr_nc)
            self.logger.debug(f"{worker_str}Using NetCDF attributes file: {attr_nc}")

        esmr.save_csv = False
        esmr.sort_ID = False
        esmr.save_temp_shp = False
        esmr.numcpu = 1

        return esmr

    def _detect_file_variables(self, file: Path, worker_str: str) -> list:
        """Detect SUMMA forcing variables in the file."""
        try:
            with nc4.Dataset(file, 'r') as ncid:
                all_summa_vars = [
                    'airpres', 'LWRadAtm', 'SWRadAtm', 'pptrate',
                    'airtemp', 'spechum', 'windspd'
                ]
                available_vars = [v for v in all_summa_vars if v in ncid.variables]

                if not available_vars:
                    self.logger.error(
                        f"{worker_str}No SUMMA variables found in {file.name}. "
                        f"Available variables: {list(ncid.variables.keys())}"
                    )
                    return []

                if 'time' not in ncid.dimensions:
                    self.logger.error(f"{worker_str}Input file {file.name} has no time dimension!")
                    return []

                self.logger.debug(
                    f"{worker_str}Detected {len(available_vars)} variables in {file.name}: {available_vars}"
                )
                return available_vars

        except Exception as e:
            self.logger.error(f"{worker_str}Error opening {file.name} for variable detection: {e}")
            return []
        finally:
            gc.collect()

    def _move_output_file(
        self,
        temp_dir: Path,
        output_file: Path,
        input_file: Path,
        worker_str: str
    ) -> bool:
        """Find and move output file from temp directory."""
        exclude_patterns = ['attributes', 'metadata', 'static', 'constants', 'params', 'remapping']
        all_temp_files = list(temp_dir.glob("*.nc"))
        temp_output_files = [
            f for f in all_temp_files
            if not any(pattern in f.name.lower() for pattern in exclude_patterns)
        ]

        if temp_output_files:
            temp_output = temp_output_files[0]

            # Validate before moving
            is_valid = self.file_validator.validate(temp_output, worker_str)

            if not is_valid:
                self.logger.error(
                    f"{worker_str}EASYMORE created invalid output for input {input_file.name}. "
                    f"Output file: {temp_output.name}."
                )
                all_created = list(temp_dir.glob("*"))
                self.logger.error(f"{worker_str}Files created by EASYMORE: {[f.name for f in all_created]}")
                return False

            # Ensure output directory exists
            self.output_dir.mkdir(parents=True, exist_ok=True)

            # Move to final location
            shutil.move(str(temp_output), str(output_file))
            self.logger.debug(f"{worker_str}Moved {temp_output.name} to {output_file.name}")
            return True

        elif output_file.exists():
            self.logger.debug(f"{worker_str}Output file already exists: {output_file.name}")
            return True
        else:
            self.logger.error(
                f"{worker_str}EASYMORE created NO valid output files for input {input_file.name}. "
                f"Files in temp dir: {[f.name for f in all_temp_files]}"
            )
            return False
