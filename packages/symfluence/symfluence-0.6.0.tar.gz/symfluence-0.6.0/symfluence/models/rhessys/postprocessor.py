"""
RHESSys Model Postprocessor

Handles output extraction, processing, and analysis for RHESSys model outputs.
"""
import logging
from pathlib import Path
from typing import Dict, Optional

import pandas as pd

from symfluence.models.registry import ModelRegistry
from symfluence.models.base import BaseModelPostProcessor

logger = logging.getLogger(__name__)


@ModelRegistry.register_postprocessor('RHESSys')
class RHESSysPostProcessor(BaseModelPostProcessor):
    """
    Postprocessor for RHESSys model outputs within SYMFLUENCE.

    Handles output extraction, processing, and analysis.
    Inherits common functionality from BaseModelPostProcessor.

    RHESSys outputs include:
    - Basin daily files (*_basin.daily): Basin-averaged water balance
    - Patch daily files (*_patch.daily): Patch-level outputs
    - Hillslope daily files (*_hillslope.daily): Hillslope-level outputs
    - Growth output files (*_grow_*.daily): Carbon/nitrogen cycling

    The primary output variable is streamflow (discharge) from the basin daily file.
    """

    def _get_model_name(self) -> str:
        """Return model name for RHESSys."""
        return "RHESSys"

    def _setup_model_specific_paths(self) -> None:
        """Setup RHESSys-specific paths."""
        self.rhessys_output_dir = self.sim_dir

    def extract_results(self) -> Dict[str, Path]:
        """
        Extract and process all RHESSys results.

        Returns:
            Dict[str, Path]: Paths to processed result files
        """
        self.logger.info("Extracting RHESSys results")

        results = {}
        try:
            # Process streamflow
            streamflow_path = self.extract_streamflow()
            if streamflow_path:
                results['streamflow'] = streamflow_path
                self.logger.info("Streamflow extracted successfully")

            # Compare fire perimeters if WMFire is enabled
            self._compare_fire_perimeters_if_enabled(results)

        except Exception as e:
            self.logger.error(f"Error extracting RHESSys results: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise

        return results

    def _compare_fire_perimeters_if_enabled(self, results: Dict[str, Path]) -> None:
        """Compare fire perimeters if WMFire is enabled and perimeter data exists."""
        try:
            # Check if WMFire is enabled
            wmfire_config = None
            if (hasattr(self.config, 'model') and
                hasattr(self.config.model, 'rhessys') and
                self.config.model.rhessys is not None):
                rhessys_config = self.config.model.rhessys
                if hasattr(rhessys_config, 'use_wmfire') and rhessys_config.use_wmfire:
                    wmfire_config = getattr(rhessys_config, 'wmfire', None)

            if wmfire_config is None:
                return

            # Check if perimeter data is configured
            has_perimeter = (
                (hasattr(wmfire_config, 'perimeter_shapefile') and wmfire_config.perimeter_shapefile) or
                (hasattr(wmfire_config, 'perimeter_dir') and wmfire_config.perimeter_dir)
            )

            if not has_perimeter:
                return

            self.logger.info("WMFire enabled with perimeter data - comparing fire perimeters")
            metrics = self.compare_fire_perimeters(visualize=True)

            if metrics:
                self.logger.info(f"Fire perimeter comparison: IoU={metrics.get('iou', 0):.3f}, "
                               f"Dice={metrics.get('dice', 0):.3f}")
                results['fire_perimeter_metrics'] = metrics

        except Exception as e:
            self.logger.warning(f"Could not compare fire perimeters: {e}")

    def extract_streamflow(self) -> Optional[Path]:
        """
        Extract simulated streamflow from RHESSys output and save to CSV.

        Reads from:
        1. rhessys_results.csv (if generated by runner)
        2. rhessys_basin.daily (standard RHESSys output)

        Returns:
            Optional[Path]: Path to the saved CSV file if successful, None otherwise
        """
        try:
            self.logger.info("Processing RHESSys streamflow results")

            # First try the pre-generated results CSV
            results_csv = self.sim_dir / "rhessys_results.csv"
            if results_csv.exists():
                self.logger.info(f"Reading from results CSV: {results_csv}")
                df = pd.read_csv(results_csv, index_col=0, parse_dates=True)
                if 'streamflow_cms' in df.columns:
                    q_sim = df['streamflow_cms']
                    return self.save_streamflow_to_results(
                        q_sim,
                        model_column_name='RHESSys_discharge_cms'
                    )

            # Try standard RHESSys basin daily output
            basin_daily = self.sim_dir / "rhessys_basin.daily"
            if basin_daily.exists():
                self.logger.info(f"Reading from basin daily file: {basin_daily}")
                q_sim = self._read_basin_daily(basin_daily)
                if q_sim is not None:
                    return self.save_streamflow_to_results(
                        q_sim,
                        model_column_name='RHESSys_discharge_cms'
                    )

            # Try any basin daily file
            basin_files = list(self.sim_dir.glob("*_basin.daily"))
            if basin_files:
                self.logger.info(f"Reading from: {basin_files[0]}")
                q_sim = self._read_basin_daily(basin_files[0])
                if q_sim is not None:
                    return self.save_streamflow_to_results(
                        q_sim,
                        model_column_name='RHESSys_discharge_cms'
                    )

            self.logger.error(f"No RHESSys output files found in {self.sim_dir}")
            return None

        except Exception as e:
            self.logger.error(f"Error extracting streamflow: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    def _get_basin_area_m2(self) -> float:
        """
        Get total basin area in m² from shapefile.

        Returns:
            Basin area in square meters
        """
        try:
            import geopandas as gpd

            # Try to find HRU shapefile
            catchment_dir = self.config.data_directory / "shapefiles" / "catchment"
            domain_name = self.config.domain_config.domain_name

            # Try different shapefile naming conventions
            shp_patterns = [
                f"{domain_name}_HRUs_GRUs.shp",
                f"{domain_name}_HRUs.shp",
                f"{domain_name}_catchment.shp",
                "*.shp"
            ]

            shp_path = None
            for pattern in shp_patterns:
                matches = list(catchment_dir.glob(pattern))
                if matches:
                    shp_path = matches[0]
                    break

            if shp_path is None:
                self.logger.warning("No catchment shapefile found, using default area")
                return 2.2e9  # Default ~2200 km²

            gdf = gpd.read_file(shp_path)

            # Reproject to UTM for accurate area calculation
            if gdf.crs and gdf.crs.is_geographic:
                gdf_proj = gdf.to_crs(gdf.estimate_utm_crs())
                total_area_m2 = gdf_proj.geometry.area.sum()
            else:
                total_area_m2 = gdf.geometry.area.sum()

            self.logger.info(f"Basin area: {total_area_m2/1e6:.2f} km²")
            return total_area_m2

        except Exception as e:
            self.logger.warning(f"Could not calculate basin area: {e}, using default")
            return 2.2e9  # Default ~2200 km²

    def _read_basin_daily(self, filepath: Path) -> Optional[pd.Series]:
        """
        Read RHESSys basin daily output file.

        RHESSys outputs streamflow in mm/day (depth over the basin).
        This method converts to m³/s using the basin area.

        Conversion: Q (m³/s) = Q (mm/day) × basin_area (m²) / (1000 × 86400)

        Args:
            filepath: Path to basin daily file

        Returns:
            Pandas Series of streamflow (cms) indexed by date
        """
        try:
            # RHESSys basin daily format varies, try different approaches
            df = pd.read_csv(filepath, sep=r'\s+', comment='#')

            # Construct date from year/month/day columns
            if all(col in df.columns for col in ['year', 'month', 'day']):
                df['date'] = pd.to_datetime(df[['year', 'month', 'day']])
                df.set_index('date', inplace=True)
            elif 'DATE' in df.columns:
                df['date'] = pd.to_datetime(df['DATE'])
                df.set_index('date', inplace=True)

            # Find streamflow column
            q_col = None
            for col in ['streamflow', 'Qout', 'discharge', 'streamflow_m3s', 'Q']:
                if col in df.columns:
                    q_col = col
                    break

            if q_col is None:
                # Look for any column containing 'stream' or 'flow'
                for col in df.columns:
                    if 'stream' in col.lower() or 'flow' in col.lower() or col.lower() == 'q':
                        q_col = col
                        break

            if q_col is None:
                self.logger.error(f"No streamflow column found in {filepath}")
                self.logger.info(f"Available columns: {list(df.columns)}")
                return None

            q_mm_day = df[q_col]

            self.logger.info(f"Raw streamflow from column '{q_col}' (mm/day): "
                           f"mean={q_mm_day.mean():.4f}, min={q_mm_day.min():.4f}, max={q_mm_day.max():.4f}")

            # Convert mm/day to m³/s
            # Q (m³/s) = Q (mm/day) × basin_area (m²) / (1000 mm/m × 86400 s/day)
            basin_area_m2 = self._get_basin_area_m2()
            conversion_factor = basin_area_m2 / (1000.0 * 86400.0)

            q_cms = q_mm_day * conversion_factor

            self.logger.info(f"Converted streamflow (m³/s): "
                           f"mean={q_cms.mean():.4f}, min={q_cms.min():.4f}, max={q_cms.max():.4f}")
            self.logger.info(f"Conversion factor: {conversion_factor:.6f} (basin area: {basin_area_m2/1e6:.2f} km²)")

            return q_cms

        except Exception as e:
            self.logger.error(f"Error reading basin daily file: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    def extract_water_balance(self) -> Optional[pd.DataFrame]:
        """
        Extract full water balance from RHESSys output.

        Returns:
            DataFrame with water balance components (P, ET, Q, storage change)
        """
        try:
            basin_files = list(self.sim_dir.glob("*_basin.daily"))
            if not basin_files:
                return None

            df = pd.read_csv(basin_files[0], sep=r'\s+')

            # Construct date
            if all(col in df.columns for col in ['year', 'month', 'day']):
                df['date'] = pd.to_datetime(df[['year', 'month', 'day']])
                df.set_index('date', inplace=True)

            # Extract available water balance components
            wb_cols = {}
            col_mapping = {
                'precip': ['precip', 'rain', 'precipitation', 'P'],
                'evap': ['evap', 'evaporation', 'ET', 'aet'],
                'trans': ['trans', 'transpiration'],
                'streamflow': ['streamflow', 'Qout', 'discharge', 'Q'],
                'baseflow': ['baseflow', 'Qbase', 'gw_Qout'],
                'snow': ['snowpack', 'swe', 'snow_water_equiv'],
            }

            for comp, candidates in col_mapping.items():
                for cand in candidates:
                    if cand in df.columns:
                        wb_cols[comp] = df[cand]
                        break

            if wb_cols:
                return pd.DataFrame(wb_cols)
            return None

        except Exception as e:
            self.logger.error(f"Error extracting water balance: {e}")
            return None

    def extract_litter_pools(self) -> Optional[Path]:
        """
        Extract and aggregate litter carbon pools from RHESSys output.

        Reads patch-level litter pools (litr1c-litr4c) and aggregates
        to basin level for fire fuel load calculations.

        Returns:
            Path to saved litter pools CSV if successful, None otherwise
        """
        try:
            # Find patch daily output files
            patch_files = list(self.sim_dir.glob("*_patch.daily"))
            grow_patch_files = list(self.sim_dir.glob("*_grow_patch.daily"))

            # Prefer grow_patch files (have carbon pools)
            output_file = None
            if grow_patch_files:
                output_file = grow_patch_files[0]
            elif patch_files:
                output_file = patch_files[0]
            else:
                self.logger.warning("No patch output files found for litter extraction")
                return None

            self.logger.info(f"Extracting litter pools from: {output_file}")

            # Read the file
            df = pd.read_csv(output_file, sep=r'\s+', skipinitialspace=True)

            # Check for litter columns
            litter_cols = ['litr1c', 'litr2c', 'litr3c', 'litr4c']
            available_cols = [col for col in litter_cols if col in df.columns]

            if not available_cols:
                self.logger.warning(f"No litter columns found. Available: {df.columns.tolist()}")
                return None

            self.logger.info(f"Found litter pools: {available_cols}")

            # Construct date
            if all(col in df.columns for col in ['year', 'month', 'day']):
                df['date'] = pd.to_datetime(df[['year', 'month', 'day']])

            # Aggregate to basin level (mean across patches per day)
            if 'date' in df.columns:
                basin_litter = df.groupby('date')[available_cols].mean()
            else:
                basin_litter = df[available_cols].mean()

            # Calculate total litter carbon
            if isinstance(basin_litter, pd.DataFrame):
                basin_litter['total_litrc'] = basin_litter[available_cols].sum(axis=1)
            else:
                basin_litter['total_litrc'] = sum(basin_litter[col] for col in available_cols)

            # Save to CSV
            output_path = self.sim_dir / "rhessys_litter_pools.csv"
            if isinstance(basin_litter, pd.DataFrame):
                basin_litter.to_csv(output_path)
            else:
                pd.DataFrame([basin_litter]).to_csv(output_path, index=False)

            self.logger.info(f"Litter pools saved to: {output_path}")
            return output_path

        except Exception as e:
            self.logger.error(f"Error extracting litter pools: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    def generate_fuel_grids(self) -> Optional[Path]:
        """
        Generate spatially variable fuel load grids from RHESSys litter output.

        Converts litter carbon pools to fuel loads using FuelCalculator
        and writes fuel grids for fire modeling.

        Returns:
            Path to saved fuel grid if successful, None otherwise
        """
        try:
            # Check WMFire configuration
            wmfire_config = None
            try:
                if hasattr(self.config, 'model') and hasattr(self.config.model, 'rhessys'):
                    if self.config.model.rhessys and hasattr(self.config.model.rhessys, 'wmfire'):
                        wmfire_config = self.config.model.rhessys.wmfire
            except AttributeError:
                pass

            # Only generate if fuel_source is rhessys_litter
            if wmfire_config is None or wmfire_config.fuel_source != 'rhessys_litter':
                self.logger.debug("Fuel source not set to rhessys_litter, skipping fuel grid generation")
                return None

            self.logger.info("Generating fuel grids from RHESSys litter output")

            # Import FuelCalculator
            try:
                from symfluence.models.wmfire import FuelCalculator
            except ImportError:
                self.logger.warning("FuelCalculator not available")
                return None

            # Extract litter pools
            litter_path = self.extract_litter_pools()
            if litter_path is None:
                return None

            # Read litter data
            litter_df = pd.read_csv(litter_path, index_col=0, parse_dates=True)

            # Initialize fuel calculator
            carbon_ratio = wmfire_config.carbon_to_fuel_ratio if wmfire_config else 2.0
            fuel_calc = FuelCalculator(carbon_to_fuel_ratio=carbon_ratio)

            # Calculate fuel load time series
            fuel_loads = []
            for idx, row in litter_df.iterrows():
                pools = {
                    'litr1c': row.get('litr1c', 0),
                    'litr2c': row.get('litr2c', 0),
                    'litr3c': row.get('litr3c', 0),
                    'litr4c': row.get('litr4c', 0),
                }
                fuel_load = fuel_calc.calculate_fuel_load(pools)
                fuel_loads.append({'date': idx, 'fuel_load_kg_m2': fuel_load})

            fuel_df = pd.DataFrame(fuel_loads).set_index('date')

            # Save fuel loads
            output_path = self.sim_dir / "rhessys_fuel_loads.csv"
            fuel_df.to_csv(output_path)

            self.logger.info(f"Fuel loads saved to: {output_path}")
            self.logger.info(f"Mean fuel load: {fuel_df['fuel_load_kg_m2'].mean():.3f} kg/m²")

            return output_path

        except Exception as e:
            self.logger.error(f"Error generating fuel grids: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

    def compare_fire_perimeters(self, visualize: bool = True) -> Optional[Dict]:
        """
        Compare simulated fire perimeter with observed perimeters.

        Loads observed perimeters from configuration and compares with
        simulated fire output from WMFire.

        Args:
            visualize: If True, create comparison map

        Returns:
            Dictionary with comparison metrics or None
        """
        try:
            # Check WMFire configuration
            wmfire_config = None
            try:
                if hasattr(self.config, 'model') and hasattr(self.config.model, 'rhessys'):
                    if self.config.model.rhessys and hasattr(self.config.model.rhessys, 'wmfire'):
                        wmfire_config = self.config.model.rhessys.wmfire
            except AttributeError:
                pass

            if wmfire_config is None:
                self.logger.debug("WMFire not configured, skipping perimeter comparison")
                return None

            # Import validator
            try:
                from symfluence.models.wmfire import FirePerimeterValidator
            except ImportError:
                self.logger.warning("FirePerimeterValidator not available")
                return None

            validator = FirePerimeterValidator(self.logger)

            # Find observed perimeter
            observed_gdf = None
            perimeter_source = None

            # Check for specific perimeter shapefile
            if wmfire_config.perimeter_shapefile:
                perimeter_path = Path(wmfire_config.perimeter_shapefile)
                if perimeter_path.exists():
                    observed_gdf = validator.load_perimeters(perimeter_path)
                    perimeter_source = perimeter_path
                else:
                    self.logger.warning(f"Perimeter shapefile not found: {perimeter_path}")

            # Check for perimeter directory
            if observed_gdf is None and wmfire_config.perimeter_dir:
                perimeter_dir = Path(wmfire_config.perimeter_dir)
                if perimeter_dir.exists():
                    observed_gdf = validator.load_perimeters(perimeter_dir)
                    perimeter_source = perimeter_dir

            # Check default location
            if observed_gdf is None:
                try:
                    domain_path = self.config.data_directory / f"domain_{self.config.domain.name}"
                    default_perim_dir = domain_path / 'shapefiles' / 'perimiters'
                    if default_perim_dir.exists():
                        observed_gdf = validator.load_perimeters(default_perim_dir)
                        perimeter_source = default_perim_dir
                except (FileNotFoundError, OSError, KeyError, ValueError):
                    pass

            if observed_gdf is None:
                self.logger.info("No observed fire perimeters found for comparison")
                return None

            self.logger.info(f"Loaded observed perimeters from: {perimeter_source}")

            # Look for simulated fire output
            # WMFire outputs fire spread grid files
            fire_output_dir = self.sim_dir
            simulated_files = list(fire_output_dir.glob("*fire*.shp")) + \
                            list(fire_output_dir.glob("*burn*.shp"))

            if not simulated_files:
                self.logger.warning("No simulated fire perimeter shapefile found")
                # Could also try to reconstruct from fire grid outputs
                return None

            import geopandas as gpd
            simulated_gdf = gpd.read_file(simulated_files[0])
            self.logger.info(f"Loaded simulated perimeter from: {simulated_files[0]}")

            # Compare perimeters
            metrics = validator.compare_perimeters(simulated_gdf, observed_gdf)

            # Create visualization
            if visualize and metrics:
                viz_path = self.sim_dir / "fire_perimeter_comparison.png"
                fire_name = perimeter_source.stem if hasattr(perimeter_source, 'stem') else 'Fire'
                validator.create_comparison_map(
                    simulated_gdf,
                    observed_gdf,
                    viz_path,
                    title=f"{fire_name} Perimeter Comparison"
                )

            # Save metrics
            if metrics:
                import json
                metrics_path = self.sim_dir / "fire_perimeter_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(metrics, f, indent=2)
                self.logger.info(f"Perimeter metrics saved to: {metrics_path}")

            return metrics

        except Exception as e:
            self.logger.error(f"Error comparing fire perimeters: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
