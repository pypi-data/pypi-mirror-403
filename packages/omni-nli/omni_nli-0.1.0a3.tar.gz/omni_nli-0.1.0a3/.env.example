# ==============================================================================
# Omni-NLI Configuration
# ==============================================================================

# Server settings
HOST=127.0.0.1
PORT=8000
LOG_LEVEL=INFO

# Enable debug mode (shows detailed error tracebacks; should be disabled in production)
DEBUG=False

# ==============================================================================
# Backend Configuration
# ==============================================================================

# Ollama (local LLM server)
# Default: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

# HuggingFace (optional; token is needed for gated models)
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=

# Optional: cache directory for downloaded HuggingFace models
# If empty, Hugging Face defaults are used (OS-agnostic; respects HF_HOME/HF_HUB_CACHE).
HF_CACHE_DIR=

# OpenRouter API (needed if using OpenRouter as backend)
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# ==============================================================================
# Model Configuration
# ==============================================================================

# Default backend provider to use for NLI evaluation
# Options: ollama, huggingface, openrouter
DEFAULT_BACKEND=huggingface

# Each provider must have a default model.
# These are used when the request doesn't specify a model.
# Examples:
#   - For Ollama: qwen3:8b, deepseek-r1:7b, phi4:latest
#   - For HuggingFace: microsoft/Phi-3.5-mini-instruct, Qwen/Qwen2.5-1.5B-Instruct
#   - For OpenRouter: openai/gpt-5-mini, openai/gpt-5.2, arcee-ai/trinity-large-preview:free
OLLAMA_DEFAULT_MODEL=qwen3:8b
HUGGINGFACE_DEFAULT_MODEL=microsoft/Phi-3.5-mini-instruct
OPENROUTER_DEFAULT_MODEL=openai/gpt-5-mini

# ==============================================================================
# Token Budget
# ==============================================================================

# Maximum tokens for reasoning/thinking traces
MAX_THINKING_TOKENS=4096

# Whether to return the raw thinking trace in the API response (default: False)
RETURN_THINKING_TRACE=False

# ==============================================================================
# Caching
# ==============================================================================

# Number of provider instances to cache.
# NOTE: This setting must be configured via environment variable, not CLI flag,
# because it is evaluated at module import time.
PROVIDER_CACHE_SIZE=8
