# ==============================================================================
# Omni-NLI Configuration
# ==============================================================================

# Server settings
HOST=127.0.0.1
PORT=8000
LOG_LEVEL=INFO

# ==============================================================================
# Backend Configuration
# ==============================================================================

# Ollama (local LLM server)
# Default: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

# HuggingFace (optional - for gated models)
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=

# OpenRouter API (optional - for frontier reasoning models)
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# ==============================================================================
# Model Configuration
# ==============================================================================

# Default backend provider to use for NLI evaluation
# Options: ollama, huggingface, openrouter
DEFAULT_BACKEND=ollama

# Default model to use for NLI evaluation
# This should be a model name compatible with your chosen backend
# Examples:
#   - For Ollama: llama3.2, mistral, qwen2.5
#   - For HuggingFace: meta-llama/Llama-3.2-3B-Instruct
#   - For OpenRouter: anthropic/claude-3.5-sonnet, openai/gpt-4o-mini
DEFAULT_MODEL=llama3.2

# ==============================================================================
# Token Budget
# ==============================================================================

# Maximum tokens for reasoning/thinking traces
MAX_THINKING_TOKENS=4096

# Maximum total tokens per request
MAX_TOTAL_TOKENS=8192

# ==============================================================================
# Caching
# ==============================================================================

# Number of provider instances to cache
PROVIDER_CACHE_SIZE=8
