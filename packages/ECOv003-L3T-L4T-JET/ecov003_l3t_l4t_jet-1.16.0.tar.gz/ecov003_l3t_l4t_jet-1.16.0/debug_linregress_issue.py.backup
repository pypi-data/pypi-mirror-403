"""
Debug Script for monte_carlo_sensitivity linregress AttributeError

ISSUE SUMMARY:
--------------
sensitivity_analysis() with use_joint_run=True fails with AttributeError when
using real ECOv002 Cal-Val data with process_JET_table. The error occurs at
line 276 in _sensitivity_analysis_joint when scipy.stats.linregress receives
arrays with insufficient variation.

ERROR STACK TRACE (from JET Sensitivity notebook):
--------------------------------------------------
File .../monte_carlo_sensitivity/sensitivity_analysis.py:64, in sensitivity_analysis
    return _sensitivity_analysis_joint(...)

File .../monte_carlo_sensitivity/sensitivity_analysis.py:276, in _sensitivity_analysis_joint
    r2 = scipy.stats.linregress(
        variable_perturbation_df.input_perturbation_std,
        variable_perturbation_df.output_perturbation_std
    )[2] ** 2

File .../scipy/stats/_stats_py.py:10524, in linregress
    ssxm, ssxym, _, ssym = np.cov(x, y, bias=1).flat

File .../numpy/lib/function_base.py:2724, in cov
    avg, w_sum = average(X, axis=1, weights=w, returned=True)

File .../numpy/lib/function_base.py:557, in average
    if scl.shape != avg_as_array.shape:
AttributeError: 'float' object has no attribute 'shape'

ROOT CAUSE:
-----------
When using the ACTUAL ECOv002 Cal-Val data with certain input variables
(e.g., AOT, COT, vapor_gccm, ozone_cm, elevation_m, canopy_height_meters),
the perturbations result in arrays that cause np.cov() to fail internally,
returning a float instead of an array for the scale variable.

REPRODUCTION:
-------------
This script uses the REAL ECOv002 Cal-Val data and process_JET_table function
to reproduce the exact error from the notebook. It's portable and can be moved
to the monte-carlo-sensitivity repository since it conditionally imports the
ECOv003 package and falls back to mock data if unavailable.

SUCCESS CRITERIA:
-----------------
The issue is FIXED when this script runs without errors and prints:
"✓ SUCCESS: All tests passed! The linregress issue is resolved."
"""

import sys
import os
import warnings
import numpy as np
import pandas as pd
import scipy.stats
from typing import Callable, Optional

# Try to import ECOv003 components for real data
HAVE_ECOV003 = False
try:
    from ECOv003_L3T_L4T_JET import process_JET_table, load_ECOv002_calval_JET_inputs
    HAVE_ECOV003 = True
except ImportError:
    pass


def create_test_input_data(n_samples: int = 100) -> pd.DataFrame:
    """
    Create test dataset that mimics ECOv002 Cal-Val structure.
    """
    np.random.seed(42)
    
    data = {
        'ST_C': np.random.uniform(20, 40, n_samples),
        'NDVI': np.random.uniform(0.1, 0.8, n_samples),
        'albedo': np.random.uniform(0.1, 0.3, n_samples),
        'Ta_C': np.random.uniform(15, 35, n_samples),
        'RH': np.random.uniform(0.3, 0.8, n_samples),
        'constant_var': np.ones(n_samples) * 42.0,  # Constant variable
        'low_variance': np.random.normal(100, 0.001, n_samples),  # Very low variance
    }
    
    return pd.DataFrame(data)


def insensitive_forward_process(input_df: pd.DataFrame) -> pd.DataFrame:
    """
    Simulates a forward process where outputs are insensitive to some inputs.
    This creates the condition where perturbations don't affect outputs,
    leading to constant standardized perturbation values.
    """
    output_df = input_df.copy()
    
    # Output that's sensitive to ST_C (works fine)
    output_df['sensitive_output'] = 200 + 5 * input_df['ST_C'] + np.random.normal(0, 10, len(input_df))
    
    # Output that's completely insensitive to inputs (causes the bug)
    # All perturbations result in same value -> zero variance -> linregress fails
    output_df['insensitive_output'] = np.full(len(input_df), 500.0)
    
    # Output with very low sensitivity (may cause numerical issues)
    output_df['low_sensitivity_output'] = 300 + 0.0001 * input_df['NDVI'] + np.random.normal(0, 0.0001, len(input_df))
    
    return output_df


def sensitive_forward_process(input_df: pd.DataFrame) -> pd.DataFrame:
    """
    Well-behaved forward process with clear input-output relationships.
    This should always work correctly.
    """
    output_df = input_df.copy()
    
    output_df['output_1'] = 200 + 5 * input_df['ST_C'] + np.random.normal(0, 10, len(input_df))
    output_df['output_2'] = 100 + 3 * input_df['NDVI'] + 2 * input_df['albedo'] + np.random.normal(0, 5, len(input_df))
    output_df['output_3'] = 50 + 1.5 * input_df['Ta_C'] + np.random.normal(0, 3, len(input_df))
    
    return output_df


def test_linregress_edge_cases():
    """
    Test that demonstrates the exact linregress failure conditions.
    """
    print("\n" + "="*70)
    print("TEST 1: Demonstrating linregress Edge Cases")
    print("="*70)
    
    print("\n1a. Well-behaved data (should work):")
    try:
        x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
        y = np.array([2.0, 4.0, 6.0, 8.0, 10.0])
        result = scipy.stats.linregress(x, y)
        print(f"   ✓ linregress succeeded: r^2 = {result.rvalue**2:.4f}")
    except Exception as e:
        print(f"   ✗ Unexpected failure: {e}")
        return False
    
    print("\n1b. Constant x array (triggers the bug):")
    try:
        x = np.array([5.0, 5.0, 5.0, 5.0, 5.0])  # No variation
        y = np.array([2.0, 4.0, 6.0, 8.0, 10.0])
        result = scipy.stats.linregress(x, y)
        print(f"   ✓ linregress handled constant x (unexpected success)")
    except AttributeError as e:
        print(f"   ✗ EXPECTED ERROR: AttributeError: {str(e)[:60]}...")
        print("      This is the bug we're trying to avoid!")
    except Exception as e:
        print(f"   ✗ Different error: {type(e).__name__}: {str(e)[:60]}...")
    
    print("\n1c. Constant y array (triggers the bug):")
    try:
        x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
        y = np.array([5.0, 5.0, 5.0, 5.0, 5.0])  # No variation
        result = scipy.stats.linregress(x, y)
        print(f"   ✓ linregress handled constant y (unexpected success)")
    except AttributeError as e:
        print(f"   ✗ EXPECTED ERROR: AttributeError: {str(e)[:60]}...")
        print("      This is the bug we're trying to avoid!")
    except Exception as e:
        print(f"   ✗ Different error: {type(e).__name__}: {str(e)[:60]}...")
    
    print("\n1d. Single data point:")
    try:
        x = np.array([1.0])
        y = np.array([2.0])
        result = scipy.stats.linregress(x, y)
        print(f"   ✓ linregress handled single point (unexpected success)")
    except Exception as e:
        print(f"   ✗ EXPECTED ERROR: {type(e).__name__}: {str(e)[:60]}...")
    
    print("\n1e. Near-constant arrays (may cause numerical issues):")
    try:
        x = np.array([1.0, 1.0000001, 1.0000002, 1.0000003, 1.0000004])
        y = np.array([2.0, 2.0000001, 2.0000002, 2.0000003, 2.0000004])
        result = scipy.stats.linregress(x, y)
        print(f"   ✓ linregress handled near-constant: r^2 = {result.rvalue**2:.4f}")
    except Exception as e:
        print(f"   ✗ Failed: {type(e).__name__}: {str(e)[:60]}...")
    
    return True


def test_sensitivity_analysis_with_real_ecov003_data():
    """
    Test using REAL ECOv002 Cal-Val data and process_JET_table function.
    This reproduces the EXACT error from the JET Sensitivity notebook.
    This SHOULD FAIL in the unfixed version and PASS after the fix.
    """
    print("\n" + "="*70)
    print("TEST 2: Reproducing sensitivity_analysis Issue with REAL Data")
    print("="*70)
    
    if not HAVE_ECOV003:
        print("\n⚠ SKIPPED: ECOv003_L3T_L4T_JET module not available")
        print("  This test requires the local ECOv003 package.")
        print("  Run this script from the ECOv003-L3T-L4T-JET directory.")
        return None  # Neither pass nor fail - just skipped
    
    try:
        from monte_carlo_sensitivity import sensitivity_analysis, divide_absolute_by_unperturbed
        
        # Load REAL ECOv002 Cal-Val data
        print("\n→ Loading ECOv002 Cal-Val data...")
        input_df = load_ECOv002_calval_JET_inputs()
        
        # Apply same filters as notebook
        input_df = input_df[input_df.ST_C <= 50]
        input_df = input_df[input_df.NDVI.apply(lambda NDVI: NDVI > 0.05)]
        
        print(f"✓ Loaded and filtered ECOv002 data: {len(input_df)} rows")
        
        # Use EXACT variables from notebook
        input_variables = [
            "ST_C",
            "NDVI",
            "albedo",
            "Ta_C",
            "RH",
            "AOT",
            "COT",
            "vapor_gccm",
            "ozone_cm",
            "elevation_m",
            "canopy_height_meters"
        ]
        
        output_variables = [
            "Rn_Wm2",
            "ET_daylight_kg",
            "GPP_inst_g_m2_s"
        ]
        
        print(f"  Input variables: {input_variables}")
        print(f"  Output variables: {output_variables}")
        
        # Test with REAL process_JET_table function
        print("\n→ Running sensitivity_analysis with REAL process_JET_table...")
        print("  (This should trigger the AttributeError in unfixed version)")
        
        perturbation_df, sensitivity_metrics_df = sensitivity_analysis(
            input_df=input_df,
            input_variables=input_variables,
            output_variables=output_variables,
            forward_process=process_JET_table,
            normalization_function=divide_absolute_by_unperturbed,
            use_joint_run=True  # Explicitly use joint run (triggers bug)
        )
        
        print("✓ SUCCESS: sensitivity_analysis completed with real data!")
        print(f"  Perturbation DataFrame shape: {perturbation_df.shape}")
        print(f"  Sensitivity metrics shape: {sensitivity_metrics_df.shape}")
        print("\n  Sample sensitivity metrics:")
        print(sensitivity_metrics_df.head(15).to_string(index=False))
        
        return True
        
    except AttributeError as e:
        if "'float' object has no attribute 'shape'" in str(e):
            print(f"\n✗ EXPECTED ERROR (unfixed): AttributeError")
            print(f"   {str(e)}")
            print("\n   This is the EXACT bug from the JET Sensitivity notebook!")
            print("   It occurs at line 276 in _sensitivity_analysis_joint")
            print("   when linregress receives arrays with insufficient variation.")
            return False
        else:
            print(f"\n✗ Different AttributeError: {e}")
            raise
    except ValueError as e:
        print(f"\n✗ ValueError: {e}")
        print("   (May be related to the same underlying issue)")
        return False
    except Exception as e:
        print(f"\n✗ UNEXPECTED ERROR: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        raise


def test_sensitivity_analysis_with_mock_insensitive_outputs():
    """
    Fallback test with mock data (used when ECOv003 package unavailable).
    This SHOULD FAIL in the current version and PASS after the fix.
    """
    print("\n" + "="*70)
    print("TEST 2b: Mock Test with Insensitive Outputs")
    print("="*70)
    
    try:
        from monte_carlo_sensitivity import sensitivity_analysis, divide_absolute_by_unperturbed
        
        input_df = create_test_input_data(n_samples=50)
        
        print(f"\n✓ Created test input DataFrame with {len(input_df)} rows")
        print(f"  Variables: {list(input_df.columns)}")
        
        # Test with insensitive forward process
        print("\n→ Running sensitivity_analysis with insensitive outputs...")
        print("  (This should trigger the AttributeError in unfixed version)")
        
        perturbation_df, sensitivity_metrics_df = sensitivity_analysis(
            input_df=input_df,
            input_variables=['ST_C', 'NDVI', 'constant_var'],
            output_variables=['sensitive_output', 'insensitive_output'],
            forward_process=insensitive_forward_process,
            normalization_function=divide_absolute_by_unperturbed,
            use_joint_run=True  # Explicitly use joint run
        )
        
        print("✓ SUCCESS: sensitivity_analysis handled insensitive outputs!")
        print(f"  Perturbation DataFrame shape: {perturbation_df.shape}")
        print(f"  Sensitivity metrics shape: {sensitivity_metrics_df.shape}" joint run
        )
        
        print("✓ SUCCESS: sensitivity_analysis handled insensitive outputs!")
        print(f"  Perturbation DataFrame shape: {perturbation_df.shape}")
        print(f"  Sensitivity metrics shape: {sensitivity_metrics_df.shape}")
        print("\n  Sensitivity metrics columns:")
        print(f"  {list(sensitivity_metrics_df.columns)}")
        print("\n  Sensitivity metrics preview:")
        print(sensitivity_metrics_df.to_string())
        
        # Verify that insensitive combinations are handled
        insensitive_rows = sensitivity_metrics_df[
            (sensitivity_metrics_df['output_variable'] == 'insensitive_output')
        ]
        if not insensitive_rows.empty:
            print(f"\n  ✓ Found {len(insensitive_rows)} insensitive output metrics")
            print("    (Should be handled gracefully with NaN or similar)")
        
        return True
        
    except AttributeError as e:
        if "'float' object has no attribute 'shape'" in str(e):
            print(f"✗ EXPECTED ERROR (unfixed): AttributeError")
            print(f"   {str(e)}")
            print("\n   This is the bug we're trying to fix!")
            print("   It occurs when linregress receives constant arrays.")
            return False
        else:
            print(f"✗ Different AttributeError: {e}")
            raise
    except ValueError as e:
        print(f"✗ ValueError: {e}")
        print("   (May be related to the same underlying issue)")
        return False
    except Exception as e:
        print(f"✗ UNEXPECTED ERROR: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        raise


def test_sensitivity_analysis_with_normal_outputs():
    """
    Baseline test to ensure normal operation still works.
    This should ALWAYS PASS (before and after the fix).
    """
    print("\n" + "="*70)
    print("TEST 3: Baseline Test with Normal Outputs")
    print("="*70)
    
    try:
        from monte_carlo_sensitivity import sensitivity_analysis, divide_absolute_by_unperturbed
        
        input_df = create_test_input_data(n_samples=100)
        
        print(f"\n✓ Created test input DataFrame")
        
        # Run with well-behaved forward process
        print("\n→ Running sensitivity_analysis with sensitive outputs...")
        
        perturbation_df, sensitivity_metrics_df = sensitivity_analysis(
            input_df=input_df,
            input_variables=['ST_C', 'NDVI', 'albedo'],
            output_variables=['output_1', 'output_2'],
            forward_process=sensitive_forward_process,
            normalization_function=divide_absolute_by_unperturbed,
            use_joint_run=True
        )
        
        print("✓ SUCCESS: Works correctly with sensitive outputs (as expected)")
        print(f"  Perturbation DataFrame shape: {perturbation_df.shape}")
        print(f"  Sensitivity metrics shape: {sensitivity_metrics_df.shape}")
        
        # Verify outputs are reasonable
        assert not sensitivity_metrics_df.empty, "Sensitivity metrics should not be empty"
        
        print("\n  Sample sensitivity metrics:")
        print(sensitivity_metrics_df.head(6).to_string())
        
        return True
        
    except Exception as e:
        print(f"✗ UNEXPECTED FAILURE: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return False


def demonstrate_fix():
    """
    Demonstrates the proper way to handle edge cases in linregress.
    Shows the SOLUTION that should be implemented in the package.
    """
    print("\n" + "="*70)
    print("SOLUTION DEMONSTRATION: Safe linregress Wrapper")
    print("="*70)
    
    def safe_linregress(x: np.ndarray, y: np.ndarray, min_variance: float = 1e-10) -> dict:
        """
        Safely compute linear regression with proper edge case handling.
        JET Sensitivity Analysis linregress AttributeError")
    print("#"*70)
    
    print("\nPURPOSE:")
    print("-" * 70)
    print("This script reproduces the AttributeError from the JET Sensitivity")
    print("notebook when running sensitivity_analysis() with use_joint_run=True.")
    print()
    print("The script prioritizes using REAL ECOv002 Cal-Val data and the actual")
    print("process_JET_table function to reproduce the exact error condition.")
    print()
    if HAVE_ECOV003:
        print("✓ ECOv003 package detected - will use REAL data")
    else:
        print("⚠ ECOv003 package not found - will use mock data")
        print("  For best results, run from ECOv003-L3T-L4T-JET directory")
    print()
    print("The script includes:")
    print("  1. Demonstration of linregress edge cases")
    print("  2. Reproduction using REAL ECOv002 data (if available)")
    print("  3. Baseline test with normal data")
    print("  4. Demonstration of the solution")
    print()
    print("SUCCESS CRITERIA: Test 2 must pass for the bug to be fixed.")
    print("-" * 70)
    
    try:
        import monte_carlo_sensitivity
        print(f"\n✓ monte_carlo_sensitivity package found")
        print(f"  Location: {monte_carlo_sensitivity.__file__}")
        if hasattr(monte_carlo_sensitivity, '__version__'):
            print(f"  Version: {monte_carlo_sensitivity.__version__}")
    except ImportError:
        print("\n✗ ERROR: monte_carlo_sensitivity package not found!")
        print("  Please install: pip install monte-carlo-sensitivity")
        sys.exit(1)
    
    # Check versions
    print(f"\n  numpy: {np.__version__}")
    print(f"  scipy: {scipy.__version__}")
    print(f"  pandas: {pd.__version__}")
    
    # Run tests
    results = []
    
    # Test 1: linregress edge cases
    test1_passed = test_linregress_edge_cases()
    results.append(('linregress edge cases', test1_passed))
    
    # Test 2: The bug reproduction (critical test) - try real data first
    print("\n" + "="*70)
    print("CRITICAL TEST: Reproducing the Notebook Error")
    print("="*70)
    
    if HAVE_ECOV003:
        print("\nAttempting with REAL ECOv002 data and process_JET_table...")
        test2_passed = test_sensitivity_analysis_with_real_ecov003_data()
        if test2_passed is None:  # Skipped
            print("\nFalling back to mock data test...")
            test2_passed = test_sensitivity_analysis_with_mock_insensitive_outputs()
    else:
        print("\nUsing mock data (ECOv003 package not available)...")
        test2_passed = test_sensitivity_analysis_with_mock_insensitive_outputs()
    
    results.append(('Real ECOv002 data reproduction' if HAVE_ECOV003 else 'Mock insensitive outputs', test2_passed))
    
    # Test 3: Baseline with normal data
    test3_passed = test_sensitivity_analysis_with_normal_outputs()
    results.append(('Normal outputs baseline', test3_passed))
    
    # Demonstrate solution
    solution_works = demonstrate_fix()
    results.append(('Solution demonstration', solution_works))
    
    # Print summary
    print("\n" + "#"*70)
    print("# TEST SUMMARY")
    print("#"*70)
    
    for test_name, passed in results:
        if passed is None:
            status = "⊘ SKIP"
        elif passed:
            status = "✓ PASS"
        else:
            status = "✗ FAIL"
        print(f"  {status}: {test_name}")
    
    # Overall status
    critical_test_passed = results[1][1]  # Test 2 is the critical one
    
    print("\n" + "="*70)
    if critical_test_passed:
        print("✓ SUCCESS: The linregress issue is FIXED!")
        print("="*70)
        print("\nThe monte_carlo_sensitivity package now properly handles:")
        print("  • Real ECOv002 Cal-Val data with process_JET_table")
        print("  • Variables with insufficient variation")
        print("  • Insensitive input-output combinations")
        print("  • Edge cases in correlation calculations")
        print("\nThe JET Sensitivity notebook should now run successfully.")
        return 0
    else:
        print("✗ FAILURE: The linregress issue still exists.")
        print("="*70)
        print("\nRECOMMENDED FIX for monte_carlo_sensitivity package:")
        print("-" * 70)
        print("\nIn sensitivity_analysis.py, around line 276, replace:")
        print()
        print("  r2 = scipy.stats.linregress(")
        print("      variable_perturbation_df.input_perturbation_std,")
        print("      variable_perturbation_df.output_perturbation_std")
        print("  )[2] ** 2")
        print()
        print("With safe version:")
        print()
        print("  # Extract arrays")
        print("  x = np.asarray(variable_perturbation_df.input_perturbation_std, dtype=np.float64)")
        print("  y = np.asarray(variable_perturbation_df.output_perturbation_std, dtype=np.float64)")
        print()
        print("  # Remove NaN/inf")
        print("  mask = np.isfinite(x) & np.isfinite(y)")
        print("  x, y = x[mask], y[mask]")
        print()
        print("  # Check for sufficient variation")
        print("  min_variance = 1e-10")
        print("  if len(x) < 2 or np.var(x) < min_variance or np.var(y) < min_variance:")
        print("      r2 = np.nan")
        print("      warnings.warn(f'Insufficient variation for {input_variable}->{output_variable}')")
        print("  else:")
        print("      try:")
        print("          r2 = scipy.stats.linregress(x, y)[2] ** 2")
        print("      except (AttributeError, ValueError):")
        print("          r2 = np.nan")
        print()
        print("Apply similar checks for pearsonr and other statistical functions.")
        print("-" * 70)
        print("\nThis script is portable - copy it to monte-carlo-sensitivity repo for debugging."un=True fails when scipy.stats.linregress receives data")
    print("with insufficient variation (constant arrays, single points, etc.).")
    print()
    print("The error occurs when model outputs are insensitive to input")
    print("perturbations, leading to constant standardized perturbation values.")
    print()
    print("The script includes:")
    print("  1. Demonstration of linregress edge cases")
    print("  2. Reproduction of the exact error")
    print("  3. Baseline test with normal data")
    print("  4. Demonstration of the solution")
    print()
    print("SUCCESS CRITERIA: Test 2 must pass for the bug to be fixed.")
    print("-" * 70)
    
    try:
        import monte_carlo_sensitivity
        print(f"\n✓ monte_carlo_sensitivity package found")
        print(f"  Location: {monte_carlo_sensitivity.__file__}")
    except ImportError:
        print("\n✗ ERROR: monte_carlo_sensitivity package not found!")
        print("  Please install the package or run this in the package directory.")
        sys.exit(1)
    
    # Run tests
    results = []
    
    # Test 1: linregress edge cases
    test1_passed = test_linregress_edge_cases()
    results.append(('linregress edge cases', test1_passed))
    
    # Test 2: The bug reproduction (critical test)
    test2_passed = test_sensitivity_analysis_with_insensitive_outputs()
    results.append(('Insensitive outputs handling', test2_passed))
    
    # Test 3: Baseline with normal data
    test3_passed = test_sensitivity_analysis_with_normal_outputs()
    results.append(('Normal outputs baseline', test3_passed))
    
    # Demonstrate solution
    solution_works = demonstrate_fix()
    results.append(('Solution demonstration', solution_works))
    
    # Print summary
    print("\n" + "#"*70)
    print("# TEST SUMMARY")
    print("#"*70)
    
    for test_name, passed in results:
        status = "✓ PASS" if passed else "✗ FAIL"
        print(f"  {status}: {test_name}")
    
    # Overall status
    critical_test_passed = results[1][1]  # Test 2 is the critical one
    
    print("\n" + "="*70)
    if critical_test_passed:
        print("✓ SUCCESS: All tests passed! The linregress issue is resolved.")
        print("="*70)
        print("\nThe monte_carlo_sensitivity package now properly handles:")
        print("  • Constant perturbation values")
        print("  • Insensitive model outputs")
        print("  • Insufficient data variation")
        print("  • Edge cases in correlation calculation")
        return 0
    else:
        print("✗ FAILURE: The linregress issue still exists.")
        print("="*70)
        print("\nRECOMMENDED FIX:")
        print("-" * 70)
        print("In sensitivity_analysis.py, around line 247, replace:")
        print()
        print("  r2 = scipy.stats.linregress(")
        print("      variable_perturbation_df.input_perturbation_std,")
        print("      variable_perturbation_df.output_perturbation_std")
        print("  )[2] ** 2")
        print()
        print("With safe version:")
        print()
        print("  # Extract arrays")
        print("  x = np.asarray(variable_perturbation_df.input_perturbation_std, dtype=np.float64)")
        print("  y = np.asarray(variable_perturbation_df.output_perturbation_std, dtype=np.float64)")
        print()
        print("  # Remove NaN/inf")
        print("  mask = np.isfinite(x) & np.isfinite(y)")
        print("  x, y = x[mask], y[mask]")
        print()
        print("  # Check for sufficient variation")
        print("  min_variance = 1e-10")
        print("  if len(x) < 2 or np.var(x) < min_variance or np.var(y) < min_variance:")
        print("      r2 = np.nan")
        print("      warnings.warn(")
        print("          f\"Insufficient variation for {input_variable}->{output_variable}\"")
        print("      )")
        print("  else:")
        print("      try:")
        print("          r2 = scipy.stats.linregress(x, y)[2] ** 2")
        print("      except Exception:")
        print("          r2 = np.nan")
        print()
        print("Similarly for pearsonr at line 203 and other statistical calculations.")
        print("-" * 70)
        return 1


if __name__ == "__main__":
    sys.exit(main())
