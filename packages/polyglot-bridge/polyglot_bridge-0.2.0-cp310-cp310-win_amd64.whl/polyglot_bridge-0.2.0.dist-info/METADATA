Metadata-Version: 2.4
Name: polyglot-bridge
Version: 0.2.0
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Rust
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Software Development :: Libraries
Summary: High-performance ML data pipeline accelerator - Rust-powered computational functions with seamless Python integration
Keywords: rust,python,performance,ml,data-pipeline,pyo3,ffi
Author-email: Septian Faturohman <farizseptian24@gmail.com>
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
Project-URL: Documentation, https://github.com/nirvagold/polyglot-bridge#readme
Project-URL: Homepage, https://github.com/nirvagold/polyglot-bridge
Project-URL: Issues, https://github.com/nirvagold/polyglot-bridge/issues
Project-URL: Repository, https://github.com/nirvagold/polyglot-bridge

# üî• Polyglot Bridge: The Inference King

**Stop wasting CPU cycles on Python-NumPy roundtrips.**

Polyglot Bridge is a specialized Rust-powered accelerator for **ML inference pipelines**. We don't replace NumPy‚Äîwe DOMINATE where it matters: **fused operations** and **parallel transforms**.

[![Rust](https://img.shields.io/badge/rust-1.93%2B-orange.svg)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## üéØ The Problem

Your ML inference pipeline is slow because:
- **NumPy can't fuse operations** - Every operation = Python roundtrip
- **Single-threaded transforms** - Wasting your CPU cores
- **Memory copies everywhere** - Killing performance

## ‚ö° The Solution

**Polyglot Bridge** eliminates Python overhead with:
- **Fused ML kernels** - Linear+ReLU+Norm in ONE Rust call
- **Automatic parallelization** - All cores, zero threading code
- **Zero-copy NumPy** - Direct memory access, no copies

---

## üèÜ Where We DOMINATE

| Operation | NumPy | Polyglot Bridge | **Speedup** |
|-----------|-------|-----------------|-------------|
| **Layer Normalization** | 7.98ms | 4.01ms | **1.99x faster** üî• |
| **Parallel Transform** (10M) | 39.23ms | 22.18ms | **1.77x faster** ‚ö° |
| **Softmax** (1000√ó1000) | 24.32ms | 17.56ms | **1.38x faster** üöÄ |

**Average: 1.71x faster** on production ML operations.

> Benchmarked on Windows, Python 3.14, 8-core CPU. See `tests/inference_benchmark.py` for reproduction.

---

## üì¶ Installation

```bash
pip install polyglot-bridge
```

**Requirements:** Python 3.8+, NumPy. No Rust installation needed.

---

## üöÄ Quickstart

### Fused Operations (THE KILLER FEATURE)

```python
import numpy as np
import polyglot_bridge

# Your neural network layer
input = np.random.randn(1000, 512).astype(np.float64)
weights = np.random.randn(512, 256).astype(np.float64)
bias = np.random.randn(256).astype(np.float64)

# ‚ùå NumPy way (slow - 3 Python roundtrips)
output = np.maximum(0, np.dot(input, weights) + bias)

# ‚úÖ Polyglot way (fast - 1 Rust call)
output = polyglot_bridge.fused_linear_relu(input, weights, bias)
```

### Parallel Transforms (THE CLEANER)

```python
# Process 10 million elements
data = np.random.rand(10_000_000)

# ‚ùå NumPy (single-threaded)
result = data * 2.5

# ‚úÖ Polyglot (all CPU cores)
result = polyglot_bridge.parallel_map_numpy(data, 2.5)  # 1.68x faster
```

### Layer Normalization (THE STABILIZER)

```python
# Transformer layer normalization
x = np.random.randn(512, 768).astype(np.float64)

# ‚ùå NumPy (multiple operations)
mean = x.mean(axis=1, keepdims=True)
var = x.var(axis=1, keepdims=True)
normalized = (x - mean) / np.sqrt(var + 1e-5)

# ‚úÖ Polyglot (fused, 2.11x faster)
normalized = polyglot_bridge.fused_layer_norm(x, 1e-5)
```

---

## üéØ When to Use Polyglot Bridge

### ‚úÖ USE IT FOR:
- **ML inference pipelines** - Fused ops eliminate Python overhead
- **Large-scale data preprocessing** - Parallel transforms dominate
- **Production workloads** - Where milliseconds = money
- **Transformer models** - Layer norm, softmax, dropout+residual

### ‚ùå DON'T USE IT FOR:
- **Pure matrix multiplication** - NumPy's BLAS is faster (20 years of optimization)
- **Small datasets** (<1000 elements) - FFI overhead dominates
- **DataFrame operations** - Use Polars instead

---

## üî• Complete API

### Fused Operations
```python
# Linear transformation
fused_linear(input, weights, bias) ‚Üí output

# Linear + ReLU activation
fused_linear_relu(input, weights, bias) ‚Üí output

# Layer normalization
fused_layer_norm(x, eps=1e-5) ‚Üí normalized

# Softmax activation
fused_softmax(logits) ‚Üí probabilities

# Dropout + residual connection
fused_dropout_add(x, residual, mask, scale) ‚Üí output
```

### Parallel Operations
```python
# Element-wise transform (all CPU cores)
parallel_map_numpy(array, factor) ‚Üí transformed

# Activations
relu_numpy(array) ‚Üí activated
sigmoid_numpy(array) ‚Üí activated
```

### Zero-Copy NumPy
```python
# Matrix multiplication (f32/f64)
matmul_numpy(a, b) ‚Üí result
matmul_numpy_f32(a, b) ‚Üí result  # 2x faster for ML

# Aggregations
sum_of_squares_numpy(array) ‚Üí scalar
```

Full API documentation: `polyglot_bridge.pyi` (IDE autocomplete supported)

---

## üí° Real-World Example

```python
import numpy as np
import polyglot_bridge

def inference_pipeline(input_data):
    """
    2-layer neural network with Polyglot Bridge
    
    Before: 45ms (NumPy separate ops)
    After: 27ms (Polyglot fused ops)
    Speedup: 1.67x faster
    """
    # Layer 1: Linear + ReLU (FUSED)
    hidden = polyglot_bridge.fused_linear_relu(
        input_data, weights1, bias1
    )
    
    # Layer 2: Linear + Softmax (FUSED)
    logits = polyglot_bridge.fused_linear(hidden, weights2, bias2)
    probs = polyglot_bridge.fused_softmax(logits)
    
    return probs

# Process batch
predictions = inference_pipeline(batch_data)
```

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Python Application             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Zero-cost FFI (PyO3)
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Fused ML Kernels (Rust)        ‚îÇ
‚îÇ  ‚Ä¢ Linear + ReLU + Norm             ‚îÇ
‚îÇ  ‚Ä¢ Single-pass computation          ‚îÇ
‚îÇ  ‚Ä¢ Zero intermediate allocations    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Parallel Engine (Rayon)        ‚îÇ
‚îÇ  ‚Ä¢ Automatic CPU core utilization   ‚îÇ
‚îÇ  ‚Ä¢ Work-stealing scheduler          ‚îÇ
‚îÇ  ‚Ä¢ Zero manual threading            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Tech Stack:**
- Rust Edition 2024 (latest language features)
- PyO3 0.27 (Python-Rust bindings)
- Rayon 1.10 (parallelization)
- ndarray 0.17 (numerical computing)

---

## üß™ Testing

```bash
# Run inference benchmark
python tests/inference_benchmark.py

# Run unit tests
pytest tests/

# Run Rust tests
cargo test
```

---

## üõ†Ô∏è Development

```bash
# Clone repository
git clone https://github.com/nirvagold/polyglot-bridge.git
cd polyglot-bridge

# Setup environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install maturin
pip install maturin

# Build and install
maturin develop --release

# Run tests
pytest tests/
cargo test
```

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file.

---

## üôè Acknowledgments

- [PyO3](https://pyo3.rs/) - Python-Rust interoperability
- [Rayon](https://github.com/rayon-rs/rayon) - Data parallelism
- [ndarray](https://github.com/rust-ndarray/ndarray) - N-dimensional arrays

---

<div align="center">

**We don't compete. We SPECIALIZE. We WIN.**

üî• **The Inference King** üî•

[‚≠ê Star this repo](https://github.com/nirvagold/polyglot-bridge) if Polyglot Bridge accelerates your ML pipeline!

</div>

