# nanoGPT Training Recipe
#
# Train a character-level GPT model on Shakespeare dataset using Andrej Karpathy's nanoGPT
# Repository: https://github.com/karpathy/nanoGPT
#
# Requirements:
# - Vast.ai account with configured API key
# - GPU instance (RTX 3090 or better recommended)
#
# Training time: ~15-30 minutes for character-level model
#
# Usage:
#   train run nanogpt-train

var REPO_URL = https://github.com/karpathy/nanoGPT.git
var WORKDIR = /workspace/nanoGPT
var MODEL_OUT = out-shakespeare
var MAX_ITERS = 5000
var BLOCK_SIZE = 256
var BATCH_SIZE = 64
var N_LAYER = 6
var N_HEAD = 6
var N_EMBD = 384
var LOCAL_OUTPUT = ./nanogpt-output

host gpu = placeholder

# Select a GPU instance from available Vast.ai machines
vast.pick @gpu num_gpus=1 min_gpu_ram=16

# Wait for instance to be ready
vast.wait timeout=5m

# Open tmux session
tmux.open @gpu as work

# Clone the nanoGPT repository
@work > git clone $REPO_URL $WORKDIR 2>/dev/null || (cd $WORKDIR && git pull)

# Install dependencies
@work > pip install torch numpy transformers datasets tiktoken wandb tqdm

# Prepare Shakespeare dataset
@work > cd $WORKDIR/data/shakespeare_char && python prepare.py

# Start training in background
@work > cd $WORKDIR && python train.py config/train_shakespeare_char.py --block_size=$BLOCK_SIZE --batch_size=$BATCH_SIZE --n_layer=$N_LAYER --n_head=$N_HEAD --n_embd=$N_EMBD --max_iters=$MAX_ITERS --out_dir=$MODEL_OUT &

# Wait for training completion
wait @work "step $MAX_ITERS:" timeout=2h

# Test the trained model with sample generation
@work > cd $WORKDIR && python sample.py --out_dir=$MODEL_OUT --num_samples=1 --max_new_tokens=200

# Download results
@gpu:$WORKDIR/$MODEL_OUT -> $LOCAL_OUTPUT/model
@gpu:$WORKDIR/train.log -> $LOCAL_OUTPUT/train.log

# Stop the instance to save costs
vast.stop
tmux.close @work
