# coding: utf-8

"""
 Shaped API

 Welcome to Shaped's API reference docs. These provide a  detailed view of the endpoints and CLI commands that Shaped provides and brief  explanations of how they should be used.  The Shaped API has four main endpoints:  **Tables** - Provision and manage batch and real-time data connectors.   **Views** - Configure SQL transformations and AI enrichment on your input data. Use SQL to combine multiple data sources or use an LLM to add new categories, extract specific attributes from descriptions, etc.  **Engines** - Deploy and manage your relevance engines. The Engine API exposes configuration for indexing logic, input datasets, externam embeddings, and more.  **Query** - Execute queries against your engines, to return data based on an input query or rerank an existing list. The Query API exposes the retrieve, filter, score, and ranking steps of the 4-stage ranking architecture.  The base URL for each endpoint is: `https://api.shaped.ai/v2`

 Generated by OpenAPI Generator (https://openapi-generator.tech)

 Do not edit the class manually.
""" # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class BigQueryTableConfig(BaseModel):
    """
    BigQueryTableConfig
    """ # noqa: E501
    schema_type: Optional[StrictStr] = Field(default='BIGQUERY', description="Schema type discriminator for BigQuery datasets.")
    name: StrictStr = Field(description="Unique identifier for the BigQuery dataset.")
    description: Optional[StrictStr] = None
    table: StrictStr = Field(description="BigQuery table path in format 'projectID.datasetID.tableID'.")
    columns: List[StrictStr] = Field(description="List of column names to sync from table.")
    datetime_key: StrictStr = Field(description="Column name used for incremental replication.")
    schedule_interval: Optional[StrictStr] = None
    filters: Optional[List[StrictStr]] = None
    start_datetime: Optional[StrictStr] = None
    batch_size: Optional[StrictInt] = None
    unique_keys: Optional[List[StrictStr]] = None
    __properties: ClassVar[List[str]] = ["schema_type", "name", "description", "table", "columns", "datetime_key", "schedule_interval", "filters", "start_datetime", "batch_size", "unique_keys"]

    @field_validator('schema_type')
    def schema_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['BIGQUERY']):
            raise ValueError("must be one of enum values ('BIGQUERY')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of BigQueryTableConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # set to None if description (nullable) is None
        # and model_fields_set contains the field
        if self.description is None and "description" in self.model_fields_set:
            _dict['description'] = None

        # set to None if schedule_interval (nullable) is None
        # and model_fields_set contains the field
        if self.schedule_interval is None and "schedule_interval" in self.model_fields_set:
            _dict['schedule_interval'] = None

        # set to None if filters (nullable) is None
        # and model_fields_set contains the field
        if self.filters is None and "filters" in self.model_fields_set:
            _dict['filters'] = None

        # set to None if start_datetime (nullable) is None
        # and model_fields_set contains the field
        if self.start_datetime is None and "start_datetime" in self.model_fields_set:
            _dict['start_datetime'] = None

        # set to None if batch_size (nullable) is None
        # and model_fields_set contains the field
        if self.batch_size is None and "batch_size" in self.model_fields_set:
            _dict['batch_size'] = None

        # set to None if unique_keys (nullable) is None
        # and model_fields_set contains the field
        if self.unique_keys is None and "unique_keys" in self.model_fields_set:
            _dict['unique_keys'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of BigQueryTableConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "schema_type": obj.get("schema_type") if obj.get("schema_type") is not None else 'BIGQUERY',
            "name": obj.get("name"),
            "description": obj.get("description"),
            "table": obj.get("table"),
            "columns": obj.get("columns"),
            "datetime_key": obj.get("datetime_key"),
            "schedule_interval": obj.get("schedule_interval"),
            "filters": obj.get("filters"),
            "start_datetime": obj.get("start_datetime"),
            "batch_size": obj.get("batch_size"),
            "unique_keys": obj.get("unique_keys")
        })
        return _obj


