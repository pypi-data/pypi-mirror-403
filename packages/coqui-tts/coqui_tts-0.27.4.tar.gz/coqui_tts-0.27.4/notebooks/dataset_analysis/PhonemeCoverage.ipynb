{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Jupyter Notebook for phoneme coverage analysis\n",
    "\n",
    "This jupyter notebook checks dataset configured in config.json for phoneme coverage.\n",
    "As mentioned here https://github.com/mozilla/TTS/wiki/Dataset#what-makes-a-good-dataset a good phoneme coverage is recommended.\n",
    "\n",
    "Most parameters will be taken from config.json file in mozilla tts repo so please ensure it's configured correctly for your dataset.\n",
    "This notebook used lots of existring code from the TTS repo to ensure future compatibility.\n",
    "\n",
    "Many thanks to Neil Stoker supporting me on this topic :-).\n",
    "\n",
    "I provide this notebook without any warranty but it's hopefully useful for your dataset analysis.\n",
    "\n",
    "Happy TTS'ing :-)\n",
    "\n",
    "Thorsten Müller\n",
    "\n",
    "* https://github.com/thorstenMueller/deep-learning-german-tts\n",
    "* https://discourse.mozilla.org/t/contributing-my-german-voice-for-tts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import operator\n",
    "from multiprocessing import Pool, cpu_count, set_start_method\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "set_start_method(\"fork\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.shared_configs import BaseTTSConfig\n",
    "\n",
    "dataset = BaseDatasetConfig(\n",
    "    dataset_name=\"ljspeech\",\n",
    "    formatter=\"ljspeech\",\n",
    "    path=\"../../tests/data/ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\",\n",
    ")\n",
    "CONFIG = BaseTTSConfig(\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    phonemizer=\"espeak\",\n",
    "    phoneme_language=\"en-us\",\n",
    "    use_phonemes=True,\n",
    "    datasets=[dataset],\n",
    ")\n",
    "\n",
    "CHARS_TO_REMOVE = \".,:!?'\"\n",
    "\n",
    "# Alternatively, you can load an existing config file:\n",
    "# from TTS.config import load_config\n",
    "# CONFIG = load_config(\"path/to/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load some properties from config.json\n",
    "CONFIG_METADATA = load_tts_samples(CONFIG.datasets, eval_split=False)[0]\n",
    "RUN_NAME = CONFIG.run_name + \" (\" + CONFIG.run_description + \")\"\n",
    "\n",
    "# Needed to convert text to phonemes and phonemes to ids\n",
    "tokenizer, config = TTSTokenizer.init_from_config(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print some debug information on loaded config values\n",
    "print(\" > Run name: \" + RUN_NAME)\n",
    "print(\" > Dataset files: \" + str(len(CONFIG_METADATA)))\n",
    "print(\" > Phoneme language: \" + CONFIG.phoneme_language)\n",
    "print(\" > Phonemizer: \" + CONFIG.phonemizer)\n",
    "print(\" > Used text cleaner: \" + CONFIG.text_cleaner)\n",
    "print(\" > Enable eos bos chars: \" + str(CONFIG.enable_eos_bos_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phoneme_from_sequence(text):\n",
    "    temp_list = []\n",
    "    if len(text[\"text\"]) > 0:\n",
    "        temp_text = text[\"text\"].rstrip(\"\\n\")\n",
    "        for rm_bad_chars in CHARS_TO_REMOVE:\n",
    "            temp_text = temp_text.replace(rm_bad_chars, \"\")\n",
    "        seq = tokenizer.text_to_ids(temp_text)\n",
    "        text = tokenizer.ids_to_text(seq)\n",
    "        text = text.replace(\" \", \"\")\n",
    "        temp_list.append(text)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get phonemes from metadata\n",
    "phonemes = []\n",
    "\n",
    "with Pool(cpu_count() - 1) as p:\n",
    "    phonemes = list(tqdm(p.imap(get_phoneme_from_sequence, CONFIG_METADATA), total=len(CONFIG_METADATA)))\n",
    "    phonemes = [i for sub in phonemes for i in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "phonemeString = s.join(phonemes)\n",
    "\n",
    "d = {}\n",
    "collections._count_elements(d, phonemeString)\n",
    "sorted_d = dict(sorted(d.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "# remove useless keys\n",
    "sorted_d.pop(\" \", None)\n",
    "sorted_d.pop(\"ˈ\", None)\n",
    "\n",
    "phonemesSum = len(phonemeString.replace(\" \", \"\"))\n",
    "\n",
    "print(\"Dataset contains \" + str(len(sorted_d)) + \" different ipa phonemes.\")\n",
    "print(\"Dataset consists of \" + str(phonemesSum) + \" phonemes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"5 rarest phonemes\")\n",
    "\n",
    "rareList = dict(sorted(sorted_d.items(), key=operator.itemgetter(1), reverse=False)[:5])\n",
    "for key, value in rareList.items():\n",
    "    print(key + \" --> \" + str(value) + \" occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# create plot from analysis result\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for key, value in sorted_d.items():\n",
    "    x.append(key)\n",
    "    y.append(value)\n",
    "\n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.title(\"Phoneme coverage for \" + RUN_NAME, fontsize=50)\n",
    "plt.xticks(fontsize=50)\n",
    "plt.yticks(fontsize=50)\n",
    "plt.barh(x, y, align=\"center\", alpha=1.0)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"phoneme\", fontsize=50)\n",
    "plt.xlabel(\"occurrences\", fontsize=50)\n",
    "\n",
    "for i, v in enumerate(y):\n",
    "    plt.text(v + 2, i - 0.2, str(v), fontsize=20)\n",
    "    plt.text(v + 2, i + 0.2, \"(\" + str(round(100 / phonemesSum * v, 2)) + \"%)\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
