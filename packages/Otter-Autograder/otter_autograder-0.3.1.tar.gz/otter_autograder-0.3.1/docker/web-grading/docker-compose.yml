services:
  ollama:
    image: ollama/ollama:latest
    container_name: autograder-ollama
    # Only start this service if using local Ollama (not host)
    # To use host Ollama, set OLLAMA_HOST=http://host.docker.internal:11434 in .env
    profiles:
      - local-ollama
    # No external ports needed - web service communicates via internal docker network
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        # Install curl for healthcheck
        apt-get update >/dev/null 2>&1 && apt-get install -y curl >/dev/null 2>&1

        # Start Ollama in background
        /bin/ollama serve &
        OLLAMA_PID=$$!

        # Wait for Ollama to be ready
        echo "Waiting for Ollama to start..."
        for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15; do
          if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Ollama is ready!"
            break
          fi
          sleep 2
        done

        # Pull model in background (don't block startup)
        (
          echo "Checking for model qwen3-vl:2b..."
          if ! /bin/ollama list | grep -q "qwen3-vl:2b"; then
            echo "Pulling model qwen3-vl:2b in background..."
            /bin/ollama pull qwen3-vl:2b
            echo "Model pulled successfully!"
          else
            echo "Model qwen3-vl:2b already present"
          fi
        ) &

        # Keep Ollama running
        wait $$OLLAMA_PID
    volumes:
      # Persistent storage for models
      - ollama-data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/bin/sh", "-c", "curl -f http://localhost:11434/api/tags && /bin/ollama list | grep -q 'qwen3-vl:2b'"]
      interval: 10s
      timeout: 5s
      start_period: 300s  # 5 minutes for initial model pull
      retries: 3
    # Uncomment for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  web:
    build:
      context: ../../  # Build from Autograder directory
      dockerfile: docker/web-grading/Dockerfile
    image: autograder-web-grading:latest
    container_name: autograder-web-grading
    ports:
      - "8765:8765"
    env_file:
      - .env  # Load environment variables from .env file
    environment:
      # Override database path to use Docker volume
      - GRADING_DB_PATH=/data/grading.db
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app/web_grading
      # Ollama configuration
      # Use host.docker.internal:11434 to connect to Ollama on host machine
      # Or ollama:11434 to use the containerized Ollama service
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen3-vl:2b}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-300}
    volumes:
      # Persistent storage for database
      - grading-data:/data
      # Mount code for live development (no rebuild needed for code changes)
      - ../../Autograder:/app/Autograder
      - ../../WebUI:/app/web_grading
      # Optional: Mount config files (uncomment if needed)
      # - ./config:/app/config:ro
    restart: unless-stopped
    # Note: depends_on removed to support both local and host Ollama
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8765/api/health', timeout=2)"]
      interval: 30s
      timeout: 3s
      start_period: 10s
      retries: 3

volumes:
  grading-data:
    driver: local
    # Optional: Specify custom location
    # driver_opts:
    #   type: none
    #   device: /path/to/your/data
    #   o: bind
  ollama-data:
    driver: local
