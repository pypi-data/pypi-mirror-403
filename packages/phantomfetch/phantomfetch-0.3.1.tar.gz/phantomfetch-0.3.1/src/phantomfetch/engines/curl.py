import random
import time
from typing import Any, ClassVar, cast

from curl_cffi.requests import AsyncSession, RequestsError
from loguru import logger

from ..telemetry import get_tracer
from ..types import Cookie, Proxy, Response

tracer = get_tracer()


class CurlEngine:
    """
    curl_cffi based HTTP engine with anti-detection and retry logic.
    """

    BROWSER_VERSIONS: ClassVar[list[str]] = [
        "chrome124",
        "chrome120",
        "safari17_0",
        "safari15_3",
    ]

    USER_AGENTS: ClassVar[dict[str, list[str]]] = {}  # Deprecated: let curl_cffi handle it

    RETRY_STATUS_CODES: ClassVar[set[int]] = {429, 500, 502, 503, 504}

    def __init__(
        self,
        timeout: float = 30.0,
        max_retries: int = 3,
        retry_backoff_base: float = 2.0,
        verify_ssl: bool = True,
    ):
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_backoff_base = retry_backoff_base
        self.verify_ssl = verify_ssl

    async def fetch(
        self,
        url: str,
        proxy: Proxy | None = None,
        headers: dict[str, str] | None = None,
        cookies: dict[str, str] | list[Cookie] | None = None,
        timeout: float | None = None,
        max_retries: int | None = None,
        retry_on: set[int] | None = None,
        retry_backoff: float | None = None,
        referer: str | None = None,
        allow_redirects: bool = True,
    ) -> Response:
        """
        Fetch URL with automatic retry and anti-detection.
        """
        timeout = timeout or self.timeout
        max_retries = max_retries or self.max_retries
        retry_status_codes = retry_on or self.RETRY_STATUS_CODES
        backoff_base = retry_backoff or self.retry_backoff_base

        start = time.perf_counter()
        last_error: str | None = None
        last_status: int = 0
        last_body: bytes = b""
        last_headers: dict[str, str] = {}

        for attempt in range(max_retries):
            # Select modern impersonation target
            impersonate = random.choice(self.BROWSER_VERSIONS)
            
            # Simplified headers: rely on curl_cffi defaults for the impersonated browser
            # Only set Referer or explicit overrides.
            request_headers = {}
            if referer:
                request_headers["Referer"] = referer

            if headers:
                request_headers.update(headers)

            with tracer.start_as_current_span("phantomfetch.engine.curl") as span:
                span.set_attribute("url.full", url)
                if proxy:
                    span.set_attribute("phantomfetch.proxy", proxy.url)
                    if proxy.vendor:
                        span.set_attribute("phantomfetch.proxy.vendor", proxy.vendor)
                    if proxy.proxy_type:
                        span.set_attribute("phantomfetch.proxy.type", proxy.proxy_type)
                    if proxy.location:
                        span.set_attribute("phantomfetch.proxy.location", proxy.location)
                    if proxy.provider:
                        span.set_attribute("phantomfetch.proxy.provider", proxy.provider)

                span.set_attribute(
                    "phantomfetch.curl.impersonate", impersonate
                )
                
                # Initialize curl_kwargs
                curl_kwargs = {
                    "impersonate": impersonate,
                    "timeout": timeout or self.timeout,
                    "allow_redirects": allow_redirects,
                }

                # Add headers to kwargs
                curl_kwargs["headers"] = request_headers

                if cookies:
                    if isinstance(cookies, dict):
                        curl_kwargs["cookies"] = cookies
                    elif isinstance(cookies, list):
                        curl_kwargs["cookies"] = {c.name: c.value for c in cookies}

                if proxy:
                    curl_kwargs["proxies"] = cast(
                        Any, {"http": proxy.url, "https": proxy.url}
                    )

                logger.debug(f"[curl] Attempt {attempt + 1}/{max_retries}: {url} (impersonate={impersonate})")

                try:
                    async with AsyncSession() as session:
                        resp = await session.get(url, **curl_kwargs)
                        # ... (success/error handling kept logically same, snipped for brevity in replacement)
                        # Wait, I need to include the body of the loop to ensure logic is preserved.
                        
                        last_status = resp.status_code
                        last_body = resp.content
                        last_headers = dict(cast(Any, resp.headers))

                        final_cookies = []
                        for name, value in session.cookies.items():
                            final_cookies.append(Cookie(name=name, value=value))

                        span.set_attribute("http.status_code", resp.status_code)

                        if 200 <= resp.status_code < 400:
                            logger.debug(f"[curl] Success: {url} [{resp.status_code}]")
                            return Response(
                                url=str(resp.url),
                                status=resp.status_code,
                                body=resp.content,
                                headers=dict(cast(Any, resp.headers)),
                                engine="curl",
                                elapsed=time.perf_counter() - start,
                                proxy_used=proxy.url if proxy else None,
                                cookies=final_cookies,
                            )

                        if resp.status_code in retry_status_codes:
                            last_error = f"HTTP {resp.status_code}"
                            logger.warning(
                                f"[curl] Retryable {resp.status_code}, attempt {attempt + 1}"
                            )
                            if attempt < max_retries - 1:
                                await self._backoff(attempt, backoff_base)
                                continue

                        return Response(
                            url=str(resp.url),
                            status=resp.status_code,
                            body=resp.content,
                            headers=dict(cast(Any, resp.headers)),
                            engine="curl",
                            elapsed=time.perf_counter() - start,
                            proxy_used=proxy.url if proxy else None,
                            error=f"HTTP {resp.status_code}",
                            cookies=final_cookies,
                        )

                except RequestsError as e:
                    last_error = str(e)
                    logger.warning(f"[curl] RequestsError: {e}, attempt {attempt + 1}")
                    span.record_exception(e)
                    if attempt < max_retries - 1:
                        await self._backoff(attempt, backoff_base)
                        continue
                except Exception as e:
                    logger.error(f"[curl] Error: {e}")
                    span.record_exception(e)
                    return Response(
                        url=url,
                        status=0,
                        body=b"",
                        engine="curl",
                        elapsed=time.perf_counter() - start,
                        proxy_used=proxy.url if proxy else None,
                        error=str(e),
                    )
        
        return Response(
            url=url,
            status=last_status,
            body=last_body,
            headers=last_headers,
            engine="curl",
            elapsed=time.perf_counter() - start,
            proxy_used=proxy.url if proxy else None,
            error=last_error or "Max retries exhausted",
        )

    # Removed _get_browser_config and _build_headers as they are now obsolete precedence logic
    # kept purely for backward compat if needed? No, we deleted usage.


    async def _backoff(self, attempt: int, backoff_base: float | None = None) -> None:
        """Exponential backoff with jitter"""
        import asyncio

        base = backoff_base or self.retry_backoff_base
        wait = base**attempt * (0.5 + random.random())
        logger.debug(f"[curl] Backoff: {wait:.2f}s")
        await asyncio.sleep(wait)
