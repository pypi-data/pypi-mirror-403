"""
Linear Discriminant Analysis.
"""

from __future__ import annotations

from typing import Optional, Literal

import numpy as np
from scipy import linalg

from nalyst.core.foundation import BaseLearner, ClassifierMixin, TransformerMixin
from nalyst.core.validation import check_array, check_is_trained


class LinearDiscriminantAnalysis(ClassifierMixin, TransformerMixin, BaseLearner):
    """
    Linear Discriminant Analysis.

    A classifier with a linear decision boundary, generated by fitting
    class conditional densities and using Bayes' rule.

    Parameters
    ----------
    solver : {"svd", "lsqr", "eigen"}, default="svd"
        Solver to use.
    shrinkage : float or "auto", optional
        Shrinkage parameter.
    priors : array-like of shape (n_classes,), optional
        Class priors.
    n_components : int, optional
        Number of components for dimensionality reduction.
    store_covariance : bool, default=False
        Store covariance matrix.
    tol : float, default=1e-4
        Threshold for rank estimation.

    Attributes
    ----------
    coef_ : ndarray of shape (n_features,) or (n_classes, n_features)
        Coefficients.
    intercept_ : ndarray of shape (n_classes,)
        Intercept term.
    covariance_ : ndarray of shape (n_features, n_features)
        Covariance matrix.
    means_ : ndarray of shape (n_classes, n_features)
        Class means.
    priors_ : ndarray of shape (n_classes,)
        Class priors.
    scalings_ : ndarray of shape (n_features, rank)
        Scaling of features.
    classes_ : ndarray of shape (n_classes,)
        Class labels.
    n_features_in_ : int
        Number of features.

    Examples
    --------
    >>> from nalyst.discriminant import LinearDiscriminantAnalysis
    >>> lda = LinearDiscriminantAnalysis()
    >>> lda.train(X, y)
    >>> predictions = lda.infer(X_test)
    >>> X_reduced = lda.apply(X)
    """

    def __init__(
        self,
        solver: Literal["svd", "lsqr", "eigen"] = "svd",
        shrinkage: Optional[float] = None,
        priors: Optional[np.ndarray] = None,
        n_components: Optional[int] = None,
        store_covariance: bool = False,
        tol: float = 1e-4,
    ):
        self.solver = solver
        self.shrinkage = shrinkage
        self.priors = priors
        self.n_components = n_components
        self.store_covariance = store_covariance
        self.tol = tol

    def train(self, X: np.ndarray, y: np.ndarray) -> "LinearDiscriminantAnalysis":
        """
        Fit LDA model.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Training data.
        y : ndarray of shape (n_samples,)
            Target values.

        Returns
        -------
        self : LinearDiscriminantAnalysis
            Fitted estimator.
        """
        X = check_array(X)
        y = np.asarray(y)

        self.classes_ = np.unique(y)
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)
        self.n_features_in_ = n_features

        # Compute class priors
        if self.priors is None:
            self.priors_ = np.array([np.sum(y == c) / n_samples for c in self.classes_])
        else:
            self.priors_ = np.asarray(self.priors)

        # Compute class means
        self.means_ = np.zeros((n_classes, n_features))
        for i, c in enumerate(self.classes_):
            self.means_[i] = np.mean(X[y == c], axis=0)

        # Compute overall mean
        self.xbar_ = np.mean(X, axis=0)

        if self.solver == "svd":
            self._train_svd(X, y)
        elif self.solver == "eigen":
            self._train_eigen(X, y)
        else:
            self._train_lsqr(X, y)

        return self

    def _train_svd(self, X: np.ndarray, y: np.ndarray):
        """Fit using SVD solver."""
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)

        # Center data
        Xc = X - self.xbar_

        # SVD
        U, S, Vt = linalg.svd(Xc, full_matrices=False)

        # Determine rank
        rank = np.sum(S > self.tol * S[0])

        # Compute scaling
        self.scalings_ = Vt[:rank].T / S[:rank]

        # Project class means
        fac = 1.0 / (n_samples - n_classes)
        X_class_means = np.sqrt(n_samples * self.priors_)[:, np.newaxis] * self.means_
        X_class_means -= self.xbar_

        # SVD of between-class scatter
        _, S2, Vt2 = linalg.svd(X_class_means @ self.scalings_, full_matrices=False)

        n_components = min(n_classes - 1, n_features, self.n_components or n_classes - 1)

        self.scalings_ = self.scalings_ @ Vt2[:n_components].T

        # Compute coefficients
        self._compute_coef()

    def _train_eigen(self, X: np.ndarray, y: np.ndarray):
        """Fit using eigenvalue decomposition."""
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)

        # Compute within-class scatter
        Sw = np.zeros((n_features, n_features))
        for i, c in enumerate(self.classes_):
            Xi = X[y == c]
            Xi_centered = Xi - self.means_[i]
            Sw += Xi_centered.T @ Xi_centered

        # Regularization
        if self.shrinkage is not None:
            if self.shrinkage == "auto":
                shrinkage = self._estimate_shrinkage(X, y)
            else:
                shrinkage = self.shrinkage
            Sw = (1 - shrinkage) * Sw + shrinkage * np.trace(Sw) / n_features * np.eye(n_features)

        # Compute between-class scatter
        Sb = np.zeros((n_features, n_features))
        for i, c in enumerate(self.classes_):
            ni = np.sum(y == c)
            diff = self.means_[i] - self.xbar_
            Sb += ni * np.outer(diff, diff)

        if self.store_covariance:
            self.covariance_ = Sw / (n_samples - n_classes)

        # Generalized eigenvalue problem
        eigenvalues, eigenvectors = linalg.eigh(Sb, Sw)

        # Sort by eigenvalue descending
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        n_components = min(n_classes - 1, n_features, self.n_components or n_classes - 1)
        self.scalings_ = eigenvectors[:, :n_components]

        self._compute_coef()

    def _train_lsqr(self, X: np.ndarray, y: np.ndarray):
        """Fit using least squares solver."""
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)

        # Compute within-class covariance
        Sw = np.zeros((n_features, n_features))
        for i, c in enumerate(self.classes_):
            Xi = X[y == c]
            Xi_centered = Xi - self.means_[i]
            Sw += Xi_centered.T @ Xi_centered

        Sw /= (n_samples - n_classes)

        if self.store_covariance:
            self.covariance_ = Sw

        # Regularization
        if self.shrinkage is not None:
            if self.shrinkage == "auto":
                shrinkage = self._estimate_shrinkage(X, y)
            else:
                shrinkage = self.shrinkage
            Sw = (1 - shrinkage) * Sw + shrinkage * np.trace(Sw) / n_features * np.eye(n_features)

        # Solve
        self.coef_ = linalg.solve(Sw, (self.means_ - self.xbar_).T).T
        self.intercept_ = -0.5 * np.sum(self.means_ * self.coef_, axis=1) + np.log(self.priors_)

    def _compute_coef(self):
        """Compute coefficients from scalings."""
        n_classes = len(self.classes_)

        # Project means
        means_scaled = self.means_ @ self.scalings_

        # Coefficients
        self.coef_ = self.scalings_ @ means_scaled.T
        self.coef_ = self.coef_.T

        # Intercept
        self.intercept_ = -0.5 * np.sum(means_scaled ** 2, axis=1) + np.log(self.priors_)

    def _estimate_shrinkage(self, X: np.ndarray, y: np.ndarray) -> float:
        """Estimate optimal shrinkage using Ledoit-Wolf."""
        n_samples, n_features = X.shape

        # Compute sample covariance
        X_centered = X - np.mean(X, axis=0)
        S = X_centered.T @ X_centered / n_samples

        mu = np.trace(S) / n_features

        # Squared Frobenius norm
        delta = np.sum((S - mu * np.eye(n_features)) ** 2)

        # Estimate shrinkage
        shrinkage = min(1, (delta + mu ** 2) / (n_samples * delta + 1e-10))

        return shrinkage

    def infer(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class labels.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
            Predicted class labels.
        """
        scores = self.decision_function(X)
        if len(self.classes_) == 2:
            return self.classes_[(scores > 0).astype(int)]
        return self.classes_[np.argmax(scores, axis=1)]

    def infer_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Estimate probability.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        proba : ndarray of shape (n_samples, n_classes)
            Probability of each class.
        """
        from scipy.special import softmax

        scores = self.decision_function(X)
        if len(self.classes_) == 2:
            proba_pos = 1 / (1 + np.exp(-scores))
            return np.column_stack([1 - proba_pos, proba_pos])

        return softmax(scores, axis=1)

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        """
        Apply decision function.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        scores : ndarray of shape (n_samples,) or (n_samples, n_classes)
            Decision function values.
        """
        check_is_trained(self, "coef_")
        X = check_array(X)

        scores = X @ self.coef_.T + self.intercept_

        if len(self.classes_) == 2:
            return scores[:, 1] - scores[:, 0]
        return scores

    def apply(self, X: np.ndarray) -> np.ndarray:
        """
        Project data to maximize class separation.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Data to transform.

        Returns
        -------
        X_new : ndarray of shape (n_samples, n_components)
            Transformed data.
        """
        check_is_trained(self, "scalings_")
        X = check_array(X)

        X_centered = X - self.xbar_
        return X_centered @ self.scalings_
