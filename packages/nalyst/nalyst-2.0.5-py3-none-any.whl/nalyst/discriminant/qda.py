"""
Quadratic Discriminant Analysis.
"""

from __future__ import annotations

from typing import Optional

import numpy as np
from scipy import linalg

from nalyst.core.foundation import BaseLearner, ClassifierMixin
from nalyst.core.validation import check_array, check_is_trained


class QuadraticDiscriminantAnalysis(ClassifierMixin, BaseLearner):
    """
    Quadratic Discriminant Analysis.

    A classifier with a quadratic decision boundary, generated by fitting
    class conditional densities and using Bayes' rule.

    Parameters
    ----------
    priors : array-like of shape (n_classes,), optional
        Class priors.
    reg_param : float, default=0.0
        Regularization parameter.
    store_covariance : bool, default=False
        Store class covariance matrices.
    tol : float, default=1e-4
        Threshold for rank estimation.

    Attributes
    ----------
    covariance_ : list of ndarray of shape (n_features, n_features)
        Class covariance matrices.
    means_ : ndarray of shape (n_classes, n_features)
        Class means.
    priors_ : ndarray of shape (n_classes,)
        Class priors.
    rotations_ : list of ndarray
        Rotation matrices.
    scalings_ : list of ndarray
        Scaling values.
    classes_ : ndarray of shape (n_classes,)
        Class labels.

    Examples
    --------
    >>> from nalyst.discriminant import QuadraticDiscriminantAnalysis
    >>> qda = QuadraticDiscriminantAnalysis()
    >>> qda.train(X, y)
    >>> predictions = qda.infer(X_test)
    """

    def __init__(
        self,
        priors: Optional[np.ndarray] = None,
        reg_param: float = 0.0,
        store_covariance: bool = False,
        tol: float = 1e-4,
    ):
        self.priors = priors
        self.reg_param = reg_param
        self.store_covariance = store_covariance
        self.tol = tol

    def train(self, X: np.ndarray, y: np.ndarray) -> "QuadraticDiscriminantAnalysis":
        """
        Fit QDA model.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Training data.
        y : ndarray of shape (n_samples,)
            Target values.

        Returns
        -------
        self : QuadraticDiscriminantAnalysis
            Fitted estimator.
        """
        X = check_array(X)
        y = np.asarray(y)

        self.classes_ = np.unique(y)
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)
        self.n_features_in_ = n_features

        # Compute class priors
        if self.priors is None:
            self.priors_ = np.array([np.sum(y == c) / n_samples for c in self.classes_])
        else:
            self.priors_ = np.asarray(self.priors)

        # Compute class means
        self.means_ = np.zeros((n_classes, n_features))

        # Compute class covariances
        self.rotations_ = []
        self.scalings_ = []

        if self.store_covariance:
            self.covariance_ = []

        for i, c in enumerate(self.classes_):
            Xi = X[y == c]
            ni = len(Xi)

            self.means_[i] = np.mean(Xi, axis=0)

            # Center
            Xi_centered = Xi - self.means_[i]

            # Covariance
            cov = Xi_centered.T @ Xi_centered / (ni - 1)

            # Regularization
            if self.reg_param > 0:
                cov = (1 - self.reg_param) * cov + \
                      self.reg_param * np.trace(cov) / n_features * np.eye(n_features)

            if self.store_covariance:
                self.covariance_.append(cov)

            # SVD decomposition
            U, S, Vt = linalg.svd(cov)

            # Clip small eigenvalues
            S = np.maximum(S, self.tol)

            self.rotations_.append(U)
            self.scalings_.append(S)

        return self

    def infer(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class labels.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
            Predicted class labels.
        """
        proba = self.infer_proba(X)
        return self.classes_[np.argmax(proba, axis=1)]

    def infer_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Return probability estimates.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        proba : ndarray of shape (n_samples, n_classes)
            Probability of each class.
        """
        check_is_trained(self, "means_")
        X = check_array(X)

        n_samples = X.shape[0]
        n_classes = len(self.classes_)

        # Log likelihood for each class
        log_likelihood = np.zeros((n_samples, n_classes))

        for i in range(n_classes):
            # Center data
            X_centered = X - self.means_[i]

            # Transform by rotation
            X_rotated = X_centered @ self.rotations_[i]

            # Mahalanobis distance
            X_scaled = X_rotated / np.sqrt(self.scalings_[i])
            mahal = np.sum(X_scaled ** 2, axis=1)

            # Log determinant
            log_det = np.sum(np.log(self.scalings_[i]))

            # Log likelihood
            log_likelihood[:, i] = -0.5 * (mahal + log_det) + np.log(self.priors_[i])

        # Convert to probabilities (softmax)
        log_likelihood -= np.max(log_likelihood, axis=1, keepdims=True)
        likelihood = np.exp(log_likelihood)
        proba = likelihood / np.sum(likelihood, axis=1, keepdims=True)

        return proba

    def decision_function(self, X: np.ndarray) -> np.ndarray:
        """
        Apply decision function.

        Parameters
        ----------
        X : ndarray of shape (n_samples, n_features)
            Input data.

        Returns
        -------
        scores : ndarray of shape (n_samples, n_classes)
            Log-likelihood of each class.
        """
        check_is_trained(self, "means_")
        X = check_array(X)

        n_samples = X.shape[0]
        n_classes = len(self.classes_)

        scores = np.zeros((n_samples, n_classes))

        for i in range(n_classes):
            X_centered = X - self.means_[i]
            X_rotated = X_centered @ self.rotations_[i]
            X_scaled = X_rotated / np.sqrt(self.scalings_[i])
            mahal = np.sum(X_scaled ** 2, axis=1)
            log_det = np.sum(np.log(self.scalings_[i]))

            scores[:, i] = -0.5 * (mahal + log_det) + np.log(self.priors_[i])

        return scores
