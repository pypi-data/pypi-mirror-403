import asyncio
import logging
import os
from pathlib import Path
from typing import List, Optional, Union
from urllib.parse import urlparse

from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from project_root_finder import root
from trafilatura import fetch_url, html2txt

from .parser_prompt import prompt

logger = logging.getLogger(__name__)


class SiteParser:
    def __init__(self, additional_instructions: Optional[str] = None, bot_id: Optional[str] = None):
        self.bot_id = bot_id
        self.api_key = self._load_api_key()

        model = ChatOpenAI(model="gpt-5-mini", temperature=0, api_key=self.api_key)

        self.chain = prompt | model | StrOutputParser()
        self.additional_instructions = additional_instructions

    def _load_api_key(self) -> str:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ OPENAI_API_KEY Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ SupabaseClient"""
        env_candidates = []

        if self.bot_id:
            env_candidates.append(root / "bots" / self.bot_id / ".env")

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ .env Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
        env_candidates.append(root / ".env")

        for env_path in env_candidates:
            if env_path and env_path.exists():
                load_dotenv(env_path)
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    logger.info(f"ğŸ”‘ OPENAI_API_KEY Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ¸Ğ· {env_path}")
                    return api_key

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ĞµĞ³Ğ¾ Ğ² .env")
        return api_key

    def _text_from_site(self, url: str):
        logger.info(f"ğŸŒ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ: {url}")
        try:
            html = fetch_url(url)
            if not html:
                raise ValueError("ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹")

            text = html2txt(html)
            if not text:
                raise ValueError("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ HTML Ğ² Ñ‚ĞµĞºÑÑ‚")

            return text
        except Exception as exc:
            logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ {url}: {exc}")
            return ""

    async def _clean_text(self, text: str):
        add_prompt = self.additional_instructions if self.additional_instructions else "Ğ”Ğ¾Ğ¿ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½ĞµÑ‚!"

        response = await self.chain.ainvoke({"additional_instructions": add_prompt, "text": text})

        return response

    async def parser(
        self,
        url: Union[str, List[str]],
        max_workers: int = 5,
        to_files: bool = False,
    ) -> Union[str, List[Path]]:
        """
        ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸:
        1. Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ HTML Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ URL
        2. ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Ñ‚ĞµĞºÑÑ‚
        3. ĞŸÑ€Ğ¾Ğ³Ğ¾Ğ½ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· LLM Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸
        4. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
        """
        logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²")
        if isinstance(url, str):
            urls: List[str] = [url]
        else:
            urls = list(url)

        if not urls:
            return [] if to_files else ""

        total = len(urls)
        concurrency = max(1, min(max_workers, total))
        logger.info(f"âš™ï¸ ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: {concurrency}, Ğ²ÑĞµĞ³Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº: {total}")

        # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
        semaphore = asyncio.Semaphore(concurrency)

        async def process_link(idx: int, link: str):
            position = f"[{idx + 1}/{total}]"
            logger.info(f"â¡ï¸ {position} Ğ¡Ñ‚Ğ°Ñ€Ñ‚ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°: {link}")
            async with semaphore:
                raw_text = await asyncio.to_thread(self._text_from_site, link)
                if not raw_text:
                    logger.warning(f"âš ï¸ {position} ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚: {link}")
                    return link, ""

                cleaned_text = await self._clean_text(raw_text)
                cleaned = cleaned_text.strip() if cleaned_text else ""
                if cleaned:
                    logger.info(f"âœ… {position} Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾, Ğ´Ğ»Ğ¸Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°: {len(cleaned)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
                else:
                    logger.warning(f"âš ï¸ {position} ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸")
                remaining = total - (idx + 1)
                if remaining > 0:
                    logger.info(f"â³ ĞÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ~{remaining} ÑÑÑ‹Ğ»Ğ¾Ğº")
                return link, cleaned

        tasks = [asyncio.create_task(process_link(i, link)) for i, link in enumerate(urls)]
        processed_results = await asyncio.gather(*tasks)

        if to_files:
            if not self.bot_id:
                raise ValueError("Ğ”Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ bot_id")

            output_dir = root / "bots" / self.bot_id / "parser"
            output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ Ğ¢ĞµĞºÑÑ‚Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {output_dir}")

            saved_files: List[Path] = []
            for link, text in processed_results:
                if not text:
                    continue
                filename = self._build_filename_from_url(link)
                file_path = output_dir / f"{filename}.txt"
                file_path.write_text(text, encoding="utf-8")
                saved_files.append(file_path)
                logger.info(f"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {file_path.name}")

            logger.info(f"ğŸ“¦ Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {len(saved_files)}")
            return saved_files

        processed_chunks = [text for _, text in processed_results if text]
        success_count = len(processed_chunks)
        fail_count = total - success_count
        logger.info(f"ğŸ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½. Ğ£ÑĞ¿ĞµÑ…Ğ¾Ğ²: {success_count}, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ²: {fail_count}")

        return "\n\n".join(processed_chunks)

    def _build_filename_from_url(self, link: str) -> str:
        parsed = urlparse(link)
        last_segment = parsed.path.rstrip("/").split("/")[-1] or "index"
        safe_segment = "".join(ch for ch in last_segment if ch.isalnum() or ch in ("-", "_"))
        return safe_segment or "page"
