import asyncio
import logging
import os
import time
from pathlib import Path
from typing import List, Optional, Union
from urllib.parse import urlparse

from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from project_root_finder import root
from trafilatura import fetch_url, html2txt

from .parser_prompt import prompt
from .sitemap import search_sitemap

logger = logging.getLogger(__name__)


def _log_or_print(level: int, message: str):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· logger, ĞµÑĞ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ñ‡ĞµÑ€ĞµĞ· print"""
    if logger.handlers or logging.root.handlers:
        if level == logging.INFO:
            logger.info(message)
        elif level == logging.WARNING:
            logger.warning(message)
        elif level == logging.ERROR:
            logger.error(message)
        else:
            logger.debug(message)
    else:
        print(message)


class SiteParser:
    def __init__(self, additional_instructions: Optional[str] = None, bot_id: Optional[str] = None):
        self.bot_id = bot_id
        self.api_key = self._load_api_key()

        model = ChatOpenAI(model="gpt-5-mini", temperature=0, api_key=self.api_key)

        self.chain = prompt | model | StrOutputParser()
        self.additional_instructions = additional_instructions

    def _load_api_key(self) -> str:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ OPENAI_API_KEY Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ SupabaseClient"""
        env_candidates = []

        if self.bot_id:
            env_candidates.append(root / "bots" / self.bot_id / ".env")

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ .env Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
        env_candidates.append(root / ".env")

        for env_path in env_candidates:
            if env_path and env_path.exists():
                load_dotenv(env_path)
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    logger.debug(f"ğŸ”‘ OPENAI_API_KEY Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ¸Ğ· {env_path}")
                    return api_key

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ ĞµĞ³Ğ¾ Ğ² .env")
        return api_key

    def _text_from_site(self, url: str):
        try:
            html = fetch_url(url)
            if not html:
                raise ValueError("ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹")
            text = html2txt(html)
            if not text:
                raise ValueError("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ HTML Ğ² Ñ‚ĞµĞºÑÑ‚")
            return text
        except Exception as exc:
            logger.debug(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ {url}: {exc}")
            return ""

    async def _clean_text(self, text: str, url: str = ""):
        add_prompt = (
            self.additional_instructions
            if self.additional_instructions
            else "Ğ”Ğ¾Ğ¿ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½ĞµÑ‚!"
        )
        response = await self.chain.ainvoke({"additional_instructions": add_prompt, "text": text})
        return response

    async def parser(
        self,
        url: Optional[Union[str, List[str]]] = None,
        sitemap: Optional[str] = None,
        sitemap_regex: Optional[str] = None,
        sitemap_limit: Optional[int] = None,
        sitemap_include_source: bool = False,
        max_workers: int = 5,
        to_files: bool = True,
    ) -> Union[str, List[Path]]:
        """
        ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸:
        1. Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ sitemap, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸Ğ· Ğ½ĞµĞ³Ğ¾
        2. Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ HTML Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ URL
        3. ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Ñ‚ĞµĞºÑÑ‚
        4. ĞŸÑ€Ğ¾Ğ³Ğ¾Ğ½ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· LLM Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸
        5. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ‹

        Args:
            url: URL Ğ¸Ğ»Ğ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº URL Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° (ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ sitemap)
            sitemap: URL sitemap Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° ÑÑÑ‹Ğ»Ğ¾Ğº
            sitemap_regex: Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸Ğ· sitemap
            sitemap_limit: Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸Ğ· sitemap
            sitemap_include_source: Ğ’ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ URL sitemap Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº
            max_workers: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
            to_files: Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ‹ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ True)
        """
        start_time = time.time()
        _log_or_print(logging.INFO, "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²")

        # Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ sitemap, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸Ğ· Ğ½ĞµĞ³Ğ¾
        if sitemap:
            _log_or_print(logging.INFO, f"ğŸ—ºï¸ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ sitemap: {sitemap}")
            urls = search_sitemap(
                url=sitemap,
                regex=sitemap_regex,
                limit=sitemap_limit,
                include_source=sitemap_include_source
            )
            if not urls:
                _log_or_print(logging.WARNING, "âš ï¸ Sitemap Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» ÑÑÑ‹Ğ»Ğ¾Ğº")
                return [] if to_files else ""
        elif url:
            if isinstance(url, str):
                urls: List[str] = [url]
            else:
                urls = list(url)
        else:
            raise ValueError("ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ±Ğ¾ url, Ğ»Ğ¸Ğ±Ğ¾ sitemap")

        if not urls:
            return [] if to_files else ""

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾)
        output_dir = None
        if to_files:
            if not self.bot_id:
                raise ValueError("Ğ”Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ bot_id")
            output_dir = root / "bots" / self.bot_id / "parser"
            output_dir.mkdir(parents=True, exist_ok=True)
            _log_or_print(logging.INFO, f"ğŸ“ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ²: {output_dir}")

        total = len(urls)
        concurrency = max(1, min(max_workers, total))
        _log_or_print(logging.INFO, f"âš™ï¸ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° {total} ÑÑÑ‹Ğ»Ğ¾Ğº (Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾: {concurrency})")

        # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡
        semaphore = asyncio.Semaphore(concurrency)

        # Ğ¡Ñ‡ĞµÑ‚Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
        completed_count = 0
        completed_lock = asyncio.Lock()
        saved_files: List[Path] = []
        saved_lock = asyncio.Lock()
        success_count = 0

        async def process_link(idx: int, link: str):
            nonlocal completed_count, success_count
            position = f"[{idx + 1}/{total}]"

            async with semaphore:
                raw_text = await asyncio.to_thread(self._text_from_site, link)
                if not raw_text:
                    async with completed_lock:
                        completed_count += 1
                        current_percent = int((completed_count / total) * 100)
                    _log_or_print(logging.WARNING, f"âš ï¸ {position} ({current_percent}%) ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚")
                    return link, ""

                cleaned_text = await self._clean_text(raw_text, link)
                cleaned = cleaned_text.strip() if cleaned_text else ""

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» ÑÑ€Ğ°Ğ·Ñƒ, ĞµÑĞ»Ğ¸ to_files=True
                if to_files and cleaned and output_dir:
                    filename = self._build_filename_from_url(link)
                    file_path = output_dir / f"{filename}.txt"
                    await asyncio.to_thread(file_path.write_text, cleaned, encoding="utf-8")
                    async with saved_lock:
                        saved_files.append(file_path)

                async with completed_lock:
                    completed_count += 1
                    current_percent = int((completed_count / total) * 100)
                    remaining = total - completed_count
                    if cleaned:
                        success_count += 1
                
                if cleaned:
                    _log_or_print(
                        logging.INFO,
                        f"âœ… {position} ({current_percent}%) Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ | "
                        f"ĞÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ: {remaining} | "
                        f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {len(cleaned)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²"
                    )
                else:
                    _log_or_print(logging.WARNING, f"âš ï¸ {position} ({current_percent}%) ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚")

                return link, cleaned

        tasks = [asyncio.create_task(process_link(i, link)) for i, link in enumerate(urls)]
        processed_results = await asyncio.gather(*tasks)

        elapsed_time = time.time() - start_time

        if to_files:
            _log_or_print(
                logging.INFO,
                f"ğŸ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {len(saved_files)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² | "
                f"Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {success_count}/{total} ({int(success_count/total*100)}%) | "
                f"Ğ’Ñ€ĞµĞ¼Ñ: {elapsed_time:.2f} ÑĞµĞº"
            )
            return saved_files

        # Ğ”Ğ»Ñ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ±ĞµĞ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ‹ - ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        processed_chunks = [text for _, text in processed_results if text]
        final_success_count = len(processed_chunks)
        _log_or_print(
            logging.INFO,
            f"ğŸ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {final_success_count}/{total} ({int(final_success_count/total*100)}%) | "
            f"Ğ’Ñ€ĞµĞ¼Ñ: {elapsed_time:.2f} ÑĞµĞº"
        )

        return "\n\n".join(processed_chunks)

    def _build_filename_from_url(self, link: str) -> str:
        parsed = urlparse(link)
        last_segment = parsed.path.rstrip("/").split("/")[-1] or "index"
        safe_segment = "".join(ch for ch in last_segment if ch.isalnum() or ch in ("-", "_"))
        return safe_segment or "page"
