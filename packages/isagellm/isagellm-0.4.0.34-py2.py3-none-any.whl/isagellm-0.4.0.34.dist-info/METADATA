Metadata-Version: 2.4
Name: isagellm
Version: 0.4.0.34
Summary: sageLLM: Modular LLM inference engine with PD separation for domestic computing power
Author: IntelliStream Team
License: Proprietary - IntelliStream
Project-URL: Homepage, https://github.com/IntelliStream/sagellm
Project-URL: Documentation, https://github.com/IntelliStream/sagellm#readme
Project-URL: Repository, https://github.com/IntelliStream/sagellm
Keywords: llm,inference,ascend,huawei,npu,cuda,domestic,pd-separation
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: ==3.11.*
Description-Content-Type: text/markdown
Requires-Dist: isagellm-protocol<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-backend<0.5.0,>=0.4.0.4
Requires-Dist: isagellm-core<0.5.0,>=0.4.0.8
Requires-Dist: isagellm-control-plane<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-gateway<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-kv-cache<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-comm<0.5.0,>=0.4.0.0
Requires-Dist: isagellm-compression<0.5.0,>=0.4.0.0
Requires-Dist: click>=8.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: pyyaml>=6.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: ruff>=0.8.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: isage-pypi-publisher>=0.2.0; extra == "dev"

# sageLLM

## Protocol Compliance (Mandatory)

- MUST follow Protocol v0.1:
  https://github.com/intellistream/sagellm-docs/blob/main/docs/specs/protocol_v0.1.md
- Any globally shared definitions (fields, error codes, metrics, IDs, schemas) MUST be added to
  Protocol first.

<p align="center">
  <strong>ğŸš€ Modular LLM Inference Engine for Domestic Computing Power</strong>
</p>

<p align="center">
  Ollama-like experience for Chinese hardware ecosystems (Huawei Ascend, NVIDIA)
</p>

______________________________________________________________________

## âœ¨ Features

- ğŸ¯ **One-Click Install** - `pip install isagellm` gets you started immediately
- ğŸ§  **CPU-First** - Default CPU engine, no GPU required
- ğŸ‡¨ğŸ‡³ **Domestic Hardware** - First-class support for Huawei Ascend NPU
- ğŸ“Š **Observable** - Built-in metrics (TTFT, TBT, throughput, KV usage)
- ğŸ§© **Plugin System** - Extend with custom backends and engines

## ğŸ“¦ Quick Install

```bash
# Install sageLLM (CPU-first, no GPU required)
pip install isagellm

# With Control Plane (request routing & scheduling)
pip install 'isagellm[control-plane]'

# With API Gateway (OpenAI-compatible REST API)
pip install 'isagellm[gateway]'

# Full server (Control Plane + Gateway)
pip install 'isagellm[server]'

# With CUDA support
pip install 'isagellm[cuda]'

# All features
pip install 'isagellm[all]'
```

### ğŸš€ å›½å†…åŠ é€Ÿå®‰è£… PyTorchï¼ˆæ¨èï¼‰

ç”±äº PyTorch CUDA ç‰ˆæœ¬ä»å®˜æ–¹æºä¸‹è½½è¾ƒæ…¢ï¼ˆ~800MBï¼‰ï¼Œæˆ‘ä»¬åœ¨ GitHub Releases æä¾›é¢„å…ˆä¸‹è½½çš„ wheelsï¼š

```bash
# æ–¹æ³• 1ï¼šä½¿ç”¨ sagellm CLI (æ¨èï¼Œæœ€ç®€å•)
pip install isagellm
sage-llm install cuda --github     # ä» GitHub ä¸‹è½½ï¼Œå¿«é€Ÿ
sage-llm install cuda              # ä»å®˜æ–¹æºä¸‹è½½ï¼ˆé»˜è®¤ï¼‰

# æ–¹æ³• 2ï¼šç›´æ¥ä½¿ç”¨ pip --find-links
pip install torch==2.5.1+cu121 torchvision torchaudio \
  --find-links https://github.com/intellistream/sagellm-pytorch-wheels/releases/download/v2.5.1-cu121/ \
  --trusted-host github.com
```

**å…¶ä»–æ”¯æŒçš„åç«¯**ï¼š

- `sage-llm install ascend` - åä¸ºæ˜‡è…¾ NPU
- `sage-llm install kunlun` - ç™¾åº¦æ˜†ä»‘ XPU
- `sage-llm install haiguang` - æµ·å…‰ DCU
- `sage-llm install cpu` - CPU-onlyï¼ˆæœ€å°ä¸‹è½½ï¼‰

ğŸ’¡ **ä¸ºä»€ä¹ˆä½¿ç”¨ GitHub åŠ é€Ÿï¼Ÿ**

- âœ… å›½å†…è®¿é—®é€Ÿåº¦å¿«ï¼ˆGitHub CDNï¼‰
- âœ… æ— éœ€é…ç½®é•œåƒæº
- âœ… å®˜æ–¹ wheelsï¼Œ100% å¯ä¿¡

ğŸ“¦ **Wheels ä»“åº“**: https://github.com/intellistream/sagellm-pytorch-wheels

## ğŸš€ Quick Start

### CLI (åƒ vLLM/Ollama ä¸€æ ·ç®€å•)

```bash
# ä¸€é”®å¯åŠ¨ï¼ˆå®Œæ•´æ ˆï¼šGateway + Engineï¼‰
pip install 'isagellm[gateway]'
sage-llm serve --model Qwen2-7B

# âœ… OpenAI API è‡ªåŠ¨å¯ç”¨
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-7B",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
sage-llm info

# å•æ¬¡æ¨ç†ï¼ˆä¸å¯åŠ¨æœåŠ¡å™¨ï¼‰
sage-llm run -p "What is LLM inference?"

# é«˜çº§ç”¨æ³•ï¼šåˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆåˆ†åˆ«å¯åŠ¨å„ç»„ä»¶ï¼‰
sage-llm serve --engine-only --port 9000   # ä»…å¼•æ“
sage-llm gateway --port 8000                # ä»… Gateway
```

### Python API (Control Plane - Recommended)

```python
import asyncio

from sagellm import ControlPlaneManager, BackendConfig, EngineConfig

# Install with: pip install 'isagellm[control-plane]'
async def main() -> None:
    manager = ControlPlaneManager(
        backend_config=BackendConfig(kind="cpu", device="cpu"),
        engine_configs=[
            EngineConfig(
                kind="cpu",
                model="sshleifer/tiny-gpt2",
                model_path="sshleifer/tiny-gpt2"
            )
        ]
    )

    await manager.start()
    try:
        # Requests are automatically routed to available engines
        response = await manager.execute_request(
            prompt="Hello, world!",
            max_tokens=128
        )
        print(response.output_text)
        print(f"TTFT: {response.metrics.ttft_ms:.2f} ms")
        print(f"Throughput: {response.metrics.throughput_tps:.2f} tokens/s")
    finally:
        await manager.stop()


asyncio.run(main())
```

**âš ï¸ Important:** Direct engine creation (`create_engine()`) is not exported from the umbrella
package. All production code must use `ControlPlaneManager` for proper request routing, scheduling,
and lifecycle management.

### Configuration

```yaml
# ~/.sage-llm/config.yaml
backend:
  kind: cpu  # Options: cpu, pytorch-cuda, pytorch-ascend
  device: cpu

engine:
  kind: cpu
  model: sshleifer/tiny-gpt2

control_plane:
  endpoint: "localhost:8080"
```

## ğŸ“Š Metrics & Validation

sageLLM provides comprehensive performance metrics:

```json
{
  "ttft_ms": 45.2,
  "tbt_ms": 12.5,
  "throughput_tps": 80.0,
  "peak_mem_mb": 24576,
  "kv_used_tokens": 4096,
  "prefix_hit_rate": 0.85
}
```

Run benchmarks:

```bash
sage-llm demo --workload year1 --output metrics.json
```

## ğŸ—ï¸ Architecture

```
isagellm (umbrella package)
â”œâ”€â”€ isagellm-protocol       # Protocol v0.1 types
â”‚   â””â”€â”€ Request, Response, Metrics, Error, StreamEvent
â”œâ”€â”€ isagellm-backend        # Hardware abstraction (L1 - Foundation)
â”‚   â””â”€â”€ BackendProvider, CPUBackend, (CUDABackend, AscendBackend)
â”œâ”€â”€ isagellm-comm           # Communication primitives (L2 - Infrastructure)
â”‚   â””â”€â”€ Topology, CollectiveOps (all_reduce/gather), P2P (send/recv), Overlap
â”œâ”€â”€ isagellm-kv-cache       # KV cache management (L2 - Optional)
â”‚   â””â”€â”€ PrefixCache, MemoryPool, EvictionPolicies, Predictor, KV Transfer
â”œâ”€â”€ isagellm-compression    # Inference acceleration (quantization, sparsity, etc.) (L2 - Optional)
â”‚   â””â”€â”€ Quantization, Sparsity, SpeculativeDecoding, Fusion
â”œâ”€â”€ isagellm-core           # Engine core & runtime (L3)
â”‚   â””â”€â”€ Config, Engine, Factory, DemoRunner, Adapters (vLLM/LMDeploy)
â”œâ”€â”€ isagellm-control-plane  # Request routing & scheduling (L4 - Optional)
â”‚   â””â”€â”€ ControlPlaneManager, Router, Policies, Lifecycle
â””â”€â”€ isagellm-gateway        # OpenAI-compatible REST API (L5 - Optional)
    â””â”€â”€ FastAPI server, /v1/chat/completions, Session management
```

## ğŸ”§ Development

### Quick Setup (Development Mode)

```bash
# Clone all repositories
./scripts/clone-all-repos.sh

# Install all packages in editable mode
./quickstart.sh

# Open all repos in VS Code Multi-root Workspace
code sagellm.code-workspace
```

**ğŸ“– See [WORKSPACE_GUIDE.md](WORKSPACE_GUIDE.md) for Multi-root Workspace usage.**

### Testing

```bash
# Clone and setup
git clone https://github.com/IntelliStream/sagellm.git
cd sagellm
pip install -e ".[dev]"

# Run tests
pytest -v

# Format & lint
ruff format .
ruff check . --fix

# Type check
mypy src/sagellm/

# Verify dependency hierarchy
python scripts/verify_dependencies.py
```

### ğŸ“– Development Resources

- **[DEPLOYMENT_GUIDE.md](docs/DEPLOYMENT_GUIDE.md)** - å®Œæ•´éƒ¨ç½²ä¸é…ç½®æŒ‡å—
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - æ•…éšœæ’æŸ¥å¿«é€Ÿå‚è€ƒ
- **[ENVIRONMENT_VARIABLES.md](docs/ENVIRONMENT_VARIABLES.md)** - ç¯å¢ƒå˜é‡å®Œæ•´å‚è€ƒ
- **[DEVELOPER_GUIDE.md](docs/DEVELOPER_GUIDE.md)** - å¼€å‘è€…æŒ‡å—
- **[WORKSPACE_GUIDE.md](docs/WORKSPACE_GUIDE.md)** - Multi-root Workspace ä½¿ç”¨
- **[INFERENCE_FLOW.md](docs/INFERENCE_FLOW.md)** - æ¨ç†æµç¨‹è¯¦è§£
- **[PR_CHECKLIST.md](docs/PR_CHECKLIST.md)** - Pull Request æ£€æŸ¥æ¸…å•

______________________________________________________________________

## ğŸ“š Documentation Index

### ç”¨æˆ·æ–‡æ¡£

- [å¿«é€Ÿå¼€å§‹](README.md#-quick-start) - 5 åˆ†é’Ÿä¸Šæ‰‹
- [éƒ¨ç½²æŒ‡å—](docs/DEPLOYMENT_GUIDE.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [é…ç½®å‚è€ƒ](docs/DEPLOYMENT_GUIDE.md#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E) - å®Œæ•´é…ç½®é€‰é¡¹
- [ç¯å¢ƒå˜é‡](docs/ENVIRONMENT_VARIABLES.md) - ç¯å¢ƒå˜é‡å‚è€ƒ
- [æ•…éšœæ’æŸ¥](docs/TROUBLESHOOTING.md) - å¸¸è§é—®é¢˜è§£å†³

### å¼€å‘è€…æ–‡æ¡£

- [å¼€å‘æŒ‡å—](docs/DEVELOPER_GUIDE.md) - è´¡çŒ®ä»£ç 
- [æ¶æ„è®¾è®¡](README.md#-architecture) - ç³»ç»Ÿæ¶æ„
- [Workspace ä½¿ç”¨](docs/WORKSPACE_GUIDE.md) - Multi-root å·¥ä½œåŒº
- [PR æ£€æŸ¥æ¸…å•](docs/PR_CHECKLIST.md) - æäº¤å‰æ£€æŸ¥

### API æ–‡æ¡£

- OpenAI å…¼å®¹ API - å‚è§ [sagellm-gateway](https://github.com/intellistream/sagellm-gateway)
- Python API - å‚è§ [API_REFERENCE.md](docs/API_REFERENCE.md)ï¼ˆå¾…è¡¥å……ï¼‰

### å­åŒ…æ–‡æ¡£

- [sagellm-protocol](https://github.com/intellistream/sagellm-protocol) - åè®®å®šä¹‰

- [sagellm-backend](https://github.com/intellistream/sagellm-backend) - åç«¯æŠ½è±¡

- [sagellm-core](https://github.com/intellistream/sagellm-core) - å¼•æ“æ ¸å¿ƒ

- [sagellm-control-plane](https://github.com/intellistream/sagellm-control-plane) - æ§åˆ¶é¢

- [sagellm-gateway](https://github.com/intellistream/sagellm-gateway) - API ç½‘å…³

- [sagellm-benchmark](https://github.com/intellistream/sagellm-benchmark) - åŸºå‡†æµ‹è¯•

- [**DEVELOPER_GUIDE.md**](DEVELOPER_GUIDE.md) - æ¶æ„è§„èŒƒä¸å¼€å‘æŒ‡å—

- [**PR_CHECKLIST.md**](PR_CHECKLIST.md) - Pull Request å®¡æŸ¥æ¸…å•

- [**scripts/verify_dependencies.py**](scripts/verify_dependencies.py) - ä¾èµ–å±‚æ¬¡éªŒè¯

## ğŸ“š Package Details

| Package          | PyPI Name           | Import Name        | Description                     |
| ---------------- | ------------------- | ------------------ | ------------------------------- |
| sagellm          | `isagellm`          | `sagellm`          | Umbrella package (install this) |
| sagellm-protocol | `isagellm-protocol` | `sagellm_protocol` | Protocol v0.1 types             |
| sagellm-core     | `isagellm-core`     | `sagellm_core`     | Runtime & config                |
| sagellm-backend  | `isagellm-backend`  | `sagellm_backend`  | Hardware abstraction            |

## ğŸ“„ License

Proprietary - IntelliStream. Internal use only.

______________________________________________________________________

<p align="center">
  <sub>Built with â¤ï¸ by IntelliStream Team for domestic AI infrastructure</sub>
</p>
# test
