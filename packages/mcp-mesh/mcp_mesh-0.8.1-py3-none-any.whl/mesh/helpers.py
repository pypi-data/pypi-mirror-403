"""
Helper decorators for common mesh patterns.

This module provides convenience decorators that build on top of the core
mesh decorators to simplify common patterns like zero-code LLM providers.
"""

import logging
from typing import Any, Dict, List, Optional

from _mcp_mesh.shared.logging_config import format_log_value

logger = logging.getLogger(__name__)


def _extract_vendor_from_model(model: str) -> str | None:
    """
    Extract vendor name from LiteLLM model string.

    LiteLLM uses vendor/model format (e.g., "anthropic/claude-sonnet-4-5").
    This extracts the vendor for provider handler selection.

    Args:
        model: LiteLLM model string

    Returns:
        Vendor name (e.g., "anthropic", "openai") or None if not extractable

    Examples:
        "anthropic/claude-sonnet-4-5" -> "anthropic"
        "openai/gpt-4o" -> "openai"
        "gpt-4" -> None (no vendor prefix)
    """
    if not model:
        return None

    if "/" in model:
        vendor = model.split("/")[0].lower().strip()
        return vendor

    return None


def llm_provider(
    model: str,
    capability: str = "llm",
    tags: Optional[list[str]] = None,
    version: str = "1.0.0",
    **litellm_kwargs: Any,
):
    """
    Zero-code LLM provider decorator.

    Creates a mesh-registered LLM provider that automatically:
    - Registers as MCP tool (@app.tool) for direct MCP calls
    - Registers in mesh network (@mesh.tool) for dependency injection
    - Wraps LiteLLM with standard MeshLlmRequest interface
    - Returns raw string response (caller handles parsing)

    The decorated function becomes a placeholder - the decorator generates
    a process_chat(request: MeshLlmRequest) -> str function that handles
    all LLM provider logic.

    Args:
        model: LiteLLM model name (e.g., "anthropic/claude-sonnet-4-5")
        capability: Capability name for mesh registration (default: "llm")
        tags: Tags for mesh registration (e.g., ["claude", "fast", "+budget"])
        version: Version string for mesh registration (default: "1.0.0")
        **litellm_kwargs: Additional kwargs to pass to litellm.completion()

    Usage:
        from fastmcp import FastMCP
        import mesh

        app = FastMCP("LLM Provider")

        @mesh.llm_provider(
            model="anthropic/claude-sonnet-4-5",
            capability="llm",
            tags=["claude", "test"],
            version="1.0.0",
        )
        def claude_provider():
            '''Zero-code Claude provider.'''
            pass  # Implementation is in the decorator

        @mesh.agent(name="my-provider", auto_run=True)
        class MyProviderAgent:
            pass

    The generated process_chat function signature:
        def process_chat(request: MeshLlmRequest) -> str:
            '''
            Auto-generated LLM handler.

            Args:
                request: MeshLlmRequest with messages, tools, model_params

            Returns:
                Raw LLM response content as string
            '''

    Testing:
        # Direct MCP call
        curl -X POST http://localhost:9019/mcp \\
          -H "Content-Type: application/json" \\
          -d '{
            "jsonrpc": "2.0",
            "id": 1,
            "method": "tools/call",
            "params": {
              "name": "process_chat",
              "arguments": {
                "request": {
                  "messages": [
                    {"role": "system", "content": "You are helpful."},
                    {"role": "user", "content": "Say hello."}
                  ]
                }
              }
            }
          }'

    Raises:
        RuntimeError: If FastMCP 'app' not found in module
        ImportError: If litellm not installed
    """

    def decorator(func):
        # Import here to avoid circular imports
        import sys

        from mesh import tool
        from mesh.types import MeshLlmRequest

        # Find FastMCP app in current module
        current_module = sys.modules.get(func.__module__)
        if not current_module or not hasattr(current_module, "app"):
            raise RuntimeError(
                f"@mesh.llm_provider requires FastMCP 'app' in module {func.__module__}. "
                f"Example: app = FastMCP('LLM Provider')"
            )

        app = current_module.app

        # Extract vendor from model name using LiteLLM
        vendor = "unknown"
        try:
            import litellm

            _, vendor, _, _ = litellm.get_llm_provider(model=model)
            logger.info(
                f"‚úÖ Extracted vendor '{vendor}' from model '{model}' "
                f"using LiteLLM detection"
            )
        except (ImportError, AttributeError, ValueError, KeyError) as e:
            # Fallback: try to extract from model prefix
            # ImportError: litellm not installed
            # AttributeError: get_llm_provider doesn't exist
            # ValueError: invalid model format
            # KeyError: model not in provider mapping
            if "/" in model:
                vendor = model.split("/")[0]
                logger.warning(
                    f"‚ö†Ô∏è  Could not extract vendor using LiteLLM ({e}), "
                    f"falling back to prefix extraction: '{vendor}'"
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Could not extract vendor from model '{model}', "
                    f"using 'unknown'"
                )

        # Generate the LLM handler function
        def process_chat(request: MeshLlmRequest) -> dict[str, Any]:
            """
            Auto-generated LLM handler.

            Args:
                request: MeshLlmRequest with messages, tools, model_params

            Returns:
                Full message dict with content, role, and tool_calls (if present)
            """
            import litellm

            # Determine effective model (check for consumer override - issue #308)
            effective_model = model  # Default to provider's model
            model_params_copy = (
                dict(request.model_params) if request.model_params else {}
            )

            if "model" in model_params_copy:
                override_model = model_params_copy.pop(
                    "model"
                )  # Remove to avoid duplication

                if override_model:
                    # Validate vendor compatibility
                    override_vendor = _extract_vendor_from_model(override_model)

                    if override_vendor and override_vendor != vendor:
                        # Vendor mismatch - log warning and fall back to provider's model
                        logger.warning(
                            f"‚ö†Ô∏è Model override '{override_model}' ignored - vendor mismatch "
                            f"(override vendor: '{override_vendor}', provider vendor: '{vendor}'). "
                            f"Using provider's default model: '{model}'"
                        )
                    else:
                        # Vendor matches or can't be determined - use override
                        effective_model = override_model
                        logger.info(
                            f"üîÑ Using model override '{effective_model}' "
                            f"(requested by consumer)"
                        )

            # Issue #459: Handle output_schema for vendor-specific structured output
            # Convert to response_format for vendors that support it
            output_schema = model_params_copy.pop("output_schema", None)
            output_type_name = model_params_copy.pop("output_type_name", None)

            # Vendors that support structured output via response_format
            supported_structured_output_vendors = (
                "openai",
                "azure",  # Azure OpenAI uses same format as OpenAI
                "gemini",
                "vertex_ai",  # Vertex AI Gemini uses same format as Gemini
                "anthropic",
            )

            if output_schema:
                if vendor in supported_structured_output_vendors:
                    # Apply vendor-specific response_format for structured output
                    from _mcp_mesh.engine.provider_handlers import make_schema_strict

                    if vendor == "anthropic":
                        # Claude: doesn't require all properties in 'required', uses strict=False
                        schema = make_schema_strict(
                            output_schema, add_all_required=False
                        )
                        strict_mode = False
                    else:
                        # OpenAI/Azure/Gemini/Vertex: require all properties in 'required', uses strict=True
                        schema = make_schema_strict(
                            output_schema, add_all_required=True
                        )
                        strict_mode = True

                    model_params_copy["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": output_type_name or "Response",
                            "schema": schema,
                            "strict": strict_mode,
                        },
                    }
                    logger.debug(
                        f"üéØ Applied {vendor} response_format for structured output: "
                        f"{output_type_name} (strict={strict_mode})"
                    )
                else:
                    # Vendor doesn't support structured output - warn user
                    logger.warning(
                        f"‚ö†Ô∏è Structured output schema '{output_type_name or 'Response'}' "
                        f"was provided but vendor '{vendor}' does not support response_format. "
                        f"The schema will be ignored and the LLM may return unstructured output."
                    )

            # Build litellm.completion arguments
            completion_args: dict[str, Any] = {
                "model": effective_model,
                "messages": request.messages,
                **litellm_kwargs,
            }

            # Add optional request parameters
            if request.tools:
                completion_args["tools"] = request.tools

            if model_params_copy:
                completion_args.update(model_params_copy)

            # Call LiteLLM
            try:
                # Log full request
                logger.debug(
                    f"üì§ LLM provider request: {format_log_value(completion_args)}"
                )

                response = litellm.completion(**completion_args)

                # Log full response
                logger.debug(f"üì• LLM provider response: {format_log_value(response)}")

                message = response.choices[0].message

                # Build message dict with all necessary fields for agentic loop
                # Handle content - it can be a string or list of content blocks
                content = message.content
                if isinstance(content, list):
                    # Extract text from content blocks (robust handling)
                    text_parts = []
                    for block in content:
                        if block is None:
                            continue  # Skip None blocks
                        elif isinstance(block, dict):
                            # Extract text field, ensure it's a string
                            text_value = block.get("text", "")
                            text_parts.append(
                                str(text_value) if text_value is not None else ""
                            )
                        else:
                            # Convert any other type to string
                            try:
                                text_parts.append(str(block))
                            except Exception:
                                # If str() fails, skip this block
                                logger.warning(
                                    f"Unable to convert content block to string: {type(block)}"
                                )
                                continue
                    content = "".join(text_parts)

                message_dict: dict[str, Any] = {
                    "role": message.role,
                    "content": content if content else "",
                }

                # Include tool_calls if present (critical for agentic loop support!)
                if hasattr(message, "tool_calls") and message.tool_calls:
                    message_dict["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            },
                        }
                        for tc in message.tool_calls
                    ]

                # Issue #311: Include usage metadata for cost tracking
                if hasattr(response, "usage") and response.usage:
                    usage = response.usage
                    message_dict["_mesh_usage"] = {
                        "prompt_tokens": getattr(usage, "prompt_tokens", 0) or 0,
                        "completion_tokens": getattr(usage, "completion_tokens", 0)
                        or 0,
                        "model": effective_model,
                    }

                logger.info(
                    f"LLM provider {func.__name__} processed request "
                    f"(model={effective_model}, messages={len(request.messages)}, "
                    f"tool_calls={len(message_dict.get('tool_calls', []))})"
                )

                return message_dict

            except Exception as e:
                logger.error(f"LLM provider {func.__name__} failed: {e}")
                raise

        # Preserve original function's docstring metadata
        if func.__doc__:
            process_chat.__doc__ = func.__doc__ + "\n\n" + (process_chat.__doc__ or "")

        # FIX for issue #227: Preserve original function name to avoid conflicts
        # when multiple @mesh.llm_provider decorators are used in the same agent.
        # FastMCP uses __name__ as the tool name, so without this fix all providers
        # would be registered as "process_chat" and overwrite each other.
        process_chat.__name__ = func.__name__
        process_chat.__qualname__ = func.__qualname__

        # CRITICAL: Apply @mesh.tool() FIRST (before FastMCP caches the function)
        # This ensures mesh DI wrapper is in place when FastMCP caches the function
        # Decorators are applied bottom-up, so mesh wrapper must be innermost
        process_chat = tool(
            capability=capability,
            tags=tags,
            version=version,
            vendor=vendor,  # Pass vendor to registry for provider handler selection
        )(process_chat)

        # Then apply @app.tool() for MCP registration (caches the wrapped version)
        process_chat = app.tool()(process_chat)

        logger.info(
            f"‚úÖ Created LLM provider '{func.__name__}' "
            f"(model={model}, capability={capability}, tags={tags}, vendor={vendor})"
        )

        # Return the generated function (replaces the placeholder)
        return process_chat

    return decorator
