#!/usr/bin/env python3
"""
Inference script for {{ framework_display }} {{ task_type }} model
"""

import os
import sys
import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any, Union, List

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import yaml

{% if deployment == "fastapi" %}
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Union
import uvicorn
{% endif %}

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))

from src.data.data_loader import DataLoader
from src.models.{{ task_type }}_model import {{ task_type.title() }}Model


def setup_logging(config: Dict[str, Any]) -> None:
    """Setup logging configuration"""
    log_config = config.get("logging", {})
    log_level = getattr(logging, log_config.get("level", "INFO"))
    log_format = log_config.get("format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    
    logging.basicConfig(
        level=log_level,
        format=log_format,
        handlers=[
            logging.FileHandler(log_config.get("file", "logs/inference.log")),
            logging.StreamHandler() if log_config.get("console", True) else logging.NullHandler()
        ]
    )


def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from YAML file"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_model(model_path: str, model_config: Dict[str, Any]) -> nn.Module:
    """Load trained PyTorch model"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model not found at {model_path}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = {{ task_type.title() }}Model(model_config).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    logging.info(f"Model loaded from {model_path}")
    return model


def preprocess_data(data: Union[pd.DataFrame, np.ndarray]) -> torch.Tensor:
    """Preprocess input data"""
    if isinstance(data, pd.DataFrame):
        data = data.values
    
    # Convert to tensor
    tensor_data = torch.FloatTensor(data)
    
    return tensor_data


def predict_single(model: nn.Module, data: torch.Tensor, device: torch.device) -> Union[int, float]:
    """Make prediction for single sample"""
    if data.dim() == 1:
        data = data.unsqueeze(0)
    
    data = data.to(device)
    
    with torch.no_grad():
        output = model(data)
        
        {% if task_type == "classification" %}
        # For classification, return the predicted class
        _, predicted = torch.max(output.data, 1)
        return int(predicted.item())
        {% elif task_type == "regression" %}
        # For regression, return the predicted value
        return float(output.squeeze().item())
        {% elif task_type == "timeseries" %}
        # For time series, return the predicted value
        return float(output.squeeze().item())
        {% endif %}


def predict_batch(model: nn.Module, data: torch.Tensor, device: torch.device) -> List[Union[int, float]]:
    """Make predictions for batch of samples"""
    data = data.to(device)
    
    with torch.no_grad():
        output = model(data)
        
        {% if task_type == "classification" %}
        # For classification, return the predicted classes
        _, predicted = torch.max(output.data, 1)
        return [int(pred.item()) for pred in predicted]
        {% elif task_type == "regression" %}
        # For regression, return the predicted values
        return [float(val.item()) for val in output.squeeze()]
        {% elif task_type == "timeseries" %}
        # For time series, return the predicted values
        return [float(val.item()) for val in output.squeeze()]
        {% endif %}


def predict_with_probabilities(model: nn.Module, data: torch.Tensor, device: torch.device) -> Dict[str, Any]:
    """Make predictions with probabilities (classification only)"""
    {% if task_type == "classification" %}
    data = data.to(device)
    
    with torch.no_grad():
        output = model(data)
        probabilities = torch.softmax(output, dim=1)
        _, predicted = torch.max(output.data, 1)
        
        return {
            "predictions": [int(pred.item()) for pred in predicted],
            "probabilities": [probs.cpu().numpy().tolist() for probs in probabilities]
        }
    {% else %}
    raise NotImplementedError("Probability predictions only available for classification")
    {% endif %}


def save_predictions(predictions: List[Union[int, float]], output_path: str) -> None:
    """Save predictions to file"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(predictions, f, indent=2)
    
    logging.info(f"Predictions saved to {output_path}")


{% if deployment == "fastapi" %}
# FastAPI app
app = FastAPI(title="{{ project_name }} API", version="0.1.0")

# Pydantic models
{% if task_type == "classification" %}
class PredictionRequest(BaseModel):
    features: List[float]

class PredictionResponse(BaseModel):
    prediction: int
    confidence: float

class BatchPredictionRequest(BaseModel):
    data: List[List[float]]

class BatchPredictionResponse(BaseModel):
    predictions: List[int]

class ProbabilityRequest(BaseModel):
    data: List[List[float]]

class ProbabilityResponse(BaseModel):
    predictions: List[int]
    probabilities: List[List[float]]
{% elif task_type == "regression" or task_type == "timeseries" %}
class PredictionRequest(BaseModel):
    features: List[float]

class PredictionResponse(BaseModel):
    prediction: float

class BatchPredictionRequest(BaseModel):
    data: List[List[float]]

class BatchPredictionResponse(BaseModel):
    predictions: List[float]
{% endif %}

# Global variables for model
model = None
config = None
device = None


@app.on_event("startup")
async def startup_event():
    """Load model on startup"""
    global model, config, device
    
    config_path = "configs/config.yaml"
    config = load_config(config_path)
    setup_logging(config)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model_path = "models/production/{{ task_type }}_model.pth"
    model = load_model(model_path, config["model"])
    
    logging.info("API startup completed")


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Make single prediction"""
    try:
        data = preprocess_data(np.array(request.features))
        prediction = predict_single(model, data, device)
        
        {% if task_type == "classification" %}
        # Get confidence (max probability)
        data_batch = data.unsqueeze(0)
        with torch.no_grad():
            output = model(data_batch.to(device))
            probabilities = torch.softmax(output, dim=1)
            confidence = float(torch.max(probabilities).item())
        
        return PredictionResponse(prediction=prediction, confidence=confidence)
        {% else %}
        return PredictionResponse(prediction=prediction)
        {% endif %}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/predict-batch", response_model=BatchPredictionResponse)
async def predict_batch_endpoint(request: BatchPredictionRequest):
    """Make batch predictions"""
    try:
        data = preprocess_data(np.array(request.data))
        predictions = predict_batch(model, data, device)
        
        return BatchPredictionResponse(predictions=predictions)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


{% if task_type == "classification" %}
@app.post("/predict-probabilities", response_model=ProbabilityResponse)
async def predict_probabilities(request: ProbabilityRequest):
    """Make predictions with probabilities"""
    try:
        data = preprocess_data(np.array(request.data))
        result = predict_with_probabilities(model, data, device)
        
        return ProbabilityResponse(
            predictions=result["predictions"],
            probabilities=result["probabilities"]
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
{% endif %}


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "model_loaded": model is not None,
        "device": str(device)
    }


if __name__ == "__main__":
    uvicorn.run(
        "src.inference:app",
        host=config["deployment"]["host"],
        port=config["deployment"]["port"],
        reload=True
    )
{% else %}
def main() -> None:
    """Main inference function"""
    parser = argparse.ArgumentParser(description="Run inference with {{ framework_display }} {{ task_type }} model")
    parser.add_argument("--config", type=str, default="configs/config.yaml", help="Path to configuration file")
    parser.add_argument("--input-path", type=str, required=True, help="Path to input data")
    parser.add_argument("--output-path", type=str, help="Path to save predictions")
    parser.add_argument("--model-path", type=str, default="models/production/{{ task_type }}_model.pth", help="Path to trained model")
    {% if task_type == "classification" %}
    parser.add_argument("--probabilities", action="store_true", help="Return prediction probabilities")
    {% endif %}
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    setup_logging(config)
    
    logging.info("Starting inference process")
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")
    
    # Load model
    model = load_model(args.model_path, config["model"])
    
    # Load data
    data_loader = DataLoader(config)
    if args.input_path.endswith('.csv'):
        data = data_loader.load_from_file(args.input_path)
    else:
        data = np.load(args.input_path)
    
    logging.info(f"Loaded data with shape: {data.shape}")
    
    # Preprocess data
    data_tensor = preprocess_data(data)
    
    # Make predictions
    if data_tensor.shape[0] == 1:
        # Single prediction
        prediction = predict_single(model, data_tensor, device)
        predictions = [prediction]
        logging.info(f"Single prediction: {prediction}")
        
        {% if task_type == "classification" %}
        if args.probabilities:
            result = predict_with_probabilities(model, data_tensor, device)
            logging.info(f"Probabilities: {result['probabilities'][0]}")
        {% endif %}
    else:
        # Batch predictions
        {% if task_type == "classification" %}
        if args.probabilities:
            result = predict_with_probabilities(model, data_tensor, device)
            predictions = result["predictions"]
            logging.info(f"Batch predictions with probabilities: {len(predictions)} predictions made")
        else:
            predictions = predict_batch(model, data_tensor, device)
            logging.info(f"Batch predictions: {len(predictions)} predictions made")
        {% else %}
        predictions = predict_batch(model, data_tensor, device)
        logging.info(f"Batch predictions: {len(predictions)} predictions made")
        {% endif %}
    
    # Save predictions if output path provided
    if args.output_path:
        {% if task_type == "classification" %}
        if args.probabilities:
            output_data = {
                "predictions": predictions,
                "probabilities": result["probabilities"]
            }
        else:
            output_data = {"predictions": predictions}
        
        with open(args.output_path, 'w') as f:
            json.dump(output_data, f, indent=2)
        {% else %}
        save_predictions(predictions, args.output_path)
        {% endif %}
    else:
        print("Predictions:")
        for i, pred in enumerate(predictions):
            print(f"Sample {i}: {pred}")
    
    logging.info("Inference process completed successfully")


if __name__ == "__main__":
    main()
{% endif %}
