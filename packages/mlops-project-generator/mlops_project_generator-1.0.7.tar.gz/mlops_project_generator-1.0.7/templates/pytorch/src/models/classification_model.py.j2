"""
Classification model implementation for PyTorch
"""

import torch
import torch.nn as nn
from typing import Dict, Any


class ClassificationModel(nn.Module):
    """
    Neural network for classification tasks
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize classification model
        
        Args:
            config: Configuration dictionary
        """
        super(ClassificationModel, self).__init__()
        
        self.config = config
        self.hidden_layers = config.get("hidden_layers", [128, 64, 32])
        self.dropout = config.get("dropout", 0.2)
        self.num_classes = config.get("num_classes", 2)
        self.input_dim = config.get("input_dim", 20)  # Default, will be updated
        
        # Build network layers
        layers = []
        
        # Input layer
        prev_dim = self.input_dim
        
        # Hidden layers
        for hidden_dim in self.hidden_layers:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout)
            ])
            prev_dim = hidden_dim
        
        # Output layer
        layers.append(nn.Linear(prev_dim, self.num_classes))
        
        self.network = nn.Sequential(*layers)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor (logits)
        """
        return self.network(x)
    
    def set_input_dim(self, input_dim: int):
        """Set input dimension (for dynamic input size)"""
        if input_dim != self.input_dim:
            self.input_dim = input_dim
            # Rebuild the first layer
            layers = []
            prev_dim = self.input_dim
            
            # Hidden layers
            for hidden_dim in self.hidden_layers:
                layers.extend([
                    nn.Linear(prev_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(self.dropout)
                ])
                prev_dim = hidden_dim
            
            # Output layer
            layers.append(nn.Linear(prev_dim, self.num_classes))
            
            self.network = nn.Sequential(*layers)
            self._initialize_weights()


class ResidualBlock(nn.Module):
    """Residual block for deeper networks"""
    
    def __init__(self, dim: int, dropout: float = 0.2):
        super(ResidualBlock, self).__init__()
        self.block = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim),
            nn.Dropout(dropout)
        )
        self.relu = nn.ReLU()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.relu(x + self.block(x))


class DeepClassificationModel(nn.Module):
    """
    Deeper neural network with residual connections for classification
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize deep classification model
        
        Args:
            config: Configuration dictionary
        """
        super(DeepClassificationModel, self).__init__()
        
        self.config = config
        self.hidden_layers = config.get("hidden_layers", [256, 128, 64])
        self.dropout = config.get("dropout", 0.3)
        self.num_classes = config.get("num_classes", 2)
        self.input_dim = config.get("input_dim", 20)
        self.num_residual_blocks = config.get("num_residual_blocks", 2)
        
        # Input projection
        self.input_projection = nn.Linear(self.input_dim, self.hidden_layers[0])
        
        # Residual blocks
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(self.hidden_layers[0], self.dropout)
            for _ in range(self.num_residual_blocks)
        ])
        
        # Transition layers
        transition_layers = []
        for i in range(len(self.hidden_layers) - 1):
            transition_layers.extend([
                nn.Linear(self.hidden_layers[i], self.hidden_layers[i + 1]),
                nn.ReLU(),
                nn.Dropout(self.dropout)
            ])
        
        self.transition = nn.Sequential(*transition_layers) if transition_layers else nn.Identity()
        
        # Output layer
        self.output_layer = nn.Linear(self.hidden_layers[-1], self.num_classes)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor (logits)
        """
        x = self.input_projection(x)
        x = torch.relu(x)
        
        # Apply residual blocks
        for block in self.residual_blocks:
            x = block(x)
        
        x = self.transition(x)
        x = self.output_layer(x)
        
        return x
    
    def set_input_dim(self, input_dim: int):
        """Set input dimension (for dynamic input size)"""
        if input_dim != self.input_dim:
            self.input_dim = input_dim
            self.input_projection = nn.Linear(self.input_dim, self.hidden_layers[0])
            self._initialize_weights()
