# -*- coding: utf-8 -*-
"""
Understand and analyze videos using the best available backend.

Supports multiple backends with automatic selection:
- Gemini: Uses native video understanding with inline_data (preferred)
- OpenAI: Uses key frame extraction with vision API

Backend Selection Priority:
1. Same backend as the calling agent (if specified and has API key)
2. Default priority list: Gemini → OpenAI (first with available API key)

This can be configured via the multimodal settings in agent config.
"""

import base64
import json
import mimetypes
import os
from pathlib import Path
from typing import List, Optional

from dotenv import load_dotenv

from massgen.context.task_context import format_prompt_with_context
from massgen.logger_config import logger
from massgen.tool._multimodal_tools.backend_selector import get_backend
from massgen.tool._result import ExecutionResult, TextContent


def _validate_path_access(path: Path, allowed_paths: Optional[List[Path]] = None) -> None:
    """
    Validate that a path is within allowed directories.

    Args:
        path: Path to validate
        allowed_paths: List of allowed base paths (optional)

    Raises:
        ValueError: If path is not within allowed directories
    """
    if not allowed_paths:
        return  # No restrictions

    for allowed_path in allowed_paths:
        try:
            path.relative_to(allowed_path)
            return  # Path is within this allowed directory
        except ValueError:
            continue

    raise ValueError(f"Path not in allowed directories: {path}")


def _extract_key_frames(video_path: Path, num_frames: int = 8) -> List[str]:
    """
    Extract key frames from a video file and resize them to fit OpenAI Vision API limits.

    Args:
        video_path: Path to the video file
        num_frames: Number of key frames to extract

    Returns:
        List of base64-encoded frame images (resized to fit 768px x 2000px limits)

    Raises:
        ImportError: If opencv-python is not installed
        Exception: If frame extraction fails
    """
    try:
        import cv2
    except ImportError:
        raise ImportError(
            "opencv-python is required for video frame extraction. " "Please install it with: pip install opencv-python",
        )

    # OpenAI Vision API limits for images (same as understand_image)
    max_short_side = 768  # Maximum pixels for short side
    max_long_side = 2000  # Maximum pixels for long side

    # Open the video file
    video = cv2.VideoCapture(str(video_path))

    if not video.isOpened():
        raise Exception(f"Failed to open video file: {video_path}")

    try:
        # Get total number of frames
        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))

        if total_frames == 0:
            raise Exception(f"Video file has no frames: {video_path}")

        # Calculate frame indices to extract (evenly spaced)
        frame_indices = []
        if num_frames >= total_frames:
            # If requesting more frames than available, use all frames
            frame_indices = list(range(total_frames))
        else:
            # Extract evenly spaced frames
            step = total_frames / num_frames
            frame_indices = [int(i * step) for i in range(num_frames)]

        # Extract frames
        frames_base64 = []
        for frame_idx in frame_indices:
            # Set video position to the frame
            video.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)

            # Read the frame
            ret, frame = video.read()

            if not ret:
                continue

            # Check and resize frame if needed to fit OpenAI Vision API limits
            height, width = frame.shape[:2]
            short_side = min(width, height)
            long_side = max(width, height)

            if short_side > max_short_side or long_side > max_long_side:
                # Calculate scale factor to fit within dimension constraints
                short_scale = max_short_side / short_side if short_side > max_short_side else 1.0
                long_scale = max_long_side / long_side if long_side > max_long_side else 1.0
                scale_factor = min(short_scale, long_scale) * 0.95  # 0.95 for safety margin

                new_width = int(width * scale_factor)
                new_height = int(height * scale_factor)

                # Resize frame using LANCZOS (high quality)
                frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)

            # Encode frame to JPEG with quality=85 (same as understand_image)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 85]
            ret, buffer = cv2.imencode(".jpg", frame, encode_param)

            if not ret:
                continue

            # Convert to base64
            frame_base64 = base64.b64encode(buffer).decode("utf-8")
            frames_base64.append(frame_base64)

        if not frames_base64:
            raise Exception("Failed to extract any frames from video")

        return frames_base64

    finally:
        # Release the video capture object
        video.release()


def _get_mime_type(file_path: Path) -> str:
    """Get MIME type for a video file."""
    mime_type, _ = mimetypes.guess_type(str(file_path))
    if mime_type:
        return mime_type
    # Fallback
    ext = file_path.suffix.lower()
    fallbacks = {
        ".mp4": "video/mp4",
        ".mov": "video/quicktime",
        ".avi": "video/x-msvideo",
        ".mkv": "video/x-matroska",
        ".webm": "video/webm",
        ".m4v": "video/mp4",
        ".flv": "video/x-flv",
        ".wmv": "video/x-ms-wmv",
    }
    return fallbacks.get(ext, "video/mp4")


async def _process_with_gemini(
    video_path: Path,
    prompt: str,
    model: str = "gemini-3-flash-preview",
) -> str:
    """
    Process video using Gemini's native video understanding.

    Args:
        video_path: Path to the video file
        prompt: Prompt for analysis (string or dict with 'question' key)
        model: Gemini model to use

    Returns:
        Text analysis from Gemini
    """
    from google import genai
    from google.genai import types

    # Handle dict prompt (model sometimes outputs {"question": "..."})
    if isinstance(prompt, dict):
        prompt = prompt.get("question", str(prompt))

    # Read video data
    with open(video_path, "rb") as f:
        video_data = f.read()
    mime_type = _get_mime_type(video_path)

    # Create Gemini client
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY not found in environment")

    client = genai.Client(api_key=api_key)

    logger.info(f"[understand_video] Using Gemini {model} for video: {video_path.name}")

    # Use types.Part for proper SDK format
    contents = [
        types.Part.from_text(text=prompt),
        types.Part.from_bytes(data=video_data, mime_type=mime_type),
    ]

    # Make API call
    response = client.models.generate_content(
        model=model,
        contents=contents,
    )

    return response.text


async def _process_with_openai(
    video_path: Path,
    prompt: str,
    model: str = "gpt-4.1",
    num_frames: int = 8,
) -> str:
    """
    Process video using OpenAI's vision API with frame extraction.

    Args:
        video_path: Path to the video file
        prompt: Prompt for analysis
        model: OpenAI model to use
        num_frames: Number of frames to extract

    Returns:
        Text analysis from OpenAI
    """
    from openai import AsyncOpenAI

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment")

    client = AsyncOpenAI(api_key=api_key)

    # Extract frames from video
    frames_base64 = _extract_key_frames(video_path, num_frames)

    logger.info(
        f"[understand_video] Using OpenAI {model} for video: {video_path.name} " f"({len(frames_base64)} frames)",
    )

    # Build content array with prompt and all frames
    content = [{"type": "input_text", "text": prompt}]

    for frame_base64 in frames_base64:
        content.append(
            {
                "type": "input_image",
                "image_url": f"data:image/jpeg;base64,{frame_base64}",
            },
        )

    # Call OpenAI API
    response = await client.responses.create(
        model=model,
        input=[{"role": "user", "content": content}],
    )

    return response.output_text if hasattr(response, "output_text") else str(response.output)


async def _process_with_anthropic(
    video_path: Path,
    prompt: str,
    model: str = "claude-sonnet-4-5",
    num_frames: int = 8,
) -> str:
    """
    Process video using Anthropic's Claude vision API with frame extraction.

    Args:
        video_path: Path to the video file
        prompt: Prompt for analysis
        model: Claude model to use
        num_frames: Number of frames to extract

    Returns:
        Text analysis from Claude
    """
    import anthropic

    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not found in environment")

    client = anthropic.Anthropic(api_key=api_key)

    # Extract frames from video
    frames_base64 = _extract_key_frames(video_path, num_frames)

    logger.info(
        f"[understand_video] Using Anthropic {model} for video: {video_path.name} " f"({len(frames_base64)} frames)",
    )

    # Build content array with frames and prompt
    content = []
    for frame_base64 in frames_base64:
        content.append(
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": frame_base64,
                },
            },
        )
    content.append({"type": "text", "text": prompt})

    # Call Claude API
    response = client.messages.create(
        model=model,
        max_tokens=4096,
        messages=[{"role": "user", "content": content}],
    )

    return response.content[0].text


async def _process_with_grok(
    video_path: Path,
    prompt: str,
    model: str = "grok-4-1-fast-reasoning",
    num_frames: int = 8,
) -> str:
    """
    Process video using Grok's vision API with frame extraction.
    Grok uses OpenAI-compatible API.

    Args:
        video_path: Path to the video file
        prompt: Prompt for analysis
        model: Grok model to use
        num_frames: Number of frames to extract

    Returns:
        Text analysis from Grok
    """
    from openai import AsyncOpenAI

    api_key = os.getenv("XAI_API_KEY")
    if not api_key:
        raise ValueError("XAI_API_KEY not found in environment")

    client = AsyncOpenAI(api_key=api_key, base_url="https://api.x.ai/v1")

    # Extract frames from video
    frames_base64 = _extract_key_frames(video_path, num_frames)

    logger.info(
        f"[understand_video] Using Grok {model} for video: {video_path.name} " f"({len(frames_base64)} frames)",
    )

    # Build content array with prompt and all frames
    content = [{"type": "text", "text": prompt}]
    for frame_base64 in frames_base64:
        content.append(
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{frame_base64}"},
            },
        )

    # Call Grok API (OpenAI-compatible)
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": content}],
    )

    return response.choices[0].message.content


async def _process_with_openrouter(
    video_path: Path,
    prompt: str,
    model: str = "openai/gpt-4.1",
    num_frames: int = 8,
) -> str:
    """
    Process video using OpenRouter's API with frame extraction.
    OpenRouter uses OpenAI-compatible API.

    Args:
        video_path: Path to the video file
        prompt: Prompt for analysis
        model: Model to use (with provider prefix)
        num_frames: Number of frames to extract

    Returns:
        Text analysis from OpenRouter
    """
    from openai import AsyncOpenAI

    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError("OPENROUTER_API_KEY not found in environment")

    client = AsyncOpenAI(api_key=api_key, base_url="https://openrouter.ai/api/v1")

    # Extract frames from video
    frames_base64 = _extract_key_frames(video_path, num_frames)

    logger.info(
        f"[understand_video] Using OpenRouter {model} for video: {video_path.name} " f"({len(frames_base64)} frames)",
    )

    # Build content array with prompt and all frames
    content = [{"type": "text", "text": prompt}]
    for frame_base64 in frames_base64:
        content.append(
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{frame_base64}"},
            },
        )

    # Call OpenRouter API (OpenAI-compatible)
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": content}],
    )

    return response.choices[0].message.content


async def understand_video(
    video_path: str,
    prompt: str = "What's happening in this video? Please describe the content, actions, and any important details you observe.",
    num_frames: int = 8,
    model: Optional[str] = None,
    backend_type: Optional[str] = None,
    allowed_paths: Optional[List[str]] = None,
    agent_cwd: Optional[str] = None,
    task_context: Optional[str] = None,
) -> ExecutionResult:
    """
    Understand and analyze a video using the best available backend.

    Backend Selection Priority:
    1. Same backend as the calling agent (if backend_type specified and has API key)
    2. Default priority list: Gemini → OpenAI (first with available API key)

    Supports multiple backends:
    - Gemini: Uses native video understanding with inline_data (preferred, no frame extraction)
    - OpenAI: Extracts key frames and uses vision API

    Args:
        video_path: Path to the video file (MP4, AVI, MOV, etc.)
                   - Relative path: Resolved relative to workspace
                   - Absolute path: Must be within allowed directories
        prompt: Question or instruction about the video (default: asks for general description)
        num_frames: Number of key frames to extract (only used for OpenAI, default: 8)
                   - Higher values provide more detail but increase API costs
                   - Recommended range: 4-16 frames
        model: Model to use. If not specified, uses default from backend selector:
               - Gemini: "gemini-3-flash-preview"
               - OpenAI: "gpt-4.1"
        backend_type: Preferred backend ("gemini" or "openai"). If specified and
                      has API key, this backend is used. Otherwise falls through
                      to priority list.
        allowed_paths: List of allowed base paths for validation (optional)
        agent_cwd: Agent's current working directory (automatically injected, optional)
        task_context: Context string or key used to augment the prompt (Optional[str])
                  - Accepts named contexts (e.g., "short_summary", "detailed_analysis") or raw context text.
                  - If None (default), no context-based augmentation is applied.

    Returns:
        ExecutionResult containing:
        - success: Whether operation succeeded
        - operation: "understand_video"
        - video_path: Path to the analyzed video
        - prompt: The prompt used
        - model: Model used for analysis
        - backend: Backend used ("gemini" or "openai")
        - response: The model's understanding/description of the video

    Examples:
        understand_video("demo.mp4")
        → Returns detailed description using best available backend

        understand_video("tutorial.mp4", "What steps are shown in this tutorial?")
        → Returns analysis of tutorial steps

        understand_video("demo.mp4", backend_type="gemini")
        → Prefers Gemini if available, otherwise falls back to OpenAI

    Security:
        - Requires valid API key for the chosen backend
        - Video file must exist and be readable
        - Supports common video formats (MP4, AVI, MOV, MKV, etc.)

    Note:
        - Gemini processes the full video natively (preferred)
        - OpenAI extracts still frames; audio content is not analyzed
    """
    try:
        # Convert allowed_paths from strings to Path objects
        allowed_paths_list = [Path(p) for p in allowed_paths] if allowed_paths else None

        # Load environment variables
        script_dir = Path(__file__).parent.parent.parent.parent
        env_path = script_dir / ".env"
        if env_path.exists():
            load_dotenv(env_path)
        else:
            load_dotenv()

        # Use backend selector to choose the best available backend
        backend_config = get_backend(
            media_type="video",
            preferred_backend=backend_type,
            preferred_model=model,
        )

        if not backend_config:
            result = {
                "success": False,
                "operation": "understand_video",
                "error": "No video backend available. Please set GOOGLE_API_KEY/GEMINI_API_KEY or OPENAI_API_KEY.",
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )

        selected_backend = backend_config.name
        selected_model = backend_config.model

        logger.info(
            f"[understand_video] Selected backend: {selected_backend}/{selected_model} " f"(preferred: {backend_type})",
        )

        # Resolve video path
        base_dir = Path(agent_cwd) if agent_cwd else Path.cwd()

        if Path(video_path).is_absolute():
            vid_path = Path(video_path).resolve()
        else:
            vid_path = (base_dir / video_path).resolve()

        # Validate video path
        _validate_path_access(vid_path, allowed_paths_list)

        if not vid_path.exists():
            result = {
                "success": False,
                "operation": "understand_video",
                "error": f"Video file does not exist: {vid_path}",
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )

        # Check if file is likely a video (by extension)
        video_extensions = [".mp4", ".avi", ".mov", ".mkv", ".flv", ".wmv", ".webm", ".m4v", ".mpg", ".mpeg"]
        if vid_path.suffix.lower() not in video_extensions:
            result = {
                "success": False,
                "operation": "understand_video",
                "error": f"File does not appear to be a video file: {vid_path}. Supported formats: {', '.join(video_extensions)}",
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )

        # Inject task context into prompt if available
        augmented_prompt = format_prompt_with_context(prompt, task_context)

        # Process video with the selected backend
        try:
            if selected_backend == "gemini":
                response_text = await _process_with_gemini(
                    video_path=vid_path,
                    prompt=augmented_prompt,
                    model=selected_model,
                )
            elif selected_backend == "claude":
                response_text = await _process_with_anthropic(
                    video_path=vid_path,
                    prompt=augmented_prompt,
                    model=selected_model,
                    num_frames=num_frames,
                )
            elif selected_backend == "grok":
                response_text = await _process_with_grok(
                    video_path=vid_path,
                    prompt=augmented_prompt,
                    model=selected_model,
                    num_frames=num_frames,
                )
            elif selected_backend == "openrouter":
                response_text = await _process_with_openrouter(
                    video_path=vid_path,
                    prompt=augmented_prompt,
                    model=selected_model,
                    num_frames=num_frames,
                )
            else:  # openai (default)
                response_text = await _process_with_openai(
                    video_path=vid_path,
                    prompt=augmented_prompt,
                    model=selected_model,
                    num_frames=num_frames,
                )

            result = {
                "success": True,
                "operation": "understand_video",
                "video_path": str(vid_path),
                "prompt": prompt,
                "model": selected_model,
                "backend": selected_backend,
                "response": response_text,
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )

        except ImportError as import_error:
            result = {
                "success": False,
                "operation": "understand_video",
                "error": str(import_error),
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )
        except Exception as api_error:
            result = {
                "success": False,
                "operation": "understand_video",
                "error": f"Video processing error: {str(api_error)}",
            }
            return ExecutionResult(
                output_blocks=[TextContent(data=json.dumps(result, indent=2))],
            )

    except Exception as e:
        result = {
            "success": False,
            "operation": "understand_video",
            "error": f"Failed to understand video: {str(e)}",
        }
        return ExecutionResult(
            output_blocks=[TextContent(data=json.dumps(result, indent=2))],
        )
