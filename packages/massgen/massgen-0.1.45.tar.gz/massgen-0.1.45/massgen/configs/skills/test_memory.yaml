# Minimal config to test memory filesystem mode
# Run with: uv run massgen --config massgen/configs/skills/test_memory.yaml "Create a website about Bob Dylan"

agents:
  - id: "agent_a"
    backend:
      # ==================== NATIVE BACKENDS ====================
      # type: "chatcompletion"
      # base_url: "https://openrouter.ai/api/v1"
      # model: "x-ai/grok-4.1-fast"
      # type: "openai"
      # model: "gpt-5-mini"
      type: "gemini"
      model: "gemini-3-pro-preview"
      # type: "grok"
      # model: "grok-4-1-fast-reasoning"
      # type: "claude_code"
      # model: "claude-opus-4-5-20251101"
      # type: "claude"
      # model: "claude-sonnet-4-20250514"
      # type: "azure_openai"
      # model: "gpt-4o"
      # base_url: "https://YOUR_RESOURCE.openai.azure.com"
      # api_version: "2024-02-15-preview"
      # type: "zai"
      # model: "glm-4.5"
      # base_url: "https://api.z.ai/api/paas/v4/"

      # ==================== LOCAL BACKENDS ====================
      # type: "lmstudio"
      # model: "local-model"
      # base_url: "http://localhost:1234/v1"
      # type: "vllm"
      # model: "your-model"
      # base_url: "http://localhost:8000/v1"
      # type: "sglang"
      # model: "your-model"
      # base_url: "http://localhost:30000/v1"

      # ==================== CHATCOMPLETION PROVIDERS ====================
      # type: "chatcompletion"
      # base_url: "https://api.cerebras.ai/v1"
      # model: "llama-4-scout-17b-16e-instruct"
      # type: "chatcompletion"
      # base_url: "https://api.together.xyz/v1"
      # model: "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"
      # type: "chatcompletion"
      # base_url: "https://api.fireworks.ai/inference/v1"
      # model: "accounts/fireworks/models/llama-v3p1-405b-instruct"
      # type: "chatcompletion"
      # base_url: "https://api.groq.com/openai/v1"
      # model: "llama-3.3-70b-versatile"
      # type: "chatcompletion"
      # base_url: "https://api.moonshot.cn/v1"
      # model: "moonshot-v1-128k"
      # type: "chatcompletion"
      # base_url: "https://api.studio.nebius.ai/v1"
      # model: "Qwen/Qwen3-4B-fast"
      # type: "chatcompletion"
      # base_url: "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
      # model: "qwen-max"

      cwd: "workspace1"  # Required for filesystem mode
      enable_mcp_command_line: true
      command_line_execution_mode: "docker"

  # - id: "agent_b"
  #   backend:
  #     type: "gemini"
  #     model: "gemini-2.5-pro"
  #     cwd: "workspace2"  # Required for filesystem mode
  #     enable_mcp_command_line: true
  #     command_line_execution_mode: "docker"

orchestrator:
  snapshot_storage: "snapshots"
  agent_temporary_workspace: "temp_workspaces"

  coordination:
    # Enable task planning
    enable_agent_task_planning: true
    task_planning_filesystem_mode: true
    # Enable memory filesystem mode
    enable_memory_filesystem_mode: true
    # Enable automatic persona generation for agent diversity
    # persona_generator:
    #   enabled: true
    #   backend:
    #     type: openai
    #     model: gpt-4o-mini
    #   strategy: complementary
