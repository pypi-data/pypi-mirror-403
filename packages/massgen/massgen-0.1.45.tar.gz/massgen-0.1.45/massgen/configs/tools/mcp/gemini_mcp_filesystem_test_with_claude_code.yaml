# MassGen Configuration
# Usage:
# massgen --config @examples/tools/mcp/gemini_mcp_filesystem_test_with_claude_code "Create a presentation that teaches a reinforcement learning algorithm and output it in LaTeX Beamer format. No figures should be added."
agents:
  - id: "gemini_agent"
    backend:
      type: "gemini"
      model: "gemini-2.5-pro"
      cwd: "workspace1"  # Working directory for file operations
      enable_web_search: true

  - id: "claude_code"
    backend:
      type: "claude_code"
      model: "claude-sonnet-4-20250514"
      cwd: "workspace2"  # Working directory for file operations

orchestrator:
    snapshot_storage: "snapshots"  # Directory to store workspace snapshots
    agent_temporary_workspace: "temp_workspaces"  # Directory for temporary agent workspaces

ui:
  display_type: "textual_terminal"
  logging_enabled: true

# ====================
# MEMORY CONFIGURATION
# ====================
memory:
  # Enable/disable persistent memory (default: true)
  enabled: true
  # # Debug mode
  # debug: true

  # Memory configuration
  conversation_memory:
    enabled: true  # Short-term conversation tracking (recommended: always true)

  persistent_memory:
    enabled: true  # Long-term knowledge storage (set to false to disable)
    on_disk: true  # Persist across restarts
    # session_name: "test_session"  # Optional - if not specified, auto-generates unique ID
                                     # Format: agent_storyteller_20251023_143022_a1b2c3
                                     # Specify to continue a specific session

    # Vector store backend (default: qdrant)
    vector_store: "qdrant"

    # LLM configuration for memory operations (fact extraction)
    # RECOMMENDED: Use mem0's native LLMs (no adapter overhead, no async complexity)
    llm:
      provider: "openai"  # Options: openai, anthropic, groq, together, etc.
      model: "gpt-4.1-nano-2025-04-14"  # Fast and cheap model for memory ops (mem0's default)

    # Embedding configuration (uses mem0's native embedders)
    # RECOMMENDED: Specify provider and model for clarity
    embedding:
      provider: "openai"  # Options: openai, together, azure_openai, gemini, huggingface, etc.
      model: "text-embedding-3-small"  # OpenAI's efficient embedding model

    # Qdrant client configuration
    # IMPORTANT: For multi-agent setups, use server mode to avoid concurrent access errors
    qdrant:
      mode: "server"  # Options: "server" (recommended for multi-agent) or "local" (single agent only)
      host: "localhost"  # Qdrant server host (default: localhost)
      port: 6333         # Qdrant server port (default: 6333)
      # For local mode (single agent only):
      # mode: "local"
      # path: ".massgen/qdrant"  # Local storage path

  # # Enable detailed recording for testing
  # recording:
  #   record_all_tool_calls: true   # Capture ALL MCP tools
  #   record_reasoning: true         # Capture reasoning separately

  # Context window management thresholds
  compression:
    trigger_threshold: 0.25  # Compress when context usage exceeds 25%
    target_ratio: 0.10       # Target 10% of context after compression

  # Memory retrieval configuration
  retrieval:
    limit: 5              # Number of memory facts to retrieve from mem0 (default: 5)
    exclude_recent: true  # Only retrieve after compression to avoid duplicates (default: true)