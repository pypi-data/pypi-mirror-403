# Example Configuration: Context Window Management with Memory
#
# Use Case: Demonstrates automatic context compression when approaching token limits
#
# This configuration demonstrates:
# - Automatic context window monitoring and compression
# - Token-aware conversation management (75% threshold, 40% target)
# - Persistent memory integration for long-term knowledge retention
# - Graceful handling when context window fills up
# - Multi-agent collaboration with shared context management
#
# Run with:
# uv run massgen --config @examples/memory/gpt5mini_high_reasoning_gemini.yaml "Analyze the pros and cons of using LLMs in commercial applications."

# ====================
# AGENT DEFINITIONS
# ====================
agents:
  - id: "agent_a"
    backend:
      # Use GPT-5-mini with medium reasoning
      type: "openai"
      model: "gpt-5-mini"
      text:
        verbosity: "medium"
      reasoning:
        effort: "high"
        summary: "auto"
      enable_web_search: true

  - id: "agent_b"
    backend:
      # Use Gemini 2.5 Flash for cost-effective testing
      type: "gemini"
      model: "gemini-2.5-flash"
      enable_web_search: true

# ====================
# MEMORY CONFIGURATION
# ====================
memory:
  # Enable/disable persistent memory (default: true)
  enabled: true

  # Memory configuration
  conversation_memory:
    enabled: true  # Short-term conversation tracking (recommended: always true)

  persistent_memory:
    enabled: true  # Long-term knowledge storage (set to false to disable)
    on_disk: true  # Persist across restarts
    # session_name: "test_session"  # Optional - if not specified, auto-generates unique ID
                                     # Format: agent_storyteller_20251023_143022_a1b2c3
                                     # Specify to continue a specific session

    # Vector store backend (default: qdrant)
    vector_store: "qdrant"

    # LLM configuration for memory operations (fact extraction)
    # RECOMMENDED: Use mem0's native LLMs (no adapter overhead, no async complexity)
    llm:
      provider: "openai"  # Options: openai, anthropic, groq, together, etc.
      model: "gpt-4.1-nano-2025-04-14"  # Fast and cheap model for memory ops (mem0's default)

    # Embedding configuration (uses mem0's native embedders)
    # RECOMMENDED: Specify provider and model for clarity
    embedding:
      provider: "openai"  # Options: openai, together, azure_openai, gemini, huggingface, etc.
      model: "text-embedding-3-small"  # OpenAI's efficient embedding model

    # Qdrant client configuration
    # IMPORTANT: For multi-agent setups, use server mode to avoid concurrent access errors
    qdrant:
      mode: "server"  # Options: "server" (recommended for multi-agent) or "local" (single agent only)
      host: "localhost"  # Qdrant server host (default: localhost)
      port: 6333         # Qdrant server port (default: 6333)
      # For local mode (single agent only):
      # mode: "local"
      # path: ".massgen/qdrant"  # Local storage path

  # Context window management thresholds
  compression:
    trigger_threshold: 0.25  # Compress when context usage exceeds 25%
    target_ratio: 0.10       # Target 10% of context after compression

  # Memory retrieval configuration
  retrieval:
    limit: 5              # Number of memory facts to retrieve from mem0 (default: 5)
    exclude_recent: true  # Only retrieve after compression to avoid duplicates (default: true)

# ====================
# ORCHESTRATOR CONFIGURATION
# ====================
orchestrator:
  # Multi-turn mode to enable interactive storytelling

  # Agent workspace for any file operations
  agent_temporary_workspace: "memory_test_workspaces"
  snapshot_storage: "memory_test_snapshots"

# ====================
# UI CONFIGURATION
# ====================
ui:
  display_type: "textual_terminal"
  logging_enabled: true