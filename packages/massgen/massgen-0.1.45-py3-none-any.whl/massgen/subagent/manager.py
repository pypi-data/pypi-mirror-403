# -*- coding: utf-8 -*-
"""
Subagent Manager for MassGen

Manages the lifecycle of subagents: creation, workspace setup, execution, and result collection.
"""

import asyncio
import json
import logging
import shutil
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import yaml

from massgen.structured_logging import (
    log_subagent_complete,
    log_subagent_spawn,
    trace_subagent_execution,
)
from massgen.subagent.models import (
    SUBAGENT_DEFAULT_TIMEOUT,
    SUBAGENT_MAX_TIMEOUT,
    SUBAGENT_MIN_TIMEOUT,
    SubagentConfig,
    SubagentDisplayData,
    SubagentOrchestratorConfig,
    SubagentPointer,
    SubagentResult,
    SubagentState,
)

logger = logging.getLogger(__name__)


class SubagentManager:
    """
    Manages subagent lifecycle, workspaces, and execution.

    Responsible for:
    - Creating isolated workspaces for subagents
    - Spawning and executing subagent tasks
    - Collecting and formatting results
    - Tracking active subagents
    - Cleanup on completion

    Subagents cannot spawn their own subagents (no nesting).
    """

    def __init__(
        self,
        parent_workspace: str,
        parent_agent_id: str,
        orchestrator_id: str,
        parent_agent_configs: List[Dict[str, Any]],
        max_concurrent: int = 3,
        default_timeout: int = SUBAGENT_DEFAULT_TIMEOUT,
        min_timeout: int = SUBAGENT_MIN_TIMEOUT,
        max_timeout: int = SUBAGENT_MAX_TIMEOUT,
        subagent_orchestrator_config: Optional[SubagentOrchestratorConfig] = None,
        log_directory: Optional[str] = None,
        parent_context_paths: Optional[List[Dict[str, str]]] = None,
        parent_coordination_config: Optional[Dict[str, Any]] = None,
        agent_temporary_workspace: Optional[str] = None,
    ):
        """
        Initialize SubagentManager.

        Args:
            parent_workspace: Path to parent agent's workspace
            parent_agent_id: ID of the parent agent
            orchestrator_id: ID of the orchestrator
            parent_agent_configs: List of parent agent configurations to inherit.
                Each config should have 'id' and 'backend' keys.
            max_concurrent: Maximum concurrent subagents (default 3)
            default_timeout: Default timeout in seconds (default 300)
            min_timeout: Minimum allowed timeout in seconds (default 60)
            max_timeout: Maximum allowed timeout in seconds (default 600)
            subagent_orchestrator_config: Configuration for subagent orchestrator mode.
                When enabled, subagents use a full Orchestrator with multiple agents
                instead of a single ConfigurableAgent.
            log_directory: Path to main run's log directory for subagent logs.
                Subagent logs will be written to {log_directory}/subagents/{subagent_id}/
            parent_context_paths: List of context paths from the parent orchestrator.
                These are passed to subagents as read-only context paths so they can
                access the same codebase/files as the parent agent.
            parent_coordination_config: Coordination config from the parent orchestrator.
                Used to inherit settings like enable_agent_task_planning for subagents.
            agent_temporary_workspace: Path to agent temporary workspace parent directory.
                When provided, list_subagents() will scan all agent temp workspaces to
                show subagents from all agents (following workspace visibility principles).
        """
        self.parent_workspace = Path(parent_workspace)
        self.parent_agent_id = parent_agent_id
        self.orchestrator_id = orchestrator_id
        self.parent_agent_configs = parent_agent_configs or []
        self.max_concurrent = max_concurrent
        self.default_timeout = default_timeout
        self.min_timeout = min_timeout
        self.max_timeout = max_timeout
        self._subagent_orchestrator_config = subagent_orchestrator_config
        self._parent_context_paths = parent_context_paths or []
        self._parent_coordination_config = parent_coordination_config or {}
        self._agent_temporary_workspace = Path(agent_temporary_workspace) if agent_temporary_workspace else None

        # Log directory for subagent logs (in main run's log dir)
        self._log_directory = Path(log_directory) if log_directory else None
        if self._log_directory:
            self._subagent_logs_base = self._log_directory / "subagents"
            self._subagent_logs_base.mkdir(parents=True, exist_ok=True)
        else:
            self._subagent_logs_base = None

        # Base path for all subagent workspaces
        self.subagents_base = self.parent_workspace / "subagents"
        self.subagents_base.mkdir(parents=True, exist_ok=True)

        # Track active and completed subagents
        self._subagents: Dict[str, SubagentState] = {}
        # Track background tasks for non-blocking execution
        self._background_tasks: Dict[str, asyncio.Task] = {}
        # Track active subprocess handles for graceful cancellation
        self._active_processes: Dict[str, asyncio.subprocess.Process] = {}
        # Track session IDs for each subagent (for continuation support)
        self._subagent_sessions: Dict[str, str] = {}  # subagent_id -> session_id
        self._semaphore = asyncio.Semaphore(max_concurrent)
        # Callbacks to invoke when background subagents complete
        self._completion_callbacks: List[Callable[[str, SubagentResult], None]] = []

        logger.info(
            f"[SubagentManager] Initialized for parent {parent_agent_id}, "
            f"workspace: {self.subagents_base}, max_concurrent: {max_concurrent}, "
            f"timeout: {default_timeout}s (min: {min_timeout}s, max: {max_timeout}s)" + (f", log_dir: {self._subagent_logs_base}" if self._subagent_logs_base else ""),
        )

    def _clamp_timeout(self, timeout: Optional[int]) -> int:
        """
        Clamp timeout to configured min/max range.

        Args:
            timeout: Requested timeout in seconds (None uses default)

        Returns:
            Timeout clamped to [min_timeout, max_timeout] range
        """
        effective_timeout = timeout if timeout is not None else self.default_timeout
        return max(self.min_timeout, min(self.max_timeout, effective_timeout))

    def register_completion_callback(
        self,
        callback: Callable[[str, "SubagentResult"], None],
    ) -> None:
        """
        Register a callback to be invoked when any background subagent completes.

        The callback receives the subagent_id and SubagentResult when a background
        subagent finishes execution (success, timeout, or error). This is used
        to notify the Orchestrator about completed async subagents so results
        can be injected into the parent agent's context.

        Args:
            callback: Function that takes (subagent_id: str, result: SubagentResult)

        Example:
            def on_complete(subagent_id: str, result: SubagentResult):
                print(f"Subagent {subagent_id} completed: {result.status}")

            manager.register_completion_callback(on_complete)
        """
        self._completion_callbacks.append(callback)
        logger.debug(
            f"[SubagentManager] Registered completion callback, " f"total callbacks: {len(self._completion_callbacks)}",
        )

    def _invoke_completion_callbacks(
        self,
        subagent_id: str,
        result: "SubagentResult",
    ) -> None:
        """
        Invoke all registered completion callbacks for a finished subagent.

        Errors in individual callbacks are caught and logged but don't
        prevent other callbacks from executing.

        Args:
            subagent_id: ID of the completed subagent
            result: The subagent's execution result
        """
        for callback in self._completion_callbacks:
            try:
                callback(subagent_id, result)
            except Exception as e:
                logger.error(
                    f"[SubagentManager] Completion callback error for {subagent_id}: {e}",
                    exc_info=True,
                )

    def _create_workspace(self, subagent_id: str) -> Path:
        """
        Create isolated workspace for a subagent.

        Args:
            subagent_id: Unique subagent identifier

        Returns:
            Path to the subagent's workspace directory
        """
        subagent_dir = self.subagents_base / subagent_id
        workspace = subagent_dir / "workspace"
        workspace.mkdir(parents=True, exist_ok=True)

        # Create metadata file
        metadata = {
            "subagent_id": subagent_id,
            "parent_agent_id": self.parent_agent_id,
            "created_at": datetime.now().isoformat(),
            "workspace_path": str(workspace),
        }
        metadata_file = subagent_dir / "_metadata.json"
        metadata_file.write_text(json.dumps(metadata, indent=2))

        logger.info(f"[SubagentManager] Created workspace for {subagent_id}: {workspace}")
        return workspace

    def _get_subagent_log_dir(self, subagent_id: str) -> Optional[Path]:
        """
        Get or create the log directory for a subagent.

        Args:
            subagent_id: Subagent identifier

        Returns:
            Path to subagent log directory, or None if logging not configured
        """
        if not self._subagent_logs_base:
            return None

        log_dir = self._subagent_logs_base / subagent_id
        log_dir.mkdir(parents=True, exist_ok=True)
        return log_dir

    def _create_live_logs_symlink(self, subagent_id: str, workspace: Path) -> None:
        """Create a symlink to the subprocess's live log directory.

        This enables the TUI to stream logs in real-time during subagent execution.
        The symlink points to workspace/.massgen/massgen_logs/ where the subprocess
        writes its logs. After completion, full_logs/ contains the copied logs.

        Args:
            subagent_id: Subagent identifier
            workspace: Subagent workspace path
        """
        if not self._subagent_logs_base:
            return

        log_dir = self._subagent_logs_base / subagent_id
        log_dir.mkdir(parents=True, exist_ok=True)

        live_logs_link = log_dir / "live_logs"
        target = workspace / ".massgen" / "massgen_logs"

        # Create the target directory so the symlink is valid immediately
        # The subprocess will populate it when it starts
        target.mkdir(parents=True, exist_ok=True)

        try:
            # Remove existing symlink if present
            if live_logs_link.is_symlink():
                live_logs_link.unlink()
            elif live_logs_link.exists():
                # Not a symlink but exists - skip to avoid data loss
                logger.warning(f"[SubagentManager] live_logs exists but is not a symlink: {live_logs_link}")
                return

            live_logs_link.symlink_to(target)
            logger.debug(f"[SubagentManager] Created live_logs symlink: {live_logs_link} -> {target}")
        except Exception as e:
            logger.warning(f"[SubagentManager] Failed to create live_logs symlink: {e}")

    def _append_conversation(
        self,
        subagent_id: str,
        role: str,
        content: str,
        agent_id: Optional[str] = None,
    ) -> None:
        """
        Append a message to the conversation log.

        Args:
            subagent_id: Subagent identifier
            role: Message role (user, assistant, system)
            content: Message content
            agent_id: Optional agent ID for multi-agent orchestrator mode
        """
        log_dir = self._get_subagent_log_dir(subagent_id)
        if not log_dir:
            return

        conversation_file = log_dir / "conversation.json"

        # Read existing conversation
        conversation = []
        if conversation_file.exists():
            try:
                conversation = json.loads(conversation_file.read_text())
            except json.JSONDecodeError:
                pass

        # Append new message
        message = {
            "timestamp": datetime.now().isoformat(),
            "role": role,
            "content": content,
        }
        if agent_id:
            message["agent_id"] = agent_id

        conversation.append(message)
        conversation_file.write_text(json.dumps(conversation, indent=2))

    def _save_subagent_to_registry(
        self,
        subagent_id: str,
        session_id: str,
        config: SubagentConfig,
        result: SubagentResult,
    ) -> None:
        """Save subagent metadata to parent workspace registry.

        This registry tracks all subagents spawned in the current parent session,
        enabling continuation and discovery across turns.

        Registry format matches MCP server's _save_subagents_to_filesystem() format:
        {
            "parent_agent_id": "...",
            "orchestrator_id": "...",
            "subagents": [{"subagent_id": "...", ...}, ...]
        }

        Args:
            subagent_id: Unique subagent identifier
            session_id: Session ID for continuation
            config: Subagent configuration
            result: Execution result
        """
        registry_file = self.subagents_base / "_registry.json"

        # Load existing registry or create new one with list format
        if registry_file.exists():
            try:
                registry = json.loads(registry_file.read_text())
                # Ensure subagents is a list (handle old dict format)
                if isinstance(registry.get("subagents"), dict):
                    # Convert old dict format to list format
                    old_dict = registry["subagents"]
                    registry["subagents"] = [{"subagent_id": k, **v} for k, v in old_dict.items()]
            except json.JSONDecodeError:
                logger.warning("[SubagentManager] Failed to parse registry, creating new one")
                registry = {
                    "parent_agent_id": self.parent_agent_id,
                    "orchestrator_id": self.orchestrator_id,
                    "subagents": [],
                }
        else:
            registry = {
                "parent_agent_id": self.parent_agent_id,
                "orchestrator_id": self.orchestrator_id,
                "subagents": [],
            }

        # Ensure metadata fields exist
        registry.setdefault("parent_agent_id", self.parent_agent_id)
        registry.setdefault("orchestrator_id", self.orchestrator_id)

        # Update or append subagent entry in list
        subagent_data = {
            "subagent_id": subagent_id,
            "session_id": session_id,
            "task": config.task[:200],  # Truncate for readability
            "status": result.status,
            "workspace": str(result.workspace_path),
            "created_at": datetime.now().isoformat(),
            "execution_time_seconds": result.execution_time_seconds,
            "success": result.success,
            "continuable": True,
            "source_agent": self.parent_agent_id,
        }

        # Find and update existing entry, or append new one
        subagents_list = registry["subagents"]
        found = False
        for i, entry in enumerate(subagents_list):
            if entry.get("subagent_id") == subagent_id:
                subagents_list[i] = subagent_data
                found = True
                break

        if not found:
            subagents_list.append(subagent_data)

        # Write back to disk
        registry_file.write_text(json.dumps(registry, indent=2))
        logger.debug(f"[SubagentManager] Saved subagent {subagent_id} to registry")

    def _copy_context_files(
        self,
        subagent_id: str,
        context_files: List[str],
        workspace: Path,
    ) -> List[str]:
        """
        Copy context files from parent workspace to subagent workspace.

        Also automatically copies CONTEXT.md if it exists in parent workspace,
        ensuring subagents have task context for external API calls.

        Args:
            subagent_id: Subagent identifier
            context_files: List of relative paths to copy
            workspace: Subagent workspace path

        Returns:
            List of successfully copied files
        """
        copied = []

        # Auto-copy CONTEXT.md if it exists (for task context)
        context_md = self.parent_workspace / "CONTEXT.md"
        if context_md.exists() and context_md.is_file():
            dst = workspace / "CONTEXT.md"
            try:
                shutil.copy2(context_md, dst)
                copied.append("CONTEXT.md")
                logger.info(f"[SubagentManager] Auto-copied CONTEXT.md for {subagent_id}")
            except Exception as e:
                logger.warning(f"[SubagentManager] Failed to copy CONTEXT.md: {e}")

        for rel_path in context_files:
            src = self.parent_workspace / rel_path
            if not src.exists():
                logger.warning(f"[SubagentManager] Context file not found: {src}")
                continue

            # Preserve directory structure
            dst = workspace / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)

            if src.is_file():
                shutil.copy2(src, dst)
                copied.append(rel_path)
            elif src.is_dir():
                shutil.copytree(
                    src,
                    dst,
                    dirs_exist_ok=True,
                    symlinks=True,
                    ignore_dangling_symlinks=True,
                )
                copied.append(rel_path)

        logger.info(f"[SubagentManager] Copied {len(copied)} context files for {subagent_id}")
        return copied

    def _build_subagent_system_prompt(
        self,
        config: SubagentConfig,
        workspace: Optional[Path] = None,
    ) -> tuple[str, Optional[str]]:
        """
        Build system prompt for subagent.

        Subagents get a minimal system prompt focused on their specific task.
        They cannot spawn their own subagents.

        Args:
            config: Subagent configuration
            workspace: Optional workspace path to read CONTEXT.md from

        Returns:
            Tuple of (system_prompt, context_warning).
            context_warning is set if CONTEXT.md was truncated.
        """
        base_prompt = config.system_prompt

        # Load task context from CONTEXT.md (required)
        context_section = ""
        task_context = None
        context_warning = None

        # Try to read CONTEXT.md from workspace using shared utility
        if workspace:
            from massgen.context.task_context import load_task_context_with_warning

            task_context, context_warning = load_task_context_with_warning(str(workspace))

        # CONTEXT.md is required for subagents
        # Note: This should have been validated earlier in spawn_subagent/spawn_subagent_background
        # If we reach here without task_context, something went wrong in the validation
        if not task_context:
            logger.warning(
                "[SubagentManager] CONTEXT.md missing in system prompt builder " "(should have been validated earlier)",
            )
            # Use empty context rather than crashing
            context_section = ""
        else:
            context_section = f"""
**Task Context:**
{task_context}

"""

        subagent_prompt = f"""## Subagent Context

You are a subagent spawned to work on a specific task. Your workspace is isolated and independent.
{context_section}
**Important:**
- Focus only on the task you were given
- Create any necessary files in your workspace
- You cannot spawn additional subagents
- Do not ask the human or request human input; subagents cannot broadcast to humans

**Output Requirements:**
- In your final answer, clearly list all files you want the parent agent to see along with their FULL ABSOLUTE PATHS. You can also list directories if needed.
- You should NOT list every single file as the parent agent does not need to know every file you created -- this context isolation is a main feature of subagents.
- The parent agent will copy files from your workspace based on your answer
- Format file paths clearly, e.g.: "Files created: /path/to/file1.md, /path/to/file2.py"

**Your Task:**
{config.task}
"""
        if base_prompt:
            subagent_prompt = f"{base_prompt}\n\n{subagent_prompt}"

        return subagent_prompt, context_warning

    async def _execute_subagent(
        self,
        config: SubagentConfig,
        workspace: Path,
    ) -> SubagentResult:
        """
        Execute a subagent task - routes to single agent or orchestrator mode.

        Args:
            config: Subagent configuration
            workspace: Path to subagent workspace

        Returns:
            SubagentResult with execution outcome
        """
        start_time = time.time()

        # Capture context warning early so it's available for all error paths
        from massgen.context.task_context import load_task_context_with_warning

        _, context_warning = load_task_context_with_warning(str(workspace))

        try:
            # Always use orchestrator mode for subagent execution
            return await self._execute_with_orchestrator(
                config,
                workspace,
                start_time,
                context_warning,
            )

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            log_dir = self._get_subagent_log_dir(config.id)
            # Attempt to recover completed work from workspace
            return self._create_timeout_result_with_recovery(
                subagent_id=config.id,
                workspace=workspace,
                timeout_seconds=execution_time,
                log_path=str(log_dir) if log_dir else None,
                warning=context_warning,
            )

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"[SubagentManager] Error executing subagent {config.id}: {e}")
            return SubagentResult.create_error(
                subagent_id=config.id,
                error=str(e),
                workspace_path=str(workspace),
                execution_time_seconds=execution_time,
                warning=context_warning,
            )

    async def _execute_with_orchestrator(
        self,
        config: SubagentConfig,
        workspace: Path,
        start_time: float,
        context_warning: Optional[str] = None,
    ) -> SubagentResult:
        """
        Execute subagent by spawning a separate MassGen process.

        This approach avoids nested MCP/async issues by running the subagent
        as a completely independent MassGen instance with its own YAML config.

        Args:
            config: Subagent configuration
            workspace: Path to subagent workspace
            start_time: Execution start time
            context_warning: Warning message if CONTEXT.md was truncated

        Returns:
            SubagentResult with execution outcome
        """
        orch_config = self._subagent_orchestrator_config

        # Build context paths from config.context_files
        # These are ALWAYS read-only - subagents cannot write to context paths.
        # If the parent agent needs changes from the subagent, it should copy
        # the desired files from the subagent's workspace after completion.
        context_paths: List[Dict[str, str]] = []
        if config.context_files:
            for ctx_file in config.context_files:
                src_path = Path(ctx_file)
                if src_path.exists():
                    context_paths.append(
                        {
                            "path": str(src_path.resolve()),
                            "permission": "read",
                        },
                    )
                    logger.info(f"[SubagentManager] Adding read-only context path: {src_path}")
                else:
                    logger.warning(f"[SubagentManager] Context file not found: {ctx_file}")

        # Generate temporary YAML config for the subagent
        subagent_yaml = self._generate_subagent_yaml_config(config, workspace, context_paths)
        yaml_path = workspace / f"subagent_config_{config.id}.yaml"
        yaml_path.write_text(yaml.dump(subagent_yaml, default_flow_style=False))

        num_agents = orch_config.num_agents if orch_config else 1
        logger.info(
            f"[SubagentManager] Executing subagent {config.id} via subprocess " f"({num_agents} agents), config: {yaml_path}",
        )

        # Build the task - system prompt already includes the task at the end
        # Pass workspace to read CONTEXT.md for task context
        # Note: context_warning is passed in from _execute_subagent, so we ignore the one from _build_subagent_system_prompt
        system_prompt, _ = self._build_subagent_system_prompt(config, workspace)
        full_task = system_prompt

        # Build command to run MassGen as subprocess
        # Use --automation for minimal output and --output-file to capture the answer
        # DON'T use --session-id for initial spawn (that's for restoring existing sessions)
        # We'll extract the auto-generated session ID from the subprocess status afterward
        answer_file = workspace / "answer.txt"
        cmd = [
            "uv",
            "run",
            "massgen",
            "--config",
            str(yaml_path),
            "--automation",  # Silent mode with minimal output
            "--output-file",
            str(answer_file),  # Write final answer to file
            full_task,
        ]

        process: Optional[asyncio.subprocess.Process] = None
        try:
            # Use async subprocess for graceful cancellation support
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(workspace),
            )

            # Track the process for potential cancellation
            self._active_processes[config.id] = process

            # Create symlink to live logs for TUI streaming
            # The subprocess writes to workspace/.massgen/massgen_logs/
            # We symlink this to _subagent_logs_base/{id}/live_logs for easy access
            self._create_live_logs_symlink(config.id, workspace)

            # Wait with timeout (clamped to configured min/max)
            timeout = self._clamp_timeout(config.timeout_seconds)
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=timeout,
                )
            except asyncio.TimeoutError:
                # Kill the process on timeout
                logger.warning(f"[SubagentManager] Subagent {config.id} timed out, terminating...")
                process.terminate()
                try:
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except asyncio.TimeoutError:
                    process.kill()
                    await process.wait()
                raise
            finally:
                # Remove from active processes
                self._active_processes.pop(config.id, None)

            if process.returncode == 0:
                # Read answer from the output file
                if answer_file.exists():
                    answer = answer_file.read_text().strip()
                else:
                    # Fallback to stdout if file wasn't created
                    answer = stdout.decode() if stdout else ""

                execution_time = time.time() - start_time

                # Get token usage, log path, and session ID from subprocess's status.json
                token_usage, subprocess_log_dir, session_id = self._parse_subprocess_status(workspace)

                # Track session ID for continuation support
                if session_id:
                    self._subagent_sessions[config.id] = session_id
                    logger.info(f"[SubagentManager] Tracked session ID for {config.id}: {session_id}")

                # Write reference to subprocess log directory
                self._write_subprocess_log_reference(config.id, subprocess_log_dir)

                # Get log directory path for the result
                log_dir = self._get_subagent_log_dir(config.id)

                return SubagentResult.create_success(
                    subagent_id=config.id,
                    answer=answer,
                    workspace_path=str(workspace),
                    execution_time_seconds=execution_time,
                    token_usage=token_usage,
                    log_path=str(log_dir) if log_dir else None,
                    warning=context_warning,
                )
            else:
                stderr_text = stderr.decode() if stderr else ""
                stdout_text = stdout.decode() if stdout else ""
                error_msg = stderr_text.strip() or f"Subprocess exited with code {process.returncode}"

                # Log detailed error information for debugging
                logger.error(
                    f"[SubagentManager] Subagent {config.id} failed with exit code {process.returncode}\n"
                    f"Command: {' '.join(cmd)}\n"
                    f"Working directory: {workspace}\n"
                    f"STDERR: {stderr_text[:1000]}\n"  # First 1000 chars of stderr
                    f"STDOUT: {stdout_text[:500]}",  # First 500 chars of stdout
                )

                # Still try to get log path for debugging
                _, subprocess_log_dir, _ = self._parse_subprocess_status(workspace)
                self._write_subprocess_log_reference(config.id, subprocess_log_dir, error=error_msg)
                log_dir = self._get_subagent_log_dir(config.id)
                return SubagentResult.create_error(
                    subagent_id=config.id,
                    error=error_msg,
                    workspace_path=str(workspace),
                    execution_time_seconds=time.time() - start_time,
                    log_path=str(log_dir) if log_dir else None,
                    warning=context_warning,
                )

        except asyncio.TimeoutError:
            logger.error(f"[SubagentManager] Subagent {config.id} timed out")
            # Still copy logs even on timeout - they contain useful debugging info
            _, subprocess_log_dir, _ = self._parse_subprocess_status(workspace)
            self._write_subprocess_log_reference(config.id, subprocess_log_dir, error="Subagent timed out")
            log_dir = self._get_subagent_log_dir(config.id)
            # Attempt to recover completed work from workspace
            return self._create_timeout_result_with_recovery(
                subagent_id=config.id,
                workspace=workspace,
                timeout_seconds=timeout,  # Use the clamped timeout
                log_path=str(log_dir) if log_dir else None,
                warning=context_warning,
            )
        except asyncio.CancelledError:
            # Handle graceful cancellation (e.g., from Ctrl+C)
            logger.warning(f"[SubagentManager] Subagent {config.id} cancelled")
            if process and process.returncode is None:
                process.terminate()
                try:
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except asyncio.TimeoutError:
                    process.kill()
            self._active_processes.pop(config.id, None)
            # Still copy logs even on cancellation - they contain useful debugging info
            _, subprocess_log_dir, _ = self._parse_subprocess_status(workspace)
            self._write_subprocess_log_reference(config.id, subprocess_log_dir, error="Subagent cancelled")
            log_dir = self._get_subagent_log_dir(config.id)
            # Attempt to recover completed work from workspace
            return self._create_timeout_result_with_recovery(
                subagent_id=config.id,
                workspace=workspace,
                timeout_seconds=time.time() - start_time,
                log_path=str(log_dir) if log_dir else None,
                warning=context_warning,
            )
        except Exception as e:
            logger.error(f"[SubagentManager] Subagent {config.id} error: {e}")
            # Still copy logs even on error - they contain useful debugging info
            _, subprocess_log_dir, _ = self._parse_subprocess_status(workspace)
            self._write_subprocess_log_reference(config.id, subprocess_log_dir, error=str(e))
            log_dir = self._get_subagent_log_dir(config.id)
            return SubagentResult.create_error(
                subagent_id=config.id,
                error=str(e),
                workspace_path=str(workspace),
                execution_time_seconds=time.time() - start_time,
                log_path=str(log_dir) if log_dir else None,
                warning=context_warning,
            )

    def _generate_subagent_yaml_config(
        self,
        config: SubagentConfig,
        workspace: Path,
        context_paths: Optional[List[Dict[str, str]]] = None,
    ) -> Dict[str, Any]:
        """
        Generate a YAML config dict for the subagent MassGen process.

        Inherits relevant settings from parent agent configs but adjusts
        paths and disables subagent nesting.

        Args:
            config: Subagent configuration
            workspace: Workspace path for the subagent
            context_paths: Optional list of context path configs for file access

        Returns:
            Dictionary suitable for YAML serialization
        """
        orch_config = self._subagent_orchestrator_config
        refine = config.metadata.get("refine", True)

        # Determine agent configs to use:
        # 1. If subagent_orchestrator.agents is specified, use those
        # 2. Otherwise, inherit from parent agent configs
        if orch_config and orch_config.agents:
            # Use explicitly configured agents
            source_agents = orch_config.agents
        else:
            # Inherit parent agent configs (default behavior)
            source_agents = self.parent_agent_configs

        # Build agent configs - each agent needs a unique workspace directory
        agents = []
        num_agents = len(source_agents) if source_agents else 1

        for i in range(num_agents):
            # Create unique workspace for each agent
            agent_workspace = workspace / f"agent_{i+1}"
            agent_workspace.mkdir(parents=True, exist_ok=True)

            # Get source config for this agent
            if source_agents and i < len(source_agents):
                source_config = source_agents[i]
            else:
                source_config = {}

            # Build agent ID - use source id or auto-generate
            agent_id = source_config.get("id", f"{config.id}_agent_{i+1}")

            # Get backend config from source or use defaults
            source_backend = source_config.get("backend", {})

            # Get first parent backend as fallback for missing values
            fallback_backend = self.parent_agent_configs[0].get("backend", {}) if self.parent_agent_configs else {}

            backend_config = {
                "type": source_backend.get("type") or fallback_backend.get("type", "openai"),
                "model": source_backend.get("model") or config.model or fallback_backend.get("model"),
                "cwd": str(agent_workspace),  # Each agent gets unique workspace
                # Inherit relevant backend settings from first parent
                "enable_mcp_command_line": fallback_backend.get("enable_mcp_command_line", False),
                "command_line_execution_mode": fallback_backend.get("command_line_execution_mode", "local"),
            }

            # Handle enable_web_search: orchestrator config > inherit from parent
            # Note: This is set in YAML config, not by agents at runtime
            if orch_config and orch_config.enable_web_search is not None:
                backend_config["enable_web_search"] = orch_config.enable_web_search
            elif "enable_web_search" in fallback_backend:
                backend_config["enable_web_search"] = fallback_backend["enable_web_search"]

            # Inherit Docker settings if using docker mode
            if backend_config["command_line_execution_mode"] == "docker":
                docker_settings = [
                    "command_line_docker_image",
                    "command_line_docker_network_mode",
                    "command_line_docker_enable_sudo",
                    "command_line_docker_credentials",
                ]
                for setting in docker_settings:
                    if setting in fallback_backend:
                        backend_config[setting] = fallback_backend[setting]

            # Inherit code-based tools settings
            code_tools_settings = [
                "enable_code_based_tools",
                "exclude_file_operation_mcps",
                "shared_tools_directory",
                "auto_discover_custom_tools",
                "exclude_custom_tools",
                "direct_mcp_servers",
            ]
            for setting in code_tools_settings:
                if setting in fallback_backend:
                    backend_config[setting] = fallback_backend[setting]

            # Add base_url if specified (source or fallback)
            base_url = source_backend.get("base_url") or fallback_backend.get("base_url")
            if base_url:
                backend_config["base_url"] = base_url

            # Copy reasoning config if present (from source or fallback)
            if "reasoning" in source_backend:
                backend_config["reasoning"] = source_backend["reasoning"]
            elif "reasoning" in fallback_backend and "type" not in source_backend:
                backend_config["reasoning"] = fallback_backend["reasoning"]

            agent_config = {
                "id": agent_id,
                "backend": backend_config,
            }

            agents.append(agent_config)

        # Build coordination config - disable subagents to prevent nesting
        coord_settings = orch_config.coordination.copy() if orch_config and orch_config.coordination else {}
        coord_settings["enable_subagents"] = False  # CRITICAL: prevent nesting
        # Subagents should not broadcast to humans (e.g., ask_others with broadcast=human)
        if coord_settings.get("broadcast") == "human":
            coord_settings["broadcast"] = False

        # Inherit planning settings from parent if not explicitly set in subagent config
        # This allows subagents to use planning tools when the parent has them enabled
        planning_settings_to_inherit = [
            "enable_agent_task_planning",
            "task_planning_filesystem_mode",
        ]
        for setting in planning_settings_to_inherit:
            if setting not in coord_settings and setting in self._parent_coordination_config:
                coord_settings[setting] = self._parent_coordination_config[setting]
                logger.info(
                    f"[SubagentManager] Inherited {setting}={self._parent_coordination_config[setting]} from parent",
                )

        orchestrator_config = {
            "snapshot_storage": str(workspace / "snapshots"),
            "agent_temporary_workspace": str(workspace / "temp"),
            "coordination": coord_settings,
        }

        # Apply max_new_answers limit to prevent runaway iterations
        # This must be at the top level of orchestrator config (not inside coordination)
        if orch_config and orch_config.max_new_answers:
            orchestrator_config["max_new_answers_per_agent"] = orch_config.max_new_answers

        # Apply refinement overrides for quick mode (matches TUI behavior)
        if not refine:
            orchestrator_config["max_new_answers_per_agent"] = 1
            orchestrator_config["skip_final_presentation"] = True
            if num_agents == 1:
                orchestrator_config["skip_voting"] = True
            else:
                orchestrator_config["disable_injection"] = True
                orchestrator_config["defer_voting_until_all_answered"] = True

        # Merge context paths: parent context paths + task-specific context paths
        # Parent context paths are always read-only (subagents can read the codebase)
        # Task-specific context paths from context_files are also read-only
        merged_context_paths: List[Dict[str, str]] = []

        # Add parent context paths first (always read-only for subagents)
        if self._parent_context_paths:
            for parent_path in self._parent_context_paths:
                # Force read-only for subagents to prevent uncontrolled writes
                merged_context_paths.append(
                    {
                        "path": parent_path.get("path", ""),
                        "permission": "read",  # Always read for subagents
                    },
                )
            logger.info(
                f"[SubagentManager] Inherited {len(self._parent_context_paths)} context paths from parent",
            )

        # Add task-specific context paths (from context_files parameter)
        if context_paths:
            existing_paths = {p.get("path") for p in merged_context_paths}
            for ctx_path in context_paths:
                if ctx_path.get("path") not in existing_paths:
                    merged_context_paths.append(ctx_path)

        if merged_context_paths:
            orchestrator_config["context_paths"] = merged_context_paths

        yaml_config = {
            "agents": agents,
            "orchestrator": orchestrator_config,
        }

        # Configure per-round timeouts for subagents, with parent inheritance
        subagent_round_timeouts = self._parent_coordination_config.get("subagent_round_timeouts") or {}
        parent_round_timeouts = self._parent_coordination_config.get("parent_round_timeouts") or {}
        effective_round_timeouts = {}
        if parent_round_timeouts:
            effective_round_timeouts.update(parent_round_timeouts)
        for key, value in subagent_round_timeouts.items():
            if value is not None:
                effective_round_timeouts[key] = value
        if effective_round_timeouts:
            timeout_settings = {}
            for key in (
                "initial_round_timeout_seconds",
                "subsequent_round_timeout_seconds",
                "round_timeout_grace_seconds",
            ):
                if key in effective_round_timeouts and effective_round_timeouts[key] is not None:
                    timeout_settings[key] = effective_round_timeouts[key]
            if timeout_settings:
                yaml_config["timeout_settings"] = timeout_settings

        return yaml_config

    def _parse_subprocess_status(self, workspace: Path) -> tuple[Dict[str, Any], Optional[str], Optional[str]]:
        """
        Parse token usage, log path, and session ID from the subprocess's status.json.

        Args:
            workspace: Workspace path where status.json might be

        Returns:
            Tuple of (token_usage dict, subprocess_log_dir path or None, session_id or None)
        """
        # Look for status.json in the subprocess's .massgen logs
        massgen_logs = workspace / ".massgen" / "massgen_logs"
        if not massgen_logs.exists():
            return {}, None, None

        # Find most recent log directory
        for log_dir in sorted(massgen_logs.glob("log_*"), reverse=True):
            status_file = log_dir / "turn_1" / "attempt_1" / "status.json"
            if status_file.exists():
                try:
                    data = json.loads(status_file.read_text())
                    costs = data.get("costs", {})
                    token_usage = {
                        "input_tokens": costs.get("total_input_tokens", 0),
                        "output_tokens": costs.get("total_output_tokens", 0),
                        "estimated_cost": costs.get("total_estimated_cost", 0.0),
                    }
                    # Extract session_id from meta section
                    session_id = data.get("meta", {}).get("session_id")
                    return token_usage, str(log_dir / "turn_1" / "attempt_1"), session_id
                except Exception:
                    pass
        return {}, None, None

    def _write_subprocess_log_reference(
        self,
        subagent_id: str,
        subprocess_log_dir: Optional[str],
        error: Optional[str] = None,
    ) -> None:
        """
        Write a reference file pointing to the subprocess's log directory
        and copy the full subprocess logs to the main log directory.

        This ensures logs are preserved even if the agent cleans up subagent
        workspaces during execution.

        Args:
            subagent_id: Subagent identifier
            subprocess_log_dir: Path to subprocess's log directory
            error: Optional error message if subprocess failed
        """
        log_dir = self._get_subagent_log_dir(subagent_id)
        if not log_dir:
            return

        reference_file = log_dir / "subprocess_logs.json"
        reference_data = {
            "subagent_id": subagent_id,
            "subprocess_log_dir": subprocess_log_dir,
            "timestamp": datetime.now().isoformat(),
        }
        if error:
            reference_data["error"] = error

        reference_file.write_text(json.dumps(reference_data, indent=2))

        # Copy the full subprocess logs to the main log directory
        # This preserves logs even if the agent deletes subagents/ during cleanup
        if subprocess_log_dir:
            subprocess_log_path = Path(subprocess_log_dir)
            if subprocess_log_path.exists() and subprocess_log_path.is_dir():
                dest_logs_dir = log_dir / "full_logs"
                try:
                    if dest_logs_dir.exists():
                        shutil.rmtree(dest_logs_dir)
                    # Copy with symlinks=True to handle any symlinks gracefully
                    shutil.copytree(
                        subprocess_log_path,
                        dest_logs_dir,
                        symlinks=True,
                        ignore_dangling_symlinks=True,
                    )
                    logger.info(f"[SubagentManager] Copied subprocess logs for {subagent_id} to {dest_logs_dir}")
                except Exception as e:
                    logger.warning(f"[SubagentManager] Failed to copy subprocess logs for {subagent_id}: {e}")

        # Also copy the subagent workspace (config, generated files)
        # This preserves the subagent's working directory including its config
        subagent_workspace = self.subagents_base / subagent_id / "workspace"
        if subagent_workspace.exists() and subagent_workspace.is_dir():
            dest_workspace_dir = log_dir / "workspace"
            try:
                if dest_workspace_dir.exists():
                    shutil.rmtree(dest_workspace_dir)
                # Copy workspace, skipping symlinks at top level but preserving content
                dest_workspace_dir.mkdir(parents=True, exist_ok=True)
                for item in subagent_workspace.iterdir():
                    if item.is_symlink():
                        continue  # Skip symlinks (shared_tools, etc.)
                    dest_item = dest_workspace_dir / item.name
                    if item.is_file():
                        shutil.copy2(item, dest_item)
                    elif item.is_dir():
                        # Skip .massgen logs (already copied above) and large dirs
                        if item.name in (".massgen", "node_modules", ".pnpm-store", "__pycache__"):
                            continue
                        shutil.copytree(
                            item,
                            dest_item,
                            symlinks=True,
                            ignore_dangling_symlinks=True,
                        )
                logger.info(f"[SubagentManager] Copied subagent workspace for {subagent_id} to {dest_workspace_dir}")
            except Exception as e:
                logger.warning(f"[SubagentManager] Failed to copy subagent workspace for {subagent_id}: {e}")

    async def spawn_subagent(
        self,
        task: str,
        subagent_id: Optional[str] = None,
        model: Optional[str] = None,
        timeout_seconds: Optional[int] = None,
        context_files: Optional[List[str]] = None,
        system_prompt: Optional[str] = None,
        refine: bool = True,
    ) -> SubagentResult:
        """
        Spawn a single subagent to work on a task.

        Context is loaded from CONTEXT.md in the workspace (required).

        Args:
            task: The task for the subagent
            subagent_id: Optional custom ID
            model: Optional model override
            timeout_seconds: Optional timeout (uses default if not specified)
            context_files: Optional files to copy to subagent workspace
            system_prompt: Optional custom system prompt
            refine: If True (default), allow multi-round coordination and refinement.
                    If False, return first answer without iteration (faster).

        Returns:
            SubagentResult with execution outcome
        """
        # Create config with clamped timeout
        clamped_timeout = self._clamp_timeout(timeout_seconds)
        config = SubagentConfig.create(
            task=task,
            parent_agent_id=self.parent_agent_id,
            subagent_id=subagent_id,
            model=model,
            timeout_seconds=clamped_timeout,
            context_files=context_files or [],
            system_prompt=system_prompt,
            metadata={"refine": refine},
        )

        logger.info(f"[SubagentManager] Spawning subagent {config.id} for task: {task[:100]}...")

        # Log subagent spawn event for structured logging
        log_subagent_spawn(
            subagent_id=config.id,
            parent_agent_id=self.parent_agent_id,
            task=task,
            model=config.model,
            timeout_seconds=config.timeout_seconds,
            context_files=config.context_files,
            execution_mode="foreground",
        )

        # Create workspace
        workspace = self._create_workspace(config.id)

        # Copy context files (always called to auto-copy CONTEXT.md even if no explicit context_files)
        self._copy_context_files(config.id, config.context_files or [], workspace)

        # Verify CONTEXT.md exists (required for subagents)
        context_md = workspace / "CONTEXT.md"
        if not context_md.exists():
            error_msg = "CONTEXT.md not found in workspace. " "Before spawning subagents, create a CONTEXT.md file with task context. " "This helps subagents understand what they're working on."
            logger.error(f"[SubagentManager] {error_msg}")
            return SubagentResult.create_error(
                subagent_id=config.id,
                error=error_msg,
                workspace_path=str(workspace),
            )

        # Track state
        state = SubagentState(
            config=config,
            status="running",
            workspace_path=str(workspace),
            started_at=datetime.now(),
        )
        self._subagents[config.id] = state

        # Initialize conversation logging (status comes from full_logs/status.json)
        self._append_conversation(config.id, "user", task)

        # Execute with semaphore and timeout, wrapped in tracing span
        with trace_subagent_execution(
            subagent_id=config.id,
            parent_agent_id=self.parent_agent_id,
            task=task,
            model=config.model,
            timeout_seconds=config.timeout_seconds,
        ) as span:
            async with self._semaphore:
                try:
                    result = await asyncio.wait_for(
                        self._execute_subagent(config, workspace),
                        timeout=config.timeout_seconds,
                    )
                except asyncio.TimeoutError:
                    # Attempt to recover completed work from workspace
                    log_dir = self._get_subagent_log_dir(config.id)
                    # Load context warning for the result
                    from massgen.context.task_context import (
                        load_task_context_with_warning,
                    )

                    _, context_warning = load_task_context_with_warning(str(workspace))
                    result = self._create_timeout_result_with_recovery(
                        subagent_id=config.id,
                        workspace=workspace,
                        timeout_seconds=config.timeout_seconds,
                        log_path=str(log_dir) if log_dir else None,
                        warning=context_warning,
                    )

            # Set span attributes based on result
            span.set_attribute("subagent.success", result.success)
            span.set_attribute("subagent.status", result.status)
            span.set_attribute("subagent.execution_time_seconds", result.execution_time_seconds)

        # Update state - use result.status directly for recovered states
        # Status can be: completed, completed_but_timeout, partial, timeout, error
        if result.success:
            state.status = "completed"
        elif result.status in ("timeout", "completed_but_timeout", "partial"):
            state.status = result.status
        else:
            state.status = "failed"
        state.result = result

        # Log conversation on success
        if result.success and result.answer:
            self._append_conversation(config.id, "assistant", result.answer)

        logger.info(
            f"[SubagentManager] Subagent {config.id} finished with status: {result.status}, " f"time: {result.execution_time_seconds:.2f}s",
        )

        # Log subagent completion event for structured logging
        log_subagent_complete(
            subagent_id=config.id,
            parent_agent_id=self.parent_agent_id,
            status=result.status,
            execution_time_seconds=result.execution_time_seconds,
            success=result.success,
            token_usage=result.token_usage,
            error_message=result.error,
            answer_preview=result.answer[:200] if result.answer else None,
        )

        # Save to registry for continuation support
        session_id = self._subagent_sessions.get(config.id)
        if session_id:
            self._save_subagent_to_registry(config.id, session_id, config, result)

        return result

    async def spawn_parallel(
        self,
        tasks: List[Dict[str, Any]],
        timeout_seconds: Optional[int] = None,
        refine: bool = True,
    ) -> List[SubagentResult]:
        """
        Spawn multiple subagents to run in parallel.

        Context is loaded from CONTEXT.md in the workspace (required).

        Args:
            tasks: List of task configurations, each with:
                   - task (required): Task description
                   - subagent_id (optional): Custom ID
                   - model (optional): Model override
                   - context_files (optional): Files to copy
            timeout_seconds: Optional timeout for all subagents
            refine: If True (default), allow multi-round coordination and refinement.
                    If False, return first answer without iteration (faster).

        Returns:
            List of SubagentResults in same order as input tasks
        """
        logger.info(f"[SubagentManager] Spawning {len(tasks)} subagents in parallel")

        # Create coroutines for each task
        coroutines = []
        for task_config in tasks:
            coro = self.spawn_subagent(
                task=task_config["task"],
                subagent_id=task_config.get("subagent_id"),
                model=task_config.get("model"),
                timeout_seconds=timeout_seconds or task_config.get("timeout_seconds"),
                context_files=task_config.get("context_files"),
                system_prompt=task_config.get("system_prompt"),
                refine=refine,
            )
            coroutines.append(coro)

        # Execute all in parallel (semaphore limits concurrency)
        results = await asyncio.gather(*coroutines, return_exceptions=True)

        # Convert exceptions to error results
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                task_id = tasks[i].get("subagent_id", f"sub_{i}")
                final_results.append(
                    SubagentResult.create_error(
                        subagent_id=task_id,
                        error=str(result),
                    ),
                )
            else:
                final_results.append(result)

        return final_results

    def spawn_subagent_background(
        self,
        task: str,
        subagent_id: Optional[str] = None,
        model: Optional[str] = None,
        timeout_seconds: Optional[int] = None,
        context_files: Optional[List[str]] = None,
        system_prompt: Optional[str] = None,
        refine: bool = True,
    ) -> Dict[str, Any]:
        """
        Spawn a subagent in the background (non-blocking).

        Returns immediately with subagent info. When the subagent completes,
        registered completion callbacks are invoked to notify about results.
        Use get_subagent_status() or get_subagent_result() to check progress.

        Args:
            task: The task for the subagent
            subagent_id: Optional custom ID
            model: Optional model override
            timeout_seconds: Optional timeout (uses default if not specified)
            context_files: Optional files to copy to subagent workspace
            system_prompt: Optional custom system prompt

        Returns:
            Dictionary with subagent_id and status_file path
        """
        # Create config with clamped timeout
        clamped_timeout = self._clamp_timeout(timeout_seconds)
        config = SubagentConfig.create(
            task=task,
            parent_agent_id=self.parent_agent_id,
            subagent_id=subagent_id,
            model=model,
            timeout_seconds=clamped_timeout,
            context_files=context_files or [],
            system_prompt=system_prompt,
            metadata={"refine": refine},
        )

        logger.info(f"[SubagentManager] Spawning background subagent {config.id} for task: {task[:100]}...")

        # Log subagent spawn event for structured logging
        log_subagent_spawn(
            subagent_id=config.id,
            parent_agent_id=self.parent_agent_id,
            task=task,
            model=config.model,
            timeout_seconds=config.timeout_seconds,
            context_files=config.context_files,
            execution_mode="background",
        )

        # Create workspace
        workspace = self._create_workspace(config.id)

        # Copy context files (always called to auto-copy CONTEXT.md even if no explicit context_files)
        self._copy_context_files(config.id, config.context_files or [], workspace)

        # Verify CONTEXT.md exists (required for subagents)
        context_md = workspace / "CONTEXT.md"
        if not context_md.exists():
            error_msg = "CONTEXT.md not found in workspace. " "Before spawning subagents, create a CONTEXT.md file with task context. " "This helps subagents understand what they're working on."
            logger.error(f"[SubagentManager] {error_msg}")
            # For background mode, we can't return SubagentResult directly
            # Store the error state and return info dict
            state = SubagentState(
                config=config,
                status="error",
                workspace_path=str(workspace),
                started_at=datetime.now(),
            )
            self._subagents[config.id] = state
            # Return error info
            return {
                "subagent_id": config.id,
                "status": "error",
                "workspace": str(workspace),
                "error": error_msg,
            }

        # Track state
        state = SubagentState(
            config=config,
            status="running",
            workspace_path=str(workspace),
            started_at=datetime.now(),
        )
        self._subagents[config.id] = state

        # Initialize conversation logging (status comes from full_logs/status.json)
        self._append_conversation(config.id, "user", task)

        # Create background task
        async def _run_background():
            async with self._semaphore:
                try:
                    result = await asyncio.wait_for(
                        self._execute_subagent(config, workspace),
                        timeout=config.timeout_seconds,
                    )
                except asyncio.TimeoutError:
                    # Attempt to recover completed work from workspace
                    log_dir = self._get_subagent_log_dir(config.id)
                    # Load context warning for the result
                    from massgen.context.task_context import (
                        load_task_context_with_warning,
                    )

                    _, context_warning = load_task_context_with_warning(str(workspace))
                    result = self._create_timeout_result_with_recovery(
                        subagent_id=config.id,
                        workspace=workspace,
                        timeout_seconds=config.timeout_seconds,
                        log_path=str(log_dir) if log_dir else None,
                        warning=context_warning,
                    )
                except Exception as e:
                    # Load context warning for the result
                    from massgen.context.task_context import (
                        load_task_context_with_warning,
                    )

                    _, context_warning = load_task_context_with_warning(str(workspace))
                    result = SubagentResult.create_error(
                        subagent_id=config.id,
                        error=str(e),
                        workspace_path=str(workspace),
                        warning=context_warning,
                    )

            # Update state - use result.status directly for recovered states
            # Status can be: completed, completed_but_timeout, partial, timeout, error
            if result.success:
                state.status = "completed"
            elif result.status in ("timeout", "completed_but_timeout", "partial"):
                state.status = result.status
            else:
                state.status = "failed"
            state.result = result

            # Invoke registered completion callbacks to notify about async completion
            self._invoke_completion_callbacks(config.id, result)

            # Log conversation on success
            if result.success and result.answer:
                self._append_conversation(config.id, "assistant", result.answer)

            logger.info(
                f"[SubagentManager] Background subagent {config.id} finished with status: {result.status}, " f"time: {result.execution_time_seconds:.2f}s",
            )

            # Log subagent completion event for structured logging
            log_subagent_complete(
                subagent_id=config.id,
                parent_agent_id=self.parent_agent_id,
                status=result.status,
                execution_time_seconds=result.execution_time_seconds,
                success=result.success,
                token_usage=result.token_usage,
                error_message=result.error,
                answer_preview=result.answer[:200] if result.answer else None,
            )

            # Save to registry for continuation support
            session_id = self._subagent_sessions.get(config.id)
            if session_id:
                self._save_subagent_to_registry(config.id, session_id, config, result)

            # Clean up task reference
            if config.id in self._background_tasks:
                del self._background_tasks[config.id]

            return result

        # Schedule the background task
        bg_task = asyncio.create_task(_run_background())
        self._background_tasks[config.id] = bg_task

        # Get status file path (now points to full_logs/status.json)
        status_file = None
        if self._subagent_logs_base:
            status_file = str(self._subagent_logs_base / config.id / "full_logs" / "status.json")

        return {
            "subagent_id": config.id,
            "status": "running",
            "workspace": str(workspace),
            "status_file": status_file,
        }

    async def continue_subagent(
        self,
        subagent_id: str,
        new_message: str,
        timeout_seconds: Optional[int] = None,
    ) -> SubagentResult:
        """Continue a previously spawned subagent with a new message.

        Uses the existing --session-id mechanism to restore the conversation
        and append the new message. This allows continuing timed-out, failed,
        or even completed subagents for refinement or follow-up questions.

        Args:
            subagent_id: ID of the subagent to continue
            new_message: New message to append to the conversation
            timeout_seconds: Optional timeout override

        Returns:
            SubagentResult with execution outcome
        """
        start_time = time.time()

        # Load registry to find the subagent
        # First check our own registry
        subagent_entry = None
        registry_file = self.subagents_base / "_registry.json"

        if registry_file.exists():
            try:
                registry = json.loads(registry_file.read_text())
                # Registry format: {"subagents": [{"subagent_id": "...", ...}, ...]}
                subagents_list = registry.get("subagents", [])
                for entry in subagents_list:
                    if entry.get("subagent_id") == subagent_id:
                        subagent_entry = entry
                        break
            except json.JSONDecodeError:
                logger.warning("[SubagentManager] Failed to parse own registry file")

        # If not found in our registry and we have temp workspaces, search all agent registries
        if not subagent_entry and self._agent_temporary_workspace and self._agent_temporary_workspace.exists():
            for agent_dir in self._agent_temporary_workspace.iterdir():
                if not agent_dir.is_dir():
                    continue

                agent_registry_file = agent_dir / "subagents" / "_registry.json"
                if not agent_registry_file.exists():
                    continue

                try:
                    agent_registry = json.loads(agent_registry_file.read_text())
                    subagents_list = agent_registry.get("subagents", [])
                    for entry in subagents_list:
                        if entry.get("subagent_id") == subagent_id:
                            subagent_entry = entry
                            registry = agent_registry
                            registry_file = agent_registry_file
                            logger.info(
                                f"[SubagentManager] Found subagent {subagent_id} in agent {agent_dir.name}'s registry",
                            )
                            break
                    if subagent_entry:
                        break
                except json.JSONDecodeError:
                    logger.warning(f"[SubagentManager] Failed to parse registry for agent {agent_dir.name}")

        # If still not found, return error
        if not subagent_entry:
            return SubagentResult.create_error(
                subagent_id=subagent_id,
                error=f"Subagent {subagent_id} not found in any registry.",
            )

        session_id = subagent_entry.get("session_id")
        if not session_id:
            return SubagentResult.create_error(
                subagent_id=subagent_id,
                error=f"Subagent {subagent_id} has no session_id in registry. Cannot continue.",
            )

        # Get the existing workspace from the registry
        workspace = Path(subagent_entry.get("workspace", ""))
        if not workspace.exists():
            return SubagentResult.create_error(
                subagent_id=subagent_id,
                error=f"Subagent workspace not found: {workspace}",
            )

        logger.info(
            f"[SubagentManager] Continuing subagent {subagent_id} with session {session_id}, " f"message: {new_message[:100]}...",
        )

        # Use clamped timeout
        timeout = self._clamp_timeout(timeout_seconds)

        # Build command to continue the session
        # Use --session-id to restore the conversation, then append new message
        answer_file = workspace / "answer_continued.txt"
        cmd = [
            "uv",
            "run",
            "massgen",
            "--session-id",
            session_id,  # Restore existing session
            "--automation",
            "--output-file",
            str(answer_file),
            new_message,  # New message to append
        ]

        process: Optional[asyncio.subprocess.Process] = None
        try:
            # Use async subprocess for graceful cancellation support
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(workspace),
            )

            # Track the process for potential cancellation
            # Use a continuation-specific ID to avoid conflicts
            continuation_id = f"{subagent_id}_cont_{int(time.time())}"
            self._active_processes[continuation_id] = process

            # Wait with timeout
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=timeout,
                )
            except asyncio.TimeoutError:
                # Kill the process on timeout
                logger.warning(f"[SubagentManager] Continuation of {subagent_id} timed out, terminating...")
                process.terminate()
                try:
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except asyncio.TimeoutError:
                    process.kill()
                    await process.wait()

                execution_time = time.time() - start_time
                log_dir = self._get_subagent_log_dir(subagent_id)
                return SubagentResult.create_error(
                    subagent_id=subagent_id,
                    error=f"Continuation timed out after {timeout}s",
                    workspace_path=str(workspace),
                    execution_time_seconds=execution_time,
                    log_path=str(log_dir) if log_dir else None,
                )
            finally:
                # Remove from active processes
                self._active_processes.pop(continuation_id, None)

            if process.returncode == 0:
                # Read answer from the output file
                if answer_file.exists():
                    answer = answer_file.read_text().strip()
                else:
                    # Fallback to stdout if file wasn't created
                    answer = stdout.decode() if stdout else ""

                execution_time = time.time() - start_time

                # Get token usage and log path from subprocess's status.json
                token_usage, subprocess_log_dir, session_id = self._parse_subprocess_status(workspace)

                # Write reference to subprocess log directory
                self._write_subprocess_log_reference(subagent_id, subprocess_log_dir)

                # Get log directory path for the result
                log_dir = self._get_subagent_log_dir(subagent_id)

                # Update registry with new status
                subagent_entry["status"] = "completed"
                subagent_entry["last_continued_at"] = datetime.now().isoformat()
                registry_file.write_text(json.dumps(registry, indent=2))

                return SubagentResult.create_success(
                    subagent_id=subagent_id,
                    answer=answer,
                    workspace_path=str(workspace),
                    execution_time_seconds=execution_time,
                    token_usage=token_usage,
                    log_path=str(log_dir) if log_dir else None,
                )
            else:
                stderr_text = stderr.decode() if stderr else ""
                stdout_text = stdout.decode() if stdout else ""
                error_msg = stderr_text.strip() or f"Subprocess exited with code {process.returncode}"

                # Log detailed error information for debugging
                logger.error(
                    f"[SubagentManager] Continuation of {subagent_id} failed with exit code {process.returncode}\n"
                    f"Command: {' '.join(cmd)}\n"
                    f"Working directory: {workspace}\n"
                    f"STDERR: {stderr_text[:1000]}\n"  # First 1000 chars of stderr
                    f"STDOUT: {stdout_text[:500]}",  # First 500 chars of stdout
                )

                # Still try to get log path for debugging
                _, subprocess_log_dir, _ = self._parse_subprocess_status(workspace)
                self._write_subprocess_log_reference(subagent_id, subprocess_log_dir, error=error_msg)
                log_dir = self._get_subagent_log_dir(subagent_id)

                return SubagentResult.create_error(
                    subagent_id=subagent_id,
                    error=error_msg,
                    workspace_path=str(workspace),
                    execution_time_seconds=time.time() - start_time,
                    log_path=str(log_dir) if log_dir else None,
                )

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"[SubagentManager] Error continuing subagent {subagent_id}: {e}")
            return SubagentResult.create_error(
                subagent_id=subagent_id,
                error=str(e),
                workspace_path=str(workspace),
                execution_time_seconds=execution_time,
            )

    def get_subagent_status(self, subagent_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the current status of a subagent.

        Reads from full_logs/status.json (written by Orchestrator) and transforms
        the rich status into a simplified view for MCP consumers.

        Args:
            subagent_id: Subagent identifier

        Returns:
            Simplified status dictionary, or None if not found
        """
        # First check in-memory state exists
        state = self._subagents.get(subagent_id)
        if not state:
            return None

        # Try to read from full_logs/status.json (the single source of truth)
        if self._subagent_logs_base:
            status_file = self._subagent_logs_base / subagent_id / "full_logs" / "status.json"
            if status_file.exists():
                try:
                    raw_status = json.loads(status_file.read_text())
                    return self._transform_orchestrator_status(subagent_id, raw_status, state)
                except json.JSONDecodeError:
                    pass

        # Fall back to in-memory state if file doesn't exist yet
        return {
            "subagent_id": subagent_id,
            "status": "pending" if state.status == "running" else state.status,
            "task": state.config.task,
            "workspace": state.workspace_path,
            "started_at": state.started_at.isoformat() if state.started_at else None,
        }

    def get_subagent_display_data(self, subagent_id: str) -> Optional[SubagentDisplayData]:
        """
        Get display data for a subagent (for TUI rendering).

        Returns a SubagentDisplayData object with progress, file count, and
        last log line - optimized for the SubagentCard widget.

        Args:
            subagent_id: Subagent identifier

        Returns:
            SubagentDisplayData if found, None otherwise
        """

        state = self._subagents.get(subagent_id)
        if not state:
            return None

        # Calculate elapsed time and progress
        elapsed = 0.0
        if state.started_at:
            elapsed = (datetime.now() - state.started_at).total_seconds()

        timeout = state.config.timeout_seconds
        progress = min(100, int(elapsed / timeout * 100)) if timeout > 0 else 0

        # Determine status
        status = state.status
        if status == "completed_but_timeout":
            status = "timeout"
        elif status == "partial":
            status = "error"

        # Count workspace files
        workspace_file_count = 0
        workspace_path = Path(state.workspace_path) if state.workspace_path else None
        if workspace_path and workspace_path.exists():
            try:
                workspace_file_count = sum(1 for p in workspace_path.rglob("*") if p.is_file())
            except (OSError, IOError):
                pass

        # Get last log line
        last_log_line = ""
        if self._subagent_logs_base and workspace_path:
            log_path = self._subagent_logs_base / subagent_id / "full_logs" / "massgen.log"
            if log_path.exists():
                try:
                    # Tail last line efficiently
                    with open(log_path, "rb") as f:
                        f.seek(0, 2)  # End of file
                        size = f.tell()
                        if size > 0:
                            # Read last ~500 bytes
                            f.seek(max(0, size - 500))
                            content = f.read().decode("utf-8", errors="replace")
                            lines = content.strip().split("\n")
                            if lines:
                                last_log_line = lines[-1][:100]  # Truncate
                except (OSError, IOError):
                    pass

        # Get error message if failed
        error = None
        if state.result and state.result.error:
            error = state.result.error

        # Get answer preview if completed
        answer_preview = None
        if state.result and state.result.answer:
            answer_preview = state.result.answer[:200]
            if len(state.result.answer) > 200:
                answer_preview += "..."

        return SubagentDisplayData(
            id=subagent_id,
            task=state.config.task,
            status=status,
            progress_percent=progress,
            elapsed_seconds=elapsed,
            timeout_seconds=float(timeout),
            workspace_path=state.workspace_path or "",
            workspace_file_count=workspace_file_count,
            last_log_line=last_log_line,
            error=error,
            answer_preview=answer_preview,
        )

    def _transform_orchestrator_status(
        self,
        subagent_id: str,
        raw_status: Dict[str, Any],
        state: SubagentState,
    ) -> Dict[str, Any]:
        """
        Transform Orchestrator's rich status.json into simplified status for MCP.

        Args:
            subagent_id: Subagent identifier
            raw_status: Raw status from full_logs/status.json
            state: In-memory subagent state

        Returns:
            Simplified status dictionary
        """
        # Extract coordination info
        coordination = raw_status.get("coordination", {})
        phase = coordination.get("phase")
        completion_pct = coordination.get("completion_percentage")

        # Derive simple status from phase
        # If state.result exists, use its status (for completed/timeout cases)
        if state.result:
            derived_status = state.result.status
        elif phase in ("initial_answer", "enforcement", "presentation"):
            derived_status = "running"
        else:
            derived_status = "pending"

        # Extract costs
        costs = raw_status.get("costs", {})
        token_usage = {}
        if costs:
            token_usage = {
                "input_tokens": costs.get("total_input_tokens", 0),
                "output_tokens": costs.get("total_output_tokens", 0),
                "estimated_cost": costs.get("total_estimated_cost", 0.0),
            }

        # Extract elapsed time
        meta = raw_status.get("meta", {})
        elapsed_seconds = meta.get("elapsed_seconds", 0.0)

        result = {
            "subagent_id": subagent_id,
            "status": derived_status,
            "phase": phase,
            "completion_percentage": completion_pct,
            "task": state.config.task,
            "workspace": state.workspace_path,
            "elapsed_seconds": elapsed_seconds,
            "token_usage": token_usage,
        }

        # Add started_at if available
        if state.started_at:
            result["started_at"] = state.started_at.isoformat()

        return result

    async def wait_for_subagent(self, subagent_id: str, timeout: Optional[float] = None) -> Optional[SubagentResult]:
        """
        Wait for a background subagent to complete.

        Args:
            subagent_id: Subagent identifier
            timeout: Optional timeout in seconds

        Returns:
            SubagentResult if completed, None if not found or timeout
        """
        task = self._background_tasks.get(subagent_id)
        if not task:
            # Check if already completed
            state = self._subagents.get(subagent_id)
            if state and state.result:
                return state.result
            return None

        try:
            if timeout:
                return await asyncio.wait_for(task, timeout=timeout)
            else:
                return await task
        except asyncio.TimeoutError:
            return None

    def list_subagents(self) -> List[Dict[str, Any]]:
        """
        List all subagents spawned by this manager.

        When agent_temporary_workspace is provided, this will scan all agent temp workspaces
        and merge their registries to show subagents from all agents (following workspace
        visibility principles - agents can only see subagents from other agents after seeing
        their answers via injection).

        Returns:
            List of subagent info dictionaries, each containing:
                - subagent_id: Unique identifier
                - status: Current status (running, completed, timeout, failed)
                - workspace: Path to subagent workspace
                - task: Task description (truncated)
                - session_id: Session ID for continuation
                - continuable: Whether this subagent can be continued
                - source_agent: Agent ID that spawned this subagent (when merging registries)
                - created_at, execution_time_seconds, success, last_continued_at, etc.
        """
        subagents: Dict[str, Dict[str, Any]] = {}

        # First, load this agent's own registry
        registry_file = self.subagents_base / "_registry.json"
        if registry_file.exists():
            try:
                registry = json.loads(registry_file.read_text())
                # Registry format: {"subagents": [{"subagent_id": "...", ...}, ...]}
                subagents_list = registry.get("subagents", [])
                for entry in subagents_list:
                    subagent_id = entry.get("subagent_id")
                    if not subagent_id:
                        continue
                    subagents[subagent_id] = {
                        "subagent_id": subagent_id,
                        "status": entry.get("status"),
                        "workspace": entry.get("workspace"),
                        "task": entry.get("task"),
                        "created_at": entry.get("created_at"),
                        "execution_time_seconds": entry.get("execution_time_seconds"),
                        "success": entry.get("success"),
                        "session_id": entry.get("session_id"),
                        "last_continued_at": entry.get("last_continued_at"),
                        "continuable": bool(entry.get("session_id")),
                        "source_agent": self.parent_agent_id,
                    }
            except json.JSONDecodeError:
                logger.warning("[SubagentManager] Failed to parse registry for list_subagents")

        # If agent_temporary_workspace is provided, scan all agent temp workspaces
        # to merge registries from all agents
        if self._agent_temporary_workspace and self._agent_temporary_workspace.exists():
            for agent_dir in self._agent_temporary_workspace.iterdir():
                if not agent_dir.is_dir():
                    continue

                agent_id = agent_dir.name
                agent_registry_file = agent_dir / "subagents" / "_registry.json"

                if not agent_registry_file.exists():
                    continue

                try:
                    agent_registry = json.loads(agent_registry_file.read_text())
                    # Registry format: {"subagents": [{"subagent_id": "...", ...}, ...]}
                    subagents_list = agent_registry.get("subagents", [])
                    for entry in subagents_list:
                        subagent_id = entry.get("subagent_id")
                        if not subagent_id:
                            continue

                        # Use a prefixed key to avoid collisions between agents
                        # (each agent can have its own subagent with the same ID)
                        prefixed_id = f"{agent_id}_{subagent_id}"

                        # Skip if we already have this subagent from our own registry
                        # (our own registry takes precedence)
                        if subagent_id in subagents and agent_id == self.parent_agent_id:
                            continue

                        subagents[prefixed_id] = {
                            "subagent_id": subagent_id,
                            "status": entry.get("status"),
                            "workspace": entry.get("workspace"),
                            "task": entry.get("task"),
                            "created_at": entry.get("created_at"),
                            "execution_time_seconds": entry.get("execution_time_seconds"),
                            "success": entry.get("success"),
                            "session_id": entry.get("session_id"),
                            "last_continued_at": entry.get("last_continued_at"),
                            "continuable": bool(entry.get("session_id")),
                            "source_agent": agent_id,
                        }
                except json.JSONDecodeError:
                    logger.warning(
                        f"[SubagentManager] Failed to parse registry for agent {agent_id}",
                    )

        # Update with currently tracked subagents (in-memory state takes precedence)
        for subagent_id, state in self._subagents.items():
            task_preview = state.config.task[:100] + ("..." if len(state.config.task) > 100 else "")
            current_entry = subagents.get(subagent_id, {})
            current_entry.update(
                {
                    "subagent_id": subagent_id,
                    "status": state.status,
                    "workspace": state.workspace_path,
                    "started_at": state.started_at.isoformat() if state.started_at else None,
                    "task": task_preview,
                    "session_id": self._subagent_sessions.get(subagent_id, current_entry.get("session_id")),
                    "continuable": bool(self._subagent_sessions.get(subagent_id, current_entry.get("session_id"))),
                    "source_agent": self.parent_agent_id,
                },
            )
            subagents[subagent_id] = current_entry

        return list(subagents.values())

    def get_subagent_result(self, subagent_id: str) -> Optional[SubagentResult]:
        """
        Get result for a specific subagent.

        Args:
            subagent_id: Subagent identifier

        Returns:
            SubagentResult if subagent exists and completed, None otherwise
        """
        state = self._subagents.get(subagent_id)
        if state and state.result:
            return state.result
        return None

    def get_subagent_costs_summary(self) -> Dict[str, Any]:
        """
        Get aggregated cost summary for all subagents.

        Returns:
            Dictionary with total costs and per-subagent breakdown
        """
        total_input_tokens = 0
        total_output_tokens = 0
        total_estimated_cost = 0.0
        subagent_details = []

        for subagent_id, state in self._subagents.items():
            if state.result and state.result.token_usage:
                tu = state.result.token_usage
                input_tokens = tu.get("input_tokens", 0)
                output_tokens = tu.get("output_tokens", 0)
                cost = tu.get("estimated_cost", 0.0)

                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                total_estimated_cost += cost

                subagent_details.append(
                    {
                        "subagent_id": subagent_id,
                        "status": state.result.status,
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens,
                        "estimated_cost": round(cost, 6),
                        "execution_time_seconds": state.result.execution_time_seconds,
                    },
                )

        return {
            "total_subagents": len(self._subagents),
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "total_estimated_cost": round(total_estimated_cost, 6),
            "subagents": subagent_details,
        }

    def get_subagent_pointer(self, subagent_id: str) -> Optional[SubagentPointer]:
        """
        Get pointer for a subagent (for plan.json tracking).

        Args:
            subagent_id: Subagent identifier

        Returns:
            SubagentPointer if subagent exists, None otherwise
        """
        state = self._subagents.get(subagent_id)
        if not state:
            return None

        pointer = SubagentPointer(
            id=subagent_id,
            task=state.config.task,
            workspace=state.workspace_path,
            status=state.status,
            created_at=state.config.created_at,
        )

        if state.result:
            pointer.mark_completed(state.result)

        return pointer

    def cleanup_subagent(self, subagent_id: str, remove_workspace: bool = False) -> bool:
        """
        Clean up a subagent.

        Args:
            subagent_id: Subagent identifier
            remove_workspace: If True, also remove the workspace directory

        Returns:
            True if cleanup successful, False if subagent not found
        """
        if subagent_id not in self._subagents:
            return False

        if remove_workspace:
            workspace_dir = self.subagents_base / subagent_id
            if workspace_dir.exists():
                shutil.rmtree(workspace_dir)
                logger.info(f"[SubagentManager] Removed workspace for {subagent_id}")

        del self._subagents[subagent_id]
        return True

    async def cancel_all_subagents(self) -> int:
        """
        Cancel all running subagent processes gracefully.

        This should be called when the parent process receives a termination
        signal (e.g., Ctrl+C) to ensure all child processes are cleaned up.

        Returns:
            Number of subagents that were cancelled
        """
        cancelled_count = 0
        for subagent_id, process in list(self._active_processes.items()):
            if process.returncode is None:  # Still running
                logger.warning(f"[SubagentManager] Cancelling subagent {subagent_id}...")
                try:
                    process.terminate()
                    try:
                        await asyncio.wait_for(process.wait(), timeout=5.0)
                    except asyncio.TimeoutError:
                        logger.warning(f"[SubagentManager] Force killing subagent {subagent_id}")
                        process.kill()
                        await process.wait()
                    cancelled_count += 1
                except Exception as e:
                    logger.error(f"[SubagentManager] Error cancelling {subagent_id}: {e}")

        self._active_processes.clear()

        # Also cancel any background tasks
        for task_id, task in list(self._background_tasks.items()):
            if not task.done():
                logger.warning(f"[SubagentManager] Cancelling background task {task_id}...")
                task.cancel()
                cancelled_count += 1

        return cancelled_count

    def cleanup_all(self, remove_workspaces: bool = False) -> int:
        """
        Clean up all subagents.

        Args:
            remove_workspaces: If True, also remove workspace directories

        Returns:
            Number of subagents cleaned up
        """
        count = len(self._subagents)
        subagent_ids = list(self._subagents.keys())

        for subagent_id in subagent_ids:
            self.cleanup_subagent(subagent_id, remove_workspace=remove_workspaces)

        return count

    # =========================================================================
    # Timeout Recovery Methods
    # =========================================================================

    def _extract_status_from_workspace(self, workspace: Path) -> Dict[str, Any]:
        """
        Extract coordination status from a subagent's workspace.

        Reads the status.json file from the subagent's full_logs directory
        to determine completion state, winner, and costs.

        Args:
            workspace: Path to the subagent's workspace directory

        Returns:
            Dictionary with:
            - phase: Coordination phase (initial_answer, enforcement, presentation)
            - completion_percentage: 0-100 progress
            - winner: Agent ID of winner if selected
            - votes: Vote counts by agent
            - has_completed_work: True if any useful work was done
            - costs: Token usage costs if available
        """
        result = {
            "phase": None,
            "completion_percentage": None,
            "winner": None,
            "votes": {},
            "has_completed_work": False,
            "costs": {},
            "historical_workspaces": {},
            "historical_workspaces_raw": [],  # Raw list with agentId/timestamp for log path lookup
        }

        # Try multiple locations for status.json:
        # 1. full_logs/status.json in workspace (standard location)
        # 2. .massgen/.../status.json in workspace (nested orchestrator logs)
        status_file = workspace / "full_logs" / "status.json"
        if not status_file.exists():
            # Try to find status.json in nested .massgen logs
            massgen_logs = workspace / ".massgen" / "massgen_logs"
            if massgen_logs.exists():
                # Find most recent log directory
                log_dirs = sorted(massgen_logs.glob("log_*"), reverse=True)
                for log_dir in log_dirs:
                    nested_status = log_dir / "turn_1" / "attempt_1" / "status.json"
                    if nested_status.exists():
                        status_file = nested_status
                        break

        if not status_file.exists():
            return result

        try:
            status_data = json.loads(status_file.read_text())

            # Extract coordination phase (nested structure)
            coordination = status_data.get("coordination", {})
            result["phase"] = coordination.get("phase")
            result["completion_percentage"] = coordination.get("completion_percentage")

            # Extract winner and votes (nested structure)
            results_data = status_data.get("results", {})
            result["winner"] = results_data.get("winner")
            result["votes"] = results_data.get("votes", {})

            # Extract historical workspaces for answer lookup
            # Key by both answerLabel (for votes) and agentId (for winner)
            historical_list = status_data.get("historical_workspaces", [])
            if isinstance(historical_list, list):
                result["historical_workspaces_raw"] = historical_list  # Keep raw for log path lookup
                workspaces_dict = {}
                for i, ws in enumerate(historical_list):
                    if isinstance(ws, dict) and ws.get("workspacePath"):
                        path = ws.get("workspacePath", "")
                        # Key by answerLabel (matches votes dict keys like "agent2.1")
                        answer_label = ws.get("answerLabel")
                        if answer_label:
                            workspaces_dict[answer_label] = path
                        # Also key by agentId (matches winner field)
                        agent_id = ws.get("agentId", ws.get("answerId", f"agent_{i}"))
                        if agent_id:
                            workspaces_dict[agent_id] = path
                result["historical_workspaces"] = workspaces_dict
            else:
                result["historical_workspaces"] = historical_list

            # Extract costs (nested structure)
            costs_data = status_data.get("costs", {})
            if costs_data:
                result["costs"] = {
                    "input_tokens": costs_data.get("total_input_tokens", 0),
                    "output_tokens": costs_data.get("total_output_tokens", 0),
                    "estimated_cost": costs_data.get("total_estimated_cost", 0.0),
                }

            # Determine if there's completed work
            phase = result["phase"]
            if phase == "presentation":
                result["has_completed_work"] = True
            elif phase == "enforcement":
                # In enforcement phase, we have answers if there are votes or workspaces
                result["has_completed_work"] = bool(result["votes"]) or bool(result["historical_workspaces"])
            elif phase == "initial_answer":
                # In initial phase, check if any workspaces exist
                result["has_completed_work"] = bool(result["historical_workspaces"])

        except (json.JSONDecodeError, OSError) as e:
            logger.warning(f"[SubagentManager] Failed to read status.json: {e}")

        return result

    def _extract_answer_from_workspace(
        self,
        workspace: Path,
        winner_agent_id: Optional[str] = None,
        votes: Optional[Dict[str, int]] = None,
        historical_workspaces: Optional[Dict[str, str]] = None,
        historical_workspaces_raw: Optional[List[Dict[str, Any]]] = None,
        log_path: Optional[Path] = None,
    ) -> Optional[str]:
        """
        Extract the best available answer from a subagent's workspace.

        Follows the same selection logic as the orchestrator's graceful timeout:
        1. If winner_agent_id is set, use that agent's answer
        2. If votes exist, select agent with most votes (ties broken by registration order)
        3. Fall back to first registered agent with an answer
        4. Check answer.txt if no agent workspaces

        Args:
            workspace: Path to the subagent's workspace
            winner_agent_id: Agent ID of explicit winner (from status.json)
            votes: Vote counts by agent (for selection when no winner)
            historical_workspaces: Pre-extracted workspace paths from status.json (optional)
            historical_workspaces_raw: Raw list from status.json with agentId/timestamp (optional)
            log_path: Path to log directory for checking full_logs/{agentId}/{timestamp}/answer.txt

        Returns:
            Answer text if found, None otherwise
        """
        # First check for answer.txt (written by orchestrator on completion)
        answer_file = workspace / "answer.txt"
        if answer_file.exists():
            try:
                return answer_file.read_text().strip()
            except OSError:
                pass

        # Use provided historical_workspaces or try to extract from workspace
        if historical_workspaces is None:
            status = self._extract_status_from_workspace(workspace)
            historical_workspaces = status.get("historical_workspaces", {})

        # If winner specified but not in historical_workspaces, try standard path
        if winner_agent_id and winner_agent_id not in historical_workspaces:
            standard_winner_path = workspace / "workspaces" / winner_agent_id
            if standard_winner_path.exists():
                historical_workspaces[winner_agent_id] = str(standard_winner_path)

        # If no historical workspaces, try to discover from standard workspaces dir
        if not historical_workspaces:
            workspaces_dir = workspace / "workspaces"
            if workspaces_dir.exists():
                for agent_dir in workspaces_dir.iterdir():
                    if agent_dir.is_dir():
                        historical_workspaces[agent_dir.name] = str(agent_dir)

        if not historical_workspaces:
            return None

        # Determine which agent's answer to use
        selected_agent = None

        if winner_agent_id and winner_agent_id in historical_workspaces:
            selected_agent = winner_agent_id
        elif votes:
            # Select by vote count (most votes wins, ties broken by dict order)
            vote_counts = votes if isinstance(votes, dict) else {}
            if vote_counts:
                max_votes = max(vote_counts.values())
                for agent_id in historical_workspaces.keys():
                    if vote_counts.get(agent_id, 0) == max_votes:
                        selected_agent = agent_id
                        break

        # Fall back to first agent in registration order
        if not selected_agent and historical_workspaces:
            selected_agent = next(iter(historical_workspaces.keys()))

        if not selected_agent:
            return None

        # Try to find answer in log directory first (persisted location)
        # Check full_logs/{agentId}/{timestamp}/answer.txt
        if log_path and historical_workspaces_raw:
            for ws_info in historical_workspaces_raw:
                agent_id = ws_info.get("agentId")
                answer_label = ws_info.get("answerLabel")
                timestamp = ws_info.get("timestamp")
                # Match by either agentId or answerLabel
                if (agent_id == selected_agent or answer_label == selected_agent) and timestamp:
                    log_answer_path = log_path / "full_logs" / agent_id / timestamp / "answer.txt"
                    if log_answer_path.exists():
                        try:
                            return log_answer_path.read_text().strip()
                        except OSError:
                            pass

        # Read answer from selected agent's workspace
        # The workspacePath points to the workspace/ subdirectory, but answer.txt
        # is in the parent directory (the timestamped snapshot directory)
        agent_workspace = Path(historical_workspaces[selected_agent])

        # Check parent directory first (where orchestrator saves answer.txt)
        parent_dir = agent_workspace.parent
        for answer_filename in ["answer.txt", "answer.md"]:
            answer_path = parent_dir / answer_filename
            if answer_path.exists():
                try:
                    return answer_path.read_text().strip()
                except OSError:
                    continue

        # Fall back to checking inside workspace
        for answer_filename in ["answer.md", "answer.txt", "response.md", "response.txt"]:
            answer_path = agent_workspace / answer_filename
            if answer_path.exists():
                try:
                    return answer_path.read_text().strip()
                except OSError:
                    continue

        return None

    def _extract_costs_from_status(self, workspace: Path) -> Dict[str, Any]:
        """
        Extract token usage costs from a subagent's status.json.

        Args:
            workspace: Path to the subagent's workspace

        Returns:
            Dictionary with input_tokens, output_tokens, estimated_cost
            Empty dict if no costs available
        """
        status = self._extract_status_from_workspace(workspace)
        return status.get("costs", {})

    def _create_timeout_result_with_recovery(
        self,
        subagent_id: str,
        workspace: Path,
        timeout_seconds: float,
        log_path: Optional[str] = None,
        warning: Optional[str] = None,
    ) -> SubagentResult:
        """
        Create a SubagentResult for a timed-out subagent, recovering any completed work.

        This method attempts to extract useful results from a subagent that
        timed out but may have completed work before the timeout.

        Args:
            subagent_id: ID of the subagent
            workspace: Path to subagent workspace
            timeout_seconds: How long the subagent ran
            log_path: Path to log directory
            warning: Warning message (e.g., context truncation)

        Returns:
            SubagentResult with recovered answer and costs if available
        """
        # Extract status - prefer log_path/full_logs/status.json if available
        status = {}
        if log_path:
            log_dir = Path(log_path)
            status = self._extract_status_from_workspace(log_dir)

        # Fall back to workspace if no status from log_path
        if not status.get("phase"):
            status = self._extract_status_from_workspace(workspace)

        # Extract answer from log workspace first (has answer.txt), then runtime workspace
        # Pass historical_workspaces from status so we don't re-read from wrong path
        historical_workspaces = status.get("historical_workspaces", {})
        historical_workspaces_raw = status.get("historical_workspaces_raw", [])
        recovered_answer = None
        if log_path:
            log_dir = Path(log_path)
            log_workspace = log_dir / "workspace"
            if log_workspace.exists():
                recovered_answer = self._extract_answer_from_workspace(
                    log_workspace,
                    winner_agent_id=status.get("winner"),
                    votes=status.get("votes"),
                    historical_workspaces=historical_workspaces,
                    historical_workspaces_raw=historical_workspaces_raw,
                    log_path=log_dir,
                )

        if not recovered_answer:
            recovered_answer = self._extract_answer_from_workspace(
                workspace,
                winner_agent_id=status.get("winner"),
                votes=status.get("votes"),
                historical_workspaces=historical_workspaces,
                historical_workspaces_raw=historical_workspaces_raw,
                log_path=Path(log_path) if log_path else None,
            )

        # Extract costs
        token_usage = status.get("costs", {})

        # Determine if this is partial or complete
        is_partial = False
        if recovered_answer is not None:
            phase = status.get("phase")
            # Partial if we have an answer but no winner and not in presentation
            if phase != "presentation" and not status.get("winner"):
                is_partial = True

        return SubagentResult.create_timeout_with_recovery(
            subagent_id=subagent_id,
            workspace_path=str(workspace),
            timeout_seconds=timeout_seconds,
            recovered_answer=recovered_answer,
            completion_percentage=status.get("completion_percentage"),
            token_usage=token_usage,
            log_path=log_path,
            is_partial=is_partial,
            warning=warning,
        )
