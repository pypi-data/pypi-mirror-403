# -*- coding: utf-8 -*-
"""
MassGen Orchestrator Agent - Chat interface that manages sub-agents internally.

The orchestrator presents a unified chat interface to users while coordinating
multiple sub-agents using the proven binary decision framework behind the scenes.

TODOs:

- Move CLI's coordinate_with_context logic to orchestrator and simplify CLI to just use orchestrator
- Implement orchestrator system message functionality to customize coordination behavior:

  * Custom voting strategies (consensus, expertise-weighted, domain-specific)
  * Message construction templates for sub-agent instructions
  * Conflict resolution approaches (evidence-based, democratic, expert-priority)
  * Workflow preferences (thorough vs fast, iterative vs single-pass)
  * Domain-specific coordination (research teams, technical reviews, creative brainstorming)
  * Dynamic agent selection based on task requirements and orchestrator instructions
"""

import asyncio
import concurrent.futures
import json
import os
import shutil
import time
import traceback
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, AsyncGenerator, Dict, List, Optional, Set, Tuple

from ._broadcast_channel import BroadcastChannel
from .agent_config import AgentConfig
from .backend.base import StreamChunk
from .chat_agent import ChatAgent
from .configs.rate_limits import get_rate_limit_config
from .coordination_tracker import CoordinationTracker

if TYPE_CHECKING:
    from .dspy_paraphraser import QuestionParaphraser
    from .subagent.models import SubagentResult

from .logger_config import get_log_session_dir  # Import to get log directory
from .logger_config import logger  # Import logger directly for INFO logging
from .logger_config import (
    log_coordination_step,
    log_orchestrator_activity,
    log_orchestrator_agent_message,
    log_stream_chunk,
    log_tool_call,
    set_log_attempt,
)
from .mcp_tools.hooks import (
    GeneralHookManager,
    HighPriorityTaskReminderHook,
    HookType,
    HumanInputHook,
    MidStreamInjectionHook,
    RoundTimeoutPostHook,
    RoundTimeoutPreHook,
    RoundTimeoutState,
    SubagentCompleteHook,
)
from .memory import ConversationMemory, PersistentMemoryBase
from .message_templates import MessageTemplates
from .persona_generator import PersonaGenerator
from .stream_chunk import ChunkType
from .structured_logging import (
    clear_current_round,
    get_tracer,
    log_agent_round_context,
    log_coordination_event,
    set_current_round,
)
from .system_message_builder import SystemMessageBuilder
from .tool import get_post_evaluation_tools, get_workflow_tools
from .utils import ActionType, AgentStatus, CoordinationStage


@dataclass
class AgentState:
    """Runtime state for an agent during coordination.

    Attributes:
        answer: The agent's current answer/summary, if any
        has_voted: Whether the agent has voted in the current round
        votes: Dictionary storing vote data for this agent
        restart_pending: Whether the agent should gracefully restart due to new answers
        is_killed: Whether this agent has been killed due to timeout/limits
        timeout_reason: Reason for timeout (if applicable)
        answer_count: Number of answers this agent has created (increments on new_answer)
        injection_count: Number of update injections this agent has received
        round_start_time: Timestamp when current round started (for per-round timeouts)
        round_timeout_hooks: Tuple of (post_hook, pre_hook) for per-round timeouts, or None
        round_timeout_state: Shared state for timeout hooks (tracks consecutive denials)
    """

    answer: Optional[str] = None
    has_voted: bool = False
    votes: Dict[str, Any] = field(default_factory=dict)
    restart_pending: bool = False
    is_killed: bool = False
    timeout_reason: Optional[str] = None
    last_context: Optional[Dict[str, Any]] = None  # Store the context sent to this agent
    paraphrase: Optional[str] = None
    answer_count: int = 0  # Track number of answers for memory archiving
    injection_count: int = 0  # Track injections received for mid-stream injection timing
    restart_count: int = 0  # Track full restarts (TUI round = restart_count + 1)
    known_answer_ids: set = field(default_factory=set)  # Agent IDs whose answers this agent has seen
    round_start_time: Optional[float] = None  # For per-round timeouts
    round_timeout_hooks: Optional[tuple] = None  # (post_hook, pre_hook) for resetting on new round
    round_timeout_state: Optional["RoundTimeoutState"] = None  # Shared timeout state


class Orchestrator(ChatAgent):
    """
    Orchestrator Agent - Unified chat interface with sub-agent coordination.

    The orchestrator acts as a single agent from the user's perspective, but internally
    coordinates multiple sub-agents using the proven binary decision framework.

    Key Features:
    - Unified chat interface (same as any individual agent)
    - Automatic sub-agent coordination and conflict resolution
    - Transparent MassGen workflow execution
    - Real-time streaming with proper source attribution
    - Graceful restart mechanism for dynamic case transitions
    - Session management

    TODO - Missing Configuration Options:
    - Option to include/exclude voting details in user messages
    - Configurable timeout settings for agent responses
    - Configurable retry limits and backoff strategies
    - Custom voting strategies beyond simple majority
    - Configurable presentation formats for final answers
    - Advanced coordination workflows (hierarchical, weighted voting, etc.)

    TODO (v0.0.14 Context Sharing Enhancement - See docs/dev_notes/v0.0.14-context.md):
    - Add permission validation logic for agent workspace access
    - Implement validate_agent_access() method to check if agent has required permission for resource
    - Replace current prompt-based access control with explicit system-level enforcement
    - Add PermissionManager integration for managing agent access rules
    - Implement audit logging for all access attempts to workspace resources
    - Support dynamic permission negotiation during runtime
    - Add configurable policy framework for permission management
    - Integrate with workspace snapshot mechanism for controlled context sharing

    Restart Behavior:
    When an agent provides new_answer, all agents gracefully restart to ensure
    consistent coordination state. This allows all agents to transition to Case 2
    evaluation with the new answers available.
    """

    def __init__(
        self,
        agents: Dict[str, ChatAgent],
        orchestrator_id: str = "orchestrator",
        session_id: Optional[str] = None,
        config: Optional[AgentConfig] = None,
        dspy_paraphraser: Optional["QuestionParaphraser"] = None,
        snapshot_storage: Optional[str] = None,
        agent_temporary_workspace: Optional[str] = None,
        previous_turns: Optional[List[Dict[str, Any]]] = None,
        winning_agents_history: Optional[List[Dict[str, Any]]] = None,
        shared_conversation_memory: Optional[ConversationMemory] = None,
        shared_persistent_memory: Optional[PersistentMemoryBase] = None,
        enable_nlip: bool = False,
        nlip_config: Optional[Dict[str, Any]] = None,
        enable_rate_limit: bool = False,
        trace_classification: str = "legacy",
        generated_personas: Optional[Dict[str, Any]] = None,
        plan_session_id: Optional[str] = None,
    ):
        """
        Initialize MassGen orchestrator.

        Args:
            agents: Dictionary of {agent_id: ChatAgent} - can be individual agents or other orchestrators
            orchestrator_id: Unique identifier for this orchestrator (default: "orchestrator")
            session_id: Optional session identifier
            config: Optional AgentConfig for customizing orchestrator behavior
            dspy_paraphraser: Optional DSPy paraphraser for multi-agent question diversity
            snapshot_storage: Optional path to store agent workspace snapshots
            agent_temporary_workspace: Optional path for agent temporary workspaces
            previous_turns: List of previous turn metadata for multi-turn conversations (loaded by CLI)
            winning_agents_history: List of previous winning agents for memory sharing
                                   Format: [{"agent_id": "agent_b", "turn": 1}, ...]
                                   Loaded from session storage to persist across orchestrator recreations
            shared_conversation_memory: Optional shared conversation memory for all agents
            shared_persistent_memory: Optional shared persistent memory for all agents
            enable_nlip: Enable NLIP (Natural Language Interaction Protocol) support
            nlip_config: Optional NLIP configuration
            enable_rate_limit: Whether to enable rate limiting and cooldown delays (default: False)
            trace_classification: "legacy" (default) preserves current content traces; "strict" emits
                                  coordination/status as non-content for server mode.
            generated_personas: Pre-generated personas from previous turn (for multi-turn persistence)
                               Format: {agent_id: GeneratedPersona, ...}
            plan_session_id: Optional plan session ID for plan execution mode (prevents workspace contamination)
        """
        super().__init__(
            session_id,
            shared_conversation_memory,
            shared_persistent_memory,
        )
        self.orchestrator_id = orchestrator_id
        self.agents = agents
        self.agent_states = {aid: AgentState() for aid in agents.keys()}
        self.config = config or AgentConfig.create_openai_config()
        self.dspy_paraphraser = dspy_paraphraser
        self._plan_session_id = plan_session_id

        # Debug: Log timeout config values
        logger.info(
            f"[Orchestrator] Timeout config: initial={self.config.timeout_config.initial_round_timeout_seconds}s, " f"subsequent={self.config.timeout_config.subsequent_round_timeout_seconds}s",
        )
        self.trace_classification = trace_classification

        # Shared memory for all agents
        self.shared_conversation_memory = shared_conversation_memory
        self.shared_persistent_memory = shared_persistent_memory

        # Get message templates from config
        self.message_templates = self.config.message_templates or MessageTemplates(
            voting_sensitivity=self.config.voting_sensitivity,
            answer_novelty_requirement=self.config.answer_novelty_requirement,
        )
        # Create system message builder for all phases (coordination, presentation, post-evaluation)
        self._system_message_builder: Optional[SystemMessageBuilder] = None  # Lazy initialization
        # Create workflow tools for agents (vote, new_answer, and optionally broadcast)
        # Will be updated with broadcast tools after coordination config is set
        # Sort agent IDs for consistent anonymous mapping (agent1, agent2, etc.)
        # This ensures consistency with coordination_tracker.get_anonymous_agent_mapping()
        self.workflow_tools = get_workflow_tools(
            valid_agent_ids=sorted(agents.keys()),
            template_overrides=getattr(
                self.message_templates,
                "_template_overrides",
                {},
            ),
            api_format="chat_completions",  # Default format, will be overridden per backend
            orchestrator=self,  # Pass self for broadcast tools
            broadcast_mode=False,  # Will be updated if broadcasts enabled
            broadcast_wait_by_default=True,
        )

        # Client-provided tools (OpenAI-style). These are passed through to backends
        # so models can request them, but are never executed by MassGen.
        self._external_tools: List[Dict[str, Any]] = []

        # MassGen-specific state
        self.current_task: Optional[str] = None
        self.workflow_phase: str = "idle"  # idle, coordinating, presenting

        # Internal coordination state
        self._coordination_messages: List[Dict[str, str]] = []
        self._selected_agent: Optional[str] = None
        self._final_presentation_content: Optional[str] = None
        self._presentation_started: bool = False  # Guard against duplicate presentations

        # Track winning agents by turn for memory sharing
        # Format: [{"agent_id": "agent_b", "turn": 1}, {"agent_id": "agent_a", "turn": 2}]
        # Restore from session storage if provided (for multi-turn persistence)
        self._winning_agents_history: List[Dict[str, Any]] = winning_agents_history or []
        if self._winning_agents_history:
            logger.info(
                f"ðŸ“š Restored {len(self._winning_agents_history)} winning agent(s) from session: {self._winning_agents_history}",
            )
        self._current_turn: int = 0

        # Timeout and resource tracking
        self.total_tokens: int = 0
        self.coordination_start_time: float = 0
        self.is_orchestrator_timeout: bool = False
        self.timeout_reason: Optional[str] = None

        # Restart feature state tracking
        self.current_attempt: int = 0
        max_restarts = self.config.coordination_config.max_orchestration_restarts
        self.max_attempts: int = 1 + max_restarts
        self.restart_pending: bool = False
        self.restart_reason: Optional[str] = None
        self.restart_instructions: Optional[str] = None
        self.previous_attempt_answer: Optional[str] = None  # Store previous winner's answer for restart context

        # Coordination state tracking for cleanup
        self._active_streams: Dict = {}
        self._active_tasks: Dict = {}

        # Human input hook for injecting user input during execution
        # Shared across all agents (one per orchestration session)
        self._human_input_hook: Optional[HumanInputHook] = None
        # Async subagent completion tracking
        # Stores pending results for each parent agent until they can be injected
        # Format: {parent_agent_id: [(subagent_id, SubagentResult), ...]}
        self._pending_subagent_results: Dict[str, List[Tuple[str, "SubagentResult"]]] = {}

        # Track which subagents have been injected to prevent duplicates
        # Format: {agent_id: set(subagent_id, ...)}
        self._injected_subagents: Dict[str, Set[str]] = {}

        # Async subagent configuration (parsed from coordination_config)
        async_subagent_config = {}
        if hasattr(self.config, "coordination_config"):
            async_subagent_config = getattr(self.config.coordination_config, "async_subagents", {}) or {}
        self._async_subagents_enabled = async_subagent_config.get("enabled", True)
        self._async_subagent_injection_strategy = async_subagent_config.get("injection_strategy", "tool_result")

        # Agent startup rate limiting (per model)
        # Load from centralized configuration file instead of hardcoding
        self._enable_rate_limit = enable_rate_limit
        self._agent_startup_times: Dict[str, List[float]] = {}  # model -> [timestamps]
        self._rate_limits: Dict[str, Dict[str, int]] = self._load_rate_limits_from_config() if enable_rate_limit else {}

        # Context sharing for agents with filesystem support
        self._snapshot_storage: Optional[str] = snapshot_storage
        self._agent_temporary_workspace: Optional[str] = agent_temporary_workspace

        # DSPy paraphrase tracking
        self._agent_paraphrases: Dict[str, str] = {}
        self._paraphrase_generation_errors: int = 0

        # Persona generation tracking
        # If personas are passed in (from previous turn), use them and mark as already generated
        self._generated_personas: Dict[str, Any] = generated_personas or {}  # agent_id -> GeneratedPersona
        self._personas_generated: bool = bool(
            generated_personas,
        )  # Skip generation if already have them
        self._original_system_messages: Dict[
            str,
            Optional[str],
        ] = {}  # agent_id -> original message
        if self._personas_generated:
            logger.info(
                f"ðŸ“ Restored {len(self._generated_personas)} persona(s) from previous turn",
            )

        # Multi-turn session tracking (loaded by CLI, not managed by orchestrator)
        self._previous_turns: List[Dict[str, Any]] = previous_turns or []

        # Coordination tracking - always enabled for analysis/debugging
        self.coordination_tracker = CoordinationTracker()
        self.coordination_tracker.initialize_session(list(agents.keys()))

        # Create snapshot storage and workspace directories if specified
        if snapshot_storage:
            self._snapshot_storage = snapshot_storage
            snapshot_path = Path(self._snapshot_storage)
            # Clean existing directory if it exists and has contents
            if snapshot_path.exists() and any(snapshot_path.iterdir()):
                shutil.rmtree(snapshot_path)
            snapshot_path.mkdir(parents=True, exist_ok=True)

        # Configure orchestration paths for each agent with filesystem support
        # Get skills configuration if skills are enabled
        skills_directory = None
        massgen_skills = []
        load_previous_session_skills = False
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "use_skills",
        ):
            if self.config.coordination_config.use_skills:
                skills_directory = self.config.coordination_config.skills_directory
                massgen_skills = self.config.coordination_config.massgen_skills
                load_previous_session_skills = getattr(
                    self.config.coordination_config,
                    "load_previous_session_skills",
                    False,
                )

        def _setup_agent_orchestration(agent_id: str, agent) -> None:
            """Setup orchestration paths for a single agent (can run in parallel)."""
            if not agent.backend.filesystem_manager:
                return

            agent.backend.filesystem_manager.setup_orchestration_paths(
                agent_id=agent_id,
                snapshot_storage=self._snapshot_storage,
                agent_temporary_workspace=self._agent_temporary_workspace,
                skills_directory=skills_directory,
                massgen_skills=massgen_skills,
                load_previous_session_skills=load_previous_session_skills,
            )
            # Setup workspace directories for massgen skills
            if hasattr(self.config, "coordination_config") and hasattr(
                self.config.coordination_config,
                "massgen_skills",
            ):
                if self.config.coordination_config.massgen_skills:
                    agent.backend.filesystem_manager.setup_massgen_skill_directories(
                        massgen_skills=self.config.coordination_config.massgen_skills,
                    )
            # Setup memory directories if memory filesystem mode is enabled
            if hasattr(self.config, "coordination_config") and hasattr(
                self.config.coordination_config,
                "enable_memory_filesystem_mode",
            ):
                if self.config.coordination_config.enable_memory_filesystem_mode:
                    agent.backend.filesystem_manager.setup_memory_directories()

                    # Restore memories from previous turn if available
                    if self._previous_turns:
                        previous_turn = self._previous_turns[-1]  # Get most recent turn
                        if "log_dir" in previous_turn:
                            from pathlib import Path as PathlibPath

                            prev_log_dir = PathlibPath(previous_turn["log_dir"])
                            # Look for final workspace from previous turn
                            prev_final_workspace = prev_log_dir / "final"
                            if prev_final_workspace.exists():
                                # Find the winning agent's workspace from previous turn
                                for agent_dir in prev_final_workspace.iterdir():
                                    if agent_dir.is_dir():
                                        prev_workspace = agent_dir / "workspace"
                                        if prev_workspace.exists():
                                            logger.info(
                                                f"[Orchestrator] Restoring memories from previous turn: {prev_workspace}",
                                            )
                                            agent.backend.filesystem_manager.restore_memories_from_previous_turn(
                                                prev_workspace,
                                            )
                                            break  # Only restore from one agent (the winner)

            # Update MCP config with agent_id for Docker mode (must be after setup_orchestration_paths)
            agent.backend.filesystem_manager.update_backend_mcp_config(
                agent.backend.config,
            )

        # Setup orchestration paths for all agents in parallel (Docker container creation is I/O bound)
        if len(self.agents) > 1:
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
                futures = {executor.submit(_setup_agent_orchestration, agent_id, agent): agent_id for agent_id, agent in self.agents.items()}
                for future in concurrent.futures.as_completed(futures):
                    agent_id = futures[future]
                    try:
                        future.result()
                    except Exception as e:
                        logger.error(f"[Orchestrator] Failed to setup orchestration for {agent_id}: {e}")
                        raise
        else:
            # Single agent - no need for threading overhead
            for agent_id, agent in self.agents.items():
                _setup_agent_orchestration(agent_id, agent)

        # Initialize broadcast channel for agent-to-agent communication
        self.broadcast_channel = BroadcastChannel(self)
        logger.info("[Orchestrator] Broadcast channel initialized")

        # Set orchestrator reference on all agents
        for agent_id, agent in self.agents.items():
            if hasattr(agent, "_orchestrator"):
                agent._orchestrator = self
                logger.debug(
                    f"[Orchestrator] Set orchestrator reference on agent: {agent_id}",
                )

        # Validate and setup skills if enabled
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "use_skills",
        ):
            if self.config.coordination_config.use_skills:
                logger.info("[Orchestrator] Skills enabled, validating configuration")
                self._validate_skills_config()
                logger.info("[Orchestrator] Skills validation complete")

        # Inject planning tools if enabled
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "enable_agent_task_planning",
        ):
            if self.config.coordination_config.enable_agent_task_planning:
                logger.info(
                    f"[Orchestrator] Injecting planning tools for {len(self.agents)} agents",
                )
                self._inject_planning_tools_for_all_agents()
                logger.info("[Orchestrator] Planning tools injection complete")

        # Inject subagent tools if enabled
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "enable_subagents",
        ):
            if self.config.coordination_config.enable_subagents:
                logger.info(
                    f"[Orchestrator] Injecting subagent tools for {len(self.agents)} agents",
                )
                self._inject_subagent_tools_for_all_agents()
                logger.info("[Orchestrator] Subagent tools injection complete")

        # Set compression target ratio on all agent backends
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "compression_target_ratio",
        ):
            compression_ratio = self.config.coordination_config.compression_target_ratio
            for agent_id, agent in self.agents.items():
                if hasattr(agent, "backend") and agent.backend:
                    agent.backend._compression_target_ratio = compression_ratio
            logger.info(
                f"[Orchestrator] Set compression_target_ratio={compression_ratio} on {len(self.agents)} agent backends",
            )

        # NLIP Configuration
        self.enable_nlip = enable_nlip
        self.nlip_config = nlip_config or {}

        # Initialize NLIP routers for agents if enabled
        if self.enable_nlip:
            self._init_nlip_routing()

        # Initialize broadcast tools (independent of NLIP)
        self._init_broadcast_tools()

    def _init_nlip_routing(self) -> None:
        """Initialize NLIP routing for all agents."""
        logger.info(
            f"[Orchestrator] Initializing NLIP routing for {len(self.agents)} agents",
        )

        nlip_enabled_count = 0
        nlip_skipped_count = 0

        for agent_id, agent in self.agents.items():
            # Check if agent has config
            if not hasattr(agent, "config"):
                logger.debug(
                    f"[Orchestrator] Agent {agent_id} has no config, skipping NLIP",
                )
                nlip_skipped_count += 1
                continue

            # Check if backend supports NLIP (has custom_tool_manager)
            backend = getattr(agent, "backend", None)
            if not backend:
                logger.debug(
                    f"[Orchestrator] Agent {agent_id} has no backend, skipping NLIP",
                )
                nlip_skipped_count += 1
                continue

            tool_manager = getattr(backend, "custom_tool_manager", None)
            if not tool_manager:
                logger.info(
                    f"[Orchestrator] Agent {agent_id} backend does not support NLIP (no custom_tool_manager), skipping",
                )
                nlip_skipped_count += 1
                continue

            # Backend supports NLIP, enable it
            agent.config.enable_nlip = True
            agent.config.nlip_config = self.nlip_config

            # Initialize NLIP router for the agent
            mcp_executor = getattr(backend, "_execute_mcp_function_with_retry", None)
            agent.config.init_nlip_router(
                tool_manager=tool_manager,
                mcp_executor=mcp_executor,
            )

            # Inject NLIP router into backend
            if hasattr(backend, "set_nlip_router"):
                backend.set_nlip_router(
                    nlip_router=agent.config.nlip_router,
                    enabled=True,
                )

            logger.info(f"[Orchestrator] NLIP routing enabled for agent: {agent_id}")
            nlip_enabled_count += 1

        logger.info(
            f"[Orchestrator] NLIP initialization complete: {nlip_enabled_count} enabled, {nlip_skipped_count} skipped",
        )

    def _init_broadcast_tools(self) -> None:
        """Initialize broadcast tools if enabled in coordination config."""
        # Update workflow tools with broadcast if enabled
        has_coord = hasattr(self.config, "coordination_config")
        has_broadcast = hasattr(self.config.coordination_config, "broadcast") if has_coord else False
        logger.info(
            f"[Orchestrator] Checking broadcast config: has_coord={has_coord}, has_broadcast={has_broadcast}",
        )
        if hasattr(self.config, "coordination_config") and hasattr(
            self.config.coordination_config,
            "broadcast",
        ):
            broadcast_mode = self.config.coordination_config.broadcast
            logger.info(
                f"[Orchestrator] Broadcast mode value: {broadcast_mode}, type: {type(broadcast_mode)}",
            )
            if broadcast_mode and broadcast_mode is not False:
                logger.info(
                    f"[Orchestrator] Broadcasting enabled (mode: {broadcast_mode}). Adding broadcast tools to workflow",
                )

                # Use blocking mode (wait=True) for both agents and human
                # Priority system prevents deadlocks by requiring agents to respond to pending broadcasts first
                wait_by_default = True
                logger.info(
                    "[Orchestrator] Using blocking broadcasts (wait=True) with priority system to prevent deadlocks",
                )

                # Get broadcast sensitivity setting
                broadcast_sensitivity = getattr(
                    self.config.coordination_config,
                    "broadcast_sensitivity",
                    "medium",
                )
                logger.info(
                    f"[Orchestrator] Broadcast sensitivity: {broadcast_sensitivity}",
                )

                # Recreate workflow tools with broadcast enabled
                # Sort agent IDs for consistent anonymous mapping with coordination_tracker
                self.workflow_tools = get_workflow_tools(
                    valid_agent_ids=sorted(self.agents.keys()),
                    template_overrides=getattr(
                        self.message_templates,
                        "_template_overrides",
                        {},
                    ),
                    api_format="chat_completions",  # Default, overridden per backend
                    orchestrator=self,
                    broadcast_mode=broadcast_mode,
                    broadcast_wait_by_default=wait_by_default,
                )
                tool_names = [t.get("function", {}).get("name", "unknown") for t in self.workflow_tools]
                logger.info(
                    f"[Orchestrator] Broadcast tools added to workflow ({len(self.workflow_tools)} total tools): {tool_names}",
                )

                # Register broadcast tools as custom tools with backends for recursive execution
                self._register_broadcast_custom_tools(
                    broadcast_mode,
                    wait_by_default,
                    broadcast_sensitivity,
                )
            else:
                logger.info("[Orchestrator] Broadcasting disabled")
        else:
            logger.info("[Orchestrator] Broadcast config not found")

    def _register_broadcast_custom_tools(
        self,
        broadcast_mode: str,
        wait_by_default: bool,
        sensitivity: str = "medium",
    ) -> None:
        """
        Register broadcast tools as custom tools with all agent backends.

        This allows broadcast tools to be executed recursively by the backend,
        avoiding the need for orchestrator-level tool handling.

        Args:
            broadcast_mode: "agents" or "human"
            wait_by_default: Default waiting behavior for broadcasts
            sensitivity: How frequently to use ask_others() ("low", "medium", "high")
        """
        from .tool.workflow_toolkits.broadcast import BroadcastToolkit

        # Create broadcast toolkit instance
        broadcast_toolkit = BroadcastToolkit(
            orchestrator=self,
            broadcast_mode=broadcast_mode,
            wait_by_default=wait_by_default,
            sensitivity=sensitivity,
        )

        # Register with each agent's backend as custom tool functions
        for agent_id, agent in self.agents.items():
            backend = agent.backend

            # Check if backend supports custom tool registration
            # Note: Some backends use custom_tool_manager, others use _custom_tool_manager
            has_tool_manager = hasattr(backend, "custom_tool_manager") or hasattr(
                backend,
                "_custom_tool_manager",
            )
            if not has_tool_manager:
                logger.warning(
                    f"[Orchestrator] Agent {agent_id} backend doesn't support custom tool manager - broadcast tools will use orchestrator handling",
                )
                continue

            # Register ask_others as a custom tool
            if not hasattr(backend, "_broadcast_toolkit"):
                backend._broadcast_toolkit = broadcast_toolkit
                # Ensure _custom_tool_names exists (some backends may not have it)
                if not hasattr(backend, "_custom_tool_names"):
                    backend._custom_tool_names = set()
                backend._custom_tool_names.add("ask_others")
                logger.info(
                    f"[Orchestrator] Registered ask_others as custom tool for agent {agent_id}",
                )

            # Register respond_to_broadcast for agents mode
            if broadcast_mode == "agents":
                backend._custom_tool_names.add("respond_to_broadcast")
                logger.info(
                    f"[Orchestrator] Registered respond_to_broadcast as custom tool for agent {agent_id}",
                )

            # Register polling tools if needed
            if not wait_by_default:
                backend._custom_tool_names.add("check_broadcast_status")
                backend._custom_tool_names.add("get_broadcast_responses")
                logger.info(
                    f"[Orchestrator] Registered polling broadcast tools for agent {agent_id}",
                )

    async def _prepare_paraphrases_for_agents(self, question: str) -> None:
        """Generate and assign DSPy paraphrases for the current question."""

        # Reset paraphrases before regenerating
        self._agent_paraphrases = {}
        for state in self.agent_states.values():
            state.paraphrase = None

        if not self.dspy_paraphraser:
            return

        if not question:
            return

        try:
            variants = await asyncio.to_thread(
                self.dspy_paraphraser.generate_variants,
                question,
            )
        except Exception as exc:
            self._paraphrase_generation_errors += 1
            logger.warning(f"Failed to generate DSPy paraphrases: {exc}")
            return

        if not variants:
            logger.warning(
                "DSPy paraphraser returned no variants; proceeding with original question for all agents.",
            )
            return

        agent_ids = list(self.agents.keys())
        if not agent_ids:
            return

        for idx, agent_id in enumerate(agent_ids):
            paraphrase = variants[idx % len(variants)]
            self._agent_paraphrases[agent_id] = paraphrase
            self.agent_states[agent_id].paraphrase = paraphrase

        # Log at INFO level so users know paraphrasing is active
        logger.info(
            f"DSPy paraphrasing enabled: {len(variants)} variant(s) generated and assigned to {len(agent_ids)} agent(s)",
        )
        for agent_id, paraphrase in self._agent_paraphrases.items():
            logger.info(f"  {agent_id}: {paraphrase}")

        log_coordination_step(
            "DSPy paraphrases prepared",
            {
                "variants": len(variants),
                "assigned_agents": self._agent_paraphrases,
            },
        )

    def get_paraphrase_status(self) -> Dict[str, Any]:
        """Return current DSPy paraphrase assignments and metrics for observability."""

        status = {
            "paraphrases": self._agent_paraphrases.copy(),
            "generation_errors": self._paraphrase_generation_errors,
            "metrics": None,
        }

        if self.dspy_paraphraser:
            try:
                status["metrics"] = self.dspy_paraphraser.get_metrics()
            except Exception as exc:  # pragma: no cover - defensive guard
                logger.debug(f"Unable to fetch DSPy paraphraser metrics: {exc}")

        return status

    def _validate_skills_config(self) -> None:
        """
        Validate skills configuration before orchestration.

        Checks that:
        1. Command line execution is enabled for at least one agent
        2. Skills directory exists and is not empty

        Raises:
            RuntimeError: If skills requirements are not met
        """
        from pathlib import Path

        # Check if command execution is available
        has_command_execution = False
        for agent_id, agent in self.agents.items():
            if hasattr(agent, "config") and agent.config:
                enable_cmd = agent.backend.config.get("enable_mcp_command_line", False)
                if enable_cmd:
                    has_command_execution = True
                    logger.info(
                        f"[Orchestrator] Agent {agent_id} has command execution enabled",
                    )
                    break

        if not has_command_execution:
            raise RuntimeError(
                "Skills require command line execution to be enabled. " "Set enable_mcp_command_line: true in at least one agent's backend config.",
            )

        # Check if skills are available (external or built-in)
        skills_dir = Path(self.config.coordination_config.skills_directory)
        logger.info(
            f"[Orchestrator] Checking skills configuration - directory: {skills_dir}",
        )

        # Check for external skills (from openskills)
        has_external_skills = skills_dir.exists() and skills_dir.is_dir() and any(skills_dir.iterdir())

        # Check for built-in skills (bundled with MassGen)
        builtin_skills_dir = Path(__file__).parent / "skills"
        has_builtin_skills = builtin_skills_dir.exists() and any(
            builtin_skills_dir.iterdir(),
        )

        # At least one type of skills must be available
        if not has_external_skills and not has_builtin_skills:
            raise RuntimeError(
                f"No skills found. To use skills:\n"
                f"Install external skills: 'npm i -g openskills && openskills install anthropics/skills --universal -y'\n"
                f"This creates '{skills_dir}' with skills like pdf, xlsx, pptx, etc.\n\n"
                f"Built-in skills (file-search, serena, semtools) should be bundled with MassGen in {builtin_skills_dir}",
            )

        logger.info(
            f"[Orchestrator] Skills available (external: {has_external_skills}, builtin: {has_builtin_skills})",
        )

    def _inject_planning_tools_for_all_agents(self) -> None:
        """
        Inject planning MCP tools into all agents.

        This method adds the planning MCP server to each agent's backend
        configuration, enabling them to create and manage task plans.
        """
        for agent_id, agent in self.agents.items():
            self._inject_planning_tools_for_agent(agent_id, agent)

    def _inject_planning_tools_for_agent(self, agent_id: str, agent: Any) -> None:
        """
        Inject planning MCP tools into a specific agent.

        Args:
            agent_id: ID of the agent
            agent: Agent instance
        """
        logger.info(f"[Orchestrator] Injecting planning tools for agent: {agent_id}")

        # Create planning MCP config
        planning_mcp_config = self._create_planning_mcp_config(agent_id, agent)
        logger.info(
            f"[Orchestrator] Created planning MCP config: {planning_mcp_config['name']}",
        )

        # Get existing mcp_servers configuration
        mcp_servers = agent.backend.config.get("mcp_servers", [])
        logger.info(
            f"[Orchestrator] Existing MCP servers for {agent_id}: {type(mcp_servers)} with {len(mcp_servers) if isinstance(mcp_servers, (list, dict)) else 0} entries",
        )

        # Handle both list format and dict format (Claude Code)
        if isinstance(mcp_servers, dict):
            # Claude Code dict format
            logger.info("[Orchestrator] Using dict format for MCP servers")
            mcp_servers[f"planning_{agent_id}"] = planning_mcp_config
        else:
            # Standard list format
            logger.info("[Orchestrator] Using list format for MCP servers")
            if not isinstance(mcp_servers, list):
                mcp_servers = []
            mcp_servers.append(planning_mcp_config)

        # Update backend config
        agent.backend.config["mcp_servers"] = mcp_servers
        logger.info(
            f"[Orchestrator] Updated MCP servers for {agent_id}, now has {len(mcp_servers) if isinstance(mcp_servers, (list, dict)) else 0} servers",
        )

    def _create_planning_mcp_config(self, agent_id: str, agent: Any) -> Dict[str, Any]:
        """
        Create MCP server configuration for planning tools.

        Args:
            agent_id: ID of the agent
            agent: Agent instance (for accessing workspace path)

        Returns:
            MCP server configuration dictionary
        """
        from pathlib import Path as PathlibPath

        import massgen.mcp_tools.planning._planning_mcp_server as planning_module

        script_path = PathlibPath(planning_module.__file__).resolve()

        args = [
            "run",
            f"{script_path}:create_server",
            "--",
            "--agent-id",
            agent_id,
            "--orchestrator-id",
            self.orchestrator_id,
        ]

        # Add workspace path if filesystem mode is enabled
        logger.info(
            f"[Orchestrator] Checking task_planning_filesystem_mode for {agent_id}",
        )
        has_coord_config = hasattr(self.config, "coordination_config")
        logger.info(f"[Orchestrator] Has coordination_config: {has_coord_config}")

        if has_coord_config:
            has_filesystem_mode = hasattr(
                self.config.coordination_config,
                "task_planning_filesystem_mode",
            )
            logger.info(
                f"[Orchestrator] Has task_planning_filesystem_mode attr: {has_filesystem_mode}",
            )
            if has_filesystem_mode:
                value = self.config.coordination_config.task_planning_filesystem_mode
                logger.info(
                    f"[Orchestrator] task_planning_filesystem_mode value: {value}",
                )

        filesystem_mode_enabled = (
            hasattr(self.config, "coordination_config")
            and hasattr(
                self.config.coordination_config,
                "task_planning_filesystem_mode",
            )
            and self.config.coordination_config.task_planning_filesystem_mode
        )

        if filesystem_mode_enabled:
            logger.info("[Orchestrator] task_planning_filesystem_mode is enabled")
            if hasattr(agent, "backend") and hasattr(agent.backend, "filesystem_manager") and agent.backend.filesystem_manager:
                if agent.backend.filesystem_manager.cwd:
                    workspace_path = str(agent.backend.filesystem_manager.cwd)
                    args.extend(["--workspace-path", workspace_path])
                    logger.info(
                        f"[Orchestrator] Enabling filesystem mode for task planning: {workspace_path}",
                    )
                else:
                    logger.warning(
                        f"[Orchestrator] Agent {agent_id} filesystem_manager.cwd is None",
                    )
            else:
                logger.warning(
                    f"[Orchestrator] Agent {agent_id} has no filesystem_manager",
                )

        # Add feature flags for auto-inserting discovery tasks
        skills_enabled = hasattr(self.config, "coordination_config") and hasattr(self.config.coordination_config, "use_skills") and self.config.coordination_config.use_skills
        if skills_enabled:
            args.append("--skills-enabled")

        auto_discovery_enabled = False
        if hasattr(agent, "backend") and hasattr(agent.backend, "config"):
            auto_discovery_enabled = agent.backend.config.get(
                "auto_discover_custom_tools",
                False,
            )
        if auto_discovery_enabled:
            args.append("--auto-discovery-enabled")

        memory_enabled = (
            hasattr(self.config, "coordination_config")
            and hasattr(
                self.config.coordination_config,
                "enable_memory_filesystem_mode",
            )
            and self.config.coordination_config.enable_memory_filesystem_mode
        )
        if memory_enabled:
            args.append("--memory-enabled")

        # Enable git commits on task completion if two-tier workspace is enabled
        coordination_config = getattr(self.config, "coordination_config", None)
        use_two_tier_workspace = bool(
            getattr(coordination_config, "use_two_tier_workspace", False),
        )
        logger.info(
            f"[Orchestrator] use_two_tier_workspace value for {agent_id}: {use_two_tier_workspace}",
        )
        if use_two_tier_workspace:
            args.append("--use-two-tier-workspace")
            logger.info(
                f"[Orchestrator] Adding --use-two-tier-workspace flag to planning MCP for {agent_id}",
            )

        logger.info(f"[Orchestrator] Planning MCP args for {agent_id}: {args}")

        config = {
            "name": f"planning_{agent_id}",
            "type": "stdio",
            "command": "fastmcp",
            "args": args,
            "env": {
                "FASTMCP_SHOW_CLI_BANNER": "false",
            },
        }

        return config

    def _inject_subagent_tools_for_all_agents(self) -> None:
        """
        Inject subagent MCP tools into all agents.

        This method adds the subagent MCP server to each agent's backend
        configuration, enabling them to spawn and manage subagents.
        """
        for agent_id, agent in self.agents.items():
            self._inject_subagent_tools_for_agent(agent_id, agent)

    def _inject_subagent_tools_for_agent(self, agent_id: str, agent: Any) -> None:
        """
        Inject subagent MCP tools into a specific agent.

        Args:
            agent_id: ID of the agent
            agent: Agent instance
        """
        # Only inject if agent has filesystem manager (needs workspace)
        if not hasattr(agent, "backend") or not hasattr(
            agent.backend,
            "filesystem_manager",
        ):
            logger.warning(
                f"[Orchestrator] Agent {agent_id} has no filesystem_manager, skipping subagent tools",
            )
            return

        if not agent.backend.filesystem_manager:
            logger.warning(
                f"[Orchestrator] Agent {agent_id} filesystem_manager is None, skipping subagent tools",
            )
            return

        if not agent.backend.filesystem_manager.cwd:
            logger.warning(
                f"[Orchestrator] Agent {agent_id} filesystem_manager.cwd is None, skipping subagent tools",
            )
            return

        logger.info(f"[Orchestrator] Injecting subagent tools for agent: {agent_id}")

        # Create subagent MCP config
        subagent_mcp_config = self._create_subagent_mcp_config(agent_id, agent)
        logger.info(
            f"[Orchestrator] Created subagent MCP config: {subagent_mcp_config['name']}",
        )

        # Get existing mcp_servers configuration
        mcp_servers = agent.backend.config.get("mcp_servers", [])
        logger.info(
            f"[Orchestrator] Existing MCP servers for {agent_id}: {type(mcp_servers)} with {len(mcp_servers) if isinstance(mcp_servers, (list, dict)) else 0} entries",
        )

        # Handle both list format and dict format (Claude Code)
        if isinstance(mcp_servers, dict):
            # Claude Code dict format
            logger.info("[Orchestrator] Using dict format for MCP servers")
            mcp_servers[f"subagent_{agent_id}"] = subagent_mcp_config
        else:
            # Standard list format
            logger.info("[Orchestrator] Using list format for MCP servers")
            if not isinstance(mcp_servers, list):
                mcp_servers = []
            mcp_servers.append(subagent_mcp_config)

        # Update backend config
        agent.backend.config["mcp_servers"] = mcp_servers
        logger.info(
            f"[Orchestrator] Updated MCP servers for {agent_id}, now has {len(mcp_servers) if isinstance(mcp_servers, (list, dict)) else 0} servers",
        )

        # Note: Subagent spawn callbacks are set up later when coordination_ui is available
        # See setup_subagent_spawn_callbacks() method

    def setup_subagent_spawn_callbacks(self) -> None:
        """Set up subagent spawn callbacks for all agents.

        This should be called AFTER coordination_ui is set on the orchestrator,
        as the callbacks need access to the display for TUI notifications.

        Called from CoordinationUI.set_orchestrator() after coordination_ui is assigned.
        """
        if not hasattr(self, "coordination_ui") or not self.coordination_ui:
            logger.debug("[Orchestrator] No coordination_ui, skipping subagent spawn callback setup")
            return

        for agent_id, agent in self.agents.items():
            if hasattr(agent, "backend") and hasattr(agent.backend, "set_subagent_spawn_callback"):
                self._setup_subagent_spawn_callback(agent_id, agent)

    def _setup_subagent_spawn_callback(self, agent_id: str, agent: Any) -> None:
        """Set up callback to notify TUI when subagent spawning starts.

        This creates a wrapper callback that captures the agent_id and forwards
        spawn notifications to the TUI display for immediate visual feedback.

        Args:
            agent_id: ID of the agent
            agent: Agent instance
        """
        # Get display if available (coordination_ui.display)
        display = None
        if hasattr(self, "coordination_ui") and self.coordination_ui:
            display = getattr(self.coordination_ui, "display", None)

        if not display:
            logger.debug(f"[Orchestrator] No display available for subagent spawn callback on {agent_id}")
            return

        if not hasattr(display, "notify_subagent_spawn_started"):
            logger.debug(f"[Orchestrator] Display doesn't support notify_subagent_spawn_started for {agent_id}")
            return

        # Create wrapper callback that captures agent_id
        def spawn_callback(tool_name: str, args: Dict[str, Any], call_id: str) -> None:
            """Forward spawn notification to TUI display."""
            try:
                display.notify_subagent_spawn_started(agent_id, tool_name, args, call_id)
                logger.debug(f"[Orchestrator] Notified TUI of subagent spawn for {agent_id}")
            except Exception as e:
                logger.debug(f"[Orchestrator] Failed to notify TUI of subagent spawn: {e}")

        # Set callback on backend
        if hasattr(agent.backend, "set_subagent_spawn_callback"):
            agent.backend.set_subagent_spawn_callback(spawn_callback)
            logger.info(f"[Orchestrator] Set subagent spawn callback for {agent_id}")
        else:
            logger.debug(f"[Orchestrator] Backend for {agent_id} doesn't support subagent spawn callback")

    def _create_subagent_mcp_config(self, agent_id: str, agent: Any) -> Dict[str, Any]:
        """
        Create MCP server configuration for subagent tools.

        Args:
            agent_id: ID of the agent
            agent: Agent instance (for accessing workspace path)

        Returns:
            MCP server configuration dictionary
        """
        import tempfile
        from pathlib import Path as PathlibPath

        import massgen.mcp_tools.subagent._subagent_mcp_server as subagent_module

        script_path = PathlibPath(subagent_module.__file__).resolve()

        workspace_path = str(agent.backend.filesystem_manager.cwd)

        # Build list of all parent agent configs to pass to subagent manager
        # This allows subagents to inherit the exact same agent setup by default
        import json

        agent_configs = []
        for aid, a in self.agents.items():
            agent_cfg = {"id": aid}
            if hasattr(a.backend, "config"):
                # Filter out non-serializable or internal keys
                backend_cfg = {k: v for k, v in a.backend.config.items() if k not in ("mcp_servers", "_config_path")}
                agent_cfg["backend"] = backend_cfg
            agent_configs.append(agent_cfg)

        # Write agent configs to temp file to avoid command line / env var length limits
        agent_configs_file = tempfile.NamedTemporaryFile(
            mode="w",
            suffix=".json",
            prefix="massgen_subagent_configs_",
            delete=False,  # Keep file until subagent reads it
        )
        json.dump(agent_configs, agent_configs_file)
        agent_configs_file.close()
        agent_configs_path = agent_configs_file.name

        # Extract context_paths from orchestrator config to pass to subagents
        # This allows subagents to read the same codebase/files as the parent
        context_paths_path = ""
        parent_context_paths = []
        if hasattr(self, "config") and isinstance(getattr(self.config, "__dict__", {}), dict):
            # Try to get context_paths from the raw config dict stored on agents
            if hasattr(agent.backend, "config") and "context_paths" in agent.backend.config:
                parent_context_paths = agent.backend.config.get("context_paths", [])

        if parent_context_paths:
            context_paths_file = tempfile.NamedTemporaryFile(
                mode="w",
                suffix=".json",
                prefix="massgen_subagent_context_paths_",
                delete=False,
            )
            json.dump(parent_context_paths, context_paths_file)
            context_paths_file.close()
            context_paths_path = context_paths_file.name
            logger.info(
                f"[Orchestrator] Passing {len(parent_context_paths)} context paths to subagent MCP",
            )

        # Extract coordination config to pass to subagents for planning tools inheritance
        coordination_config_path = ""
        if hasattr(self.config, "coordination_config") and self.config.coordination_config:
            coord_cfg = self.config.coordination_config
            # Extract relevant coordination settings that subagents should inherit
            parent_coordination_config = {}
            if hasattr(coord_cfg, "enable_agent_task_planning"):
                parent_coordination_config["enable_agent_task_planning"] = coord_cfg.enable_agent_task_planning
            if hasattr(coord_cfg, "task_planning_filesystem_mode"):
                parent_coordination_config["task_planning_filesystem_mode"] = coord_cfg.task_planning_filesystem_mode
            if hasattr(coord_cfg, "subagent_round_timeouts") and coord_cfg.subagent_round_timeouts:
                parent_coordination_config["subagent_round_timeouts"] = coord_cfg.subagent_round_timeouts

            # Include parent round timeouts for inheritance if subagent settings are omitted
            if hasattr(self.config, "timeout_config") and self.config.timeout_config:
                parent_coordination_config["parent_round_timeouts"] = {
                    "initial_round_timeout_seconds": self.config.timeout_config.initial_round_timeout_seconds,
                    "subsequent_round_timeout_seconds": self.config.timeout_config.subsequent_round_timeout_seconds,
                    "round_timeout_grace_seconds": self.config.timeout_config.round_timeout_grace_seconds,
                }

            if parent_coordination_config:
                coordination_config_file = tempfile.NamedTemporaryFile(
                    mode="w",
                    suffix=".json",
                    prefix="massgen_subagent_coordination_config_",
                    delete=False,
                )
                json.dump(parent_coordination_config, coordination_config_file)
                coordination_config_file.close()
                coordination_config_path = coordination_config_file.name
                logger.info(
                    f"[Orchestrator] Passing coordination config to subagent MCP: {list(parent_coordination_config.keys())}",
                )

        # Get subagent configuration from coordination config
        max_concurrent = 3
        default_timeout = 300
        min_timeout = 60
        max_timeout = 600
        subagent_orchestrator_config_json = "{}"
        if hasattr(self.config, "coordination_config"):
            if hasattr(self.config.coordination_config, "subagent_max_concurrent"):
                max_concurrent = self.config.coordination_config.subagent_max_concurrent
            if hasattr(self.config.coordination_config, "subagent_default_timeout"):
                default_timeout = self.config.coordination_config.subagent_default_timeout
            if hasattr(self.config.coordination_config, "subagent_min_timeout"):
                min_timeout = self.config.coordination_config.subagent_min_timeout
            if hasattr(self.config.coordination_config, "subagent_max_timeout"):
                max_timeout = self.config.coordination_config.subagent_max_timeout
            # Get subagent_orchestrator config if present
            if hasattr(self.config.coordination_config, "subagent_orchestrator"):
                so_config = self.config.coordination_config.subagent_orchestrator
                if so_config:
                    subagent_orchestrator_config_json = json.dumps(so_config.to_dict())

        # Get log directory for subagent logs
        log_directory = ""
        try:
            log_dir = get_log_session_dir()
            if log_dir:
                log_directory = str(log_dir)
        except Exception:
            pass  # Log directory not configured

        args = [
            "run",
            f"{script_path}:create_server",
            "--",
            "--agent-id",
            agent_id,
            "--orchestrator-id",
            self.orchestrator_id,
            "--workspace-path",
            workspace_path,
            "--agent-configs-file",
            agent_configs_path,
            "--max-concurrent",
            str(max_concurrent),
            "--default-timeout",
            str(default_timeout),
            "--min-timeout",
            str(min_timeout),
            "--max-timeout",
            str(max_timeout),
            "--orchestrator-config",
            subagent_orchestrator_config_json,
            "--log-directory",
            log_directory,
            "--context-paths-file",
            context_paths_path,
            "--coordination-config-file",
            coordination_config_path,
        ]

        config = {
            "name": f"subagent_{agent_id}",
            "type": "stdio",
            "command": "fastmcp",
            "args": args,
            "env": {
                "FASTMCP_SHOW_CLI_BANNER": "false",
            },
        }

        logger.info(
            f"[Orchestrator] Created subagent MCP config for {agent_id} with workspace: {workspace_path}",
        )

        return config

    async def _generate_and_inject_personas(self) -> None:
        """
        Generate diverse personas for all agents and inject into their system messages.

        This method uses a subagent (running the same models as parent) to generate
        complementary personas for each agent, increasing response diversity.
        The generated personas are prepended to existing system messages.

        The subagent approach:
        - Inherits the same models/backends as the parent config
        - Uses stripped-down config (no filesystem/command line tools)
        - If parent has N agents, subagent uses N agents to collaboratively generate personas
        """
        # Check if persona generation is enabled
        if not hasattr(self.config, "coordination_config"):
            logger.info(
                "[Orchestrator] No coordination_config, skipping persona generation",
            )
            return
        if not hasattr(self.config.coordination_config, "persona_generator"):
            logger.info(
                "[Orchestrator] No persona_generator config, skipping persona generation",
            )
            return

        pg = self.config.coordination_config.persona_generator
        logger.info(
            f"[Orchestrator] persona_generator config: type={type(pg)}, value={pg}",
        )
        if hasattr(pg, "enabled"):
            logger.info(f"[Orchestrator] persona_generator.enabled = {pg.enabled}")
        else:
            logger.info(
                f"[Orchestrator] persona_generator has no 'enabled' attr, attrs={dir(pg)}",
            )

        if not self.config.coordination_config.persona_generator.enabled:
            logger.info("[Orchestrator] Persona generation disabled in config")
            return

        # Skip if already generated (for multi-turn scenarios)
        if self._personas_generated:
            logger.info("[Orchestrator] Personas already generated, skipping")
            return

        logger.info(
            f"[Orchestrator] Generating personas for {len(self.agents)} agents via subagent",
        )

        try:
            pg_config = self.config.coordination_config.persona_generator

            # Initialize generator
            generator = PersonaGenerator(
                guidelines=pg_config.persona_guidelines,
                diversity_mode=pg_config.diversity_mode,
            )

            # Get existing system messages
            existing_messages = {}
            for agent_id, agent in self.agents.items():
                if hasattr(agent, "get_configurable_system_message"):
                    existing_messages[agent_id] = agent.get_configurable_system_message()
                else:
                    existing_messages[agent_id] = None

            # Build parent agent configs for inheritance
            parent_configs = []
            for agent_id, agent in self.agents.items():
                agent_cfg = {"id": agent_id}
                if hasattr(agent, "backend") and hasattr(agent.backend, "config"):
                    # Filter out non-serializable keys
                    backend_cfg = {k: v for k, v in agent.backend.config.items() if k not in ("mcp_servers", "_config_path")}
                    agent_cfg["backend"] = backend_cfg
                parent_configs.append(agent_cfg)

            # Get workspace path (use first agent's workspace or temp)
            parent_workspace = None
            for agent in self.agents.values():
                if hasattr(agent, "backend") and hasattr(
                    agent.backend,
                    "filesystem_manager",
                ):
                    if agent.backend.filesystem_manager and agent.backend.filesystem_manager.cwd:
                        parent_workspace = str(agent.backend.filesystem_manager.cwd)
                        break

            if not parent_workspace:
                import tempfile

                parent_workspace = tempfile.mkdtemp(prefix="massgen_persona_")
                logger.debug(
                    f"[Orchestrator] Using temp workspace for persona generation: {parent_workspace}",
                )

            # Get log directory
            log_directory = None
            try:
                log_dir = get_log_session_dir()
                if log_dir:
                    log_directory = str(log_dir)
            except Exception:
                pass

            # Generate personas via subagent
            personas = await generator.generate_personas_via_subagent(
                agent_ids=list(self.agents.keys()),
                task=self.current_task or "Complete the assigned task",
                existing_system_messages=existing_messages,
                parent_agent_configs=parent_configs,
                parent_workspace=parent_workspace,
                orchestrator_id=self.orchestrator_id,
                log_directory=log_directory,
            )

            # Store personas and original system messages for phase-based injection
            # We don't inject into agents here - we do it dynamically per execution
            # based on whether they've seen other answers (exploration vs convergence)
            self._generated_personas = personas
            self._original_system_messages = existing_messages
            self._personas_generated = True

            for agent_id, persona in personas.items():
                approach = persona.attributes.get(
                    "approach_summary",
                    persona.attributes.get("thinking_style", "unknown"),
                )
                logger.info(
                    f"[Orchestrator] Generated persona for {agent_id}: {approach}",
                )

            # Save personas to log file
            self._save_personas_to_log(personas)

            logger.info(
                f"[Orchestrator] Successfully generated and injected {len(personas)} personas",
            )

        except Exception as e:
            logger.error(f"[Orchestrator] Failed to generate personas: {e}")
            logger.warning("[Orchestrator] Continuing without persona generation")
            self._personas_generated = True  # Don't retry on failure

    def _get_persona_for_agent(
        self,
        agent_id: str,
        has_seen_answers: bool,
    ) -> Optional[str]:
        """Get the appropriate persona text for an agent based on phase.

        Args:
            agent_id: The agent ID
            has_seen_answers: True if agent has seen other agents' answers (convergence phase)

        Returns:
            The persona text to prepend, or None if no persona exists
        """
        if not self._generated_personas:
            return None

        persona = self._generated_personas.get(agent_id)
        if not persona:
            return None

        if has_seen_answers:
            # Convergence phase - use softened perspective
            return persona.get_softened_text()
        else:
            # Exploration phase - use strong perspective
            return persona.persona_text

    def get_generated_personas(self) -> Dict[str, Any]:
        """Get the generated personas for persistence across turns.

        Returns:
            Dictionary of agent_id -> GeneratedPersona
        """
        return self._generated_personas

    def _save_personas_to_log(self, personas: Dict[str, Any]) -> None:
        """
        Save generated personas to a YAML file in the log directory.

        Args:
            personas: Dictionary mapping agent_id to GeneratedPersona
        """
        try:
            import yaml

            from .logger_config import get_log_session_dir

            log_dir = get_log_session_dir()
            personas_file = log_dir / "generated_personas.yaml"

            # Convert personas to serializable dict
            personas_data = {}
            for agent_id, persona in personas.items():
                personas_data[agent_id] = {
                    "persona_text": persona.persona_text,
                    "attributes": persona.attributes,
                }

            with open(personas_file, "w") as f:
                yaml.dump(personas_data, f, default_flow_style=False, sort_keys=False)

            logger.info(f"[Orchestrator] Saved personas to {personas_file}")

        except Exception as e:
            logger.warning(f"[Orchestrator] Failed to save personas to log: {e}")

    @staticmethod
    def _get_chunk_type_value(chunk) -> str:
        """
        Extract chunk type as string, handling both legacy and typed chunks.

        Args:
            chunk: StreamChunk, TextStreamChunk, or MultimodalStreamChunk

        Returns:
            String representation of chunk type (e.g., "content", "tool_calls")
        """
        chunk_type = chunk.type

        if isinstance(chunk_type, ChunkType):
            return chunk_type.value

        return str(chunk_type)

    def _trace_tuple(
        self,
        text: str,
        *,
        kind: str = "agent_status",
        tool_call_id: str | None = None,
    ) -> tuple:
        """Map coordination/status text to a non-content type when strict tracing is enabled.

        Returns a 3-tuple (type, content, tool_call_id) to preserve tool tracking info.
        """
        if self.trace_classification == "strict":
            return (kind, text, tool_call_id)
        return ("content", text, tool_call_id)

    @staticmethod
    def _is_tool_related_content(content: str) -> bool:
        """
        Check if content is tool-related output that should be excluded from clean answer.

        Tool-related content includes:
        - Tool calls: ðŸ”§ tool_name(...)
        - Tool results: ðŸ”§ Tool âœ… Result: ... or ðŸ”§ Tool âŒ Error: ...
        - MCP status: ðŸ”§ MCP: ...
        - Backend status: Final Temp Working directory: ...

        Args:
            content: The content string to check

        Returns:
            True if content is tool-related and should be excluded from clean answer
        """
        if not content:
            return False

        # Tool calls and results from ClaudeCodeBackend
        if content.startswith("ðŸ”§ "):
            return True

        # Backend status messages
        if content.startswith("Final Temp Working directory:"):
            return True
        if content.startswith("Final Session ID:"):
            return True

        return False

    async def chat(
        self,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]] = None,
        reset_chat: bool = False,
        clear_history: bool = False,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        Main chat interface - handles user messages and coordinates sub-agents.

        Args:
            messages: List of conversation messages
            tools: Ignored by orchestrator (uses internal workflow tools)
            reset_chat: If True, reset conversation and start fresh
            clear_history: If True, clear history before processing

        Yields:
            StreamChunk: Streaming response chunks
        """
        # External (client-provided) tools: these are passed through to backends so models
        # can request them, but MassGen will NOT execute them (backends treat unknown tools
        # as provider_calls and emit StreamChunk(type="tool_calls")).
        self._external_tools = tools or []

        # Handle conversation management
        if clear_history:
            self.conversation_history.clear()
        if reset_chat:
            self.reset()

        # Process all messages to build conversation context
        conversation_context = self._build_conversation_context(messages)
        user_message = conversation_context.get("current_message")

        if not user_message:
            log_stream_chunk(
                "orchestrator",
                "error",
                "No user message found in conversation",
            )
            yield StreamChunk(
                type="error",
                error="No user message found in conversation",
            )
            return

        # Add user message to history
        self.add_to_history("user", user_message)

        # Determine what to do based on current state and conversation context
        if self.workflow_phase == "idle":
            # Emit preparation status
            yield StreamChunk(
                type="preparation_status",
                status="Preparing coordination...",
                detail="Setting up orchestrator",
            )

            # New task - start MassGen coordination with full context
            self.current_task = user_message

            # Prepare paraphrases if DSPy is enabled
            if self.dspy_paraphraser:
                yield StreamChunk(
                    type="preparation_status",
                    status="Generating prompt variants...",
                    detail="DSPy paraphrasing",
                )
            await self._prepare_paraphrases_for_agents(self.current_task)

            # Reinitialize session with user prompt now that we have it (MAS-199: includes log_path)
            log_dir = get_log_session_dir()
            log_path = str(log_dir) if log_dir else None
            self.coordination_tracker.initialize_session(
                list(self.agents.keys()),
                self.current_task,
                log_path=log_path,
            )
            self.workflow_phase = "coordinating"

            # Reset restart_pending flag at start of coordination (will be set again if restart needed)
            self.restart_pending = False

            # Clear context path write tracking at start of each turn
            self._clear_context_path_write_tracking()

            # Clear agent workspaces for new turn (if this is a multi-turn conversation with history)
            if conversation_context and conversation_context.get(
                "conversation_history",
            ):
                self._clear_agent_workspaces()

            # Check if planning mode is enabled in config
            planning_mode_config_exists = (
                self.config.coordination_config and self.config.coordination_config.enable_planning_mode if self.config and hasattr(self.config, "coordination_config") else False
            )

            if planning_mode_config_exists:
                yield StreamChunk(
                    type="preparation_status",
                    status="Analyzing task...",
                    detail="Checking for irreversible operations",
                )
                # Analyze question for irreversibility and set planning mode accordingly
                # This happens silently - users don't see this analysis
                analysis_result = await self._analyze_question_irreversibility(
                    user_message,
                    conversation_context,
                )
                has_irreversible = analysis_result["has_irreversible"]
                blocked_tools = analysis_result["blocked_tools"]

                # Set planning mode and blocked tools for all agents based on analysis
                for agent_id, agent in self.agents.items():
                    if hasattr(agent.backend, "set_planning_mode"):
                        agent.backend.set_planning_mode(has_irreversible)
                        if hasattr(agent.backend, "set_planning_mode_blocked_tools"):
                            agent.backend.set_planning_mode_blocked_tools(blocked_tools)
                        log_orchestrator_activity(
                            self.orchestrator_id,
                            f"Set planning mode for {agent_id}",
                            {
                                "planning_mode_enabled": has_irreversible,
                                "blocked_tools_count": len(blocked_tools),
                                "reason": "irreversibility analysis",
                            },
                        )

            # Starting actual coordination
            yield StreamChunk(
                type="preparation_status",
                status="Starting coordination...",
                detail=f"{len(self.agents)} agents ready",
            )

            async for chunk in self._coordinate_agents_with_timeout(
                conversation_context,
            ):
                yield chunk

        elif self.workflow_phase == "presenting":
            # Handle follow-up question with full conversation context
            async for chunk in self._handle_followup(
                user_message,
                conversation_context,
            ):
                yield chunk
        else:
            # Already coordinating - provide status update
            log_stream_chunk(
                "orchestrator",
                "content",
                "ðŸ”„ Coordinating agents, please wait...",
            )
            chunk_type = "coordination" if self.trace_classification == "strict" else "content"
            yield StreamChunk(
                type=chunk_type,
                content="ðŸ”„ Coordinating agents, please wait...",
            )
            # Note: In production, you might want to queue follow-up questions

    async def chat_simple(self, user_message: str) -> AsyncGenerator[StreamChunk, None]:
        """
        Backwards compatible simple chat interface.

        Args:
            user_message: Simple string message from user

        Yields:
            StreamChunk: Streaming response chunks
        """
        messages = [{"role": "user", "content": user_message}]
        async for chunk in self.chat(messages):
            yield chunk

    def _build_conversation_context(
        self,
        messages: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Build conversation context from message list."""
        conversation_history = []
        current_message = None

        # Process messages to extract conversation history and current message
        for message in messages:
            role = message.get("role")
            content = message.get("content", "")

            if role == "user":
                current_message = content
                # Add to history (excluding the current message)
                if len(conversation_history) > 0 or len(messages) > 1:
                    conversation_history.append(message.copy())
            elif role == "assistant":
                conversation_history.append(message.copy())
            elif role == "tool":
                # Preserve tool results for multi-turn tool calling.
                conversation_history.append(message.copy())
            elif role == "system":
                # System messages are typically not part of conversation history
                pass

        # Remove the last user message from history since that's the current message
        if conversation_history and conversation_history[-1].get("role") == "user":
            conversation_history.pop()

        return {
            "current_message": current_message,
            "conversation_history": conversation_history,
            "full_messages": messages,
        }

    async def _inject_shared_memory_context(
        self,
        messages: List[Dict[str, Any]],
        agent_id: str,
    ) -> List[Dict[str, Any]]:
        """
        Inject shared memory context into agent messages.

        This allows all agents to see shared memories including what other agents
        have stored in the shared memory.

        Args:
            messages: Original messages to send to agent
            agent_id: ID of the agent receiving the messages

        Returns:
            Messages with shared memory context injected
        """
        if not self.shared_conversation_memory and not self.shared_persistent_memory:
            # No shared memory configured, return original messages
            return messages

        memory_context_parts = []

        # Get conversation memory content
        if self.shared_conversation_memory:
            try:
                conv_messages = await self.shared_conversation_memory.get_messages()
                if conv_messages:
                    memory_context_parts.append("=== SHARED CONVERSATION MEMORY ===")
                    for msg in conv_messages[-10:]:  # Last 10 messages
                        role = msg.get("role", "unknown")
                        content = msg.get("content", "")
                        agent_source = msg.get("agent_id", "unknown")
                        memory_context_parts.append(
                            f"[{agent_source}] {role}: {content}",
                        )
            except Exception as e:
                logger.warning(f"Failed to retrieve shared conversation memory: {e}")

        # Get persistent memory content
        if self.shared_persistent_memory:
            try:
                # Extract user message for retrieval
                user_messages = [msg for msg in messages if msg.get("role") == "user"]
                if user_messages:
                    retrieved = await self.shared_persistent_memory.retrieve(
                        user_messages,
                    )
                    if retrieved:
                        memory_context_parts.append(
                            "\n=== SHARED PERSISTENT MEMORY ===",
                        )
                        memory_context_parts.append(retrieved)
            except NotImplementedError:
                # Memory backend doesn't support retrieve
                pass
            except Exception as e:
                logger.warning(f"Failed to retrieve shared persistent memory: {e}")

        # Inject memory context if we have any
        if memory_context_parts:
            memory_message = {
                "role": "system",
                "content": ("You have access to shared memory that all agents can see and contribute to.\n" + "\n".join(memory_context_parts)),
            }

            # Insert after existing system messages but before user messages
            system_count = sum(1 for msg in messages if msg.get("role") == "system")
            modified_messages = messages.copy()
            modified_messages.insert(system_count, memory_message)
            return modified_messages

        return messages

    def _merge_agent_memories_to_winner(self, winning_agent_id: str) -> None:
        """
        Merge memory directories from all agents into the winning agent's workspace.

        This ensures memories created by any agent during coordination are preserved
        in the final snapshot, regardless of which agent won.

        Args:
            winning_agent_id: ID of the agent selected as final presenter
        """
        if not hasattr(self.config, "coordination_config") or not hasattr(
            self.config.coordination_config,
            "enable_memory_filesystem_mode",
        ):
            return

        if not self.config.coordination_config.enable_memory_filesystem_mode:
            logger.debug(
                "[Orchestrator] Memory filesystem mode not enabled, skipping memory merge",
            )
            return

        winning_agent = self.agents.get(winning_agent_id)
        if not winning_agent or not hasattr(winning_agent, "backend") or not winning_agent.backend.filesystem_manager:
            logger.warning(
                f"[Orchestrator] Cannot merge memories - winning agent {winning_agent_id} has no filesystem manager",
            )
            return

        winner_memory_base = Path(winning_agent.backend.filesystem_manager.cwd) / "memory"
        winner_memory_base.mkdir(parents=True, exist_ok=True)

        logger.info(
            f"[Orchestrator] Merging memories from all agents into {winning_agent_id}'s workspace",
        )

        merged_count = 0
        for agent_id, agent in self.agents.items():
            if agent_id == winning_agent_id:
                continue  # Skip winner's own memories

            if not hasattr(agent, "backend") or not agent.backend.filesystem_manager:
                continue

            agent_memory_base = Path(agent.backend.filesystem_manager.cwd) / "memory"
            if not agent_memory_base.exists():
                continue

            # Merge short_term and long_term directories
            for tier in ["short_term", "long_term"]:
                source_tier_dir = agent_memory_base / tier
                if not source_tier_dir.exists():
                    continue

                dest_tier_dir = winner_memory_base / tier
                dest_tier_dir.mkdir(parents=True, exist_ok=True)

                # Copy all .md files from this agent's tier
                for memory_file in source_tier_dir.glob("*.md"):
                    dest_file = dest_tier_dir / memory_file.name

                    # If file already exists in winner's workspace, append with agent attribution
                    if dest_file.exists():
                        try:
                            existing_content = dest_file.read_text()
                            new_content = memory_file.read_text()
                            combined = f"{existing_content}\n\n---\n\n# From Agent {agent_id}\n\n{new_content}"
                            dest_file.write_text(combined)
                            logger.info(
                                f"[Orchestrator] Merged {memory_file.name} from {agent_id} (appended)",
                            )
                            merged_count += 1
                        except Exception as e:
                            logger.warning(
                                f"[Orchestrator] Failed to merge {memory_file.name} from {agent_id}: {e}",
                            )
                    else:
                        # File doesn't exist in winner's workspace, copy it
                        try:
                            import shutil

                            shutil.copy2(memory_file, dest_file)
                            logger.info(
                                f"[Orchestrator] Copied {memory_file.name} from {agent_id}",
                            )
                            merged_count += 1
                        except Exception as e:
                            logger.warning(
                                f"[Orchestrator] Failed to copy {memory_file.name} from {agent_id}: {e}",
                            )

        logger.info(
            f"[Orchestrator] Memory merge complete: {merged_count} files merged from other agents into {winning_agent_id}'s workspace",
        )

    async def _record_to_shared_memory(
        self,
        agent_id: str,
        content: str,
        role: str = "assistant",
    ) -> None:
        """
        Record agent's contribution to shared memory.

        Args:
            agent_id: ID of the agent contributing
            content: Content to record
            role: Role of the message (default: "assistant")
        """
        message = {
            "role": role,
            "content": content,
            "agent_id": agent_id,
            "timestamp": time.time(),
        }

        # Add to conversation memory
        if self.shared_conversation_memory:
            try:
                await self.shared_conversation_memory.add(message)
            except Exception as e:
                logger.warning(f"Failed to add to shared conversation memory: {e}")

        # Record to persistent memory
        if self.shared_persistent_memory:
            try:
                await self.shared_persistent_memory.record([message])
            except NotImplementedError:
                # Memory backend doesn't support record
                pass
            except Exception as e:
                logger.warning(f"Failed to record to shared persistent memory: {e}")

    def save_coordination_logs(self):
        """Public method to save coordination logs after final presentation is complete."""
        logger.info("[Orchestrator] save_coordination_logs called")
        # End the coordination session
        self.coordination_tracker._end_session()

        # Save coordination logs using the coordination tracker
        log_session_dir = get_log_session_dir()
        if log_session_dir:
            logger.info(f"[Orchestrator] Saving to {log_session_dir}")
            self.coordination_tracker.save_coordination_logs(log_session_dir)
            # Also save final status.json with complete token/cost data
            self.coordination_tracker.save_status_file(
                log_session_dir,
                orchestrator=self,
            )
            # Save detailed metrics files
            self.save_metrics(log_session_dir)

    def save_metrics(self, log_dir: Path):
        """Save detailed metrics files for analysis.

        Outputs:
            - metrics_events.json: Detailed event log of all tool executions and round completions
            - metrics_summary.json: Aggregated summary with per-agent and global statistics
        """
        try:
            log_dir = Path(log_dir)

            # Collect all tool metrics and round history from agents
            all_tool_events = []
            all_round_events = []
            agent_metrics = {}

            for agent_id, agent in self.agents.items():
                if hasattr(agent, "backend") and agent.backend:
                    backend = agent.backend

                    # Collect detailed tool execution events
                    if hasattr(backend, "get_tool_metrics"):
                        tool_events = backend.get_tool_metrics()
                        all_tool_events.extend(tool_events)

                    # Collect round token history
                    if hasattr(backend, "get_round_token_history"):
                        round_history = backend.get_round_token_history()
                        all_round_events.extend(round_history)

                    # Collect per-agent summaries
                    agent_metrics[agent_id] = {
                        "tool_metrics": backend.get_tool_metrics_summary() if hasattr(backend, "get_tool_metrics_summary") else None,
                        "round_history": backend.get_round_token_history() if hasattr(backend, "get_round_token_history") else None,
                        "token_usage": {
                            "input_tokens": backend.token_usage.input_tokens if backend.token_usage else 0,
                            "output_tokens": backend.token_usage.output_tokens if backend.token_usage else 0,
                            "reasoning_tokens": backend.token_usage.reasoning_tokens if backend.token_usage else 0,
                            "cached_input_tokens": backend.token_usage.cached_input_tokens if backend.token_usage else 0,
                            "estimated_cost": round(
                                backend.token_usage.estimated_cost,
                                6,
                            )
                            if backend.token_usage
                            else 0,
                        }
                        if hasattr(backend, "token_usage")
                        else None,
                    }

            # Save detailed events log
            events_file = log_dir / "metrics_events.json"
            events_data = {
                "meta": {
                    "generated_at": time.time(),
                    "session_id": log_dir.name,
                    "question": self.current_task,
                },
                "tool_executions": all_tool_events,
                "round_completions": all_round_events,
            }
            with open(events_file, "w", encoding="utf-8") as f:
                json.dump(events_data, f, indent=2, default=str)

            # Build aggregated summary
            # Aggregate tool stats
            tools_summary = {
                "total_calls": 0,
                "total_failures": 0,
                "total_execution_time_ms": 0.0,
                "by_tool": {},
            }
            for event in all_tool_events:
                tools_summary["total_calls"] += 1
                if not event.get("success", True):
                    tools_summary["total_failures"] += 1
                tools_summary["total_execution_time_ms"] += event.get(
                    "execution_time_ms",
                    0,
                )

                tool_name = event.get("tool_name", "unknown")
                if tool_name not in tools_summary["by_tool"]:
                    tools_summary["by_tool"][tool_name] = {
                        "call_count": 0,
                        "success_count": 0,
                        "failure_count": 0,
                        "total_execution_time_ms": 0.0,
                        "total_input_chars": 0,
                        "total_output_chars": 0,
                        "tool_type": event.get("tool_type", "unknown"),
                    }
                tools_summary["by_tool"][tool_name]["call_count"] += 1
                if event.get("success", True):
                    tools_summary["by_tool"][tool_name]["success_count"] += 1
                else:
                    tools_summary["by_tool"][tool_name]["failure_count"] += 1
                tools_summary["by_tool"][tool_name]["total_execution_time_ms"] += event.get("execution_time_ms", 0)
                tools_summary["by_tool"][tool_name]["total_input_chars"] += event.get(
                    "input_chars",
                    0,
                )
                tools_summary["by_tool"][tool_name]["total_output_chars"] += event.get(
                    "output_chars",
                    0,
                )

            # Calculate tool averages and token estimates
            for tool_stats in tools_summary["by_tool"].values():
                count = tool_stats["call_count"]
                if count > 0:
                    tool_stats["avg_execution_time_ms"] = round(
                        tool_stats["total_execution_time_ms"] / count,
                        2,
                    )
                    tool_stats["input_tokens_est"] = tool_stats["total_input_chars"] // 4
                    tool_stats["output_tokens_est"] = tool_stats["total_output_chars"] // 4

            # Aggregate round stats
            rounds_summary = {
                "total_rounds": len(all_round_events),
                "by_outcome": {
                    "answer": 0,
                    "vote": 0,
                    "presentation": 0,
                    "post_evaluation": 0,
                    "restarted": 0,
                    "error": 0,
                    "timeout": 0,
                },
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_reasoning_tokens": 0,
                "total_estimated_cost": 0.0,
                "avg_context_usage_pct": 0.0,
            }
            total_context_pct = 0.0
            for r in all_round_events:
                outcome = r.get("outcome", "unknown")
                if outcome in rounds_summary["by_outcome"]:
                    rounds_summary["by_outcome"][outcome] += 1
                rounds_summary["total_input_tokens"] += r.get("input_tokens", 0)
                rounds_summary["total_output_tokens"] += r.get("output_tokens", 0)
                rounds_summary["total_reasoning_tokens"] += r.get("reasoning_tokens", 0)
                rounds_summary["total_estimated_cost"] += r.get("estimated_cost", 0.0)
                total_context_pct += r.get("context_usage_pct", 0.0)

            rounds_summary["total_estimated_cost"] = round(
                rounds_summary["total_estimated_cost"],
                6,
            )
            if len(all_round_events) > 0:
                rounds_summary["avg_context_usage_pct"] = round(
                    total_context_pct / len(all_round_events),
                    2,
                )

            # Calculate total costs across all agents
            total_cost = 0.0
            total_input_tokens = 0
            total_output_tokens = 0
            total_reasoning_tokens = 0
            for am in agent_metrics.values():
                tu = am.get("token_usage")
                if tu:
                    total_cost += tu.get("estimated_cost", 0)
                    total_input_tokens += tu.get("input_tokens", 0)
                    total_output_tokens += tu.get("output_tokens", 0)
                    total_reasoning_tokens += tu.get("reasoning_tokens", 0)

            # Collect subagent costs from status files
            subagents_summary = self._collect_subagent_costs(log_dir)
            subagent_total_cost = subagents_summary.get("total_estimated_cost", 0.0)

            # Aggregate API call timing metrics
            api_timing = {
                "total_calls": 0,
                "total_time_ms": 0.0,
                "avg_time_ms": 0.0,
                "avg_ttft_ms": 0.0,
                "by_round": {},
                "by_backend": {},
            }
            total_ttft_ms = 0.0

            for agent_id, agent in self.agents.items():
                if hasattr(agent, "backend") and agent.backend:
                    backend = agent.backend
                    if hasattr(backend, "get_api_call_history"):
                        for metric in backend.get_api_call_history():
                            api_timing["total_calls"] += 1
                            api_timing["total_time_ms"] += metric.duration_ms
                            total_ttft_ms += metric.time_to_first_token_ms

                            # By round
                            round_key = f"round_{metric.round_number}"
                            if round_key not in api_timing["by_round"]:
                                api_timing["by_round"][round_key] = {
                                    "calls": 0,
                                    "time_ms": 0.0,
                                    "ttft_ms": 0.0,
                                }
                            api_timing["by_round"][round_key]["calls"] += 1
                            api_timing["by_round"][round_key]["time_ms"] += metric.duration_ms
                            api_timing["by_round"][round_key]["ttft_ms"] += metric.time_to_first_token_ms

                            # By backend
                            if metric.backend_name not in api_timing["by_backend"]:
                                api_timing["by_backend"][metric.backend_name] = {
                                    "calls": 0,
                                    "time_ms": 0.0,
                                    "ttft_ms": 0.0,
                                }
                            api_timing["by_backend"][metric.backend_name]["calls"] += 1
                            api_timing["by_backend"][metric.backend_name]["time_ms"] += metric.duration_ms
                            api_timing["by_backend"][metric.backend_name]["ttft_ms"] += metric.time_to_first_token_ms

            # Calculate averages
            if api_timing["total_calls"] > 0:
                api_timing["avg_time_ms"] = round(
                    api_timing["total_time_ms"] / api_timing["total_calls"],
                    2,
                )
                api_timing["avg_ttft_ms"] = round(
                    total_ttft_ms / api_timing["total_calls"],
                    2,
                )

            # Round timing values
            api_timing["total_time_ms"] = round(api_timing["total_time_ms"], 2)
            for round_data in api_timing["by_round"].values():
                round_data["time_ms"] = round(round_data["time_ms"], 2)
                round_data["ttft_ms"] = round(round_data["ttft_ms"], 2)
            for backend_data in api_timing["by_backend"].values():
                backend_data["time_ms"] = round(backend_data["time_ms"], 2)
                backend_data["ttft_ms"] = round(backend_data["ttft_ms"], 2)

            # Save summary file
            summary_file = log_dir / "metrics_summary.json"
            summary_data = {
                "meta": {
                    "generated_at": time.time(),
                    "session_id": log_dir.name,
                    "question": self.current_task,
                    "num_agents": len(self.agents),
                    "winner": self.coordination_tracker.final_winner,
                },
                "totals": {
                    "estimated_cost": round(total_cost + subagent_total_cost, 6),
                    "agent_cost": round(total_cost, 6),
                    "subagent_cost": round(subagent_total_cost, 6),
                    "input_tokens": total_input_tokens,
                    "output_tokens": total_output_tokens,
                    "reasoning_tokens": total_reasoning_tokens,
                },
                "tools": tools_summary,
                "rounds": rounds_summary,
                "api_timing": api_timing,
                "agents": agent_metrics,
                "subagents": subagents_summary,
            }
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(summary_data, f, indent=2, default=str)

            logger.info(f"[Orchestrator] Saved metrics files to {log_dir}")

        except Exception as e:
            logger.warning(f"Failed to save metrics files: {e}", exc_info=True)

    def _collect_subagent_costs(self, log_dir: Path) -> Dict[str, Any]:
        """
        Collect subagent costs and metrics from status.json and subprocess metrics.

        Args:
            log_dir: Path to the log directory (e.g., turn_1/attempt_1)

        Returns:
            Dictionary with total costs, timing data, and per-subagent breakdown
        """
        subagents_dir = log_dir / "subagents"
        if not subagents_dir.exists():
            return {
                "total_subagents": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_estimated_cost": 0.0,
                "total_api_time_ms": 0.0,
                "total_api_calls": 0,
                "subagents": [],
            }

        total_input_tokens = 0
        total_output_tokens = 0
        total_estimated_cost = 0.0
        total_api_time_ms = 0.0
        total_api_calls = 0
        subagent_details = []

        # Find all status.json files in subagent directories
        # Status file is at full_logs/status.json (written by subagent's Orchestrator)
        for subagent_path in subagents_dir.iterdir():
            if not subagent_path.is_dir():
                continue

            # Read from full_logs/status.json (the single source of truth)
            status_file = subagent_path / "full_logs" / "status.json"
            if not status_file.exists():
                continue

            try:
                # Read status.json for basic info
                with open(status_file, "r", encoding="utf-8") as f:
                    status_data = json.load(f)

                # Extract costs from the new structure
                costs = status_data.get("costs", {})
                input_tokens = costs.get("total_input_tokens", 0)
                output_tokens = costs.get("total_output_tokens", 0)
                cost = costs.get("total_estimated_cost", 0.0)

                # Extract timing from meta
                meta = status_data.get("meta", {})
                elapsed_seconds = meta.get("elapsed_seconds", 0.0)

                # Extract coordination info
                coordination = status_data.get("coordination", {})
                phase = coordination.get("phase", "unknown")

                total_input_tokens += input_tokens
                total_output_tokens += output_tokens
                total_estimated_cost += cost

                # Initialize subagent detail entry
                subagent_detail = {
                    "subagent_id": subagent_path.name,
                    "status": phase,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "estimated_cost": round(cost, 6),
                    "elapsed_seconds": elapsed_seconds,
                    "task": meta.get("question", "")[:100],
                }

                # Try to read subprocess metrics for API timing data
                subprocess_logs_file = subagent_path / "subprocess_logs.json"
                if subprocess_logs_file.exists():
                    try:
                        with open(subprocess_logs_file, "r", encoding="utf-8") as f:
                            subprocess_logs = json.load(f)

                        subprocess_log_dir = subprocess_logs.get("subprocess_log_dir")
                        if subprocess_log_dir:
                            # Read the subprocess's metrics_summary.json
                            metrics_file = Path(subprocess_log_dir) / "metrics_summary.json"
                            if metrics_file.exists():
                                with open(metrics_file, "r", encoding="utf-8") as f:
                                    metrics_data = json.load(f)

                                # Extract API timing data
                                api_timing = metrics_data.get("api_timing", {})
                                if api_timing:
                                    subagent_api_time = api_timing.get(
                                        "total_time_ms",
                                        0.0,
                                    )
                                    subagent_api_calls = api_timing.get(
                                        "total_calls",
                                        0,
                                    )

                                    total_api_time_ms += subagent_api_time
                                    total_api_calls += subagent_api_calls

                                    subagent_detail["api_timing"] = {
                                        "total_time_ms": round(subagent_api_time, 2),
                                        "total_calls": subagent_api_calls,
                                        "avg_time_ms": api_timing.get(
                                            "avg_time_ms",
                                            0.0,
                                        ),
                                        "avg_ttft_ms": api_timing.get(
                                            "avg_ttft_ms",
                                            0.0,
                                        ),
                                    }
                    except Exception as e:
                        logger.debug(
                            f"Failed to read subprocess metrics for {subagent_path.name}: {e}",
                        )

                subagent_details.append(subagent_detail)

            except Exception as e:
                logger.debug(f"Failed to read subagent status from {status_file}: {e}")

        return {
            "total_subagents": len(subagent_details),
            "total_input_tokens": total_input_tokens,
            "total_output_tokens": total_output_tokens,
            "total_estimated_cost": round(total_estimated_cost, 6),
            "total_api_time_ms": round(total_api_time_ms, 2),
            "total_api_calls": total_api_calls,
            "subagents": subagent_details,
        }

    def _format_planning_mode_ui(
        self,
        has_irreversible: bool,
        blocked_tools: set,
        has_isolated_workspaces: bool,
        user_question: str,
    ) -> str:
        """
        Format a nice UI box for planning mode status.

        Args:
            has_irreversible: Whether irreversible operations were detected
            blocked_tools: Set of specific blocked tool names
            has_isolated_workspaces: Whether agents have isolated workspaces
            user_question: The user's question for context

        Returns:
            Formatted string with nice box UI
        """
        if not has_irreversible:
            # Planning mode disabled - brief message
            box = "\nâ•­â”€ Coordination Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
            box += "â”‚ âœ… Planning Mode: DISABLED                                â”‚\n"
            box += "â”‚                                                            â”‚\n"
            box += "â”‚ All tools available during coordination.                  â”‚\n"
            box += "â”‚ No irreversible operations detected.                      â”‚\n"
            box += "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            return box

        # Planning mode enabled
        box = "\nâ•­â”€ Coordination Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n"
        box += "â”‚ ðŸ§  Planning Mode: ENABLED                                  â”‚\n"
        box += "â”‚                                                            â”‚\n"

        if has_isolated_workspaces:
            box += "â”‚ ðŸ”’ Workspace: Isolated (filesystem ops allowed)           â”‚\n"
            box += "â”‚                                                            â”‚\n"

        # Description
        box += "â”‚ Agents will plan and coordinate without executing         â”‚\n"
        box += "â”‚ irreversible actions. The winning agent will implement    â”‚\n"
        box += "â”‚ the plan during final presentation.                       â”‚\n"
        box += "â”‚                                                            â”‚\n"

        # Blocked tools section
        if blocked_tools:
            box += "â”‚ ðŸš« Blocked Tools:                                          â”‚\n"
            # Format tools into nice columns
            sorted_tools = sorted(blocked_tools)
            for i, tool in enumerate(sorted_tools[:5], 1):  # Show max 5 tools
                # Shorten tool name if too long
                display_tool = tool if len(tool) <= 50 else tool[:47] + "..."
                box += f"â”‚   {i}. {display_tool:<54} â”‚\n"

            if len(sorted_tools) > 5:
                remaining = len(sorted_tools) - 5
                box += f"â”‚   ... and {remaining} more tool(s)                              â”‚\n"
            box += "â”‚                                                            â”‚\n"
        else:
            box += "â”‚ ðŸš« Blocking: ALL MCP tools                                 â”‚\n"
            box += "â”‚                                                            â”‚\n"

        # Add brief analysis summary
        box += "â”‚ ðŸ“Š Analysis:                                               â”‚\n"
        # Create a brief summary from the question
        summary = user_question[:50] + "..." if len(user_question) > 50 else user_question
        # Wrap text to fit in box
        words = summary.split()
        line = "â”‚   "
        for word in words:
            if len(line) + len(word) + 1 > 60:
                box += line.ljust(61) + "â”‚\n"
                line = "â”‚   " + word + " "
            else:
                line += word + " "
        if len(line) > 4:  # If there's content
            box += line.ljust(61) + "â”‚\n"

        box += "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
        return box

    async def _analyze_question_irreversibility(
        self,
        user_question: str,
        conversation_context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Analyze if the user's question involves MCP tools with irreversible outcomes.

        This method randomly selects an available agent to analyze whether executing
        the user's question would involve MCP tool operations with irreversible outcomes
        (e.g., sending Discord messages, posting tweets, deleting files) vs reversible
        read operations (e.g., reading Discord messages, searching tweets, listing files).

        Args:
            user_question: The user's question/request
            conversation_context: Full conversation context including history

        Returns:
            Dict with:
                - has_irreversible (bool): True if irreversible operations detected
                - blocked_tools (set): Set of MCP tool names to block (e.g., {'mcp__discord__discord_send'})
                                      Empty set means block ALL MCP tools
        """
        import random

        print("=" * 80, flush=True)
        print(
            "ðŸ” [INTELLIGENT PLANNING MODE] Analyzing question for irreversibility...",
            flush=True,
        )
        print(
            f"ðŸ“ Question: {user_question[:100]}{'...' if len(user_question) > 100 else ''}",
            flush=True,
        )
        print("=" * 80, flush=True)

        # Select a random agent for analysis
        available_agents = [aid for aid, agent in self.agents.items() if agent.backend is not None]
        if not available_agents:
            # No agents available, default to safe mode (planning enabled, block ALL)
            log_orchestrator_activity(
                self.orchestrator_id,
                "No agents available for irreversibility analysis, defaulting to planning mode",
                {},
            )
            return {"has_irreversible": True, "blocked_tools": set()}

        analyzer_agent_id = random.choice(available_agents)
        analyzer_agent = self.agents[analyzer_agent_id]

        print(f"ðŸ¤– Selected analyzer agent: {analyzer_agent_id}", flush=True)

        # Check if agents have isolated workspaces
        has_isolated_workspaces = False
        workspace_info = []
        for agent_id, agent in self.agents.items():
            if agent.backend and agent.backend.filesystem_manager:
                cwd = agent.backend.filesystem_manager.cwd
                if cwd and "workspace" in os.path.basename(cwd).lower():
                    has_isolated_workspaces = True
                    workspace_info.append(f"{agent_id}: {cwd}")

        if has_isolated_workspaces:
            print(
                "ðŸ”’ Detected isolated agent workspaces - filesystem ops will be allowed",
                flush=True,
            )

        log_orchestrator_activity(
            self.orchestrator_id,
            "Analyzing question irreversibility",
            {
                "analyzer_agent": analyzer_agent_id,
                "question_preview": user_question[:100] + "..." if len(user_question) > 100 else user_question,
                "has_isolated_workspaces": has_isolated_workspaces,
            },
        )

        # Build analysis prompt - now asking for specific tool names
        workspace_context = ""
        if has_isolated_workspaces:
            workspace_context = """
IMPORTANT - ISOLATED WORKSPACES:
The agents are working in isolated temporary workspaces (directories containing "workspace" in their name).
Filesystem operations (read_file, write_file, delete_file, list_files, etc.) within these isolated workspaces are SAFE and REVERSIBLE.
They should NOT be blocked because:
- These are temporary directories specific to this coordination session
- Files created/modified are isolated from external systems
- Changes are contained within the agent's sandbox
- The workspace can be cleared after coordination

Only block filesystem operations if they explicitly target paths OUTSIDE the isolated workspace.
"""

        analysis_prompt = f"""You are analyzing whether a user's request involves operations with irreversible outcomes.

USER REQUEST:
{user_question}
{workspace_context}
CONTEXT:
Your task is to determine if executing this request would involve MCP (Model Context Protocol) tools that have irreversible outcomes, and if so, identify which specific tools should be blocked.

MCP tools follow the naming convention: mcp__<server>__<tool_name>
Examples:
- mcp__discord__discord_send (irreversible - sends messages)
- mcp__discord__discord_read_channel (reversible - reads messages)
- mcp__twitter__post_tweet (irreversible - posts publicly)
- mcp__twitter__search_tweets (reversible - searches)
- mcp__filesystem__write_file (SAFE in isolated workspace - writes to temporary files)
- mcp__filesystem__read_file (reversible - reads files)

IRREVERSIBLE OPERATIONS:
- Sending messages (discord_send, slack_send, etc.)
- Posting content publicly (post_tweet, create_post, etc.)
- Deleting files or data OUTSIDE isolated workspace (delete_file on external paths, remove_data, etc.)
- Modifying external systems (write_file to external paths, update_record, etc.)
- Creating permanent records (create_issue, add_comment, etc.)
- Executing commands that change state (run_command, execute_script, etc.)

REVERSIBLE OPERATIONS (DO NOT BLOCK):
- Reading messages or data (read_channel, get_messages, etc.)
- Searching or querying information (search_tweets, query_data, etc.)
- Listing files or resources (list_files, list_channels, etc.)
- Fetching data from APIs (get_user, fetch_data, etc.)
- Viewing information (view_channel, get_info, etc.)
- Filesystem operations IN ISOLATED WORKSPACE (write_file, read_file, delete_file, list_files when in workspace*)

Respond in this EXACT format:
IRREVERSIBLE: YES/NO
BLOCKED_TOOLS: tool1, tool2, tool3

If IRREVERSIBLE is NO, leave BLOCKED_TOOLS empty.
If IRREVERSIBLE is YES, list the specific MCP tool names that should be blocked (e.g., mcp__discord__discord_send).

Your answer:"""

        # Create messages for the analyzer
        analysis_messages = [
            {"role": "user", "content": analysis_prompt},
        ]

        try:
            # Stream response from analyzer agent (but don't show to user)
            response_text = ""
            async for chunk in analyzer_agent.backend.stream_with_tools(
                messages=analysis_messages,
                tools=[],  # No tools needed for simple analysis
                agent_id=analyzer_agent_id,
            ):
                if chunk.type == "content" and chunk.content:
                    response_text += chunk.content

            # Parse response
            response_clean = response_text.strip()
            has_irreversible = False
            blocked_tools = set()

            # Parse IRREVERSIBLE line
            found_irreversible_line = False
            for line in response_clean.split("\n"):
                line = line.strip()
                if line.startswith("IRREVERSIBLE:"):
                    found_irreversible_line = True
                    # Extract the value after the colon
                    value = line.split(":", 1)[1].strip().upper()
                    # Check if the first word is YES
                    has_irreversible = value.startswith("YES")
                elif line.startswith("BLOCKED_TOOLS:"):
                    # Extract tool names after the colon
                    tools_part = line.split(":", 1)[1].strip()
                    if tools_part:
                        # Split by comma and clean up whitespace
                        blocked_tools = {tool.strip() for tool in tools_part.split(",") if tool.strip()}

            # Fallback: If no structured format found, look for YES/NO in the response
            if not found_irreversible_line:
                print(
                    "âš ï¸  [WARNING] No 'IRREVERSIBLE:' line found, using fallback parsing",
                    flush=True,
                )
                response_upper = response_clean.upper()
                # Look for clear YES/NO indicators
                if "YES" in response_upper and "NO" not in response_upper:
                    has_irreversible = True
                elif "NO" in response_upper:
                    has_irreversible = False
                else:
                    # Default to safe mode if unclear
                    has_irreversible = True

            log_orchestrator_activity(
                self.orchestrator_id,
                "Irreversibility analysis complete",
                {
                    "analyzer_agent": analyzer_agent_id,
                    "response": response_clean[:100],
                    "has_irreversible": has_irreversible,
                    "blocked_tools_count": len(blocked_tools),
                },
            )

            # Display nice UI box for planning mode status
            ui_box = self._format_planning_mode_ui(
                has_irreversible=has_irreversible,
                blocked_tools=blocked_tools,
                has_isolated_workspaces=has_isolated_workspaces,
                user_question=user_question,
            )
            print(ui_box, flush=True)

            return {
                "has_irreversible": has_irreversible,
                "blocked_tools": blocked_tools,
            }

        except Exception as e:
            # On error, default to safe mode (planning enabled, block ALL)
            log_orchestrator_activity(
                self.orchestrator_id,
                "Irreversibility analysis failed, defaulting to planning mode",
                {"error": str(e)},
            )
            return {"has_irreversible": True, "blocked_tools": set()}

    async def _continuous_status_updates(self):
        """Background task to continuously update status.json during coordination.

        This task runs every 2 seconds to provide real-time status monitoring
        for automation tools and LLM agents.
        """
        try:
            while True:
                # Check for cancellation before sleeping
                if hasattr(self, "cancellation_manager") and self.cancellation_manager and self.cancellation_manager.is_cancelled:
                    logger.info(
                        "Cancellation detected in status update task - stopping",
                    )
                    break

                await asyncio.sleep(2)  # Update every 2 seconds

                # Check for cancellation after sleeping
                if hasattr(self, "cancellation_manager") and self.cancellation_manager and self.cancellation_manager.is_cancelled:
                    logger.info(
                        "Cancellation detected in status update task - stopping",
                    )
                    break

                log_session_dir = get_log_session_dir()
                if log_session_dir:
                    try:
                        # Run synchronous save_status_file in thread pool to avoid blocking event loop
                        # This prevents delays in WebSocket broadcasts and other async operations
                        loop = asyncio.get_running_loop()
                        await loop.run_in_executor(
                            None,  # Use default thread pool executor
                            self.coordination_tracker.save_status_file,
                            log_session_dir,
                            self,
                        )
                    except Exception as e:
                        logger.debug(f"Failed to update status file in background: {e}")

                # Update timeout status for each agent in the display
                try:
                    display = None
                    if hasattr(self, "coordination_ui") and self.coordination_ui:
                        display = getattr(self.coordination_ui, "display", None)

                    if display and hasattr(display, "update_timeout_status"):
                        for agent_id in self.agents.keys():
                            timeout_state = self.get_agent_timeout_state(agent_id)
                            if timeout_state and timeout_state.get("active_timeout"):
                                display.update_timeout_status(agent_id, timeout_state)
                except Exception as e:
                    logger.warning(f"Failed to update timeout status in display: {e}")
        except asyncio.CancelledError:
            # Task was cancelled, this is expected behavior
            pass
        except Exception as e:
            logger.warning(f"Background status update task encountered error: {e}")

    async def _coordinate_agents_with_timeout(
        self,
        conversation_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Execute coordination with orchestrator-level timeout protection.

        When restart is needed, this method completes and returns control to CLI,
        which will call coordinate() again (similar to multiturn pattern).
        """
        # Reset timing and state for this attempt
        self.coordination_start_time = time.time()
        self.total_tokens = 0
        self.is_orchestrator_timeout = False
        self.timeout_reason = None
        self._presentation_started = False  # Reset presentation guard for new attempt

        log_orchestrator_activity(
            self.orchestrator_id,
            f"Starting coordination attempt {self.current_attempt + 1}/{self.max_attempts}",
            {
                "timeout_seconds": self.config.timeout_config.orchestrator_timeout_seconds,
                "agents": list(self.agents.keys()),
                "has_restart_context": bool(self.restart_reason),
            },
        )

        # Set log attempt for directory organization (only if restart feature is enabled)
        # For restarts (attempt 2+), CLI sets this before creating the UI
        # For first attempt, we still need to set it here
        if self.config.coordination_config.max_orchestration_restarts > 0:
            from massgen.logger_config import _CURRENT_ATTEMPT

            expected_attempt = self.current_attempt + 1
            # Only set if not already set to the expected value (CLI may have set it for restarts)
            if _CURRENT_ATTEMPT != expected_attempt:
                set_log_attempt(expected_attempt)

        # Track active coordination state for cleanup
        self._active_streams = {}
        self._active_tasks = {}

        timeout_seconds = self.config.timeout_config.orchestrator_timeout_seconds

        try:
            # Use asyncio.timeout for timeout protection
            async with asyncio.timeout(timeout_seconds):
                async for chunk in self._coordinate_agents(conversation_context):
                    # Track tokens if this is a content chunk (only for string content)
                    if hasattr(chunk, "content") and chunk.content and isinstance(chunk.content, str):
                        self.total_tokens += len(
                            chunk.content.split(),
                        )  # Rough token estimation

                    yield chunk

        except asyncio.TimeoutError:
            self.is_orchestrator_timeout = True
            elapsed = time.time() - self.coordination_start_time
            self.timeout_reason = f"Time limit exceeded ({elapsed:.1f}s/{timeout_seconds}s)"
            # Track timeout for all agents that were still working
            for agent_id in self.agent_states.keys():
                if not self.agent_states[agent_id].has_voted:
                    self.coordination_tracker.track_agent_action(
                        agent_id,
                        ActionType.TIMEOUT,
                        self.timeout_reason,
                    )

            # Force cleanup of any active agent streams and tasks
            await self._cleanup_active_coordination()

        # Handle timeout by jumping to final presentation
        if self.is_orchestrator_timeout:
            async for chunk in self._handle_orchestrator_timeout():
                yield chunk

        # Exit here - if restart is needed, CLI will call coordinate() again

    async def _coordinate_agents(
        self,
        conversation_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Execute unified MassGen coordination workflow with real-time streaming."""
        # Log structured coordination event for observability
        log_coordination_event(
            "coordination_started",
            details={
                "num_agents": len(self.agents),
                "agent_ids": list(self.agents.keys()),
                "task": self.current_task[:200] if self.current_task else None,
            },
        )

        log_coordination_step(
            "Starting multi-agent coordination",
            {
                "agents": list(self.agents.keys()),
                "has_context": conversation_context is not None,
            },
        )

        # Generate and inject personas if enabled (happens once per session)
        if (
            hasattr(self.config, "coordination_config")
            and hasattr(self.config.coordination_config, "persona_generator")
            and self.config.coordination_config.persona_generator.enabled
            and not self._personas_generated
        ):
            yield StreamChunk(
                type="preparation_status",
                status="Generating personas...",
                detail="Creating unique agent identities",
            )
        await self._generate_and_inject_personas()

        # Check if we should skip coordination rounds (debug/test mode)
        if self.config.skip_coordination_rounds:
            log_stream_chunk(
                "orchestrator",
                "content",
                "âš¡ [DEBUG MODE] Skipping coordination rounds, going straight to final presentation...\n\n",
                self.orchestrator_id,
            )
            yield StreamChunk(
                type="content",
                content="âš¡ [DEBUG MODE] Skipping coordination rounds, going straight to final presentation...\n\n",
                source=self.orchestrator_id,
            )

            # Select first agent as winner (or random if needed)
            self._selected_agent = list(self.agents.keys())[0]
            log_coordination_step(
                "Skipped coordination, selected first agent",
                {"selected_agent": self._selected_agent},
            )

            # Present final answer immediately
            async for chunk in self._present_final_answer():
                yield chunk
            return

        # Emit startup status update for UI
        yield StreamChunk(
            type="system_status",
            content="Initializing coordination...",
            source=self.orchestrator_id,
        )

        log_stream_chunk(
            "orchestrator",
            "content",
            "ðŸš€ Starting multi-agent coordination...\n\n",
            self.orchestrator_id,
        )
        yield StreamChunk(
            type="content",
            content="ðŸš€ Starting multi-agent coordination...\n\n",
            source=self.orchestrator_id,
        )

        # Emit status update: preparing agent environments
        yield StreamChunk(
            type="system_status",
            content=f"Preparing {len(self.agents)} agent environments...",
            source=self.orchestrator_id,
        )

        # Start background status update task for real-time monitoring
        status_update_task = asyncio.create_task(self._continuous_status_updates())
        # Store reference so it can be cancelled from outside if needed
        self._status_update_task = status_update_task

        votes = {}  # Track votes: voter_id -> {"agent_id": voted_for, "reason": reason}

        # Initialize all agents with has_voted = False and set restart flags
        for agent_id in self.agents.keys():
            self.agent_states[agent_id].has_voted = False
            self.agent_states[agent_id].restart_pending = True

        # Emit status update: checking MCP/tool availability
        has_mcp_agents = any(hasattr(agent, "backend") and hasattr(agent.backend, "config") and agent.backend.config.get("mcp_servers") for agent in self.agents.values())
        if has_mcp_agents:
            yield StreamChunk(
                type="system_status",
                content="Connecting to MCP servers...",
                source=self.orchestrator_id,
            )

        log_stream_chunk(
            "orchestrator",
            "content",
            "## ðŸ“‹ Agents Coordinating\n",
            self.orchestrator_id,
        )
        yield StreamChunk(
            type="content",
            content="## ðŸ“‹ Agents Coordinating\n",
            source=self.orchestrator_id,
        )

        # Emit status update: coordination started
        yield StreamChunk(
            type="system_status",
            content="Agents working on task...",
            source=self.orchestrator_id,
        )

        # Emit status that agents are now starting to work
        yield StreamChunk(
            type="preparation_status",
            status="Agents working...",
            detail="Waiting for first response",
        )

        # Start streaming coordination with real-time agent output
        async for chunk in self._stream_coordination_with_agents(
            votes,
            conversation_context,
        ):
            yield chunk

        # Determine final agent based on votes
        current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}
        self._selected_agent = self._determine_final_agent_from_votes(
            votes,
            current_answers,
        )

        # Track winning agent for memory sharing in future turns
        self._current_turn += 1
        if self._selected_agent:
            winner_entry = {
                "agent_id": self._selected_agent,
                "turn": self._current_turn,
            }
            self._winning_agents_history.append(winner_entry)
            logger.info(
                f"ðŸ† Turn {self._current_turn} winner: {self._selected_agent} " f"(tracked for memory sharing)",
            )

        log_coordination_step(
            "Final agent selected",
            {"selected_agent": self._selected_agent, "votes": votes},
        )

        # Log structured event for observability
        log_coordination_event(
            "winner_selected",
            agent_id=self._selected_agent,
            details={
                "turn": self._current_turn,
                "vote_count": len(votes),
                "num_answers": len(current_answers),
            },
        )

        # Merge all agents' memories into winner's workspace before final presentation
        if self._selected_agent:
            self._merge_agent_memories_to_winner(self._selected_agent)

        # Cancel background status update task
        status_update_task.cancel()
        try:
            await status_update_task
        except asyncio.CancelledError:
            pass  # Expected

        # Present final answer
        async for chunk in self._present_final_answer():
            yield chunk

    async def _stream_coordination_with_agents(
        self,
        votes: Dict[str, Dict],
        conversation_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        Coordinate agents with real-time streaming of their outputs.

        Processes agent stream signals:
        - "content": Streams real-time agent output to user
        - "result": Records votes/answers, triggers restart_pending for other agents
        - "error": Displays error and closes agent stream (self-terminating)
        - "done": Closes agent stream gracefully

        Restart Mechanism:
        When any agent provides new_answer, all other agents get restart_pending=True
        and gracefully terminate their current work before restarting.
        """
        active_streams = {}
        active_tasks = {}  # Track active tasks to prevent duplicate task creation

        # Store references for timeout cleanup
        self._active_streams = active_streams
        self._active_tasks = active_tasks

        # Helper to check if coordination should end
        def _coordination_complete() -> bool:
            """Check if coordination is complete.

            Returns True when:
            - All agents have voted (normal case), OR
            - skip_voting=True and all agents have submitted at least one answer
            """
            all_voted = all(state.has_voted for state in self.agent_states.values())
            if all_voted:
                return True

            # Check skip_voting mode: complete when all agents have answered
            if self.config.skip_voting:
                all_answered = all(state.answer is not None for state in self.agent_states.values())
                if all_answered:
                    logger.info("[skip_voting] All agents have answered - skipping voting, proceeding to presentation")
                    return True

            return False

        # Stream agent outputs in real-time until coordination is complete
        while not _coordination_complete():
            # Start new coordination iteration
            self.coordination_tracker.start_new_iteration()

            # Check for cancellation - stop coordination immediately
            if hasattr(self, "cancellation_manager") and self.cancellation_manager and self.cancellation_manager.is_cancelled:
                logger.info(
                    "Cancellation detected in main coordination loop - stopping",
                )
                break

            # Check for orchestrator timeout - stop spawning new agents
            if self.is_orchestrator_timeout:
                break
            # Start any agents that aren't running and haven't voted yet
            current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}
            for agent_id in self.agents.keys():
                # Skip agents that are waiting for all answers before voting
                if self._is_waiting_for_all_answers(agent_id):
                    continue

                if agent_id not in active_streams and not self.agent_states[agent_id].has_voted and not self.agent_states[agent_id].is_killed:
                    # Apply rate limiting before starting agent
                    await self._apply_agent_startup_rate_limit(agent_id)

                    # Create a copy for this agent to avoid cross-agent coupling
                    # Each agent needs its own baseline to detect new answers independently
                    per_agent_answers = dict(current_answers)

                    # Track which answers this agent knows about (for vote validation)
                    self.agent_states[agent_id].known_answer_ids = set(current_answers.keys())

                    active_streams[agent_id] = self._stream_agent_execution(
                        agent_id,
                        self.current_task,
                        per_agent_answers,
                        conversation_context,
                        self._agent_paraphrases.get(agent_id),
                    )

            if not active_streams:
                break

            # Create tasks only for streams that don't already have active tasks
            for agent_id, stream in active_streams.items():
                if agent_id not in active_tasks:
                    active_tasks[agent_id] = asyncio.create_task(
                        self._get_next_chunk(stream),
                    )

            if not active_tasks:
                break

            done, _ = await asyncio.wait(
                active_tasks.values(),
                return_when=asyncio.FIRST_COMPLETED,
            )

            # Check for cancellation after wait
            if hasattr(self, "cancellation_manager") and self.cancellation_manager and self.cancellation_manager.is_cancelled:
                logger.info("Cancellation detected after asyncio.wait - cleaning up")
                # Gracefully interrupt Claude Code backends before cancelling tasks
                for agent_id, agent in self.agents.items():
                    if hasattr(agent, "backend") and hasattr(agent.backend, "interrupt"):
                        try:
                            await agent.backend.interrupt()
                        except Exception:
                            pass
                # Cancel remaining tasks
                for task in active_tasks.values():
                    task.cancel()
                break

            # Collect results from completed agents
            reset_signal = False
            voted_agents = {}
            answered_agents = {}
            completed_agent_ids = set()  # Track all agents whose tasks completed, i.e., done, error, result.

            # Process completed stream chunks
            for task in done:
                agent_id = next(aid for aid, t in active_tasks.items() if t is task)
                # Remove completed task from active_tasks
                del active_tasks[agent_id]

                try:
                    # Unpack chunk tuple - may be 2-tuple (type, data) or 3-tuple (type, data, tool_call_id)
                    chunk_tuple = await task
                    chunk_type = chunk_tuple[0]
                    chunk_data = chunk_tuple[1]
                    chunk_tool_call_id = chunk_tuple[2] if len(chunk_tuple) > 2 else None

                    if chunk_type == "content":
                        # Stream agent content in real-time with source info
                        log_stream_chunk(
                            "orchestrator",
                            "content",
                            chunk_data,
                            agent_id,
                        )
                        yield StreamChunk(
                            type="content",
                            content=chunk_data,
                            source=agent_id,
                        )

                    elif chunk_type == "coordination":
                        # Coordination traces (strict mode) - pass through as coordination type
                        log_stream_chunk(
                            "orchestrator",
                            "coordination",
                            chunk_data,
                            agent_id,
                        )
                        yield StreamChunk(
                            type="coordination",
                            content=chunk_data,
                            source=agent_id,
                        )

                    elif chunk_type == "external_tool_calls":
                        # Client-provided (non-workflow) tool calls must be surfaced to the caller
                        # and are never executed by MassGen.
                        yield StreamChunk(
                            type="tool_calls",
                            tool_calls=chunk_data,
                            source=agent_id,
                        )
                        # Close all active streams and stop coordination.
                        for aid in list(active_streams.keys()):
                            await self._close_agent_stream(aid, active_streams)
                        for t in list(active_tasks.values()):
                            t.cancel()
                        yield StreamChunk(type="done")
                        return

                    elif chunk_type == "reasoning":
                        # Stream reasoning content with proper attribution
                        log_stream_chunk(
                            "orchestrator",
                            "reasoning",
                            chunk_data,
                            agent_id,
                        )
                        yield chunk_data  # chunk_data is already a StreamChunk with source

                    elif chunk_type == "result":
                        # Agent completed with result
                        result_type, result_data = chunk_data
                        # Result ends the agent's current stream
                        completed_agent_ids.add(agent_id)
                        log_stream_chunk(
                            "orchestrator",
                            f"result.{result_type}",
                            result_data,
                            agent_id,
                        )

                        # Only emit "completed" status for votes - agents are truly done
                        # after voting. For answers, they still need to vote.
                        if result_type == "vote":
                            yield StreamChunk(
                                type="agent_status",
                                source=agent_id,
                                status="completed",
                                content="",
                            )
                        await self._close_agent_stream(agent_id, active_streams)

                        if result_type == "answer":
                            # Agent provided an answer (initial or improved)
                            agent = self.agents.get(agent_id)
                            # Get the context that was sent to this agent
                            agent_context = self.get_last_context(agent_id)
                            # Save snapshot (of workspace and answer) when agent provides new answer
                            answer_timestamp = await self._save_agent_snapshot(
                                agent_id,
                                answer_content=result_data,
                                context_data=agent_context,
                            )
                            if agent and agent.backend.filesystem_manager:
                                agent.backend.filesystem_manager.log_current_state(
                                    "after providing answer",
                                )
                            # Always record answers, even from restarting agents (orchestrator accepts them)

                            answered_agents[agent_id] = result_data
                            # Pass timestamp to coordination_tracker for mapping
                            self.coordination_tracker.add_agent_answer(
                                agent_id,
                                result_data,
                                snapshot_timestamp=answer_timestamp,
                            )
                            # End round token tracking with "answer" outcome
                            if agent and hasattr(agent.backend, "end_round_tracking"):
                                agent.backend.end_round_tracking("answer")
                            # Notify web display if available
                            if hasattr(self, "coordination_ui") and self.coordination_ui:
                                display = getattr(self.coordination_ui, "display", None)
                                if display and hasattr(display, "send_new_answer"):
                                    # Get answer count and label for this agent
                                    agent_answers = self.coordination_tracker.answers_by_agent.get(
                                        agent_id,
                                        [],
                                    )
                                    answer_number = len(agent_answers)
                                    agent_num = self.coordination_tracker._get_agent_number(
                                        agent_id,
                                    )
                                    answer_label = f"agent{agent_num}.{answer_number}"

                                    # Get workspace path from snapshot mapping
                                    workspace_path = None
                                    snapshot_mapping = self.coordination_tracker.snapshot_mappings.get(
                                        answer_label,
                                    )
                                    if snapshot_mapping:
                                        # Build absolute workspace path from mapping
                                        log_session_dir = get_log_session_dir()
                                        if log_session_dir and snapshot_mapping.get(
                                            "path",
                                        ):
                                            # path is like "agent_a/20251230_123456/answer.txt"
                                            # workspace is at "agent_a/20251230_123456/workspace"
                                            snapshot_path = snapshot_mapping["path"]
                                            if snapshot_path.endswith("/answer.txt"):
                                                workspace_rel = snapshot_path[: -len("/answer.txt")] + "/workspace"
                                            else:
                                                workspace_rel = f"{agent_id}/{answer_timestamp}/workspace"
                                            workspace_path = str(
                                                Path(log_session_dir) / workspace_rel,
                                            )

                                    display.send_new_answer(
                                        agent_id=agent_id,
                                        content=result_data,
                                        answer_number=answer_number,
                                        answer_label=answer_label,
                                        workspace_path=workspace_path,
                                    )
                                # Record answer with context for timeline visualization
                                if display and hasattr(
                                    display,
                                    "record_answer_with_context",
                                ):
                                    agent_answers = self.coordination_tracker.answers_by_agent.get(
                                        agent_id,
                                        [],
                                    )
                                    answer_number = len(agent_answers)
                                    agent_num = self.coordination_tracker._get_agent_number(
                                        agent_id,
                                    )
                                    # Use same label format as coordination_tracker: "agent1.1"
                                    answer_label = f"agent{agent_num}.{answer_number}"
                                    context_sources = self.coordination_tracker.get_agent_context_labels(
                                        agent_id,
                                    )
                                    display.record_answer_with_context(
                                        agent_id=agent_id,
                                        answer_label=answer_label,
                                        context_sources=context_sources,
                                        round_num=answer_number,
                                    )
                            # Update status file for real-time monitoring
                            # Run in executor to avoid blocking event loop
                            log_session_dir = get_log_session_dir()
                            if log_session_dir:
                                loop = asyncio.get_running_loop()
                                await loop.run_in_executor(
                                    None,
                                    self.coordination_tracker.save_status_file,
                                    log_session_dir,
                                    self,
                                )
                            restart_triggered_id = agent_id  # Last agent to provide new answer
                            reset_signal = True
                            log_stream_chunk(
                                "orchestrator",
                                "content",
                                "âœ… Answer provided\n",
                                agent_id,
                            )

                            # Track new answer event
                            log_stream_chunk(
                                "orchestrator",
                                "content",
                                "âœ… Answer provided\n",
                                agent_id,
                            )
                            yield StreamChunk(
                                type="agent_status" if self.trace_classification == "strict" else "content",
                                content="âœ… Answer provided\n",
                                source=agent_id,
                            )

                        elif result_type == "vote":
                            # Agent voted for existing answer
                            logger.debug(
                                f"VOTE BLOCK ENTERED for {agent_id}, result_data={result_data}",
                            )
                            # Ignore votes from agents with restart pending (votes are about current state)
                            # EXCEPTION 1: For single agent, if it's voting for itself after producing
                            # its first answer, accept the vote (no other agents to wait for)
                            # EXCEPTION 2: If restart_pending is stale (agent has already seen all
                            # current answers), clear it and accept the vote. This prevents infinite
                            # restart loops for fast agents that vote without tool calls (so the
                            # mid-stream injection callback never fires to clear restart_pending).
                            restart_pending = self._check_restart_pending(agent_id)
                            is_single_agent = len(self.agents) == 1
                            agent_has_answer = self.agent_states[agent_id].answer is not None
                            if restart_pending and is_single_agent and agent_has_answer:
                                # Single agent voting for itself - clear restart_pending and accept vote
                                self.agent_states[agent_id].restart_pending = False
                                restart_pending = False
                                logger.info(f"[Orchestrator] Single agent {agent_id} vote accepted (has own answer)")
                            if restart_pending:
                                # Check if there are genuinely unseen answers
                                current_answer_ids = {aid for aid, state in self.agent_states.items() if state.answer}
                                known = self.agent_states[agent_id].known_answer_ids
                                unseen = current_answer_ids - known
                                if not unseen:
                                    # No new answers the agent hasn't seen - stale restart_pending
                                    self.agent_states[agent_id].restart_pending = False
                                    restart_pending = False
                                    logger.info(
                                        f"[Orchestrator] Agent {agent_id} vote accepted (no unseen answers, clearing stale restart_pending)",
                                    )
                            if restart_pending:
                                voted_for = result_data.get("agent_id", "<unknown>")
                                reason = result_data.get("reason", "No reason provided")
                                # Track the ignored vote action
                                self.coordination_tracker.track_agent_action(
                                    agent_id,
                                    ActionType.VOTE_IGNORED,
                                    f"Voted for {voted_for} but ignored due to restart",
                                )
                                # Save in coordination tracker that we waste a vote due to restart
                                log_stream_chunk(
                                    "orchestrator",
                                    "content",
                                    f"ðŸ”„ Vote for [{voted_for}] ignored (reason: {reason}) - restarting due to new answers",
                                    agent_id,
                                )
                                yield StreamChunk(
                                    type="agent_status" if self.trace_classification == "strict" else "content",
                                    content=f"ðŸ”„ Vote for [{voted_for}] ignored (reason: {reason}) - restarting due to new answers",
                                    source=agent_id,
                                )
                                # Clear the stale vote data to prevent it leaking into final results
                                self.agent_states[agent_id].votes = {}
                            else:
                                # Save vote snapshot (includes workspace)
                                vote_timestamp = await self._save_agent_snapshot(
                                    agent_id=agent_id,
                                    vote_data=result_data,
                                    context_data=self.get_last_context(agent_id),
                                )
                                # Log workspaces for current agent
                                agent = self.agents.get(agent_id)
                                if agent and agent.backend.filesystem_manager:
                                    self.agents.get(
                                        agent_id,
                                    ).backend.filesystem_manager.log_current_state(
                                        "after voting",
                                    )
                                voted_agents[agent_id] = result_data
                                # Pass timestamp to coordination_tracker for mapping
                                self.coordination_tracker.add_agent_vote(
                                    agent_id,
                                    result_data,
                                    snapshot_timestamp=vote_timestamp,
                                )
                                # End round token tracking with "vote" outcome
                                if agent and hasattr(
                                    agent.backend,
                                    "end_round_tracking",
                                ):
                                    agent.backend.end_round_tracking("vote")
                                # Notify web display about the vote
                                logger.debug(
                                    f"Vote recorded - checking for coordination_ui: hasattr={hasattr(self, 'coordination_ui')}, coordination_ui={self.coordination_ui}",
                                )
                                if hasattr(self, "coordination_ui") and self.coordination_ui:
                                    display = getattr(
                                        self.coordination_ui,
                                        "display",
                                        None,
                                    )
                                    logger.debug(
                                        f"Got display: {display}, has update_vote_target: {hasattr(display, 'update_vote_target') if display else 'N/A'}",
                                    )
                                    if display and hasattr(
                                        display,
                                        "update_vote_target",
                                    ):
                                        logger.debug(
                                            f"Calling update_vote_target({agent_id}, {result_data.get('agent_id', '')}, ...)",
                                        )
                                        display.update_vote_target(
                                            voter_id=agent_id,
                                            target_id=result_data.get("agent_id", ""),
                                            reason=result_data.get("reason", ""),
                                        )
                                    # Record vote with context for timeline visualization
                                    if display and hasattr(
                                        display,
                                        "record_vote_with_context",
                                    ):
                                        agent_num = self.coordination_tracker._get_agent_number(
                                            agent_id,
                                        )
                                        # Count previous votes by this agent to get vote number
                                        votes_by_agent = [v for v in self.coordination_tracker.votes if v.voter_id == agent_id]
                                        vote_number = len(
                                            votes_by_agent,
                                        )  # Already recorded above, so this is the count
                                        # Use format like "vote1.1" (matches answer format "agent1.1")
                                        vote_label = f"vote{agent_num}.{vote_number}"
                                        available_answers = self.coordination_tracker.iteration_available_labels.copy()
                                        # Get the answer label that was voted for (e.g., "agent2.3")
                                        voted_for_agent = result_data.get(
                                            "agent_id",
                                            "",
                                        )
                                        voted_for_label = self.coordination_tracker.get_voted_for_label(
                                            agent_id,
                                            voted_for_agent,
                                        )
                                        display.record_vote_with_context(
                                            voter_id=agent_id,
                                            vote_label=vote_label,
                                            voted_for=voted_for_label or voted_for_agent,
                                            available_answers=available_answers,
                                            voting_round=self.coordination_tracker.current_iteration,
                                        )
                                    # Notify TUI to display vote tool card (TextualTerminalDisplay)
                                    if display and hasattr(display, "notify_vote"):
                                        display.notify_vote(
                                            voter=agent_id,
                                            voted_for=result_data.get("agent_id", ""),
                                            reason=result_data.get("reason", ""),
                                        )
                                # Update status file for real-time monitoring
                                # Run in executor to avoid blocking event loop
                                log_session_dir = get_log_session_dir()
                                logger.debug(f"Log session dir: {log_session_dir}")
                                if log_session_dir:
                                    loop = asyncio.get_running_loop()
                                    await loop.run_in_executor(
                                        None,
                                        self.coordination_tracker.save_status_file,
                                        log_session_dir,
                                        self,
                                    )

                                # Track vote event for logging only
                                # Note: The TUI displays votes via notify_vote tool card,
                                # so we use agent_status type to avoid duplicate display
                                log_stream_chunk(
                                    "orchestrator",
                                    "agent_status",
                                    f"âœ… Vote recorded for [{result_data['agent_id']}]",
                                    agent_id,
                                )
                                yield StreamChunk(
                                    type="agent_status",  # Always agent_status - TUI shows vote via tool card
                                    content=f"âœ… Vote recorded for [{result_data['agent_id']}]",
                                    source=agent_id,
                                )

                    elif chunk_type == "error":
                        # Agent error
                        self.coordination_tracker.track_agent_action(
                            agent_id,
                            ActionType.ERROR,
                            chunk_data,
                        )
                        # End round token tracking with "error" outcome
                        agent = self.agents.get(agent_id)
                        if agent and hasattr(agent.backend, "end_round_tracking"):
                            agent.backend.end_round_tracking("error")
                        # Error ends the agent's current stream
                        completed_agent_ids.add(agent_id)
                        # Mark agent as killed to prevent respawning in the while loop
                        self.agent_states[agent_id].is_killed = True
                        log_stream_chunk("orchestrator", "error", chunk_data, agent_id)
                        yield StreamChunk(
                            type="agent_status" if self.trace_classification == "strict" else "content",
                            content=f"âŒ {chunk_data}",
                            source=agent_id,
                        )
                        log_stream_chunk(
                            "orchestrator",
                            "agent_status",
                            "completed",
                            agent_id,
                        )
                        yield StreamChunk(
                            type="agent_status",
                            source=agent_id,
                            status="completed",
                            content="",
                        )
                        await self._close_agent_stream(agent_id, active_streams)

                    elif chunk_type == "debug":
                        # Debug information - forward as StreamChunk for logging
                        log_stream_chunk("orchestrator", "debug", chunk_data, agent_id)
                        yield StreamChunk(
                            type="debug",
                            content=chunk_data,
                            source=agent_id,
                        )

                    elif chunk_type == "mcp_status":
                        # MCP status messages - keep mcp_status type to preserve tool tracking
                        mcp_message = f"ðŸ”§ MCP: {chunk_data}"
                        log_stream_chunk("orchestrator", "mcp_status", chunk_data, agent_id)
                        yield StreamChunk(
                            type="mcp_status",
                            content=mcp_message,
                            source=agent_id,
                            tool_call_id=chunk_tool_call_id,
                        )

                    elif chunk_type == "custom_tool_status":
                        # Custom tool status messages - keep custom_tool_status type for tool tracking
                        custom_message = f"ðŸ”§ Custom Tool: {chunk_data}"
                        log_stream_chunk("orchestrator", "custom_tool_status", chunk_data, agent_id)
                        yield StreamChunk(
                            type="custom_tool_status",
                            content=custom_message,
                            source=agent_id,
                            tool_call_id=chunk_tool_call_id,
                        )

                    elif chunk_type == "hook_execution":
                        # Hook execution chunks - pass through for TUI display
                        # chunk_data is already a StreamChunk with hook_info and tool_call_id
                        log_stream_chunk("orchestrator", "hook_execution", str(chunk_data.hook_info), agent_id)
                        yield chunk_data

                    elif chunk_type == "agent_restart":
                        # Agent is starting a new round - notify UI to show fresh timeline
                        # chunk_data is a dict with agent_id and round
                        log_stream_chunk("orchestrator", "agent_restart", str(chunk_data), agent_id)
                        yield StreamChunk(
                            type="agent_restart",
                            content=chunk_data,
                            source=agent_id,
                        )

                    elif chunk_type == "done":
                        # Stream completed - this is just an end-of-stream marker
                        # DON'T emit "completed" status here - that's handled by the "result" handler
                        # when the agent actually provides an answer/vote.
                        # The "done" chunk just means the backend stream ended, which happens
                        # after every turn (including the first turn before any answer).
                        agent = self.agents.get(agent_id)
                        if agent and hasattr(agent.backend, "end_round_tracking"):
                            agent.backend.end_round_tracking("restarted")
                        completed_agent_ids.add(agent_id)
                        log_stream_chunk("orchestrator", "done", None, agent_id)

                        # Phase 13.1: Emit token usage update for TUI status ribbon
                        if agent and hasattr(agent.backend, "token_usage") and agent.backend.token_usage:
                            token_usage = agent.backend.token_usage
                            yield StreamChunk(
                                type="token_usage_update",
                                source=agent_id,
                                usage={
                                    "input_tokens": token_usage.input_tokens or 0,
                                    "output_tokens": token_usage.output_tokens or 0,
                                    "estimated_cost": token_usage.estimated_cost or 0,
                                },
                            )

                        # Note: Removed agent_status: completed emission here - it was causing
                        # agents to show "Done" immediately before they've done any work.
                        # Status updates are properly handled by the "result" handler.
                        await self._close_agent_stream(agent_id, active_streams)

                except Exception as e:
                    self.coordination_tracker.track_agent_action(
                        agent_id,
                        ActionType.ERROR,
                        f"Stream error - {e}",
                    )
                    # End round token tracking with "error" outcome
                    agent = self.agents.get(agent_id)
                    if agent and hasattr(agent.backend, "end_round_tracking"):
                        agent.backend.end_round_tracking("error")
                    completed_agent_ids.add(agent_id)
                    # Mark agent as killed to prevent respawning in the while loop
                    self.agent_states[agent_id].is_killed = True
                    log_stream_chunk(
                        "orchestrator",
                        "error",
                        f"âŒ Stream error - {e}",
                        agent_id,
                    )
                    error_type = "coordination" if self.trace_classification == "strict" else "content"
                    yield StreamChunk(
                        type=error_type,
                        content=f"âŒ Stream error - {e}",
                        source=agent_id,
                    )
                    await self._close_agent_stream(agent_id, active_streams)

            # Apply all state changes atomically after processing all results
            if reset_signal:
                # Reset all agents' has_voted to False (any new answer invalidates all votes)
                for state in self.agent_states.values():
                    state.has_voted = False
                    state.votes = {}  # Clear stale vote data
                votes.clear()

                # Skip restart signaling when injection is disabled (multi-agent refinement OFF)
                # Agents work independently and don't need to see each other's answers
                if not self.config.disable_injection:
                    for agent_id in self.agent_states.keys():
                        self.agent_states[agent_id].restart_pending = True

                    # Track restart signals
                    self.coordination_tracker.track_restart_signal(
                        restart_triggered_id,
                        list(self.agent_states.keys()),
                    )
                    # Note that the agent that sent the restart signal had its stream end so we should mark as completed. NOTE the below breaks it.
                    self.coordination_tracker.complete_agent_restart(restart_triggered_id)
                else:
                    logger.info(
                        "[disable_injection] Skipping restart signaling - agents work independently",
                    )
            # Set has_voted = True for agents that voted (only if no reset signal)
            else:
                for agent_id, vote_data in voted_agents.items():
                    self.agent_states[agent_id].has_voted = True
                    votes[agent_id] = vote_data

            # Update answers for agents that provided them
            for agent_id, answer in answered_agents.items():
                self.agent_states[agent_id].answer = answer

            # Update status based on what actions agents took
            for agent_id in completed_agent_ids:
                if agent_id in answered_agents:
                    self.coordination_tracker.change_status(
                        agent_id,
                        AgentStatus.ANSWERED,
                    )
                elif agent_id in voted_agents:
                    self.coordination_tracker.change_status(agent_id, AgentStatus.VOTED)
                # Errors and timeouts are already tracked via track_agent_action

        # Cancel any remaining tasks and close streams, as all agents have voted (no more new answers)
        for agent_id, task in active_tasks.items():
            if not task.done():
                self.coordination_tracker.track_agent_action(
                    agent_id,
                    ActionType.CANCELLED,
                    "All agents voted - coordination complete",
                )
            task.cancel()
        for agent_id in list(active_streams.keys()):
            await self._close_agent_stream(agent_id, active_streams)

        # Finalize token tracking for all agents
        # This estimates tokens for any streams that were interrupted (e.g., due to restart_pending)
        for agent_id, agent in self.agents.items():
            if hasattr(agent.backend, "finalize_token_tracking"):
                agent.backend.finalize_token_tracking()

    async def _copy_all_snapshots_to_temp_workspace(
        self,
        agent_id: str,
    ) -> Optional[str]:
        """Copy all agents' latest workspace snapshots to a temporary workspace for context sharing.

        TODO (v0.0.14 Context Sharing Enhancement - See docs/dev_notes/v0.0.14-context.md):
        - Validate agent permissions before restoring snapshots
        - Check if agent has read access to other agents' workspaces
        - Implement fine-grained control over which snapshots can be accessed
        - Add audit logging for snapshot access attempts

        Args:
            agent_id: ID of the Claude Code agent receiving the context

        Returns:
            Path to the agent's workspace directory if successful, None otherwise
        """
        agent = self.agents.get(agent_id)
        if not agent:
            return None

        # Check if agent has filesystem support
        if not agent.backend.filesystem_manager:
            return None

        # Create anonymous mapping for agent IDs
        # This ensures consistency with the anonymous IDs shown to agents
        agent_mapping = self.coordination_tracker.get_reverse_agent_mapping()

        # Collect snapshots from snapshot_storage directory
        all_snapshots = {}
        if self._snapshot_storage:
            snapshot_base = Path(self._snapshot_storage)
            for source_agent_id in self.agents.keys():
                source_snapshot = snapshot_base / source_agent_id
                if source_snapshot.exists() and source_snapshot.is_dir():
                    all_snapshots[source_agent_id] = source_snapshot

        # Use the filesystem manager to copy snapshots to temp workspace
        workspace_path = await agent.backend.filesystem_manager.copy_snapshots_to_temp_workspace(
            all_snapshots,
            agent_mapping,
        )
        return str(workspace_path) if workspace_path else None

    async def _save_agent_snapshot(
        self,
        agent_id: str,
        answer_content: str = None,
        vote_data: Dict[str, Any] = None,
        is_final: bool = False,
        context_data: Any = None,
    ) -> str:
        """
        Save a snapshot of an agent's working directory and answer/vote with the same timestamp.

        Creates a timestamped directory structure:
        - agent_id/timestamp/workspace/ - Contains the workspace files
        - agent_id/timestamp/answer.txt - Contains the answer text (if provided)
        - agent_id/timestamp/vote.json - Contains the vote data (if provided)
        - agent_id/timestamp/context.txt - Contains the context used (if provided)

        Note on vote-only snapshots:
            When saving a vote without an answer (vote_data only), workspace snapshots are
            intentionally skipped. During voting, agents may create temporary verification
            files (e.g., check.py, test scripts) to help evaluate answers. Saving these would
            overwrite the actual deliverable files from the previous answer snapshot. The
            vote.json and context.txt are still saved for tracking purposes.

        Args:
            agent_id: ID of the agent
            answer_content: The answer content to save (if provided)
            vote_data: The vote data to save (if provided)
            is_final: If True, save as final snapshot for presentation
            context_data: The context data to save (conversation, answers, etc.)

        Returns:
            The timestamp used for this snapshot
        """
        logger.info(
            f"[Orchestrator._save_agent_snapshot] Called for agent_id={agent_id}, has_answer={bool(answer_content)}, has_vote={bool(vote_data)}, is_final={is_final}",
        )

        agent = self.agents.get(agent_id)
        if not agent:
            logger.warning(
                f"[Orchestrator._save_agent_snapshot] Agent {agent_id} not found in agents dict",
            )
            return None

        # Generate single timestamp for answer/vote and workspace
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")

        # Save answer if provided (or create final directory structure even if empty)
        if answer_content is not None or is_final:
            try:
                log_session_dir = get_log_session_dir()
                if log_session_dir:
                    if is_final:
                        # For final, save to final directory
                        timestamped_dir = log_session_dir / "final" / agent_id
                    else:
                        # For regular snapshots, create timestamped directory
                        timestamped_dir = log_session_dir / agent_id / timestamp
                    timestamped_dir.mkdir(parents=True, exist_ok=True)
                    answer_file = timestamped_dir / "answer.txt"

                    # Write the answer content (even if empty for final snapshots)
                    content_to_write = answer_content if answer_content is not None else ""
                    answer_file.write_text(content_to_write)
                    logger.info(
                        f"[Orchestrator._save_agent_snapshot] Saved answer to {answer_file}",
                    )

            except Exception as e:
                logger.warning(
                    f"[Orchestrator._save_agent_snapshot] Failed to save answer for {agent_id}: {e}",
                )

        # Save vote if provided
        if vote_data:
            try:
                log_session_dir = get_log_session_dir()
                if log_session_dir:
                    # Create timestamped directory for vote
                    timestamped_dir = log_session_dir / agent_id / timestamp
                    timestamped_dir.mkdir(parents=True, exist_ok=True)
                    vote_file = timestamped_dir / "vote.json"

                    # Get current state for context
                    current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}

                    # Create anonymous agent mapping (agent1, agent2, etc.)
                    agent_mapping = self.coordination_tracker.get_anonymous_agent_mapping()

                    # Get answer labels from coordination tracker (e.g., "agent1.2", "agent2.1")
                    # Use the voter's context labels (what they were shown) to avoid race conditions
                    # in parallel execution where new answers may arrive while voting
                    available_answer_labels = []
                    answer_label_to_agent = {}  # Maps "agent1.2" -> "agent_a"
                    voted_for_label = None
                    voted_for_agent = vote_data.get("agent_id", "unknown")

                    if self.coordination_tracker:
                        # Get labels from voter's context (what they actually saw)
                        voter_context = self.coordination_tracker.get_agent_context_labels(agent_id)
                        for label in voter_context:
                            available_answer_labels.append(label)
                            # Extract agent number from label (e.g., "agent1.2" -> 1)
                            # and map back to agent ID
                            for aid in current_answers.keys():
                                aid_label = self.coordination_tracker.get_voted_for_label(
                                    agent_id,
                                    aid,
                                )
                                if aid_label == label:
                                    answer_label_to_agent[label] = aid

                        # Get the specific label for the voted-for agent
                        voted_for_label = self.coordination_tracker.get_voted_for_label(
                            agent_id,
                            voted_for_agent,
                        )

                    # Build comprehensive vote data
                    comprehensive_vote_data = {
                        "voter_id": agent_id,
                        "voter_anon_id": next(
                            (anon for anon, real in agent_mapping.items() if real == agent_id),
                            agent_id,
                        ),
                        "voted_for": voted_for_agent,
                        "voted_for_label": voted_for_label,  # e.g., "agent1.2"
                        "voted_for_anon": next(
                            (anon for anon, real in agent_mapping.items() if real == voted_for_agent),
                            "unknown",
                        ),
                        "reason": vote_data.get("reason", ""),
                        "timestamp": timestamp,
                        "unix_timestamp": time.time(),
                        "iteration": self.coordination_tracker.current_iteration if self.coordination_tracker else None,
                        "coordination_round": self.coordination_tracker.max_round if self.coordination_tracker else None,
                        "available_options": list(
                            current_answers.keys(),
                        ),  # agent IDs for backwards compatibility
                        "available_options_labels": available_answer_labels,  # e.g., ["agent1.2", "agent2.1"]
                        "answer_label_to_agent": answer_label_to_agent,  # Maps label -> agent_id
                        "available_options_anon": [
                            next(
                                (anon for anon, real in agent_mapping.items() if real == aid),
                                aid,
                            )
                            for aid in sorted(current_answers.keys())
                        ],
                        "agent_mapping": agent_mapping,
                        "vote_context": {
                            "total_agents": len(self.agents),
                            "agents_with_answers": len(current_answers),
                            "current_task": self.current_task,
                        },
                    }

                    # Write the comprehensive vote data
                    with open(vote_file, "w", encoding="utf-8") as f:
                        json.dump(comprehensive_vote_data, f, indent=2)
                    logger.info(
                        f"[Orchestrator._save_agent_snapshot] Saved comprehensive vote to {vote_file}",
                    )

            except Exception as e:
                logger.error(
                    f"[Orchestrator._save_agent_snapshot] Failed to save vote for {agent_id}: {e}",
                )
                logger.error(
                    f"[Orchestrator._save_agent_snapshot] Traceback: {traceback.format_exc()}",
                )

        # Save workspace snapshot with the same timestamp
        # Skip workspace saving for votes - workspace should be preserved from previous answer
        if agent.backend.filesystem_manager:
            if vote_data and not answer_content and not is_final:
                # Vote only - skip workspace snapshot to preserve previous answer's workspace
                logger.info(
                    "[Orchestrator._save_agent_snapshot] Skipping workspace snapshot for vote (preserving previous workspace)",
                )
            else:
                # Archive memories BEFORE clearing/snapshotting workspace
                workspace_path = agent.backend.filesystem_manager.get_current_workspace()
                if workspace_path:
                    self._archive_agent_memories(agent_id, Path(workspace_path))

                logger.info(
                    f"[Orchestrator._save_agent_snapshot] Agent {agent_id} has filesystem_manager, calling save_snapshot with timestamp={timestamp if not is_final else None}",
                )
                await agent.backend.filesystem_manager.save_snapshot(
                    timestamp=timestamp if not is_final else None,
                    is_final=is_final,
                )

                # Clear workspace after saving snapshot (but not for final snapshots)
                if not is_final:
                    agent.backend.filesystem_manager.clear_workspace()
                    logger.info(
                        f"[Orchestrator._save_agent_snapshot] Cleared workspace for {agent_id} after saving snapshot",
                    )
        else:
            logger.info(
                f"[Orchestrator._save_agent_snapshot] Agent {agent_id} does not have filesystem_manager",
            )

        # Save context if provided (unified context saving)
        if context_data:
            try:
                log_session_dir = get_log_session_dir()
                if log_session_dir:
                    if is_final:
                        timestamped_dir = log_session_dir / "final" / agent_id
                    else:
                        timestamped_dir = log_session_dir / agent_id / timestamp

                    # Ensure directory exists (may not have been created if no answer/vote)
                    timestamped_dir.mkdir(parents=True, exist_ok=True)
                    context_file = timestamped_dir / "context.txt"

                    # Handle different types of context data
                    if isinstance(context_data, dict):
                        # Pretty print dict/JSON data
                        context_file.write_text(
                            json.dumps(context_data, indent=2, default=str),
                        )
                    else:
                        # Save as string
                        context_file.write_text(str(context_data))

                    logger.info(
                        f"[Orchestrator._save_agent_snapshot] Saved context to {context_file}",
                    )
            except Exception as ce:
                logger.warning(
                    f"[Orchestrator._save_agent_snapshot] Failed to save context for {agent_id}: {ce}",
                )

        # Save execution trace if available (for both answer and vote snapshots)
        # Votes also contain valuable execution history (tool calls, reasoning, etc.)
        if answer_content is not None or vote_data is not None or is_final:
            try:
                if hasattr(agent.backend, "_save_execution_trace"):
                    # Save to log directory for historical tracking
                    log_session_dir = get_log_session_dir()
                    if log_session_dir:
                        if is_final:
                            timestamped_dir = log_session_dir / "final" / agent_id
                        else:
                            timestamped_dir = log_session_dir / agent_id / timestamp
                        timestamped_dir.mkdir(parents=True, exist_ok=True)
                        agent.backend._save_execution_trace(timestamped_dir)

                    # Also save to snapshot_storage so other agents can access it
                    # via temp_workspace when they receive context updates
                    if agent.backend.filesystem_manager and agent.backend.filesystem_manager.snapshot_storage:
                        snapshot_storage = agent.backend.filesystem_manager.snapshot_storage
                        snapshot_storage.mkdir(parents=True, exist_ok=True)
                        agent.backend._save_execution_trace(snapshot_storage)
                        logger.debug(
                            f"[Orchestrator._save_agent_snapshot] Saved execution trace to snapshot_storage: {snapshot_storage}",
                        )
            except Exception as te:
                logger.warning(
                    f"[Orchestrator._save_agent_snapshot] Failed to save execution trace for {agent_id}: {te}",
                )

        # Return the timestamp for tracking
        return timestamp if not is_final else "final"

    def get_last_context(self, agent_id: str) -> Any:
        """Get the last context for an agent, or None if not available."""
        return self.agent_states[agent_id].last_context if agent_id in self.agent_states else None

    async def _close_agent_stream(
        self,
        agent_id: str,
        active_streams: Dict[str, AsyncGenerator],
    ) -> None:
        """Close and remove an agent stream safely."""
        if agent_id in active_streams:
            try:
                await active_streams[agent_id].aclose()
            except Exception:
                pass  # Ignore cleanup errors
            del active_streams[agent_id]

    def _check_restart_pending(self, agent_id: str) -> bool:
        """Check if agent should restart and yield restart message if needed. This will always be called when exiting out of _stream_agent_execution()."""
        restart_pending = self.agent_states[agent_id].restart_pending
        return restart_pending

    async def _clear_framework_mcp_state(self, agent_id: str) -> None:
        """
        Clear in-memory state of framework MCP servers before agent restart.

        This ensures stateful MCPs like planning don't retain old data across
        answer submissions. Currently clears:
        - Task plans (planning MCP)

        Args:
            agent_id: ID of the agent being restarted
        """
        agent = self.agents.get(agent_id)
        if not agent or not hasattr(agent.backend, "_mcp_client") or not agent.backend._mcp_client:
            return

        # Find the planning MCP tool name for this agent
        planning_tool_name = None
        for tool_name in agent.backend._mcp_functions.keys():
            if "clear_task_plan" in tool_name and f"planning_{agent_id}" in tool_name:
                planning_tool_name = tool_name
                break

        if planning_tool_name:
            try:
                logger.info(
                    f"[Orchestrator] Clearing task plan for {agent_id} via {planning_tool_name}",
                )
                result, _ = await agent.backend._execute_mcp_function_with_retry(
                    planning_tool_name,
                    "{}",  # No arguments needed
                )
                logger.info(
                    f"[Orchestrator] Clear task plan result for {agent_id}: {result}",
                )
            except Exception as e:
                logger.warning(
                    f"[Orchestrator] Failed to clear task plan for {agent_id}: {e}",
                )

    async def _save_partial_work_on_restart(self, agent_id: str) -> Optional[str]:
        """
        Save partial work snapshot when agent is restarting due to new answers from others.
        This ensures that any work done before the restart is preserved and shared with other agents.

        Args:
            agent_id: ID of the agent being restarted

        Returns:
            The timestamp of the saved snapshot, or None if no snapshot was saved
        """
        agent = self.agents.get(agent_id)
        if not agent or not agent.backend.filesystem_manager:
            return None

        logger.info(
            f"[Orchestrator._save_partial_work_on_restart] Saving partial work for {agent_id} before restart",
        )

        # Save the partial work snapshot with context
        timestamp = await self._save_agent_snapshot(
            agent_id,
            answer_content=None,  # No complete answer yet
            context_data=self.get_last_context(agent_id),
            is_final=False,
        )

        agent.backend.filesystem_manager.log_current_state(
            "after saving partial work on restart",
        )
        return timestamp

    def _compute_plan_progress_stats(self, workspace_path: str) -> Optional[Dict[str, Any]]:
        """Compute task progress stats for an agent's workspace (plan execution mode only).

        This reads the agent's tasks/plan.json and computes how many tasks are completed
        vs total. Only works in plan-and-execute mode where tasks/plan.json exists.

        Args:
            workspace_path: Path to the agent's workspace (temp workspace copy)

        Returns:
            Dict with progress stats, or None if not in plan execution mode or files missing
        """
        try:
            workspace = Path(workspace_path)
            tasks_plan = workspace / "tasks" / "plan.json"

            # Check if this is plan execution mode (has tasks/plan.json)
            if not tasks_plan.exists():
                return None

            # Read task plan
            tasks_data = json.loads(tasks_plan.read_text())
            tasks = tasks_data.get("tasks", [])
            total_tasks = len(tasks)

            if total_tasks == 0:
                return None

            # Count by status (verified tasks count as completed for progress)
            completed = sum(1 for t in tasks if t.get("status") in ("completed", "verified"))
            in_progress = sum(1 for t in tasks if t.get("status") == "in_progress")
            pending = sum(1 for t in tasks if t.get("status") == "pending")

            return {
                "total": total_tasks,
                "completed": completed,
                "in_progress": in_progress,
                "pending": pending,
                "percent_complete": round(100 * completed / total_tasks, 1) if total_tasks > 0 else 0,
            }
        except Exception as e:
            logger.debug(f"[Orchestrator] Could not compute plan progress: {e}")
            return None

    def _build_tool_result_injection(
        self,
        agent_id: str,
        new_answers: Dict[str, str],
        existing_answers: Optional[Dict[str, str]] = None,
    ) -> str:
        """Build compact injection content for appending to tool results.

        This creates a lighter-weight update message designed to be embedded
        in tool result content rather than sent as a separate user message.
        Used for mid-stream injection after the first traditional injection.

        Args:
            agent_id: The agent receiving the injection
            new_answers: Dict mapping agent_id to their NEW answer content
            existing_answers: Dict of answers agent already knew about (to detect updates)

        Returns:
            Formatted string to append to tool result content
        """
        existing_answers = existing_answers or {}

        # Normalize workspace paths for this agent's perspective
        normalized = self._normalize_workspace_paths_in_answers(
            new_answers,
            viewing_agent_id=agent_id,
        )

        # Get viewing agent's temporary workspace path
        temp_workspace_base = None
        viewing_agent = self.agents.get(agent_id)
        if viewing_agent and viewing_agent.backend.filesystem_manager:
            temp_workspace_base = str(
                viewing_agent.backend.filesystem_manager.agent_temporary_workspace,
            )

        # Create anonymous mapping (consistent with CURRENT ANSWERS format across all agents)
        agent_mapping = self.coordination_tracker.get_reverse_agent_mapping()

        # Format answers with workspace paths
        lines = []
        updated_agents = []
        new_agents = []

        for aid, answer in normalized.items():
            anon_id = agent_mapping.get(aid, f"agent_{aid}")
            is_update = aid in existing_answers

            if is_update:
                updated_agents.append(anon_id)
            else:
                new_agents.append(anon_id)

            # Truncate long answers for injection context
            truncated = answer[:500] + "..." if len(answer) > 500 else answer

            # Include workspace path for file access
            workspace_path = os.path.join(temp_workspace_base, anon_id) if temp_workspace_base else f"temp_workspaces/{anon_id}"
            lines.append(f"  [{anon_id}] (workspace: {workspace_path}):")

            # Compute and include progress stats if in plan execution mode
            progress = self._compute_plan_progress_stats(workspace_path)
            if progress:
                lines.append(
                    f"    ðŸ“Š Progress: {progress['completed']}/{progress['total']} tasks completed "
                    f"({progress['percent_complete']}%) | {progress['in_progress']} in progress | {progress['pending']} pending",
                )
                lines.append("    âš ï¸  Note: Progress stats are INFORMATIONAL - evaluate the DELIVERABLE quality, not task count")

            lines.append(f"    {truncated}")
            lines.append("")

        # Build header based on what changed
        if updated_agents and new_agents:
            header = f"[UPDATE: {', '.join(new_agents)} submitted new answer(s); {', '.join(updated_agents)} updated their answer(s)]"
        elif updated_agents:
            header = f"[UPDATE: {', '.join(updated_agents)} updated their answer(s)]"
        else:
            header = f"[UPDATE: {', '.join(new_agents)} submitted new answer(s)]"

        injection_parts = [
            "",
            "=" * 60,
            "âš ï¸  IMPORTANT: NEW ANSWER RECEIVED - ACTION REQUIRED",
            "=" * 60,
            "",
            header,
            "",
            *lines,
            "=" * 60,
            "REQUIRED ACTION - You MUST do one of the following:",
            "=" * 60,
            "",
            "1. **ADD A TASK** to your plan: 'Evaluate agent answer(s) and decide next action'",
            "   - Use update_task_status or create a new task to track this evaluation",
            "   - Read their workspace files (paths above) to understand their solution",
            "   - Read their execution_trace.md to see their full tool usage and reasoning",
            "   - Compare their approach to yours",
            "",
            "2. **THEN CHOOSE ONE**:",
            "   a) VOTE for their answer if it's complete and correct (use vote tool)",
            "   b) BUILD on their work - improve/extend it and submit YOUR enhanced answer",
            "   c) MERGE approaches - combine the best parts of their work with yours",
            "   d) CONTINUE your own approach if you believe it's better",
            "",
            "DO NOT ignore this update - you must explicitly evaluate and decide!",
            "=" * 60,
        ]

        return "\n".join(injection_parts)

    def _on_subagent_complete(
        self,
        parent_agent_id: str,
        subagent_id: str,
        result: "SubagentResult",
    ) -> None:
        """Callback invoked when a background subagent completes.

        This is registered with SubagentManager and called asynchronously when
        any background subagent finishes execution. The result is queued for
        injection into the parent agent's context via SubagentCompleteHook.

        Args:
            parent_agent_id: ID of the parent agent that spawned the subagent
            subagent_id: ID of the completed subagent
            result: The SubagentResult from execution
        """
        if parent_agent_id not in self._pending_subagent_results:
            self._pending_subagent_results[parent_agent_id] = []
        self._pending_subagent_results[parent_agent_id].append((subagent_id, result))
        logger.info(
            f"[Orchestrator] Background subagent {subagent_id} completed for {parent_agent_id} " f"(status={result.status}, success={result.success})",
        )

    def _get_pending_subagent_results(self, agent_id: str) -> List[Tuple[str, "SubagentResult"]]:
        """Get pending subagent results for an agent by polling the MCP server.

        This is called by SubagentCompleteHook to retrieve completed subagent results
        for injection. Polls the subagent MCP server to find completed subagents,
        fetches their results, and tracks which ones have been injected.

        Args:
            agent_id: The agent to get pending results for

        Returns:
            List of (subagent_id, SubagentResult) tuples, or empty list
        """
        try:
            # Get the agent to access its MCP client
            agent = self.agents.get(agent_id)
            if not agent or not hasattr(agent, "mcp_client"):
                return []

            # Initialize injected set for this agent if needed
            if agent_id not in self._injected_subagents:
                self._injected_subagents[agent_id] = set()

            # Poll the subagent MCP server for all subagents
            list_result = agent.mcp_client.call_tool(f"mcp__subagent_{agent_id}__list_subagents", {})

            if not list_result.get("success") or not list_result.get("subagents"):
                return []

            # Find completed subagents that haven't been injected yet
            pending_results = []
            for subagent_info in list_result["subagents"]:
                subagent_id = subagent_info.get("subagent_id")
                status = subagent_info.get("status")

                # Skip if not completed or already injected
                if status != "completed" or subagent_id in self._injected_subagents[agent_id]:
                    continue

                # Fetch the subagent's result
                result_response = agent.mcp_client.call_tool(
                    f"mcp__subagent_{agent_id}__get_subagent_result",
                    {"subagent_id": subagent_id},
                )

                if result_response.get("success") and result_response.get("result"):
                    # Import SubagentResult here to avoid circular import
                    from massgen.subagent.models import SubagentResult

                    result_data = result_response["result"]
                    result = SubagentResult(
                        subagent_id=result_data.get("subagent_id", subagent_id),
                        success=result_data.get("success", False),
                        status=result_data.get("status", "unknown"),
                        answer=result_data.get("answer", ""),
                        error=result_data.get("error"),
                        workspace_path=result_data.get("workspace_path", ""),
                        execution_time_seconds=result_data.get("execution_time_seconds", 0.0),
                        token_usage=result_data.get("token_usage", {}),
                    )

                    pending_results.append((subagent_id, result))
                    # Mark as injected
                    self._injected_subagents[agent_id].add(subagent_id)
                    logger.debug(
                        f"[Orchestrator] Fetched completed subagent {subagent_id} for {agent_id} " f"(status={result.status})",
                    )

            if pending_results:
                logger.debug(
                    f"[Orchestrator] Retrieved {len(pending_results)} completed subagent(s) for {agent_id}",
                )

            return pending_results

        except Exception as e:
            logger.error(f"[Orchestrator] Error polling for completed subagents: {e}", exc_info=True)
            return []

    def _setup_hook_manager_for_agent(
        self,
        agent_id: str,
        agent: ChatAgent,
        answers: Dict[str, str],
    ) -> None:
        """Set up hooks for agent - uses native adapter for Claude Code, GeneralHookManager for others.

        This routes hook setup based on backend capabilities:
        - Backends with native hook support (Claude Code): Use NativeHookAdapter
        - Standard backends: Use GeneralHookManager

        Both paths set up the same hooks:
        1. MidStreamInjectionHook - injects answers from other agents into tool results
        2. HighPriorityTaskReminderHook - reminds to document high-priority task completions

        Args:
            agent_id: The agent identifier
            agent: The ChatAgent instance
            answers: Dict of existing answers when agent started (used to detect new answers)
        """
        # Check if backend supports native hooks (e.g., Claude Code)
        if hasattr(agent.backend, "supports_native_hooks") and agent.backend.supports_native_hooks():
            self._setup_native_hooks_for_agent(agent_id, agent, answers)
            return

        # Fall back to GeneralHookManager for standard backends
        if not hasattr(agent.backend, "set_general_hook_manager"):
            return

        # Create hook manager
        manager = GeneralHookManager()

        # Create mid-stream injection hook with closure-based callback
        mid_stream_hook = MidStreamInjectionHook()

        # Define the injection callback (captures agent_id and answers)
        # This is async to allow copying snapshots before injection
        async def get_injection_content() -> Optional[str]:
            """Check if mid-stream injection is needed and return content."""
            # Skip injection if disabled (multi-agent refinement OFF mode)
            # Agents work independently without seeing each other's work
            if self.config.disable_injection:
                return None

            if not self._check_restart_pending(agent_id):
                return None

            # In vote-only mode, skip injection and force a full restart instead.
            # Mid-stream injection can't update tool schemas, so agents in vote-only mode
            # wouldn't be able to vote for newly discovered answers (the vote enum is fixed
            # at stream start). A full restart gives them updated tool schemas.
            if self._is_vote_only_mode(agent_id):
                return None  # Let restart happen instead

            # Get CURRENT answers from agent_states
            current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}

            # Filter to only NEW answers (ones that didn't exist when this agent started)
            new_answers = {aid: ans for aid, ans in current_answers.items() if aid not in answers}

            if not new_answers:
                # No new answers to inject - agent already has full context.
                # Clear restart_pending since there's nothing new to show them.
                self.agent_states[agent_id].restart_pending = False
                return None

            # TIMING CONSTRAINT: Only use mid-stream injection after first traditional injection
            # This prevents premature convergence where agents immediately adopt the first answer
            if self.agent_states[agent_id].injection_count == 0:
                return None  # Use traditional approach for first injection

            # TIMING CONSTRAINT: Skip injection if too close to soft timeout
            if self._should_skip_injection_due_to_timeout(agent_id):
                return None  # Let restart happen instead

            # Copy snapshots from new answer agents to temp workspace BEFORE building injection
            # This ensures the workspace files are available when the agent tries to access them
            logger.info(
                f"[Orchestrator] Copying snapshots for mid-stream injection to {agent_id}",
            )
            await self._copy_all_snapshots_to_temp_workspace(agent_id)

            # Build injection content (pass existing answers to detect updates vs new)
            injection = self._build_tool_result_injection(
                agent_id,
                new_answers,
                existing_answers=answers,
            )

            # Debug: Log what's in the temp workspace for each injected agent
            viewing_agent = self.agents.get(agent_id)
            if viewing_agent and viewing_agent.backend.filesystem_manager:
                temp_workspace_base = str(
                    viewing_agent.backend.filesystem_manager.agent_temporary_workspace,
                )
                agent_mapping = self.coordination_tracker.get_reverse_agent_mapping()
                for aid in new_answers.keys():
                    anon_id = agent_mapping.get(aid, f"agent_{aid}")
                    workspace_path = os.path.join(temp_workspace_base, anon_id)
                    if os.path.exists(workspace_path):
                        try:
                            files = os.listdir(workspace_path)
                            logger.debug(
                                f"[Orchestrator] Injection workspace {workspace_path} contains: {files}",
                            )
                        except OSError as e:
                            logger.debug(
                                f"[Orchestrator] Could not list workspace {workspace_path}: {e}",
                            )
                    else:
                        logger.debug(
                            f"[Orchestrator] Injection workspace {workspace_path} does NOT exist!",
                        )

            # Clear restart_pending since injection satisfies the update need
            self.agent_states[agent_id].restart_pending = False

            # Increment injection count
            self.agent_states[agent_id].injection_count += 1

            # Update answers to include newly injected answers (prevents re-injection)
            # This mutates the captured closure variable so future callbacks see updated state
            answers.update(new_answers)

            # Update known_answer_ids so vote validation knows this agent has seen these
            self.agent_states[agent_id].known_answer_ids.update(new_answers.keys())

            # Track the injection
            logger.info(
                f"[Orchestrator] Mid-stream injection for {agent_id}: {len(new_answers)} new answer(s)",
            )
            # Log the actual injection content at debug level (may contain sensitive data)
            preview = injection[:2000] + ("..." if len(injection) > 2000 else "")
            logger.debug(f"[Orchestrator] Injection content (truncated):\n{preview}")
            self.coordination_tracker.track_agent_action(
                agent_id,
                ActionType.UPDATE_INJECTED,
                f"Mid-stream: {len(new_answers)} answer(s)",
            )

            # Update agent's context labels
            self.coordination_tracker.update_agent_context_with_new_answers(
                agent_id,
                list(new_answers.keys()),
            )

            return injection

        # Set callback on hook
        mid_stream_hook.set_callback(get_injection_content)

        # Register mid-stream injection hook first (maintains current behavior order)
        manager.register_global_hook(HookType.POST_TOOL_USE, mid_stream_hook)

        # Register high-priority task reminder hook
        reminder_hook = HighPriorityTaskReminderHook()
        manager.register_global_hook(HookType.POST_TOOL_USE, reminder_hook)

        # Register human input hook (shared across all agents)
        # Create on first agent setup, reuse for subsequent agents
        if self._human_input_hook is None:
            self._human_input_hook = HumanInputHook()
            # Share hook with display so TUI can queue input
            self._share_human_input_hook_with_display()
        manager.register_global_hook(HookType.POST_TOOL_USE, self._human_input_hook)

        # Register subagent completion hook for async result injection
        if self._async_subagents_enabled:
            subagent_hook = SubagentCompleteHook(
                injection_strategy=self._async_subagent_injection_strategy,
            )

            # Create a closure that captures agent_id for pending results retrieval
            def make_pending_getter(aid: str):
                return lambda: self._get_pending_subagent_results(aid)

            subagent_hook.set_pending_results_getter(make_pending_getter(agent_id))
            manager.register_global_hook(HookType.POST_TOOL_USE, subagent_hook)
            logger.debug(f"[Orchestrator] Registered SubagentCompleteHook for {agent_id}")
        # Register per-round timeout hooks if configured
        self._register_round_timeout_hooks(agent_id, manager)

        # Register user-configured hooks from agent backend config
        if hasattr(agent.backend, "config") and agent.backend.config:
            agent_hooks = agent.backend.config.get("hooks")
            if agent_hooks:
                manager.register_hooks_from_config(agent_hooks, agent_id=agent_id)
                logger.debug(
                    f"[Orchestrator] Registered user-configured hooks for {agent_id}",
                )

        # Set manager on backend
        agent.backend.set_general_hook_manager(manager)
        logger.debug(
            f"[Orchestrator] Set up hook manager for {agent_id} with mid-stream and reminder hooks",
        )

    def _share_human_input_hook_with_display(self) -> None:
        """Share the human input hook reference with the TUI display.

        This allows the TUI to queue user input for injection during execution.
        Called once when the human input hook is first created.
        """
        if not self._human_input_hook:
            return

        # Get display from coordination_ui if available
        display = None
        if hasattr(self, "coordination_ui") and self.coordination_ui:
            display = getattr(self.coordination_ui, "display", None)

        if not display:
            logger.debug("[Orchestrator] No display available for human input hook sharing")
            return

        # Check if display supports human input hook
        if hasattr(display, "set_human_input_hook"):
            display.set_human_input_hook(self._human_input_hook)
            logger.info("[Orchestrator] Shared human input hook with TUI display")
        else:
            logger.debug("[Orchestrator] Display does not support human input hook")

    def _register_round_timeout_hooks(
        self,
        agent_id: str,
        manager: GeneralHookManager,
    ) -> None:
        """Register per-round timeout hooks if configured.

        This creates two hooks:
        1. RoundTimeoutPostHook (soft timeout) - Injects warning message after tool calls
        2. RoundTimeoutPreHook (hard timeout) - Blocks non-terminal tools after grace period

        The hooks are stored in agent_states so they can be reset when a new round starts.

        Args:
            agent_id: The agent identifier
            manager: The GeneralHookManager to register hooks with
        """
        # Get timeout config
        timeout_config = self.config.timeout_config
        initial_timeout = timeout_config.initial_round_timeout_seconds
        subsequent_timeout = timeout_config.subsequent_round_timeout_seconds
        grace_seconds = timeout_config.round_timeout_grace_seconds

        # Skip if no round timeouts configured
        if initial_timeout is None and subsequent_timeout is None:
            return

        logger.info(
            f"[Orchestrator] Registering round timeout hooks for {agent_id}: " f"initial={initial_timeout}s, subsequent={subsequent_timeout}s, grace={grace_seconds}s",
        )

        # Create closures that read from agent state
        def get_round_start_time() -> float:
            """Get the current round start time from agent state."""
            start_time = self.agent_states[agent_id].round_start_time
            if start_time is None:
                # Fallback to current time if not set (shouldn't happen)
                logger.warning(
                    f"[Orchestrator] round_start_time is None for {agent_id}, using current time as fallback",
                )
                return time.time()
            return start_time

        def get_agent_round() -> int:
            """Get the current round number from coordination tracker."""
            return self.coordination_tracker.get_agent_round(agent_id)

        # Create shared state for coordinating soft -> hard timeout progression
        # This ensures hard timeout only fires AFTER soft timeout has been injected
        timeout_state = RoundTimeoutState()

        # Get two-tier workspace setting from coordination config
        coordination_config = getattr(self.config, "coordination_config", None)
        use_two_tier_workspace = bool(
            getattr(coordination_config, "use_two_tier_workspace", False),
        )

        # Create soft timeout hook (POST_TOOL_USE - injects warning)
        post_hook = RoundTimeoutPostHook(
            name=f"round_timeout_soft_{agent_id}",
            get_round_start_time=get_round_start_time,
            get_agent_round=get_agent_round,
            initial_timeout_seconds=initial_timeout,
            subsequent_timeout_seconds=subsequent_timeout,
            grace_seconds=grace_seconds,
            agent_id=agent_id,
            shared_state=timeout_state,
            use_two_tier_workspace=use_two_tier_workspace,
        )

        # Create hard timeout hook (PRE_TOOL_USE - blocks non-terminal tools)
        pre_hook = RoundTimeoutPreHook(
            name=f"round_timeout_hard_{agent_id}",
            get_round_start_time=get_round_start_time,
            get_agent_round=get_agent_round,
            initial_timeout_seconds=initial_timeout,
            subsequent_timeout_seconds=subsequent_timeout,
            grace_seconds=grace_seconds,
            agent_id=agent_id,
            shared_state=timeout_state,
        )

        # Register hooks
        manager.register_global_hook(HookType.POST_TOOL_USE, post_hook)
        manager.register_global_hook(HookType.PRE_TOOL_USE, pre_hook)

        # Store hook references so we can reset them on new rounds
        self.agent_states[agent_id].round_timeout_hooks = (post_hook, pre_hook)
        # Store the shared state so we can check force_terminate in the orchestrator loop
        self.agent_states[agent_id].round_timeout_state = timeout_state

        logger.debug(f"[Orchestrator] Registered round timeout hooks for {agent_id}")

    def _setup_native_hooks_for_agent(
        self,
        agent_id: str,
        agent: ChatAgent,
        answers: Dict[str, str],
    ) -> None:
        """Set up native hooks for backends that support them (e.g., Claude Code).

        This converts MassGen hooks to the backend's native format using the
        NativeHookAdapter interface. The hooks are then executed natively by
        the backend rather than through MassGen's GeneralHookManager.

        Args:
            agent_id: The agent identifier
            agent: The ChatAgent instance
            answers: Dict of existing answers when agent started (used to detect new answers)
        """
        # Get the native hook adapter from the backend
        adapter = agent.backend.get_native_hook_adapter()
        if not adapter:
            logger.warning(
                f"[Orchestrator] Backend supports native hooks but adapter unavailable for {agent_id}",
            )
            return

        # Create a GeneralHookManager to hold MassGen hooks
        # (We'll convert these to native format)
        manager = GeneralHookManager()

        # Create mid-stream injection hook with closure-based callback
        mid_stream_hook = MidStreamInjectionHook()

        # Define the injection callback (same logic as GeneralHookManager path)
        async def get_injection_content() -> Optional[str]:
            """Check if mid-stream injection is needed and return content."""
            if not self._check_restart_pending(agent_id):
                return None

            # In vote-only mode, skip injection and force a full restart instead.
            if self._is_vote_only_mode(agent_id):
                return None

            # Get CURRENT answers from agent_states
            current_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}

            # Filter to only NEW answers
            new_answers = {aid: ans for aid, ans in current_answers.items() if aid not in answers}

            if not new_answers:
                # No new answers to inject - agent already has full context.
                # Clear restart_pending since there's nothing new to show them.
                self.agent_states[agent_id].restart_pending = False
                return None

            # TIMING CONSTRAINT: Only use mid-stream injection after first traditional injection
            if self.agent_states[agent_id].injection_count == 0:
                return None

            # TIMING CONSTRAINT: Skip injection if too close to soft timeout
            if self._should_skip_injection_due_to_timeout(agent_id):
                return None  # Let restart happen instead

            # Copy snapshots from new answer agents to temp workspace
            logger.info(
                f"[Orchestrator] Copying snapshots for mid-stream injection to {agent_id}",
            )
            await self._copy_all_snapshots_to_temp_workspace(agent_id)

            # Build injection content
            injection = self._build_tool_result_injection(
                agent_id,
                new_answers,
                existing_answers=answers,
            )

            # Clear restart_pending since injection satisfies the update need
            self.agent_states[agent_id].restart_pending = False

            # Increment injection count
            self.agent_states[agent_id].injection_count += 1

            # Update answers to include newly injected answers (prevents re-injection)
            # This mutates the captured closure variable so future callbacks see updated state
            answers.update(new_answers)

            # Update known_answer_ids so vote validation knows this agent has seen these
            self.agent_states[agent_id].known_answer_ids.update(new_answers.keys())

            # Track the injection
            logger.info(
                f"[Orchestrator] Mid-stream injection (native) for {agent_id}: {len(new_answers)} new answer(s)",
            )
            self.coordination_tracker.track_agent_action(
                agent_id,
                ActionType.UPDATE_INJECTED,
                f"Mid-stream (native): {len(new_answers)} answer(s)",
            )

            # Update agent's context labels
            self.coordination_tracker.update_agent_context_with_new_answers(
                agent_id,
                list(new_answers.keys()),
            )

            return injection

        # Set callback on hook
        mid_stream_hook.set_callback(get_injection_content)

        # Register mid-stream injection hook
        manager.register_global_hook(HookType.POST_TOOL_USE, mid_stream_hook)

        # Register high-priority task reminder hook
        reminder_hook = HighPriorityTaskReminderHook()
        manager.register_global_hook(HookType.POST_TOOL_USE, reminder_hook)

        # Register human input hook (shared across all agents)
        if self._human_input_hook is None:
            self._human_input_hook = HumanInputHook()
            self._share_human_input_hook_with_display()
        manager.register_global_hook(HookType.POST_TOOL_USE, self._human_input_hook)

        # Register subagent completion hook for async result injection
        if self._async_subagents_enabled:
            subagent_hook = SubagentCompleteHook(
                injection_strategy=self._async_subagent_injection_strategy,
            )

            # Create a closure that captures agent_id for pending results retrieval
            def make_pending_getter(aid: str):
                return lambda: self._get_pending_subagent_results(aid)

            subagent_hook.set_pending_results_getter(make_pending_getter(agent_id))
            manager.register_global_hook(HookType.POST_TOOL_USE, subagent_hook)
            logger.debug(f"[Orchestrator] Registered SubagentCompleteHook (native) for {agent_id}")
        # Register per-round timeout hooks if configured
        self._register_round_timeout_hooks(agent_id, manager)

        # Register user-configured hooks from agent backend config
        agent_hooks = agent.backend.config.get("hooks")
        if agent_hooks:
            manager.register_hooks_from_config(agent_hooks, agent_id=agent_id)

        # Create context factory for hooks
        def context_factory() -> Dict[str, Any]:
            return {
                "session_id": getattr(self, "session_id", ""),
                "orchestrator_id": getattr(self, "orchestrator_id", ""),
                "agent_id": agent_id,
            }

        # Convert to native format using adapter
        native_config = adapter.build_native_hooks_config(
            manager,
            agent_id=agent_id,
            context_factory=context_factory,
        )

        # Set native hooks config on backend
        agent.backend.set_native_hooks_config(native_config)
        logger.info(
            f"[Orchestrator] Set up native hooks for {agent_id}: " f"PreToolUse={len(native_config.get('PreToolUse', []))}, " f"PostToolUse={len(native_config.get('PostToolUse', []))} hooks",
        )

    def _normalize_workspace_paths_in_answers(
        self,
        answers: Dict[str, str],
        viewing_agent_id: Optional[str] = None,
    ) -> Dict[str, str]:
        """Normalize absolute workspace paths in agent answers to accessible temporary workspace paths.

        This addresses the issue where agents working in separate workspace directories
        reference the same logical files using different absolute paths, causing them
        to think they're working on different tasks when voting.

        Converts workspace paths to temporary workspace paths where the viewing agent can actually
        access other agents' files for verification during context sharing.

        TODO: Replace with Docker volume mounts to ensure consistent paths across agents.

        Args:
            answers: Dict mapping agent_id to their answer content
            viewing_agent_id: The agent who will be reading these answers.
                            If None, normalizes to generic "workspace/" prefix.

        Returns:
            Dict with same keys but normalized answer content with accessible paths
        """
        normalized_answers = {}

        # Get viewing agent's temporary workspace path for context sharing (full absolute path)
        temp_workspace_base = None
        if viewing_agent_id:
            viewing_agent = self.agents.get(viewing_agent_id)
            if viewing_agent and viewing_agent.backend.filesystem_manager:
                temp_workspace_base = str(
                    viewing_agent.backend.filesystem_manager.agent_temporary_workspace,
                )
        # Create anonymous agent mapping for consistent directory names
        agent_mapping = self.coordination_tracker.get_reverse_agent_mapping()

        for agent_id, answer in answers.items():
            normalized_answer = answer

            # Replace all workspace paths found in the answer with accessible paths
            for other_agent_id, other_agent in self.agents.items():
                if not other_agent.backend.filesystem_manager:
                    continue

                anon_agent_id = agent_mapping.get(
                    other_agent_id,
                    f"agent_{other_agent_id}",
                )
                replace_path = os.path.join(temp_workspace_base, anon_agent_id) if temp_workspace_base else anon_agent_id
                other_workspace = str(
                    other_agent.backend.filesystem_manager.get_current_workspace(),
                )
                logger.debug(
                    f"[Orchestrator._normalize_workspace_paths_in_answers] Replacing {other_workspace} in answer from {agent_id} with path {replace_path}. original answer: {normalized_answer}",
                )
                normalized_answer = normalized_answer.replace(
                    other_workspace,
                    replace_path,
                )
                logger.debug(
                    f"[Orchestrator._normalize_workspace_paths_in_answers] Intermediate normalized answer: {normalized_answer}",
                )

            normalized_answers[agent_id] = normalized_answer

        return normalized_answers

    def _normalize_workspace_paths_for_comparison(
        self,
        content: str,
        replacement_path: str = "/workspace",
    ) -> str:
        """
        Normalize all workspace paths in content to a canonical form for equality comparison.

        Unlike _normalize_workspace_paths_in_answers which normalizes paths for specific agents,
        this method normalizes ALL workspace paths to a neutral canonical form (like '/workspace')
        so that content can be compared for equality regardless of which agent workspace it came from.

        Args:
            content: Content that may contain workspace paths

        Returns:
            Content with all workspace paths normalized to canonical form
        """
        normalized_content = content

        # Replace all agent workspace paths with canonical '/workspace/'
        for _, agent in self.agents.items():
            if not agent.backend.filesystem_manager:
                continue

            # Get this agent's workspace path
            workspace_path = str(
                agent.backend.filesystem_manager.get_current_workspace(),
            )
            normalized_content = normalized_content.replace(
                workspace_path,
                replacement_path,
            )

        return normalized_content

    def _flush_pending_subagent_results(self) -> None:
        """Flush any pending subagent results before coordination ends.

        Called during cleanup to log warnings about subagents that completed
        after the parent agent finished, or are still running when coordination ends.
        """
        if not hasattr(self, "_pending_subagent_results"):
            return

        for agent_id, pending in self._pending_subagent_results.items():
            if pending:
                logger.warning(
                    f"[Orchestrator] {len(pending)} async subagent result(s) for {agent_id} " f"were not delivered (parent finished before injection). " f"IDs: {[p[0] for p in pending]}",
                )
                # Clear the pending results since they won't be delivered
                self._pending_subagent_results[agent_id] = []

    async def _cleanup_active_coordination(self) -> None:
        """Force cleanup of active coordination streams and tasks on timeout."""
        # Flush any pending subagent results that weren't delivered
        self._flush_pending_subagent_results()

        # Cancel and cleanup active tasks
        if hasattr(self, "_active_tasks") and self._active_tasks:
            # Gracefully interrupt Claude Code backends before cancelling tasks
            for agent_id, agent in self.agents.items():
                if hasattr(agent, "backend") and hasattr(agent.backend, "interrupt"):
                    try:
                        await agent.backend.interrupt()
                    except Exception:
                        pass
            for agent_id, task in self._active_tasks.items():
                if not task.done():
                    # Only track if not already tracked by timeout above
                    if not self.is_orchestrator_timeout:
                        self.coordination_tracker.track_agent_action(
                            agent_id,
                            ActionType.CANCELLED,
                            "Coordination cleanup",
                        )
                    task.cancel()
                    try:
                        await task
                    except (asyncio.CancelledError, Exception):
                        pass  # Ignore cleanup errors
            self._active_tasks.clear()

        # Close active streams
        if hasattr(self, "_active_streams") and self._active_streams:
            for agent_id in list(self._active_streams.keys()):
                await self._close_agent_stream(agent_id, self._active_streams)

    async def _cleanup_background_shells_for_agent(self, agent_id: str) -> None:
        """Clean up background shells started by this agent at round end.

        Uses MCP tools to list and kill shells, since background shells run in
        the MCP subprocess (not the main orchestrator process).

        Args:
            agent_id: The agent identifier
        """
        agent = self.agents.get(agent_id)
        if not agent or not hasattr(agent.backend, "_mcp_client") or not agent.backend._mcp_client:
            return

        mcp_client = agent.backend._mcp_client

        try:
            # List all background shells via MCP tool
            list_result = await mcp_client.call_tool(
                "mcp__command_line__list_background_shells",
                {},
            )

            if not list_result or not isinstance(list_result, dict):
                return

            shells = list_result.get("shells", [])
            if not shells:
                return

            # Kill each running shell
            for shell_info in shells:
                shell_id = shell_info.get("shell_id")
                status = shell_info.get("status")

                if shell_id and status == "running":
                    try:
                        await mcp_client.call_tool(
                            "mcp__command_line__kill_background_shell",
                            {"shell_id": shell_id},
                        )
                        logger.info(
                            f"[Orchestrator] Killed background shell {shell_id} at round end for {agent_id}",
                        )
                    except Exception as e:
                        logger.warning(f"[Orchestrator] Failed to kill shell {shell_id}: {e}")

        except Exception as e:
            # MCP tool not available or other error - not critical
            logger.debug(f"[Orchestrator] Could not clean up background shells for {agent_id}: {e}")

    # TODO (v0.0.14 Context Sharing Enhancement - See docs/dev_notes/v0.0.14-context.md):
    # Add the following permission validation methods:
    # async def validate_agent_access(self, agent_id: str, resource_path: str, access_type: str) -> bool:
    #     """Check if agent has required permission for resource.
    #
    #     Args:
    #         agent_id: ID of the agent requesting access
    #         resource_path: Path to the resource being accessed
    #         access_type: Type of access (read, write, read-write, execute)
    #
    #     Returns:
    #         bool: True if access is allowed, False otherwise
    #     """
    #     # Implementation will check against PermissionManager
    #     pass

    def _calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """Calculate Jaccard similarity between two texts based on word tokens.

        Args:
            text1: First text to compare
            text2: Second text to compare

        Returns:
            Similarity score between 0.0 and 1.0
        """
        # Tokenize and normalize - simple word-based approach
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0  # Both empty, consider identical
        if not words1 or not words2:
            return 0.0  # One empty, one not

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0

    def _check_answer_novelty(
        self,
        new_answer: str,
        existing_answers: Dict[str, str],
    ) -> tuple[bool, Optional[str]]:
        """Check if a new answer is sufficiently different from existing answers.

        Args:
            new_answer: The proposed new answer
            existing_answers: Dictionary of existing answers {agent_id: answer_content}

        Returns:
            Tuple of (is_novel, error_message). is_novel=True if answer passes novelty check.
        """
        # Lenient mode: no checks (current behavior)
        if self.config.answer_novelty_requirement == "lenient":
            return (True, None)

        # Determine threshold based on setting
        if self.config.answer_novelty_requirement == "strict":
            threshold = 0.50  # Reject if >50% overlap (strict)
            error_msg = (
                "Your answer is too similar to existing answers (>50% overlap). Please use a fundamentally different approach, employ different tools/techniques, or vote for an existing answer."
            )
        else:  # balanced
            threshold = 0.70  # Reject if >70% overlap (balanced)
            error_msg = (
                "Your answer is too similar to existing answers (>70% overlap). "
                "Please provide a meaningfully different solution with new insights, "
                "approaches, or tools, or vote for an existing answer."
            )

        # Check similarity against all existing answers
        for agent_id, existing_answer in existing_answers.items():
            similarity = self._calculate_jaccard_similarity(new_answer, existing_answer)
            if similarity > threshold:
                logger.info(
                    f"[Orchestrator] Answer rejected: {similarity:.2%} similar to {agent_id}'s answer (threshold: {threshold:.0%})",
                )
                return (False, error_msg)

        # Answer is sufficiently novel
        return (True, None)

    def _check_answer_count_limit(self, agent_id: str) -> tuple[bool, Optional[str]]:
        """Check if agent has reached their answer count limit.

        Args:
            agent_id: The agent attempting to provide a new answer

        Returns:
            Tuple of (can_answer, error_message). can_answer=True if agent can provide another answer.
        """
        # No limit set
        if self.config.max_new_answers_per_agent is None:
            return (True, None)

        # Count how many answers this agent has provided
        answer_count = len(self.coordination_tracker.answers_by_agent.get(agent_id, []))

        if answer_count >= self.config.max_new_answers_per_agent:
            error_msg = f"You've reached the maximum of {self.config.max_new_answers_per_agent} new answer(s). Please vote for the best existing answer using the `vote` tool."
            logger.info(
                f"[Orchestrator] Answer rejected: {agent_id} has reached limit ({answer_count}/{self.config.max_new_answers_per_agent})",
            )
            return (False, error_msg)

        return (True, None)

    def _is_vote_only_mode(self, agent_id: str) -> bool:
        """Check if agent has exhausted their answer limit and must vote.

        When an agent reaches max_new_answers_per_agent, they should only
        have the vote tool available (no new_answer or broadcast tools).

        When defer_voting_until_all_answered=True, also requires ALL agents
        to have answered before voting is allowed.

        Args:
            agent_id: The agent to check

        Returns:
            True if agent must vote (has hit answer limit AND can vote now), False otherwise.
        """
        if self.config.max_new_answers_per_agent is None:
            return False
        answer_count = len(self.coordination_tracker.answers_by_agent.get(agent_id, []))
        hit_answer_limit = answer_count >= self.config.max_new_answers_per_agent

        if not hit_answer_limit:
            return False

        # If defer_voting_until_all_answered is enabled, also check that all agents have answered
        if self.config.defer_voting_until_all_answered:
            all_answered = all(state.answer is not None for state in self.agent_states.values())
            if not all_answered:
                # Agent hit their limit but others haven't answered yet
                # Return False - agent is in "waiting" state, handled by _is_waiting_for_all_answers
                return False

        return True

    def _is_waiting_for_all_answers(self, agent_id: str) -> bool:
        """Check if agent is waiting for all agents to answer before voting.

        This happens when defer_voting_until_all_answered=True and this agent
        has hit their answer limit but other agents haven't answered yet.

        Args:
            agent_id: The agent to check

        Returns:
            True if agent should wait (not run), False otherwise.
        """
        if not self.config.defer_voting_until_all_answered:
            return False

        if self.config.max_new_answers_per_agent is None:
            return False

        answer_count = len(self.coordination_tracker.answers_by_agent.get(agent_id, []))
        hit_answer_limit = answer_count >= self.config.max_new_answers_per_agent

        if not hit_answer_limit:
            return False

        # Check if all agents have answered
        all_answered = all(state.answer is not None for state in self.agent_states.values())
        if all_answered:
            return False  # Can proceed to voting

        logger.debug(
            f"[defer_voting] {agent_id} waiting for all agents to answer before voting",
        )
        return True

    def _get_buffer_content(self, agent: "ChatAgent") -> tuple[Optional[str], int]:
        """Get streaming buffer content from agent backend for enforcement tracking.

        Returns:
            Tuple of (buffer_preview: first 500 chars or None, buffer_chars: total char count)
        """
        buffer_content = None
        buffer_chars = 0

        if hasattr(agent.backend, "_get_streaming_buffer"):
            buffer_content = agent.backend._get_streaming_buffer()
            if buffer_content:
                buffer_chars = len(buffer_content)
                # Truncate preview to 500 chars
                buffer_content = buffer_content[:500] if len(buffer_content) > 500 else buffer_content

        return buffer_content, buffer_chars

    def _save_docker_logs_on_mcp_failure(
        self,
        agent: "ChatAgent",
        agent_id: str,
        mcp_status: str,
    ) -> None:
        """Save Docker container logs when MCP failure is detected.

        This helps debug why Docker-based MCP servers disconnect by capturing
        container state and logs at the time of failure.

        Args:
            agent: The ChatAgent instance.
            agent_id: Agent identifier.
            mcp_status: The MCP status that triggered this (e.g., 'mcp_tools_failed').
        """
        try:
            # Check if agent uses Docker mode
            if not hasattr(agent, "backend") or not hasattr(
                agent.backend,
                "filesystem_manager",
            ):
                return

            fm = agent.backend.filesystem_manager
            if not fm or not hasattr(fm, "docker_manager") or not fm.docker_manager:
                return

            docker_manager = fm.docker_manager

            # Get container health info
            health = docker_manager.get_container_health(agent_id)
            if not health.get("exists"):
                logger.warning(
                    f"[Docker] Container not found for {agent_id} during MCP failure - may have been cleaned up",
                )
                return

            # Log container health status
            logger.info(
                f"[Docker] Container health for {agent_id} during MCP failure ({mcp_status}): "
                f"status={health.get('status')}, running={health.get('running')}, "
                f"exit_code={health.get('exit_code')}, oom_killed={health.get('oom_killed')}, "
                f"error={health.get('error')}",
            )

            # Save logs to the session log directory
            from .logger_config import get_log_session_dir

            log_dir = get_log_session_dir()
            if log_dir:
                import time

                timestamp = time.strftime("%H%M%S")
                log_filename = f"docker_logs_{agent_id}_{mcp_status}_{timestamp}.txt"
                log_path = log_dir / log_filename
                docker_manager.save_container_logs(agent_id, log_path, tail=500)

        except (OSError, AttributeError, KeyError) as e:
            # OSError: File I/O errors when saving logs
            # AttributeError: Missing attributes on agent/backend/manager objects
            # KeyError: Missing dict keys in health info
            logger.warning(
                f"[Docker] Failed to save container logs on MCP failure: {e}",
            )

    def _get_docker_health(
        self,
        agent: "ChatAgent",
        agent_id: str,
    ) -> Optional[Dict[str, Any]]:
        """Get Docker container health info for reliability metrics.

        Args:
            agent: The ChatAgent instance.
            agent_id: Agent identifier.

        Returns:
            Docker health dict or None if not using Docker.
        """
        try:
            if not hasattr(agent, "backend") or not hasattr(
                agent.backend,
                "filesystem_manager",
            ):
                return None

            fm = agent.backend.filesystem_manager
            if not fm or not hasattr(fm, "docker_manager") or not fm.docker_manager:
                return None

            return fm.docker_manager.get_container_health(agent_id)
        except (AttributeError, KeyError) as e:
            # AttributeError: Missing attributes on agent/backend/manager objects
            # KeyError: Missing dict keys when accessing container state
            logger.debug(f"[Docker] Failed to get container health: {e}")
            return None

    def _create_tool_error_messages(
        self,
        agent: "ChatAgent",
        tool_calls: List[Dict[str, Any]],
        primary_error_msg: str,
        secondary_error_msg: str = None,
    ) -> List[Dict[str, Any]]:
        """
        Create tool error messages for all tool calls in a response.

        Args:
            agent: The ChatAgent instance for backend access
            tool_calls: List of tool calls that need error responses
            primary_error_msg: Error message for the first tool call
            secondary_error_msg: Error message for additional tool calls (defaults to primary_error_msg)

        Returns:
            List of tool result messages that can be sent back to the agent
        """
        if not tool_calls:
            return []

        if secondary_error_msg is None:
            secondary_error_msg = primary_error_msg

        enforcement_msgs = []

        # Send primary error for the first tool call
        first_tool_call = tool_calls[0]
        error_result_msg = agent.backend.create_tool_result_message(
            first_tool_call,
            primary_error_msg,
        )
        # Handle both single dict (Chat Completions) and list (Response API) returns
        if isinstance(error_result_msg, list):
            enforcement_msgs.extend(error_result_msg)
        else:
            enforcement_msgs.append(error_result_msg)

        # Send secondary error messages for any additional tool calls (API requires response to ALL calls)
        for additional_tool_call in tool_calls[1:]:
            neutral_msg = agent.backend.create_tool_result_message(
                additional_tool_call,
                secondary_error_msg,
            )
            # Handle both single dict (Chat Completions) and list (Response API) returns
            if isinstance(neutral_msg, list):
                enforcement_msgs.extend(neutral_msg)
            else:
                enforcement_msgs.append(neutral_msg)

        return enforcement_msgs

    def _load_rate_limits_from_config(self) -> Dict[str, Dict[str, int]]:
        """
        Load rate limits from centralized configuration file.

        Converts RPM (Requests Per Minute) values from rate_limits.yaml
        into agent startup rate limits for the orchestrator.

        Returns:
            Dictionary mapping model names to rate limit configs:
            {"model-name": {"max_starts": N, "time_window": 60}}
        """
        rate_limits = {}

        try:
            config = get_rate_limit_config()

            # Load Gemini models
            gemini_models = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini"]
            for model in gemini_models:
                limits = config.get_limits("gemini", model, use_defaults=True)
                rpm = limits.get("rpm")

                if rpm:
                    # Use RPM directly as max_starts for conservative limiting
                    # For very limited models (rpm <= 2), be extra conservative
                    if rpm <= 2:
                        max_starts = 1  # Very conservative for Pro (actual: 2 RPM)
                    elif rpm <= 10:
                        max_starts = max(1, rpm - 1)  # Conservative buffer
                    else:
                        max_starts = rpm

                    rate_limits[model] = {
                        "max_starts": max_starts,
                        "time_window": 60,  # Always use 60s window (1 minute)
                    }
                    logger.info(
                        f"[Orchestrator] Loaded rate limit for {model}: " f"{max_starts} starts/min (from RPM: {rpm})",
                    )

            # Fallback defaults if config loading failed
            if not rate_limits:
                logger.warning(
                    "[Orchestrator] No rate limits loaded from config, using fallback defaults",
                )
                rate_limits = {
                    "gemini-2.5-flash": {"max_starts": 9, "time_window": 60},
                    "gemini-2.5-pro": {"max_starts": 2, "time_window": 60},
                    "gemini": {"max_starts": 7, "time_window": 60},
                }

        except Exception as e:
            logger.error(f"[Orchestrator] Failed to load rate limits from config: {e}")
            # Fallback to safe defaults
            rate_limits = {
                "gemini-2.5-flash": {"max_starts": 9, "time_window": 60},
                "gemini-2.5-pro": {"max_starts": 2, "time_window": 60},
                "gemini": {"max_starts": 7, "time_window": 60},
            }

        return rate_limits

    async def _apply_agent_startup_rate_limit(self, agent_id: str) -> None:
        """
        Apply rate limiting for agent startup based on model.

        Ensures that agents using rate-limited models (like Gemini Flash/Pro)
        don't exceed the allowed startup rate.

        Args:
            agent_id: ID of the agent to start
        """
        # Skip rate limiting if not enabled
        if not self._enable_rate_limit:
            return

        agent = self.agents.get(agent_id)
        if not agent or not hasattr(agent, "backend"):
            return

        # Get model name from backend config
        model_key = None
        if hasattr(agent.backend, "config") and isinstance(agent.backend.config, dict):
            model_name = agent.backend.config.get("model", "")
            # Check for specific models first
            if "gemini-2.5-flash" in model_name.lower():
                model_key = "gemini-2.5-flash"
            elif "gemini-2.5-pro" in model_name.lower():
                model_key = "gemini-2.5-pro"
            elif "gemini" in model_name.lower():
                model_key = "gemini"

        # Fallback: try backend type
        if not model_key:
            if hasattr(agent.backend, "get_provider_name"):
                backend_type = agent.backend.get_provider_name()
                if backend_type in self._rate_limits:
                    model_key = backend_type

        # Check if this model has rate limits
        if not model_key or model_key not in self._rate_limits:
            return

        rate_limit = self._rate_limits[model_key]
        max_starts = rate_limit["max_starts"]
        time_window = rate_limit["time_window"]

        # Initialize tracking for this model if needed
        if model_key not in self._agent_startup_times:
            self._agent_startup_times[model_key] = []

        current_time = time.time()
        startup_times = self._agent_startup_times[model_key]

        # Remove timestamps outside the current window
        startup_times[:] = [t for t in startup_times if t > current_time - time_window]

        # If we've hit the limit, wait until the oldest startup falls outside the window
        if len(startup_times) >= max_starts:
            oldest_time = startup_times[0]
            wait_time = (oldest_time + time_window) - current_time

            if wait_time > 0:
                log_orchestrator_activity(
                    self.orchestrator_id,
                    f"Rate limit reached for {model_key}",
                    {
                        "agent_id": agent_id,
                        "model": model_key,
                        "current_starts": len(startup_times),
                        "max_starts": max_starts,
                        "time_window": time_window,
                        "wait_time": round(wait_time, 2),
                    },
                )
                logger.info(
                    f"[Orchestrator] Rate limit: {len(startup_times)}/{max_starts} {model_key} agents " f"started in {time_window}s window. Waiting {wait_time:.2f}s before starting {agent_id}...",
                )

                await asyncio.sleep(wait_time)

                # After waiting, clean up old timestamps again
                current_time = time.time()
                startup_times[:] = [t for t in startup_times if t > current_time - time_window]

        # Record this startup
        startup_times.append(time.time())

        log_orchestrator_activity(
            self.orchestrator_id,
            "Agent startup allowed",
            {
                "agent_id": agent_id,
                "model": model_key,
                "current_starts": len(startup_times),
                "max_starts": max_starts,
            },
        )

        # Add mandatory cooldown after startup to prevent burst API calls
        # This gives the backend rate limiter time to properly queue requests
        cooldown_delays = {
            "gemini-2.5-flash": 3.0,  # 3 second cooldown between Flash agent starts
            "gemini-2.5-pro": 10.0,  # 10 second cooldown between Pro agent starts (very limited!)
            "gemini": 5.0,  # 5 second default cooldown
        }

        if model_key in cooldown_delays:
            cooldown = cooldown_delays[model_key]
            logger.info(
                f"[Orchestrator] Applying {cooldown}s cooldown after starting {agent_id} ({model_key})",
            )
            await asyncio.sleep(cooldown)

    async def _stream_agent_execution(
        self,
        agent_id: str,
        task: str,
        answers: Dict[str, str],
        conversation_context: Optional[Dict[str, Any]] = None,
        paraphrase: Optional[str] = None,
    ) -> AsyncGenerator[tuple, None]:
        """
        Stream agent execution with real-time content and final result.

        Yields:
            ("content", str): Real-time agent output (source attribution added by caller)
            ("result", (type, data)): Final result - ("vote", vote_data) or ("answer", content)
            ("external_tool_calls", List[Dict]): Client-provided tool calls that must be surfaced externally (not executed)
            ("error", str): Error message (self-terminating)
            ("done", None): Graceful completion signal

        Restart Behavior:
            If restart_pending is True, agent gracefully terminates with "done" signal.
            restart_pending is cleared at the beginning of execution.
        """
        agent = self.agents[agent_id]

        # Get backend name for logging
        backend_name = None
        if hasattr(agent, "backend") and hasattr(agent.backend, "get_provider_name"):
            backend_name = agent.backend.get_provider_name()

        log_orchestrator_activity(
            self.orchestrator_id,
            f"Starting agent execution: {agent_id}",
            {
                "agent_id": agent_id,
                "backend": backend_name,
                "task": task if task else None,
                "paraphrased_task": paraphrase,
                "agent_view_task": paraphrase or task,
                "has_answers": bool(answers),
                "num_answers": len(answers) if answers else 0,
            },
        )

        # Add periodic heartbeat logging for stuck agents
        paraphrase_note = " (with DSPy paraphrased question)" if paraphrase else ""
        logger.info(
            f"[Orchestrator] Agent {agent_id} starting execution loop...{paraphrase_note}",
        )

        # Initialize agent state
        self.agent_states[agent_id].is_killed = False
        self.agent_states[agent_id].timeout_reason = None

        # Track whether we've notified TUI of new round (done once per real execution)
        _notified_round = False

        # Set round start time for per-round timeout tracking
        self.agent_states[agent_id].round_start_time = time.time()

        # Reset timeout hooks if they exist (for new round after restart)
        if self.agent_states[agent_id].round_timeout_hooks:
            post_hook, pre_hook = self.agent_states[agent_id].round_timeout_hooks
            post_hook.reset_for_new_round()
            pre_hook.reset_for_new_round()
            logger.debug(f"[Orchestrator] Reset round timeout hooks for {agent_id}")

        # Clean up any background shells from the previous round
        await self._cleanup_background_shells_for_agent(agent_id)

        # Note: Do NOT clear restart_pending here - let the injection logic inside the iteration
        # loop handle it (see line ~1969). This ensures agents receive updates via injection
        # instead of restarting from scratch, even if they haven't started streaming yet.
        # The injection logic will:
        # - Inject new answers if they exist (and continue working)
        # - Clear the flag if no new answers exist (agent already has full context)

        # Copy all agents' snapshots to temp workspace for context sharing
        await self._copy_all_snapshots_to_temp_workspace(agent_id)

        # Clear the agent's workspace to prepare for new execution
        # This preserves the previous agent's output for logging while giving a clean slate
        if agent.backend.filesystem_manager:
            # agent.backend.filesystem_manager.clear_workspace()  # Don't clear for now.
            agent.backend.filesystem_manager.log_current_state("before execution")

            # For single-agent mode with skip_voting (refinement OFF), enable context write access
            # from the START of coordination so the agent can write directly to context paths
            if self.config.skip_voting and self._has_write_context_paths(agent):
                logger.info(
                    f"[Orchestrator] Single-agent mode: enabling context write access from start for {agent_id}",
                )
                # Snapshot BEFORE enabling writes (to track what gets written)
                agent.backend.filesystem_manager.path_permission_manager.snapshot_writable_context_paths()
                agent.backend.filesystem_manager.path_permission_manager.set_context_write_access_enabled(True)

        # Create agent execution span for hierarchical tracing in Logfire
        # This groups all tool calls, LLM calls, and events under this agent's execution
        tracer = get_tracer()
        current_round = self.coordination_tracker.get_agent_round(agent_id)
        context_labels = self.coordination_tracker.get_agent_context_labels(agent_id)
        round_type = "voting" if answers else "initial_answer"

        span_attributes = {
            "massgen.agent_id": agent_id,
            "massgen.iteration": self.coordination_tracker.current_iteration,
            "massgen.round": current_round,
            "massgen.round_type": round_type,
            "massgen.backend": backend_name or "unknown",
            "massgen.num_context_answers": len(answers) if answers else 0,
        }
        if context_labels:
            span_attributes["massgen.context_labels"] = ",".join(context_labels)

        _agent_span_cm = tracer.span(
            f"agent.{agent_id}.round_{current_round}",
            attributes=span_attributes,
        )
        _agent_span = _agent_span_cm.__enter__()  # Capture the yielded span for set_attribute()

        # Set the round context for nested tool calls to use
        set_current_round(current_round, round_type)

        # Track outcome for span attributes (set in finally block)
        _agent_outcome = None  # "vote", "answer", or "error"
        _agent_voted_for = None  # Only set for votes
        _agent_answer_label = None  # Only set for answers (e.g., "agent1.1")
        _agent_voted_for_label = None  # Only set for votes (e.g., "agent2.1")
        _agent_error_message = None  # Only set for errors

        try:
            # Normalize workspace paths in agent answers for better comparison from this agent's perspective
            normalized_answers = self._normalize_workspace_paths_in_answers(answers, agent_id) if answers else answers

            # Log structured context for this agent's round (for observability/debugging)
            # Get agent's log directory path for hybrid access pattern (MAS-199)
            log_session_dir = get_log_session_dir()
            agent_log_path = str(log_session_dir / agent_id) if log_session_dir else None
            log_agent_round_context(
                agent_id=agent_id,
                round_number=current_round,
                round_type=round_type,
                answers_in_context=normalized_answers,
                answer_labels=context_labels,
                agent_log_path=agent_log_path,
            )

            # Log the normalized answers this agent will see
            if normalized_answers:
                logger.info(
                    f"[Orchestrator] Agent {agent_id} sees normalized answers: {normalized_answers}",
                )
            else:
                logger.info(f"[Orchestrator] Agent {agent_id} sees no existing answers")

            # Check if planning mode is enabled for coordination phase
            # Use the ACTUAL backend planning mode status (set by intelligent analysis)
            # instead of the static config setting
            is_coordination_phase = self.workflow_phase == "coordinating"
            planning_mode_enabled = agent.backend.is_planning_mode_enabled() if is_coordination_phase else False

            # Build new structured system message FIRST (before conversation building)
            logger.info(
                f"[Orchestrator] Building structured system message for {agent_id}",
            )
            # Get human Q&A history for context injection (human broadcast mode only)
            human_qa_history = None
            if hasattr(self, "broadcast_channel") and self.broadcast_channel:
                human_qa_history = self.broadcast_channel.get_human_qa_history()

            # Check if agent is in vote-only mode (reached max_new_answers_per_agent)
            # This affects both the system message and available tools
            vote_only_for_system_message = self._is_vote_only_mode(agent_id)
            if vote_only_for_system_message:
                logger.info(
                    f"[Orchestrator] Agent {agent_id} in vote-only mode for system message (answer limit reached)",
                )

            system_message = self._get_system_message_builder().build_coordination_message(
                agent=agent,
                agent_id=agent_id,
                answers=normalized_answers,
                planning_mode_enabled=planning_mode_enabled,
                use_skills=hasattr(self.config.coordination_config, "use_skills") and self.config.coordination_config.use_skills,
                enable_memory=hasattr(
                    self.config.coordination_config,
                    "enable_memory_filesystem_mode",
                )
                and self.config.coordination_config.enable_memory_filesystem_mode,
                enable_task_planning=self.config.coordination_config.enable_agent_task_planning,
                previous_turns=self._previous_turns,
                human_qa_history=human_qa_history,
                vote_only=vote_only_for_system_message,
                agent_mapping=self.coordination_tracker.get_reverse_agent_mapping(),
            )

            # Inject phase-appropriate persona if enabled
            has_seen_answers = bool(normalized_answers)
            persona_text = self._get_persona_for_agent(agent_id, has_seen_answers)
            if persona_text:
                phase = "convergence" if has_seen_answers else "exploration"
                logger.info(f"[Orchestrator] Injecting {phase} persona for {agent_id}")
                system_message = f"{persona_text}\n\n{system_message}"

            logger.info(
                f"[Orchestrator] Structured system message built for {agent_id} (length: {len(system_message)} chars)",
            )

            # Note: Broadcast communication section is now integrated in SystemMessageBuilder
            # as BroadcastCommunicationSection when broadcast is enabled in coordination config

            # Build conversation with context support (for user message and conversation history)
            # We pass the NEW system_message so it gets tracked in context JSONs
            # Sort agent IDs for consistent anonymous mapping with coordination_tracker
            sorted_answer_ids = sorted(normalized_answers.keys()) if normalized_answers else None
            # Get global agent mapping for consistent anonymous IDs across all components
            agent_mapping = self.coordination_tracker.get_reverse_agent_mapping()
            if conversation_context and conversation_context.get(
                "conversation_history",
            ):
                # Use conversation context-aware building
                conversation = self.message_templates.build_conversation_with_context(
                    current_task=task,
                    conversation_history=conversation_context.get(
                        "conversation_history",
                        [],
                    ),
                    agent_summaries=normalized_answers,
                    valid_agent_ids=sorted_answer_ids,
                    base_system_message=system_message,  # Use NEW structured message
                    paraphrase=paraphrase,
                    agent_mapping=agent_mapping,
                )
            else:
                # Fallback to standard conversation building
                conversation = self.message_templates.build_initial_conversation(
                    task=task,
                    agent_summaries=normalized_answers,
                    valid_agent_ids=sorted_answer_ids,
                    base_system_message=system_message,  # Use NEW structured message
                    paraphrase=paraphrase,
                    agent_mapping=agent_mapping,
                )

            # Inject restart context if this is a restart attempt (like multi-turn context)
            if self.restart_reason and self.restart_instructions:
                restart_context = self.message_templates.format_restart_context(
                    self.restart_reason,
                    self.restart_instructions,
                    previous_answer=self.previous_attempt_answer,
                )
                # Prepend restart context to user message
                conversation["user_message"] = restart_context + "\n\n" + conversation["user_message"]

            # Track all the context used for this agent execution
            # Now conversation["system_message"] contains the NEW structured message
            self.coordination_tracker.track_agent_context(
                agent_id,
                answers,
                conversation.get("conversation_history", []),
                conversation,
            )

            # Notify display of context received (for TUI to show context labels)
            if answers:
                context_labels = self.coordination_tracker.get_agent_context_labels(agent_id)
                if context_labels and hasattr(self, "display") and self.display and hasattr(self.display, "notify_context_received"):
                    self.display.notify_context_received(agent_id, context_labels)

            # Store the context in agent state for later use when saving snapshots
            self.agent_states[agent_id].last_context = conversation

            # Log the messages being sent to the agent with backend info
            backend_name = None
            if hasattr(agent, "backend") and hasattr(
                agent.backend,
                "get_provider_name",
            ):
                backend_name = agent.backend.get_provider_name()

            log_orchestrator_agent_message(
                agent_id,
                "SEND",
                {
                    "system": conversation["system_message"],  # NEW structured message logged
                    "user": conversation["user_message"],
                },
                backend_name=backend_name,
            )

            # Set planning mode on the agent's backend to control MCP tool execution
            if hasattr(agent.backend, "set_planning_mode"):
                agent.backend.set_planning_mode(planning_mode_enabled)
                if planning_mode_enabled:
                    logger.info(
                        f"[Orchestrator] Backend planning mode ENABLED for {agent_id} - MCP tools blocked",
                    )
                else:
                    logger.info(
                        f"[Orchestrator] Backend planning mode DISABLED for {agent_id} - MCP tools allowed",
                    )

            # Set up hook manager for mid-stream injection and reminder extraction
            self._setup_hook_manager_for_agent(agent_id, agent, answers)

            # Build proper conversation messages with system + user messages
            max_attempts = 3

            # Add broadcast guidance if enabled
            if self.config.coordination_config.broadcast and self.config.coordination_config.broadcast is not False:
                # Use blocking mode for both agents and human (priority system prevents deadlocks)
                broadcast_mode = self.config.coordination_config.broadcast
                wait_by_default = True
                broadcast_sensitivity = getattr(
                    self.config.coordination_config,
                    "broadcast_sensitivity",
                    "medium",
                )

                broadcast_guidance = self.message_templates.get_broadcast_guidance(
                    broadcast_mode=broadcast_mode,
                    wait_by_default=wait_by_default,
                    sensitivity=broadcast_sensitivity,
                )
                system_message = system_message + broadcast_guidance
                logger.info(
                    f"ðŸ“¢ [{agent_id}] Added broadcast guidance to system message",
                )

            conversation_messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": conversation["user_message"]},
            ]

            # Inject shared memory context
            conversation_messages = await self._inject_shared_memory_context(
                conversation_messages,
                agent_id,
            )

            enforcement_msg = self.message_templates.enforcement_message()

            # Update agent status to STREAMING
            self.coordination_tracker.change_status(agent_id, AgentStatus.STREAMING)

            # Start round token tracking for this agent
            # Note: round_type was computed earlier as "voting" if answers else "initial_answer"
            current_round = self.coordination_tracker.get_agent_round(agent_id)
            if hasattr(agent.backend, "start_round_tracking"):
                agent.backend.start_round_tracking(
                    round_number=current_round,
                    round_type=round_type,  # Use computed round_type (voting or initial_answer)
                    agent_id=agent_id,
                )

            # Use while loop for retry attempts
            attempt = 0
            is_first_real_attempt = True  # Track first LLM call separately from attempt counter
            while attempt < max_attempts:
                logger.info(
                    f"[Orchestrator] Agent {agent_id} workflow enforcement attempt {attempt + 1}/{max_attempts}",
                )

                if self._check_restart_pending(agent_id):
                    logger.info(
                        f"[Orchestrator] Agent {agent_id} has restart_pending flag",
                    )

                    # Clear framework MCP state before restart (e.g., task plans)
                    await self._clear_framework_mcp_state(agent_id)

                    # In vote-only mode, always restart to get updated tool schemas.
                    # Mid-stream injection can't update the vote enum, so we need a full restart.
                    if self._is_vote_only_mode(agent_id):
                        logger.info(
                            f"[Orchestrator] Agent {agent_id} in vote-only mode - forcing restart for updated vote options",
                        )
                        self.agent_states[agent_id].restart_pending = False
                        yield ("done", None)
                        return

                    # Check if this is the first time agent sees a new answer
                    if self.agent_states[agent_id].injection_count == 0:
                        # First time seeing a new answer - restart normally
                        # The mid-stream callback will handle subsequent answers via tool results
                        logger.info(
                            f"[Orchestrator] Agent {agent_id} restarting normally (first new answer)",
                        )
                        self.agent_states[agent_id].restart_pending = False
                        self.agent_states[agent_id].injection_count += 1
                        # Signal completion so coordination loop restarts agent with updated context
                        # Note: agent_restart notification is yielded at the top of _stream_agent_execution
                        yield ("done", None)
                        return
                    # else: injection_count >= 1, mid-stream callback will handle via tool results
                    # Do NOT clear restart_pending here - the callback checks this flag
                    # and will clear it after injecting content (see get_injection_content)

                # Track restarts for TUI round display - only when agent is about to do real work
                # (not if it's exiting immediately due to restart_pending)
                if not _notified_round:
                    _notified_round = True
                    self.agent_states[agent_id].restart_count += 1
                    current_round = self.agent_states[agent_id].restart_count

                    # If this is a restart (round > 1), notify the UI to show fresh timeline
                    if current_round > 1:
                        logger.info(
                            f"[Orchestrator] Agent {agent_id} starting round {current_round} (restart)",
                        )
                        yield (
                            "agent_restart",
                            {
                                "agent_id": agent_id,
                                "round": current_round,
                            },
                        )

                # TODO: Need to still log this redo enforcement msg in the context.txt, and this & others in the coordination tracker.

                # Determine which workflow tools to use for this agent
                # If agent has hit answer limit, only provide vote tool (no new_answer/broadcast)
                vote_only = self._is_vote_only_mode(agent_id)
                if vote_only:
                    # Sort agent IDs for consistent anonymous mapping with coordination_tracker
                    # Get agents with answers using global numbering for vote enum
                    anon_ids_with_answers = self.coordination_tracker.get_agents_with_answers_anon(answers) if answers else None
                    agent_workflow_tools = get_workflow_tools(
                        valid_agent_ids=sorted(self.agents.keys()),
                        template_overrides=getattr(
                            self.message_templates,
                            "_template_overrides",
                            {},
                        ),
                        api_format="chat_completions",
                        vote_only=True,
                        anon_agent_ids=anon_ids_with_answers,
                    )
                    logger.info(
                        f"[Orchestrator] Agent {agent_id} in vote-only mode (answer limit reached)",
                    )
                else:
                    agent_workflow_tools = self.workflow_tools

                # Combined tools: per-agent workflow tools + any client-provided external tools
                combined_tools = list(agent_workflow_tools) + (list(self._external_tools) if self._external_tools else [])

                if is_first_real_attempt:
                    # First attempt: orchestrator provides initial conversation
                    # But we need the agent to have this in its history for subsequent calls
                    # First attempt: provide complete conversation and reset agent's history
                    # Pass current turn and previous winners for memory sharing
                    chat_stream = agent.chat(
                        conversation_messages,
                        combined_tools,
                        reset_chat=True,
                        current_stage=CoordinationStage.INITIAL_ANSWER,
                        orchestrator_turn=self._current_turn + 1,  # Next turn number
                        previous_winners=self._winning_agents_history.copy(),
                        vote_only=vote_only,  # Pass vote-only flag for Gemini schema
                    )
                    is_first_real_attempt = False  # Only first LLM call uses this path
                else:
                    # Subsequent attempts: send enforcement message (set by error handling)

                    # Log enforcement message preview before sending to chat
                    if isinstance(enforcement_msg, list):
                        msg_preview = str(enforcement_msg)[:500]
                        logger.info(
                            f"[Orchestrator] Sending enforcement message to {agent_id} (list, {len(enforcement_msg)} items): {msg_preview}...",
                        )
                    else:
                        msg_preview = enforcement_msg[:500] if len(enforcement_msg) > 500 else enforcement_msg
                        logger.info(
                            f"[Orchestrator] Sending enforcement message to {agent_id} ({len(enforcement_msg)} chars): {msg_preview}...",
                        )

                    if isinstance(enforcement_msg, list):
                        # Tool message array
                        chat_stream = agent.chat(
                            enforcement_msg,
                            combined_tools,
                            reset_chat=False,
                            current_stage=CoordinationStage.ENFORCEMENT,
                            orchestrator_turn=self._current_turn + 1,
                            previous_winners=self._winning_agents_history.copy(),
                            vote_only=vote_only,  # Pass vote-only flag for Gemini schema
                        )
                    else:
                        # Single user message
                        enforcement_message = {
                            "role": "user",
                            "content": enforcement_msg,
                        }
                        chat_stream = agent.chat(
                            [enforcement_message],
                            combined_tools,
                            reset_chat=False,
                            current_stage=CoordinationStage.ENFORCEMENT,
                            orchestrator_turn=self._current_turn + 1,
                            previous_winners=self._winning_agents_history.copy(),
                            vote_only=vote_only,  # Pass vote-only flag for Gemini schema
                        )
                response_text = ""
                tool_calls = []
                workflow_tool_found = False
                # Determine internal tool names for this run (uses agent-specific tools to respect vote-only mode).
                internal_tool_names = {(t.get("function", {}) or {}).get("name") for t in (agent_workflow_tools or []) if isinstance(t, dict)}

                logger.info(
                    f"[Orchestrator] Agent {agent_id} starting to stream chat response...",
                )

                async for chunk in chat_stream:
                    chunk_type = self._get_chunk_type_value(chunk)
                    if chunk_type == "content":
                        response_text += chunk.content
                        # In strict mode, agent content during coordination goes to traces
                        # Only final presentation content should be the actual response
                        if self.trace_classification == "strict":
                            yield ("coordination", chunk.content)
                        else:
                            yield ("content", chunk.content)
                        # Log received content
                        backend_name = None
                        if hasattr(agent, "backend") and hasattr(
                            agent.backend,
                            "get_provider_name",
                        ):
                            backend_name = agent.backend.get_provider_name()
                        log_orchestrator_agent_message(
                            agent_id,
                            "RECV",
                            {"content": chunk.content},
                            backend_name=backend_name,
                        )
                    elif chunk_type in [
                        "reasoning",
                        "reasoning_done",
                        "reasoning_summary",
                        "reasoning_summary_done",
                    ]:
                        # Stream reasoning content as tuple format
                        reasoning_chunk = StreamChunk(
                            type=chunk.type,
                            content=chunk.content,
                            source=agent_id,
                            reasoning_delta=getattr(chunk, "reasoning_delta", None),
                            reasoning_text=getattr(chunk, "reasoning_text", None),
                            reasoning_summary_delta=getattr(
                                chunk,
                                "reasoning_summary_delta",
                                None,
                            ),
                            reasoning_summary_text=getattr(
                                chunk,
                                "reasoning_summary_text",
                                None,
                            ),
                            item_id=getattr(chunk, "item_id", None),
                            content_index=getattr(chunk, "content_index", None),
                            summary_index=getattr(chunk, "summary_index", None),
                        )
                        yield ("reasoning", reasoning_chunk)
                    elif chunk_type == "backend_status":
                        pass
                    elif chunk_type == "mcp_status":
                        # Forward MCP status messages preserving type for tool tracking
                        yield (
                            "mcp_status",
                            chunk.content,
                            getattr(chunk, "tool_call_id", None),
                        )

                        # Track MCP failures in reliability metrics
                        mcp_status = getattr(chunk, "status", None)
                        if mcp_status in (
                            "mcp_tools_failed",
                            "mcp_unavailable",
                            "mcp_error",
                        ):
                            buffer_preview, buffer_chars = self._get_buffer_content(
                                agent,
                            )

                            # Get Docker health info for reliability metrics (non-blocking)
                            docker_health = await asyncio.to_thread(
                                self._get_docker_health,
                                agent,
                                agent_id,
                            )

                            self.coordination_tracker.track_enforcement_event(
                                agent_id=agent_id,
                                reason="mcp_disconnected",
                                attempt=attempt + 1,
                                max_attempts=max_attempts,
                                tool_calls=[],
                                error_message=chunk.content[:500] if chunk.content else None,
                                buffer_preview=buffer_preview,
                                buffer_chars=buffer_chars,
                                docker_health=docker_health,
                            )

                            # Save Docker container logs on MCP failure for debugging (fire-and-forget)
                            asyncio.create_task(
                                asyncio.to_thread(
                                    self._save_docker_logs_on_mcp_failure,
                                    agent,
                                    agent_id,
                                    mcp_status,
                                ),
                            )
                    elif chunk_type == "custom_tool_status":
                        # Forward custom tool status messages preserving type for tool tracking
                        yield (
                            "custom_tool_status",
                            chunk.content,
                            getattr(chunk, "tool_call_id", None),
                        )
                    elif chunk_type == "hook_execution":
                        # Forward hook execution chunks for TUI display
                        # Include hook_info and tool_call_id for injection subcard display
                        hook_chunk = StreamChunk(
                            type="hook_execution",
                            content=chunk.content,
                            source=agent_id,
                            hook_info=getattr(chunk, "hook_info", None),
                            tool_call_id=getattr(chunk, "tool_call_id", None),
                        )
                        yield ("hook_execution", hook_chunk)
                    elif chunk_type == "debug":
                        # Forward debug chunks
                        yield ("debug", chunk.content)
                    elif chunk_type == "tool_calls":
                        # Use the correct tool_calls field
                        chunk_tool_calls = getattr(chunk, "tool_calls", []) or []
                        tool_calls.extend(chunk_tool_calls)

                        # Stream tool calls to show agent actions
                        # Get backend name for logging
                        backend_name = None
                        if hasattr(agent, "backend") and hasattr(
                            agent.backend,
                            "get_provider_name",
                        ):
                            backend_name = agent.backend.get_provider_name()

                        # Build set of client-provided external tool names
                        external_tool_names = {(t.get("function", {}) or {}).get("name") for t in (self._external_tools or []) if isinstance(t, dict)}

                        external_tool_calls = []
                        for tool_call in chunk_tool_calls:
                            tool_name = agent.backend.extract_tool_name(tool_call)
                            tool_args = agent.backend.extract_tool_arguments(tool_call)

                            # Client-provided external tools: surface to caller and end the turn
                            if tool_name and tool_name in external_tool_names:
                                external_tool_calls.append(tool_call)
                                continue

                            # Check if this is an MCP or custom tool (handled by backend)
                            is_mcp = hasattr(
                                agent.backend,
                                "is_mcp_tool_call",
                            ) and agent.backend.is_mcp_tool_call(tool_name)
                            is_custom = hasattr(
                                agent.backend,
                                "is_custom_tool_call",
                            ) and agent.backend.is_custom_tool_call(tool_name)

                            # MCP and custom tools are handled by backend - just log for UI, don't warn
                            if is_mcp or is_custom:
                                tool_type = "MCP" if is_mcp else "Custom"
                                logger.debug(
                                    f"[Orchestrator] Agent {agent_id} called {tool_type} tool '{tool_name}' (handled by backend)",
                                )
                                # Don't yield UI message here - backend streams its own status messages
                                continue

                            # Unknown tools (not workflow, not MCP, not custom, not external): log warning
                            # This handles hallucinated tool names or model prefixes like "default_api:"
                            if tool_name and tool_name not in internal_tool_names:
                                logger.warning(
                                    f"[Orchestrator] Agent {agent_id} called unknown tool '{tool_name}' - not registered as workflow, MCP, or custom tool",
                                )
                                yield self._trace_tuple(
                                    f"âš ï¸ Unknown tool: {tool_name} (not registered)",
                                    kind="coordination",
                                )
                                continue

                            if tool_name == "new_answer":
                                content = tool_args.get("content", "")
                                yield self._trace_tuple(
                                    f'ðŸ’¡ Providing answer: "{content}"',
                                    kind="coordination",
                                )
                                log_tool_call(
                                    agent_id,
                                    "new_answer",
                                    {"content": content},
                                    None,
                                    backend_name,
                                )  # Full content for debug logging
                            elif tool_name == "vote":
                                agent_voted_for = tool_args.get("agent_id", "")
                                reason = tool_args.get("reason", "")
                                log_tool_call(
                                    agent_id,
                                    "vote",
                                    {"agent_id": agent_voted_for, "reason": reason},
                                    None,
                                    backend_name,
                                )  # Full reason for debug logging

                                # Convert anonymous agent ID to real agent ID for display
                                # Use global agent mapping (consistent with vote validation)
                                agent_mapping = self.coordination_tracker.get_anonymous_agent_mapping()
                                real_agent_id = agent_mapping.get(
                                    agent_voted_for,
                                    agent_voted_for,
                                )

                                # Show which agents have answers using global numbering
                                options_anon = self.coordination_tracker.get_agents_with_answers_anon(
                                    answers,
                                )

                                yield (
                                    "coordination" if self.trace_classification == "strict" else "content",
                                    f"ðŸ—³ï¸ Voting for [{real_agent_id}] (options: {', '.join(options_anon)}) : {reason}",
                                )
                            elif tool_name == "ask_others":
                                # Broadcast tool - handled as custom tool by backend
                                question = tool_args.get("question", "")
                                yield self._trace_tuple(
                                    f"ðŸ“¢ Asking others: {question[:80]}...",
                                    kind="coordination",
                                )
                                log_tool_call(
                                    agent_id,
                                    "ask_others",
                                    tool_args,
                                    None,
                                    backend_name,
                                )
                            elif tool_name in [
                                "check_broadcast_status",
                                "get_broadcast_responses",
                            ]:
                                # Polling broadcast tools - handled as custom tools by backend
                                request_id = tool_args.get("request_id", "")
                                yield self._trace_tuple(
                                    f"ðŸ“¢ Checking broadcast {request_id[:8]}...",
                                    kind="coordination",
                                )
                                log_tool_call(
                                    agent_id,
                                    tool_name,
                                    tool_args,
                                    None,
                                    backend_name,
                                )
                            else:
                                yield self._trace_tuple(
                                    f"ðŸ”§ Using {tool_name}",
                                    kind="coordination",
                                )
                                log_tool_call(
                                    agent_id,
                                    tool_name,
                                    tool_args,
                                    None,
                                    backend_name,
                                )

                        if external_tool_calls:
                            # Surface external tool calls (do NOT execute) and terminate this agent execution.
                            yield ("external_tool_calls", external_tool_calls)
                            yield ("done", None)
                            return
                    elif chunk_type == "error":
                        # Stream error information to user interface
                        error_msg = getattr(chunk, "error", str(chunk.content)) if hasattr(chunk, "error") else str(chunk.content)
                        yield ("content", f"âŒ Error: {error_msg}\n")

                        # Track API/streaming error in reliability metrics
                        buffer_preview, buffer_chars = self._get_buffer_content(agent)
                        self.coordination_tracker.track_enforcement_event(
                            agent_id=agent_id,
                            reason="api_error",
                            attempt=attempt + 1,
                            max_attempts=max_attempts,
                            tool_calls=[],
                            error_message=error_msg[:500] if error_msg else None,
                            buffer_preview=buffer_preview,
                            buffer_chars=buffer_chars,
                        )
                    elif chunk_type == "incomplete_response_recovery":
                        # Handle incomplete response recovery - API stream ended early
                        # Buffer content is preserved in chunk.content
                        buffer_size = len(chunk.content or "") if chunk.content else 0
                        detail = getattr(chunk, "detail", "")
                        logger.info(
                            f"[Orchestrator] Agent {agent_id} recovering from incomplete response - " f"preserved {buffer_size} chars of content. {detail}",
                        )
                        # Yield status message for visibility
                        yield (
                            "content",
                            f"âš ï¸ API stream ended early - recovering with preserved context ({detail})\n",
                        )

                        # Track connection recovery in reliability metrics
                        self.coordination_tracker.track_enforcement_event(
                            agent_id=agent_id,
                            reason="connection_recovery",
                            attempt=attempt + 1,
                            max_attempts=max_attempts,
                            tool_calls=[],
                            error_message=detail,
                            buffer_preview=chunk.content[:500] if chunk.content else None,
                            buffer_chars=buffer_size,
                        )
                        # Note: The orchestrator's while loop will continue and make a new API call
                        # The buffer content has already been yielded as stream content, so it's already in the context

                    # Check if force_terminate was triggered by too many consecutive denied tool calls
                    timeout_state = self.agent_states[agent_id].round_timeout_state
                    if timeout_state and timeout_state.force_terminate:
                        logger.error(
                            f"[Orchestrator] FORCE TERMINATE for {agent_id} - "
                            f"{timeout_state.consecutive_hard_denials} consecutive denied tool calls. "
                            f"Agent stuck in denial loop, terminating turn.",
                        )
                        yield (
                            "error",
                            f"Agent terminated: {timeout_state.consecutive_hard_denials} consecutive blocked " f"tool calls after hard timeout. Agent failed to submit vote/answer.",
                        )
                        yield ("done", None)
                        return

                # Handle multiple vote calls - take the last vote (agent's final decision)
                vote_calls = [tc for tc in tool_calls if agent.backend.extract_tool_name(tc) == "vote"]
                if len(vote_calls) > 1:
                    # Take the last vote - represents the agent's final, most refined decision
                    num_votes = len(vote_calls)
                    final_vote_call = vote_calls[-1]
                    final_vote_args = agent.backend.extract_tool_arguments(
                        final_vote_call,
                    )
                    final_voted_agent = final_vote_args.get("agent_id", "unknown")

                    # Replace tool_calls with deduplicated list (all non-votes + final vote)
                    vote_calls = [final_vote_call]
                    tool_calls = [tc for tc in tool_calls if agent.backend.extract_tool_name(tc) != "vote"] + [final_vote_call]

                    logger.info(
                        f"[Orchestrator] Agent {agent_id} made {num_votes} votes - using last vote: {final_voted_agent}",
                    )
                    yield (
                        "content",
                        f"âš ï¸ Agent made {num_votes} votes - using last (final decision): {final_voted_agent}\n",
                    )

                # Check for mixed new_answer and vote calls - violates binary decision framework
                new_answer_calls = [tc for tc in tool_calls if agent.backend.extract_tool_name(tc) == "new_answer"]
                if len(vote_calls) > 0 and len(new_answer_calls) > 0:
                    if attempt < max_attempts - 1:
                        # Note: restart_pending is handled by mid-stream callback on next tool call
                        error_msg = "Cannot use both 'vote' and 'new_answer' in same response. Choose one: vote for existing answer OR provide new answer."
                        yield (
                            "content",
                            f"âŒ Retry ({attempt + 1}/{max_attempts}): {error_msg}",
                        )

                        # Track enforcement event before retry
                        buffer_preview, buffer_chars = self._get_buffer_content(agent)
                        self.coordination_tracker.track_enforcement_event(
                            agent_id=agent_id,
                            reason="vote_and_answer",
                            attempt=attempt + 1,
                            max_attempts=max_attempts,
                            tool_calls=["vote", "new_answer"],
                            error_message=error_msg,
                            buffer_preview=buffer_preview,
                            buffer_chars=buffer_chars,
                        )

                        # Send tool error response for all tool calls that caused the violation
                        enforcement_msg = self._create_tool_error_messages(
                            agent,
                            tool_calls,
                            error_msg,
                        )
                        attempt += 1  # Error counts as an attempt
                        continue  # Retry this attempt
                    else:
                        yield (
                            "error",
                            "Agent used both vote and new_answer tools in single response after max attempts",
                        )
                        yield ("done", None)
                        return

                # Process all tool calls
                if tool_calls:
                    for tool_call in tool_calls:
                        tool_name = agent.backend.extract_tool_name(tool_call)
                        tool_args = agent.backend.extract_tool_arguments(tool_call)

                        if tool_name == "vote":
                            # Fetch fresh answers from agent_states (injection may have added new ones)
                            answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}

                            # Log which agents we are choosing from
                            logger.info(
                                f"[Orchestrator] Agent {agent_id} voting from options: {list(answers.keys()) if answers else 'No answers available'}",
                            )
                            # Note: restart_pending is handled by mid-stream callback on next tool call

                            workflow_tool_found = True
                            # Vote for existing answer (requires existing answers)
                            if not answers:
                                # Invalid - can't vote when no answers exist
                                if attempt < max_attempts - 1:
                                    # Note: restart_pending is handled by mid-stream callback on next tool call
                                    error_msg = "Cannot vote when no answers exist. Use new_answer tool."
                                    yield (
                                        "content",
                                        f"âŒ Retry ({attempt + 1}/{max_attempts}): {error_msg}",
                                    )

                                    # Track enforcement event before retry
                                    buffer_preview, buffer_chars = self._get_buffer_content(agent)
                                    self.coordination_tracker.track_enforcement_event(
                                        agent_id=agent_id,
                                        reason="vote_no_answers",
                                        attempt=attempt + 1,
                                        max_attempts=max_attempts,
                                        tool_calls=["vote"],
                                        error_message=error_msg,
                                        buffer_preview=buffer_preview,
                                        buffer_chars=buffer_chars,
                                    )

                                    # Create proper tool error message for retry
                                    enforcement_msg = self._create_tool_error_messages(
                                        agent,
                                        [tool_call],
                                        error_msg,
                                    )
                                    attempt += 1  # Error counts as an attempt
                                    continue
                                else:
                                    yield (
                                        "error",
                                        "Cannot vote when no answers exist after max attempts",
                                    )
                                    yield ("done", None)
                                    return

                            voted_agent_anon = tool_args.get("agent_id")
                            reason = tool_args.get("reason", "")

                            # Convert anonymous agent ID back to real agent ID
                            # Use global agent mapping (consistent with vote tool enum and injection)
                            agent_mapping = self.coordination_tracker.get_anonymous_agent_mapping()

                            voted_agent = agent_mapping.get(
                                voted_agent_anon,
                                voted_agent_anon,
                            )

                            # Handle invalid agent_id - check if voted agent has an answer
                            if voted_agent not in answers:
                                if attempt < max_attempts - 1:
                                    # Note: restart_pending is handled by mid-stream callback on next tool call
                                    # Build valid agents list using global numbering (consistent with enum)
                                    valid_anon_agents = self.coordination_tracker.get_agents_with_answers_anon(
                                        answers,
                                    )
                                    error_msg = f"Invalid agent_id '{voted_agent_anon}'. Valid agents: {', '.join(valid_anon_agents)}"
                                    # Send tool error result back to agent
                                    yield (
                                        "content",
                                        f"âŒ Retry ({attempt + 1}/{max_attempts}): {error_msg}",
                                    )

                                    # Track enforcement event before retry
                                    buffer_preview, buffer_chars = self._get_buffer_content(agent)
                                    self.coordination_tracker.track_enforcement_event(
                                        agent_id=agent_id,
                                        reason="invalid_vote_id",
                                        attempt=attempt + 1,
                                        max_attempts=max_attempts,
                                        tool_calls=["vote"],
                                        error_message=error_msg,
                                        buffer_preview=buffer_preview,
                                        buffer_chars=buffer_chars,
                                    )

                                    # Create proper tool error message for retry
                                    enforcement_msg = self._create_tool_error_messages(
                                        agent,
                                        [tool_call],
                                        error_msg,
                                    )
                                    attempt += 1  # Error counts as an attempt
                                    continue  # Retry with updated conversation
                                else:
                                    yield (
                                        "error",
                                        f"Invalid agent_id after {max_attempts} attempts",
                                    )
                                    yield ("done", None)
                                    return
                            # Record the vote locally (but orchestrator may still ignore it)
                            self.agent_states[agent_id].votes = {
                                "agent_id": voted_agent,
                                "reason": reason,
                            }

                            # Record vote to shared memory
                            vote_message = f"Voted for {voted_agent}. Reason: {reason}"
                            await self._record_to_shared_memory(
                                agent_id=agent_id,
                                content=vote_message,
                                role="assistant",
                            )

                            # Send tool result - orchestrator will decide if vote is accepted
                            # Vote submitted (result will be shown by orchestrator)
                            _agent_outcome = "vote"
                            _agent_voted_for = voted_agent
                            # Get the answer label that this voter was shown for voted-for agent
                            _agent_voted_for_label = self.coordination_tracker.get_voted_for_label(
                                agent_id,
                                voted_agent,
                            )

                            # Record vote to execution trace (if available)
                            if hasattr(agent.backend, "_add_vote_to_trace"):
                                # Get available answer labels from voter's context
                                available_options = self.coordination_tracker.get_agent_context_labels(
                                    agent_id,
                                )
                                agent.backend._add_vote_to_trace(
                                    voted_for_agent=voted_agent,
                                    voted_for_label=_agent_voted_for_label,
                                    reason=reason,
                                    available_options=available_options,
                                )

                            yield (
                                "result",
                                ("vote", {"agent_id": voted_agent, "reason": reason}),
                            )
                            yield ("done", None)
                            return

                        elif tool_name == "new_answer":
                            workflow_tool_found = True
                            # Agent provided new answer
                            content = tool_args.get("content", response_text.strip())

                            # Check answer count limit
                            can_answer, count_error = self._check_answer_count_limit(
                                agent_id,
                            )
                            if not can_answer:
                                if attempt < max_attempts - 1:
                                    # Note: restart_pending is handled by mid-stream callback on next tool call
                                    yield (
                                        "content",
                                        f"âŒ Retry ({attempt + 1}/{max_attempts}): {count_error}",
                                    )

                                    # Track enforcement event before retry
                                    buffer_preview, buffer_chars = self._get_buffer_content(agent)
                                    self.coordination_tracker.track_enforcement_event(
                                        agent_id=agent_id,
                                        reason="answer_limit",
                                        attempt=attempt + 1,
                                        max_attempts=max_attempts,
                                        tool_calls=["new_answer"],
                                        error_message=count_error,
                                        buffer_preview=buffer_preview,
                                        buffer_chars=buffer_chars,
                                    )

                                    # Create proper tool error message for retry
                                    enforcement_msg = self._create_tool_error_messages(
                                        agent,
                                        [tool_call],
                                        count_error,
                                    )
                                    attempt += 1  # Error counts as an attempt
                                    continue
                                else:
                                    yield (
                                        "error",
                                        f"Answer count limit reached after {max_attempts} attempts",
                                    )
                                    yield ("done", None)
                                    return

                            # Check answer novelty (similarity to existing answers)
                            is_novel, novelty_error = self._check_answer_novelty(
                                content,
                                answers,
                            )
                            if not is_novel:
                                if attempt < max_attempts - 1:
                                    # Note: restart_pending is handled by mid-stream callback on next tool call
                                    yield (
                                        "content",
                                        f"âŒ Retry ({attempt + 1}/{max_attempts}): {novelty_error}",
                                    )

                                    # Track enforcement event before retry
                                    buffer_preview, buffer_chars = self._get_buffer_content(agent)
                                    self.coordination_tracker.track_enforcement_event(
                                        agent_id=agent_id,
                                        reason="answer_novelty",
                                        attempt=attempt + 1,
                                        max_attempts=max_attempts,
                                        tool_calls=["new_answer"],
                                        error_message=novelty_error,
                                        buffer_preview=buffer_preview,
                                        buffer_chars=buffer_chars,
                                    )

                                    # Create proper tool error message for retry
                                    enforcement_msg = self._create_tool_error_messages(
                                        agent,
                                        [tool_call],
                                        novelty_error,
                                    )
                                    attempt += 1  # Error counts as an attempt
                                    continue
                                else:
                                    yield (
                                        "error",
                                        f"Answer novelty requirement not met after {max_attempts} attempts",
                                    )
                                    yield ("done", None)
                                    return

                            # Check for duplicate answer
                            # Normalize both new content and existing content to neutral paths for comparison
                            normalized_new_content = self._normalize_workspace_paths_for_comparison(content)

                            for existing_agent_id, existing_content in answers.items():
                                normalized_existing_content = self._normalize_workspace_paths_for_comparison(
                                    existing_content,
                                )
                                if normalized_new_content.strip() == normalized_existing_content.strip():
                                    if attempt < max_attempts - 1:
                                        # Note: restart_pending is handled by mid-stream callback on next tool call
                                        error_msg = f"Answer already provided by {existing_agent_id}. Provide different answer or vote for existing one."
                                        yield (
                                            "content",
                                            f"âŒ Retry ({attempt + 1}/{max_attempts}): {error_msg}",
                                        )

                                        # Track enforcement event before retry
                                        buffer_preview, buffer_chars = self._get_buffer_content(agent)
                                        self.coordination_tracker.track_enforcement_event(
                                            agent_id=agent_id,
                                            reason="answer_duplicate",
                                            attempt=attempt + 1,
                                            max_attempts=max_attempts,
                                            tool_calls=["new_answer"],
                                            error_message=error_msg,
                                            buffer_preview=buffer_preview,
                                            buffer_chars=buffer_chars,
                                        )

                                        # Create proper tool error message for retry
                                        enforcement_msg = self._create_tool_error_messages(
                                            agent,
                                            [tool_call],
                                            error_msg,
                                        )
                                        attempt += 1  # Error counts as an attempt
                                        continue
                                    else:
                                        yield (
                                            "error",
                                            f"Duplicate answer provided after {max_attempts} attempts",
                                        )
                                        yield ("done", None)
                                        return
                            # Send successful tool result back to agent
                            # Answer recorded (result will be shown by orchestrator)

                            # Record to shared memory
                            await self._record_to_shared_memory(
                                agent_id=agent_id,
                                content=content,
                                role="assistant",
                            )

                            _agent_outcome = "answer"
                            # Compute the answer label that will be assigned (e.g., "agent1.1")
                            agent_num = self.coordination_tracker._get_agent_number(
                                agent_id,
                            )
                            current_answers = len(
                                self.coordination_tracker.answers_by_agent.get(
                                    agent_id,
                                    [],
                                ),
                            )
                            _agent_answer_label = f"agent{agent_num}.{current_answers + 1}"
                            yield ("result", ("answer", content))
                            yield ("done", None)
                            return
                        elif tool_name in (
                            "ask_others",
                            "check_broadcast_status",
                            "get_broadcast_responses",
                        ):
                            # Broadcast tools - check if backend already executed it
                            # For most backends, custom tools are executed during streaming
                            # For Claude Code, tools are parsed from text and need orchestrator execution
                            is_claude_code = hasattr(agent.backend, "get_provider_name") and agent.backend.get_provider_name() == "claude_code"

                            if is_claude_code and hasattr(
                                agent.backend,
                                "_broadcast_toolkit",
                            ):
                                # Claude Code: Execute broadcast tool here since backend doesn't execute it
                                import json

                                broadcast_toolkit = agent.backend._broadcast_toolkit

                                if tool_name == "ask_others":
                                    args_json = json.dumps(tool_args)
                                    yield (
                                        "content",
                                        f"ðŸ“¢ Asking others: {tool_args.get('question', '')[:80]}...\n",
                                    )
                                    result = await broadcast_toolkit.execute_ask_others(
                                        args_json,
                                        agent_id,
                                    )
                                    # Inject result back to agent's conversation
                                    result_msg = {
                                        "role": "user",
                                        "content": f"[Broadcast Response]\n{result}",
                                    }
                                    conversation_messages.append(result_msg)
                                    yield (
                                        "content",
                                        "ðŸ“¢ Received broadcast responses\n",
                                    )
                                elif tool_name == "check_broadcast_status":
                                    args_json = json.dumps(tool_args)
                                    result = await broadcast_toolkit.execute_check_broadcast_status(
                                        args_json,
                                        agent_id,
                                    )
                                    result_msg = {
                                        "role": "user",
                                        "content": f"[Broadcast Status]\n{result}",
                                    }
                                    conversation_messages.append(result_msg)
                                elif tool_name == "get_broadcast_responses":
                                    args_json = json.dumps(tool_args)
                                    result = await broadcast_toolkit.execute_get_broadcast_responses(
                                        args_json,
                                        agent_id,
                                    )
                                    result_msg = {
                                        "role": "user",
                                        "content": f"[Broadcast Responses]\n{result}",
                                    }
                                    conversation_messages.append(result_msg)

                            # Mark as workflow tool found to avoid retry enforcement
                            # The agent will continue and provide new_answer or vote after receiving broadcast response
                            workflow_tool_found = True
                            # Don't return - let the loop continue so agent can process broadcast result
                            # and provide a proper workflow response (new_answer or vote)
                        elif (hasattr(agent.backend, "is_mcp_tool_call") and agent.backend.is_mcp_tool_call(tool_name)) or (
                            hasattr(agent.backend, "is_custom_tool_call") and agent.backend.is_custom_tool_call(tool_name)
                        ):
                            # MCP and custom tools are handled by the backend
                            # Tool results are streamed separately via StreamChunks
                            # Only mark as workflow progress if agent can still provide answers.
                            # If they've hit their answer limit, they MUST vote - MCP tools shouldn't delay this.
                            can_answer, _ = self._check_answer_count_limit(agent_id)
                            if can_answer:
                                workflow_tool_found = True
                            # else: agent must vote, don't set workflow_tool_found so enforcement triggers
                        else:
                            # Non-workflow tools not yet implemented
                            yield (
                                "coordination" if self.trace_classification == "strict" else "content",
                                f"ðŸ”§ used {tool_name} tool (not implemented)",
                            )

                # Case 3: Non-workflow response, need enforcement (only if no workflow tool was found)
                if not workflow_tool_found:
                    # Note: restart_pending is handled by mid-stream callback on next tool call
                    if attempt < max_attempts - 1:
                        # Determine enforcement reason and message
                        if tool_calls:
                            # Use vote-only enforcement message if agent has hit answer limit
                            if vote_only:
                                error_msg = "You have reached your answer limit. You MUST use the `vote` tool now to vote for the best existing answer. The `new_answer` tool is no longer available."
                            else:
                                error_msg = "You must use workflow tools (vote or new_answer) to complete the task."
                            enforcement_reason = "no_workflow_tool"
                            tool_names_called = [agent.backend.extract_tool_name(tc) for tc in tool_calls]
                        else:
                            # No tool calls, just a plain text response - use default enforcement
                            error_msg = "You must use workflow tools (vote or new_answer) to complete the task."
                            enforcement_reason = "no_tool_calls"
                            tool_names_called = []

                        yield (
                            "content",
                            f"âŒ Retry ({attempt + 1}/{max_attempts}): {error_msg}",
                        )

                        # Get full buffer content for injection into retry message
                        # This allows the agent to see what it was working on before the incomplete response
                        full_buffer_content = None
                        if hasattr(agent.backend, "_get_streaming_buffer"):
                            full_buffer_content = agent.backend._get_streaming_buffer()

                        # Track enforcement event before retry (with truncated preview for logging)
                        buffer_preview = full_buffer_content[:500] if full_buffer_content and len(full_buffer_content) > 500 else full_buffer_content
                        buffer_chars = len(full_buffer_content) if full_buffer_content else 0
                        self.coordination_tracker.track_enforcement_event(
                            agent_id=agent_id,
                            reason=enforcement_reason,
                            attempt=attempt + 1,
                            max_attempts=max_attempts,
                            tool_calls=tool_names_called,
                            error_message=error_msg,
                            buffer_preview=buffer_preview,
                            buffer_chars=buffer_chars,
                        )

                        # If there were tool calls, we must provide tool results before continuing
                        # (Response API requires function_call + function_call_output pairs)
                        if tool_calls:
                            enforcement_msg = self._create_tool_error_messages(
                                agent,
                                tool_calls,
                                error_msg,
                            )
                        else:
                            # Include buffer content so agent can continue from where it left off
                            if full_buffer_content:
                                logger.info(
                                    f"[Orchestrator] Injecting {len(full_buffer_content)} chars of buffer content into enforcement retry for {agent_id}",
                                )
                            enforcement_msg = self.message_templates.enforcement_message(
                                buffer_content=full_buffer_content,
                            )
                        attempt += 1  # Error counts as an attempt
                        continue  # Retry with updated conversation
                    else:
                        # Last attempt failed, agent did not provide proper workflow response
                        yield (
                            "error",
                            f"Agent failed to use workflow tools after {max_attempts} attempts",
                        )
                        yield ("done", None)
                        return

        except Exception as e:
            _agent_outcome = "error"
            _agent_error_message = str(e)
            yield ("error", f"Agent execution failed: {str(e)}")
            yield ("done", None)
        finally:
            # Hook manager cleanup is automatic - no explicit cleanup needed
            # The GeneralHookManager is recreated for each agent run

            # Add outcome attributes to agent execution span
            if _agent_outcome:
                _agent_span.set_attribute("massgen.outcome", _agent_outcome)
            if _agent_voted_for:
                _agent_span.set_attribute("massgen.voted_for", _agent_voted_for)
            if _agent_voted_for_label:
                _agent_span.set_attribute(
                    "massgen.voted_for_label",
                    _agent_voted_for_label,
                )
            if _agent_answer_label:
                _agent_span.set_attribute("massgen.answer_label", _agent_answer_label)
            if _agent_error_message:
                _agent_span.set_attribute("massgen.error_message", _agent_error_message)

            # Add token usage and cost to agent execution span before closing
            # Note: Use "usage" instead of "tokens" to avoid logfire's security scrubbing
            if hasattr(agent.backend, "token_usage") and agent.backend.token_usage:
                token_usage = agent.backend.token_usage
                _agent_span.set_attribute(
                    "massgen.usage.input",
                    token_usage.input_tokens or 0,
                )
                _agent_span.set_attribute(
                    "massgen.usage.output",
                    token_usage.output_tokens or 0,
                )
                _agent_span.set_attribute(
                    "massgen.usage.reasoning",
                    token_usage.reasoning_tokens or 0,
                )
                _agent_span.set_attribute(
                    "massgen.usage.cached_input",
                    token_usage.cached_input_tokens or 0,
                )
                _agent_span.set_attribute(
                    "massgen.usage.cost",
                    round(token_usage.estimated_cost or 0, 6),
                )

            # Close the agent execution span for hierarchical tracing
            # Wrap in try/except to handle OpenTelemetry context issues in async generators
            try:
                _agent_span_cm.__exit__(None, None, None)
            except ValueError as e:
                # Context detach failures are expected in async generators - safe to ignore
                # The span is still closed, just the context token can't be detached
                if "context" not in str(e).lower() and "detach" not in str(e).lower():
                    logger.debug(f"Unexpected ValueError closing agent span: {e}")

            # Clear the round context
            clear_current_round()

    async def _get_next_chunk(self, stream: AsyncGenerator[tuple, None]) -> tuple:
        """Get the next chunk from an agent stream."""
        try:
            return await stream.__anext__()
        except StopAsyncIteration:
            return ("done", None)
        except Exception as e:
            return ("error", str(e))

    def _has_write_context_paths(self, agent: "ChatAgent") -> bool:
        """
        Check if agent has any context paths with write permission configured.

        Args:
            agent: The agent to check

        Returns:
            True if agent has write context paths, False otherwise
        """
        if not hasattr(agent, "backend") or not agent.backend:
            return False
        filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
        if not filesystem_manager:
            return False
        ppm = getattr(filesystem_manager, "path_permission_manager", None)
        if not ppm:
            return False
        return any(mp.will_be_writable for mp in ppm.managed_paths if mp.path_type == "context")

    def _enable_context_write_access(self, agent: "ChatAgent") -> None:
        """
        Enable write access for context paths on the given agent.

        Args:
            agent: The agent to enable write access for
        """
        if not hasattr(agent, "backend") or not agent.backend:
            return
        filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
        if not filesystem_manager:
            return
        ppm = getattr(filesystem_manager, "path_permission_manager", None)
        if not ppm:
            return
        ppm.set_context_write_access_enabled(True)
        logger.info(f"[Orchestrator] Enabled context write access for agent: {agent.agent_id}")

    def get_context_path_writes(self) -> list[str]:
        """
        Get list of files written to context paths by the final agent.

        Returns:
            List of file paths written to context paths
        """
        if not self._selected_agent:
            return []
        agent = self.agents.get(self._selected_agent)
        if not agent or not hasattr(agent, "backend") or not agent.backend:
            return []
        filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
        if not filesystem_manager:
            return []
        ppm = getattr(filesystem_manager, "path_permission_manager", None)
        if not ppm:
            return []
        return ppm.get_context_path_writes()

    def get_context_path_writes_categorized(self) -> dict[str, list[str]]:
        """
        Get categorized lists of new and modified files in context paths.

        Returns:
            Dict with 'new' and 'modified' keys, each containing a list of file paths
        """
        if not self._selected_agent:
            return {"new": [], "modified": []}
        agent = self.agents.get(self._selected_agent)
        if not agent or not hasattr(agent, "backend") or not agent.backend:
            return {"new": [], "modified": []}
        filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
        if not filesystem_manager:
            return {"new": [], "modified": []}
        ppm = getattr(filesystem_manager, "path_permission_manager", None)
        if not ppm:
            return {"new": [], "modified": []}
        return ppm.get_context_path_writes_categorized()

    def _clear_context_path_write_tracking(self) -> None:
        """Clear context path write tracking for all agents at the start of each turn."""
        for agent_id, agent in self.agents.items():
            if not hasattr(agent, "backend") or not agent.backend:
                continue
            filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
            if not filesystem_manager:
                continue
            ppm = getattr(filesystem_manager, "path_permission_manager", None)
            if ppm and hasattr(ppm, "clear_context_path_writes"):
                ppm.clear_context_path_writes()
                logger.debug(f"[Orchestrator] Cleared context path write tracking for {agent_id}")

    async def _present_final_answer(self) -> AsyncGenerator[StreamChunk, None]:
        """Present the final coordinated answer with optional post-evaluation and restart loop."""

        # Select the best agent based on current state
        if not self._selected_agent:
            self._selected_agent = self._determine_final_agent_from_states()

        if not self._selected_agent:
            error_msg = "âŒ Unable to provide coordinated answer - no successful agents"
            self.add_to_history("assistant", error_msg)
            log_stream_chunk("orchestrator", "error", error_msg)
            yield StreamChunk(type="content", content=error_msg)
            self.workflow_phase = "presenting"
            log_stream_chunk("orchestrator", "done", None)
            yield StreamChunk(type="done")
            return

        # Get vote results for presentation
        vote_results = self._get_vote_results()

        log_stream_chunk("orchestrator", "content", "## ðŸŽ¯ Final Coordinated Answer\n")
        yield StreamChunk(
            type="coordination" if self.trace_classification == "strict" else "content",
            content="## ðŸŽ¯ Final Coordinated Answer\n",
        )

        # Stream final presentation from winning agent
        log_stream_chunk(
            "orchestrator",
            "content",
            f"ðŸ† Selected Agent: {self._selected_agent}\n",
        )
        yield StreamChunk(
            type="coordination" if self.trace_classification == "strict" else "content",
            content=f"ðŸ† Selected Agent: {self._selected_agent}\n",
        )

        # Check if we should skip final presentation (quick mode - refinement OFF)
        if self.config.skip_final_presentation:
            # Check if we have write context paths - this affects whether we can skip
            agent = self.agents.get(self._selected_agent)
            has_write_context_paths = self._has_write_context_paths(agent) if agent else False
            is_single_agent_mode = self.config.skip_voting  # skip_voting implies single-agent mode

            # Decision matrix:
            # - Single agent + write paths: Enable writes directly (no LLM call), skip presentation
            # - Multi-agent + write paths: Need final presentation to copy files, fall through
            # - Multi-agent + no write paths: Safe to skip, no files need copying

            if is_single_agent_mode and has_write_context_paths:
                # Single agent mode with write context paths:
                # Write access was already enabled at start of coordination (in _stream_agent_execution)
                # and snapshot was taken then, so we just skip to the presentation logic
                logger.info(
                    "[skip_final_presentation] Single agent mode with write context paths - writes already enabled at coordination start",
                )
                # Fall through to skip logic below

            elif not is_single_agent_mode and has_write_context_paths:
                # Multi-agent mode with write context paths:
                # Need final presentation to copy winning agent's files to context paths
                logger.info(
                    "[skip_final_presentation] Multi-agent mode with write context paths - falling through to final presentation",
                )
                # Fall through to normal presentation (don't skip)
                pass  # Continue to normal presentation logic below

            # For all other cases (single agent without write paths, or multi-agent without write paths),
            # we can skip the final presentation
            if not (not is_single_agent_mode and has_write_context_paths):
                # Use existing answer directly without an additional LLM call
                existing_answer = self.agent_states[self._selected_agent].answer
                if existing_answer:
                    # Notify TUI to highlight winner (TextualTerminalDisplay)
                    if hasattr(self, "coordination_ui") and self.coordination_ui:
                        display = getattr(self.coordination_ui, "display", None)
                        if display and hasattr(display, "highlight_winner_quick"):
                            display.highlight_winner_quick(
                                winner_id=self._selected_agent,
                                vote_results=vote_results,
                            )

                    log_stream_chunk(
                        "orchestrator",
                        "content",
                        f"\n{existing_answer}\n",
                        self._selected_agent,
                    )
                    yield StreamChunk(
                        type="content",
                        content=f"\n{existing_answer}\n",
                        source=self._selected_agent,
                    )
                    self._final_presentation_content = existing_answer

                    # Force a workspace snapshot before final answer saving in single-agent skip mode
                    if is_single_agent_mode and agent and hasattr(agent, "backend") and agent.backend:
                        filesystem_manager = getattr(agent.backend, "filesystem_manager", None)
                        if filesystem_manager:
                            await filesystem_manager.save_snapshot(
                                timestamp=None,  # Use None for final snapshots
                                is_final=True,
                            )

                    # Save the final snapshot (creates final/ directory with answer.txt)
                    # This copies the agent's workspace to the final directory
                    final_context = self.get_last_context(self._selected_agent)
                    await self._save_agent_snapshot(
                        self._selected_agent,
                        answer_content=existing_answer,
                        is_final=True,
                        context_data=final_context,
                    )

                    # Track the final answer in coordination tracker
                    self.coordination_tracker.set_final_answer(
                        self._selected_agent,
                        existing_answer,
                        snapshot_timestamp="final",
                    )

                    # Compute context path writes (compare current state to snapshot taken at start)
                    if agent.backend.filesystem_manager:
                        agent.backend.filesystem_manager.path_permission_manager.compute_context_path_writes()

                    # Add to conversation history
                    self.add_to_history("assistant", existing_answer)

                    # Save coordination logs
                    self.save_coordination_logs()

                    # Update workflow phase
                    self.workflow_phase = "presenting"
                    log_stream_chunk("orchestrator", "done", None, self._selected_agent)
                    yield StreamChunk(type="done", source=self._selected_agent)
                    return  # Skip post-evaluation and all remaining logic
                else:
                    # No existing answer - fall through to normal presentation
                    logger.warning(
                        f"[skip_final_presentation] No existing answer for {self._selected_agent}, falling back to normal presentation",
                    )

        # Stream the final presentation (with full tool support)
        presentation_content = ""
        async for chunk in self.get_final_presentation(
            self._selected_agent,
            vote_results,
        ):
            if chunk.type == "content" and chunk.content:
                presentation_content += chunk.content
            yield chunk

        # NOTE: end_round_tracking("presentation") and save_coordination_logs() are now called
        # inside _handle_presentation_phase BEFORE yielding the done chunk.
        # This is necessary because UI consumers break out of the loop on "done" chunk,
        # so cleanup must happen before the done is yielded.

        # Check if post-evaluation should run
        # Skip post-evaluation on final attempt (user clarification #4)
        is_final_attempt = self.current_attempt >= (self.max_attempts - 1)
        should_evaluate = self.max_attempts > 1 and not is_final_attempt

        if should_evaluate:
            # Run post-evaluation
            final_answer_to_evaluate = self._final_presentation_content or presentation_content
            async for chunk in self.post_evaluate_answer(
                self._selected_agent,
                final_answer_to_evaluate,
            ):
                yield chunk

            # End round tracking for post-evaluation phase (moved from post_evaluate_answer finally block
            # to ensure it completes before save_coordination_logs is called)
            if self._selected_agent:
                selected_agent = self.agents.get(self._selected_agent)
                if selected_agent and hasattr(
                    selected_agent.backend,
                    "end_round_tracking",
                ):
                    selected_agent.backend.end_round_tracking("post_evaluation")

            # Check if restart was requested
            if self.restart_pending and self.current_attempt < (self.max_attempts - 1):
                # Show restart banner
                restart_banner = f"""

ðŸ”„ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ORCHESTRATION RESTART (Attempt {self.current_attempt + 2}/{self.max_attempts})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

REASON:
{self.restart_reason}

INSTRUCTIONS FOR NEXT ATTEMPT:
{self.restart_instructions}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
                log_stream_chunk("orchestrator", "status", restart_banner)
                yield StreamChunk(
                    type="restart_banner",
                    content=restart_banner,
                    source="orchestrator",
                )

                # Reset state for restart (prepare for next coordinate() call)
                self.handle_restart()

                # Don't add to history or set workflow phase - restart is pending
                # Exit here - CLI will detect restart_pending and call coordinate() again
                return

        # No restart - add final answer to conversation history
        if self._final_presentation_content:
            self.add_to_history("assistant", self._final_presentation_content)

        # NOTE: save_coordination_logs() is already called inside _handle_presentation_phase
        # before yielding the done chunk. This code path is never reached because UI breaks
        # on the done chunk from _handle_presentation_phase.

        # Update workflow phase
        self.workflow_phase = "presenting"
        log_stream_chunk("orchestrator", "done", None)
        yield StreamChunk(type="done")

    async def _handle_orchestrator_timeout(self) -> AsyncGenerator[StreamChunk, None]:
        """Handle orchestrator timeout by jumping directly to get_final_presentation."""
        # Output orchestrator timeout message first
        log_stream_chunk(
            "orchestrator",
            "content",
            f"\nâš ï¸ **Orchestrator Timeout**: {self.timeout_reason}\n",
            self.orchestrator_id,
        )
        yield StreamChunk(
            type="content",
            content=f"\nâš ï¸ **Orchestrator Timeout**: {self.timeout_reason}\n",
            source=self.orchestrator_id,
        )

        # Count available answers
        available_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer and not state.is_killed}

        log_stream_chunk(
            "orchestrator",
            "content",
            f"ðŸ“Š Current state: {len(available_answers)} answers available\n",
            self.orchestrator_id,
        )
        yield StreamChunk(
            type="content",
            content=f"ðŸ“Š Current state: {len(available_answers)} answers available\n",
            source=self.orchestrator_id,
        )

        # If no answers available, provide fallback with timeout explanation
        if len(available_answers) == 0:
            log_stream_chunk(
                "orchestrator",
                "error",
                "âŒ No answers available from any agents due to timeout. No agents had enough time to provide responses.\n",
                self.orchestrator_id,
            )
            yield StreamChunk(
                type="content",
                content="âŒ No answers available from any agents due to timeout. No agents had enough time to provide responses.\n",
                source=self.orchestrator_id,
            )
            self.workflow_phase = "presenting"
            log_stream_chunk("orchestrator", "done", None)
            yield StreamChunk(type="done")
            return

        # Determine best available agent for presentation
        current_votes = {aid: state.votes for aid, state in self.agent_states.items() if state.votes and not state.is_killed}

        self._selected_agent = self._determine_final_agent_from_votes(
            current_votes,
            available_answers,
        )

        # Jump directly to get_final_presentation
        vote_results = self._get_vote_results()
        log_stream_chunk(
            "orchestrator",
            "content",
            f"ðŸŽ¯ Jumping to final presentation with {self._selected_agent} (selected despite timeout)\n",
            self.orchestrator_id,
        )
        yield StreamChunk(
            type="content",
            content=f"ðŸŽ¯ Jumping to final presentation with {self._selected_agent} (selected despite timeout)\n",
            source=self.orchestrator_id,
        )

        async for chunk in self.get_final_presentation(
            self._selected_agent,
            vote_results,
        ):
            yield chunk

        # NOTE: end_round_tracking("presentation") and save_coordination_logs() are now called
        # inside _handle_presentation_phase BEFORE yielding the done chunk.
        # This code path is never reached because UI breaks on the done chunk.

    def _determine_final_agent_from_votes(
        self,
        votes: Dict[str, Dict],
        agent_answers: Dict[str, str],
    ) -> str:
        """Determine which agent should present the final answer based on votes."""
        if not votes:
            # No votes yet, return first agent with an answer (earliest by generation time)
            return next(iter(agent_answers)) if agent_answers else None

        # Count votes for each agent
        vote_counts = {}
        for vote_data in votes.values():
            voted_for = vote_data.get("agent_id")
            if voted_for:
                vote_counts[voted_for] = vote_counts.get(voted_for, 0) + 1

        if not vote_counts:
            return next(iter(agent_answers)) if agent_answers else None

        # Find agents with maximum votes
        max_votes = max(vote_counts.values())
        tied_agents = [agent_id for agent_id, count in vote_counts.items() if count == max_votes]

        # Break ties by agent registration order (order in agent_states dict)
        for agent_id in agent_answers.keys():
            if agent_id in tied_agents:
                return agent_id

        # Fallback to first tied agent
        return tied_agents[0] if tied_agents else next(iter(agent_answers)) if agent_answers else None

    async def get_final_presentation(
        self,
        selected_agent_id: str,
        vote_results: Dict[str, Any],
    ) -> AsyncGenerator[StreamChunk, None]:
        """Ask the winning agent to present their final answer with voting context."""
        # Guard against duplicate presentations (e.g., if timeout handler runs after presentation started)
        if self._presentation_started:
            logger.warning(
                f"Presentation already started, skipping duplicate call for {selected_agent_id}",
            )
            yield StreamChunk(
                type="status",
                content="Presentation already in progress, skipping duplicate...",
            )
            return
        self._presentation_started = True

        # Start tracking the final round
        self.coordination_tracker.start_final_round(selected_agent_id)

        if selected_agent_id not in self.agents:
            log_stream_chunk(
                "orchestrator",
                "error",
                f"Selected agent {selected_agent_id} not found",
            )
            yield StreamChunk(
                type="error",
                error=f"Selected agent {selected_agent_id} not found",
            )
            return

        agent = self.agents[selected_agent_id]

        # Create presentation span for hierarchical tracing in Logfire
        tracer = get_tracer()
        final_round = self.coordination_tracker.get_agent_round(selected_agent_id)
        backend_name = agent.backend.get_provider_name() if hasattr(agent.backend, "get_provider_name") else "unknown"

        span_attributes = {
            "massgen.agent_id": selected_agent_id,
            "massgen.iteration": self.coordination_tracker.current_iteration,
            "massgen.round": final_round,
            "massgen.round_type": "presentation",
            "massgen.backend": backend_name,
            "massgen.is_winner": True,
            "massgen.vote_count": vote_results.get("vote_counts", {}).get(
                selected_agent_id,
                0,
            ),
        }

        _presentation_span_cm = tracer.span(
            f"agent.{selected_agent_id}.presentation",
            attributes=span_attributes,
        )
        _presentation_span = _presentation_span_cm.__enter__()  # Capture yielded span for set_attribute()

        # Set the round context for nested tool calls to use
        set_current_round(final_round, "presentation")

        # Enable write access for final agent on context paths. This ensures that those paths marked `write` by the user are now writable (as all previous agents were read-only).
        if agent.backend.filesystem_manager:
            # Snapshot context paths BEFORE enabling write access (for tracking what gets written)
            agent.backend.filesystem_manager.path_permission_manager.snapshot_writable_context_paths()

            # Recreate Docker container with write access to context paths
            # The original container was created with read-only mounts for context paths
            # (to prevent race conditions during coordination). For final presentation,
            # we need write access so the agent can write to context paths via shell commands.
            if agent.backend.filesystem_manager.docker_manager:
                skills_directory = None
                massgen_skills = []
                load_previous_session_skills = False
                if self.config.coordination_config:
                    if self.config.coordination_config.use_skills:
                        skills_directory = self.config.coordination_config.skills_directory
                        massgen_skills = self.config.coordination_config.massgen_skills or []
                        load_previous_session_skills = getattr(
                            self.config.coordination_config,
                            "load_previous_session_skills",
                            False,
                        )
                agent.backend.filesystem_manager.recreate_container_for_write_access(
                    skills_directory=skills_directory,
                    massgen_skills=massgen_skills,
                    load_previous_session_skills=load_previous_session_skills,
                )

            # Enable write access in PathPermissionManager (for MCP filesystem tools)
            agent.backend.filesystem_manager.path_permission_manager.set_context_write_access_enabled(
                True,
            )

        # Reset backend planning mode to allow MCP tool execution during final presentation
        if hasattr(agent.backend, "set_planning_mode"):
            agent.backend.set_planning_mode(False)
            logger.info(
                f"[Orchestrator] Backend planning mode DISABLED for final presentation: {selected_agent_id} - MCP tools now allowed",
            )

        # Copy all agents' snapshots to temp workspace to preserve context from coordination phase
        # This allows the agent to reference and access previous work
        temp_workspace_path = await self._copy_all_snapshots_to_temp_workspace(
            selected_agent_id,
        )
        yield StreamChunk(
            type="debug",
            content=f"Restored workspace context for final presentation: {temp_workspace_path}",
            source=selected_agent_id,
        )

        # Prepare context about the voting
        vote_counts = vote_results.get("vote_counts", {})
        voter_details = vote_results.get("voter_details", {})
        is_tie = vote_results.get("is_tie", False)

        # Build voting summary -- note we only include the number of votes and reasons for the selected agent. There is no information about the distribution of votes beyond this.
        voting_summary = f"You received {vote_counts.get(selected_agent_id, 0)} vote(s)"
        if voter_details.get(selected_agent_id):
            reasons = [v["reason"] for v in voter_details[selected_agent_id]]
            voting_summary += f" with feedback: {'; '.join(reasons)}"

        if is_tie:
            voting_summary += " (tie-broken by registration order)"

        # Get all answers for context
        all_answers = {aid: s.answer for aid, s in self.agent_states.items() if s.answer}

        # Normalize workspace paths in both voting summary and all answers for final presentation. Use same function for consistency.
        normalized_voting_summary = self._normalize_workspace_paths_in_answers(
            {selected_agent_id: voting_summary},
            selected_agent_id,
        )[selected_agent_id]
        normalized_all_answers = self._normalize_workspace_paths_in_answers(
            all_answers,
            selected_agent_id,
        )

        # Use MessageTemplates to build the presentation message
        presentation_content = self.message_templates.build_final_presentation_message(
            original_task=self.current_task or "Task coordination",
            vote_summary=normalized_voting_summary,
            all_answers=normalized_all_answers,
            selected_agent_id=selected_agent_id,
        )

        # Get agent's configurable system message using the standard interface
        agent.get_configurable_system_message()

        # Check if image generation is enabled for this agent
        enable_image_generation = False
        if hasattr(agent, "config") and agent.config:
            enable_image_generation = agent.config.backend_params.get(
                "enable_image_generation",
                False,
            )
        elif hasattr(agent, "backend") and hasattr(agent.backend, "backend_params"):
            enable_image_generation = agent.backend.backend_params.get(
                "enable_image_generation",
                False,
            )

        # Extract command execution parameters
        enable_command_execution = False
        docker_mode = False
        enable_sudo = False
        concurrent_tool_execution = False
        if hasattr(agent, "config") and agent.config:
            enable_command_execution = agent.config.backend_params.get(
                "enable_mcp_command_line",
                False,
            )
            docker_mode = agent.config.backend_params.get("command_line_execution_mode", "local") == "docker"
            enable_sudo = agent.config.backend_params.get(
                "command_line_docker_enable_sudo",
                False,
            )
            concurrent_tool_execution = agent.config.backend_params.get(
                "concurrent_tool_execution",
                False,
            )
        elif hasattr(agent, "backend") and hasattr(agent.backend, "backend_params"):
            enable_command_execution = agent.backend.backend_params.get(
                "enable_mcp_command_line",
                False,
            )
            docker_mode = agent.backend.backend_params.get("command_line_execution_mode", "local") == "docker"
            enable_sudo = agent.backend.backend_params.get(
                "command_line_docker_enable_sudo",
                False,
            )
            concurrent_tool_execution = agent.backend.backend_params.get(
                "concurrent_tool_execution",
                False,
            )
        # Check if audio generation is enabled for this agent
        enable_audio_generation = False
        if hasattr(agent, "config") and agent.config:
            enable_audio_generation = agent.config.backend_params.get(
                "enable_audio_generation",
                False,
            )
        elif hasattr(agent, "backend") and hasattr(agent.backend, "backend_params"):
            enable_audio_generation = agent.backend.backend_params.get(
                "enable_audio_generation",
                False,
            )

        # Check if file generation is enabled for this agent
        enable_file_generation = False
        if hasattr(agent, "config") and agent.config:
            enable_file_generation = agent.config.backend_params.get(
                "enable_file_generation",
                False,
            )
        elif hasattr(agent, "backend") and hasattr(agent.backend, "backend_params"):
            enable_file_generation = agent.backend.backend_params.get(
                "enable_file_generation",
                False,
            )

        # Check if video generation is enabled for this agent
        enable_video_generation = False
        if hasattr(agent, "config") and agent.config:
            enable_video_generation = agent.config.backend_params.get(
                "enable_video_generation",
                False,
            )
        elif hasattr(agent, "backend") and hasattr(agent.backend, "backend_params"):
            enable_video_generation = agent.backend.backend_params.get(
                "enable_video_generation",
                False,
            )

        # Check if agent has write access to context paths (requires file delivery)
        has_irreversible_actions = False
        if agent.backend.filesystem_manager:
            context_paths = agent.backend.filesystem_manager.path_permission_manager.get_context_paths()
            # Check if any context path has write permission
            has_irreversible_actions = any(cp.get("permission") == "write" for cp in context_paths)

        # Build system message using section architecture
        base_system_message = self._get_system_message_builder().build_presentation_message(
            agent=agent,
            all_answers=all_answers,
            previous_turns=self._previous_turns,
            enable_image_generation=enable_image_generation,
            enable_audio_generation=enable_audio_generation,
            enable_file_generation=enable_file_generation,
            enable_video_generation=enable_video_generation,
            has_irreversible_actions=has_irreversible_actions,
            enable_command_execution=enable_command_execution,
            docker_mode=docker_mode,
            enable_sudo=enable_sudo,
            concurrent_tool_execution=concurrent_tool_execution,
            agent_mapping=self.coordination_tracker.get_reverse_agent_mapping(),
        )

        # Change the status of all agents that were not selected to AgentStatus.COMPLETED
        for aid, _ in self.agent_states.items():
            if aid != selected_agent_id:
                self.coordination_tracker.change_status(aid, AgentStatus.COMPLETED)

        self.coordination_tracker.set_final_agent(
            selected_agent_id,
            voting_summary,
            all_answers,
        )
        # Update status file for real-time monitoring
        # Run in executor to avoid blocking event loop
        log_session_dir = get_log_session_dir()
        if log_session_dir:
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(
                None,
                self.coordination_tracker.save_status_file,
                log_session_dir,
                self,
            )

        # Create conversation with system and user messages
        presentation_messages = [
            {
                "role": "system",
                "content": base_system_message,
            },
            {"role": "user", "content": presentation_content},
        ]

        # Store the final context in agent state for saving
        self.agent_states[selected_agent_id].last_context = {
            "messages": presentation_messages,
            "is_final": True,
            "vote_summary": voting_summary,
            "all_answers": all_answers,
            "complete_vote_results": vote_results,  # Include ALL vote data
            "vote_counts": vote_counts,
            "voter_details": voter_details,
            "all_votes": {aid: state.votes for aid, state in self.agent_states.items() if state.votes},  # All individual votes
        }

        log_stream_chunk(
            "orchestrator",
            "status",
            f"ðŸŽ¤  [{selected_agent_id}] presenting final answer\n",
        )
        yield StreamChunk(
            type="status",
            content=f"ðŸŽ¤  [{selected_agent_id}] presenting final answer\n",
        )

        # Notify TUI to show fresh final presentation view
        # Build answer labels mapping for vote display (agent_id -> "A1.1" style label)
        answer_labels = {}
        if vote_counts:
            for aid in vote_counts.keys():
                label = self.coordination_tracker.get_latest_answer_label(aid)
                if label:
                    # Convert "agent1.1" to "A1.1"
                    answer_labels[aid] = label.replace("agent", "A")

        yield StreamChunk(
            type="final_presentation_start",
            content={
                "agent_id": selected_agent_id,
                "vote_counts": vote_counts,
                "answer_labels": answer_labels,
            },
            source=selected_agent_id,
        )

        # Start round token tracking for final presentation
        final_round = self.coordination_tracker.get_agent_round(selected_agent_id)
        if hasattr(agent.backend, "start_round_tracking"):
            agent.backend.start_round_tracking(
                round_number=final_round,
                round_type="presentation",
                agent_id=selected_agent_id,
            )

        # Use agent's chat method with proper system message (reset chat for clean presentation)
        presentation_content = ""  # All content for display/logging
        clean_answer_content = ""  # Only clean text for answer.txt (excludes tool calls/results)
        final_snapshot_saved = False  # Track whether snapshot was saved during stream
        was_cancelled = False  # Track if we broke out due to cancellation

        try:
            # Track final round iterations (each chunk is like an iteration)
            async for chunk in agent.chat(
                presentation_messages,
                reset_chat=True,  # Reset conversation history for clean presentation
                current_stage=CoordinationStage.PRESENTATION,
                orchestrator_turn=self._current_turn,
                previous_winners=self._winning_agents_history.copy(),
            ):
                # Check for cancellation at the start of each chunk
                if hasattr(self, "cancellation_manager") and self.cancellation_manager and self.cancellation_manager.is_cancelled:
                    logger.info(
                        "Cancellation detected during final presentation - stopping streaming",
                    )
                    was_cancelled = True
                    # Yield a cancellation chunk so the UI knows to stop
                    yield StreamChunk(
                        type="cancelled",
                        content="Final presentation cancelled by user",
                        source=selected_agent_id,
                    )
                    break

                chunk_type = self._get_chunk_type_value(chunk)
                # Start new iteration for this chunk
                self.coordination_tracker.start_new_iteration()
                # Use the same streaming approach as regular coordination
                if chunk_type == "content" and chunk.content:
                    presentation_content += chunk.content
                    # Only add to clean answer if not tool-related content
                    if not self._is_tool_related_content(chunk.content):
                        clean_answer_content += chunk.content
                    log_stream_chunk(
                        "orchestrator",
                        "content",
                        chunk.content,
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="content",
                        content=chunk.content,
                        source=selected_agent_id,
                    )
                elif chunk_type in [
                    "reasoning",
                    "reasoning_done",
                    "reasoning_summary",
                    "reasoning_summary_done",
                ]:
                    # Stream reasoning content with proper attribution (same as main coordination)
                    reasoning_chunk = StreamChunk(
                        type=chunk_type,
                        content=chunk.content,
                        source=selected_agent_id,
                        reasoning_delta=getattr(chunk, "reasoning_delta", None),
                        reasoning_text=getattr(chunk, "reasoning_text", None),
                        reasoning_summary_delta=getattr(
                            chunk,
                            "reasoning_summary_delta",
                            None,
                        ),
                        reasoning_summary_text=getattr(
                            chunk,
                            "reasoning_summary_text",
                            None,
                        ),
                        item_id=getattr(chunk, "item_id", None),
                        content_index=getattr(chunk, "content_index", None),
                        summary_index=getattr(chunk, "summary_index", None),
                    )
                    # Use the same format as main coordination for consistency
                    log_stream_chunk(
                        "orchestrator",
                        chunk.type,
                        chunk.content,
                        selected_agent_id,
                    )
                    yield reasoning_chunk
                elif chunk_type == "backend_status":
                    import json

                    status_json = json.loads(chunk.content)
                    cwd = status_json["cwd"]
                    session_id = status_json["session_id"]
                    content = f"""Final Temp Working directory: {cwd}.
    Final Session ID: {session_id}.
    """

                    log_stream_chunk(
                        "orchestrator",
                        "content",
                        content,
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="content",
                        content=content,
                        source=selected_agent_id,
                    )
                elif chunk_type == "mcp_status":
                    # MCP status - preserve type for TUI tool tracking
                    mcp_content = f"ðŸ”§ MCP: {chunk.content}"
                    log_stream_chunk("orchestrator", "mcp_status", chunk.content, selected_agent_id)
                    yield StreamChunk(
                        type="mcp_status",
                        content=mcp_content,
                        source=selected_agent_id,
                        tool_call_id=getattr(chunk, "tool_call_id", None),
                    )
                elif chunk_type == "custom_tool_status":
                    # Custom tool status - preserve type for TUI tool tracking
                    custom_content = f"ðŸ”§ Custom Tool: {chunk.content}"
                    log_stream_chunk("orchestrator", "custom_tool_status", chunk.content, selected_agent_id)
                    yield StreamChunk(
                        type="custom_tool_status",
                        content=custom_content,
                        source=selected_agent_id,
                        tool_call_id=getattr(chunk, "tool_call_id", None),
                    )
                elif chunk_type == "hook_execution":
                    # Hook execution - pass through with source
                    log_stream_chunk(
                        "orchestrator",
                        "hook_execution",
                        str(getattr(chunk, "hook_info", "")),
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="hook_execution",
                        content=chunk.content,
                        source=selected_agent_id,
                        hook_info=getattr(chunk, "hook_info", None),
                        tool_call_id=getattr(chunk, "tool_call_id", None),
                    )
                elif chunk_type == "done":
                    # Save the final workspace snapshot (from final workspace directory)
                    # Use clean_answer_content (excludes tool calls/results) for answer.txt
                    final_answer = (
                        clean_answer_content.strip() if clean_answer_content.strip() else self.agent_states[selected_agent_id].answer
                    )  # fallback to stored answer if no clean content generated
                    final_context = self.get_last_context(selected_agent_id)
                    await self._save_agent_snapshot(
                        self._selected_agent,
                        answer_content=final_answer,
                        is_final=True,
                        context_data=final_context,
                    )

                    # Track the final answer in coordination tracker (use clean content)
                    self.coordination_tracker.set_final_answer(
                        selected_agent_id,
                        final_answer,
                        snapshot_timestamp="final",
                    )

                    # Mark snapshot as saved
                    final_snapshot_saved = True

                    # End round tracking for presentation phase BEFORE yielding done
                    # (UI consumers break out of loop on "done" chunk, so cleanup must happen first)
                    agent = self.agents.get(self._selected_agent)
                    if agent and hasattr(agent.backend, "end_round_tracking"):
                        agent.backend.end_round_tracking("presentation")

                    # Save coordination logs BEFORE yielding done
                    # (ensures all round data is captured before UI breaks out of loop)
                    self.save_coordination_logs()

                    log_stream_chunk("orchestrator", "done", None, selected_agent_id)
                    yield StreamChunk(type="done", source=selected_agent_id)
                elif chunk_type == "error":
                    log_stream_chunk(
                        "orchestrator",
                        "error",
                        chunk.error,
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="error",
                        error=chunk.error,
                        source=selected_agent_id,
                    )
                # Pass through other chunk types as-is but with source
                else:
                    if hasattr(chunk, "source"):
                        log_stream_chunk(
                            "orchestrator",
                            chunk_type,
                            getattr(chunk, "content", ""),
                            selected_agent_id,
                        )
                        yield StreamChunk(
                            type=chunk_type,
                            content=getattr(chunk, "content", ""),
                            source=selected_agent_id,
                            **{
                                k: v
                                for k, v in chunk.__dict__.items()
                                if k
                                not in [
                                    "type",
                                    "content",
                                    "source",
                                    "timestamp",
                                    "sequence_number",
                                ]
                            },
                        )
                    else:
                        log_stream_chunk(
                            "orchestrator",
                            chunk_type,
                            getattr(chunk, "content", ""),
                            selected_agent_id,
                        )
                        yield StreamChunk(
                            type=chunk_type,
                            content=getattr(chunk, "content", ""),
                            source=selected_agent_id,
                            **{
                                k: v
                                for k, v in chunk.__dict__.items()
                                if k
                                not in [
                                    "type",
                                    "content",
                                    "source",
                                    "timestamp",
                                    "sequence_number",
                                ]
                            },
                        )

        finally:
            # Ensure final snapshot is always saved (even if "done" chunk wasn't yielded)
            if not final_snapshot_saved:
                # Use clean_answer_content (excludes tool calls/results) for answer.txt
                final_answer = clean_answer_content.strip() if clean_answer_content.strip() else self.agent_states[selected_agent_id].answer
                final_context = self.get_last_context(selected_agent_id)
                await self._save_agent_snapshot(
                    self._selected_agent,
                    answer_content=final_answer,
                    is_final=True,
                    context_data=final_context,
                )

                # Track the final answer in coordination tracker (use clean content)
                self.coordination_tracker.set_final_answer(
                    selected_agent_id,
                    final_answer,
                    snapshot_timestamp="final",
                )

            # Store the final presentation content for post-evaluation and history
            # Use clean_answer_content (excludes tool calls/results)
            if clean_answer_content.strip():
                # Store the clean final answer (used by post-evaluation and conversation history)
                self._final_presentation_content = clean_answer_content.strip()
            elif not was_cancelled:
                # Only yield fallback content if NOT cancelled - yielding after cancellation
                # causes display issues since the UI has already raised CancellationRequested
                stored_answer = self.agent_states[selected_agent_id].answer
                if stored_answer:
                    fallback_content = f"\nðŸ“‹ Using stored answer as final presentation:\n\n{stored_answer}"
                    log_stream_chunk(
                        "orchestrator",
                        "content",
                        fallback_content,
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="content",
                        content=fallback_content,
                        source=selected_agent_id,
                    )
                    self._final_presentation_content = stored_answer
                else:
                    log_stream_chunk(
                        "orchestrator",
                        "error",
                        "\nâŒ No content generated for final presentation and no stored answer available.",
                        selected_agent_id,
                    )
                    yield StreamChunk(
                        type="content",
                        content="\nâŒ No content generated for final presentation and no stored answer available.",
                        source=selected_agent_id,
                    )
            else:
                # Cancelled - use stored answer without yielding
                stored_answer = self.agent_states[selected_agent_id].answer
                if stored_answer:
                    self._final_presentation_content = stored_answer

            # Note: end_round_tracking for presentation is called from _present_final_answer
            # after the async for loop completes, to ensure reliable timing before save_coordination_logs

            # Mark final round as completed
            self.coordination_tracker.change_status(
                selected_agent_id,
                AgentStatus.COMPLETED,
            )

            # Compute context path writes (compare current state to snapshot taken before presentation)
            if agent.backend.filesystem_manager:
                agent.backend.filesystem_manager.path_permission_manager.compute_context_path_writes()

            # Add token usage and cost to presentation span before closing
            if hasattr(agent.backend, "token_usage") and agent.backend.token_usage:
                token_usage = agent.backend.token_usage
                _presentation_span.set_attribute(
                    "massgen.usage.input",
                    token_usage.input_tokens or 0,
                )
                _presentation_span.set_attribute(
                    "massgen.usage.output",
                    token_usage.output_tokens or 0,
                )
                _presentation_span.set_attribute(
                    "massgen.usage.reasoning",
                    token_usage.reasoning_tokens or 0,
                )
                _presentation_span.set_attribute(
                    "massgen.usage.cached_input",
                    token_usage.cached_input_tokens or 0,
                )
                _presentation_span.set_attribute(
                    "massgen.usage.cost",
                    round(token_usage.estimated_cost or 0, 6),
                )

            # Close the presentation span for hierarchical tracing
            # Wrap in try/except to handle OpenTelemetry context issues in async generators
            try:
                _presentation_span_cm.__exit__(None, None, None)
            except ValueError as e:
                # Context detach failures are expected in async generators - safe to ignore
                if "context" not in str(e).lower() and "detach" not in str(e).lower():
                    logger.debug(
                        f"Unexpected ValueError closing presentation span: {e}",
                    )

            # Clear the round context
            clear_current_round()

        # Don't yield done here - let _present_final_answer handle final done after post-evaluation

    async def post_evaluate_answer(
        self,
        selected_agent_id: str,
        final_answer: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Post-evaluation phase where winning agent evaluates its own answer.

        The agent reviews the final answer and decides whether to submit or restart
        with specific improvement instructions.

        Args:
            selected_agent_id: The agent that won the vote and presented the answer
            final_answer: The final answer that was presented

        Yields:
            StreamChunk: Stream chunks from the evaluation process
        """
        if selected_agent_id not in self.agents:
            log_stream_chunk(
                "orchestrator",
                "error",
                f"Selected agent {selected_agent_id} not found for post-evaluation",
            )
            yield StreamChunk(
                type="error",
                error=f"Selected agent {selected_agent_id} not found",
            )
            return

        agent = self.agents[selected_agent_id]

        # Use debug override on first attempt if configured
        eval_answer = final_answer
        if self.config.debug_final_answer and self.current_attempt == 0:
            eval_answer = self.config.debug_final_answer
            log_stream_chunk(
                "orchestrator",
                "debug",
                f"Using debug override for post-evaluation: {self.config.debug_final_answer}",
            )
            yield StreamChunk(
                type="debug",
                content=f"[DEBUG MODE] Overriding answer for evaluation: {self.config.debug_final_answer}",
                source="orchestrator",
            )

        # Build evaluation message
        evaluation_content = f"""{self.message_templates.format_original_message(self.current_task or "Task")}

FINAL ANSWER TO EVALUATE:
{eval_answer}

Review this answer carefully and determine if it fully addresses the original task. Use your available tools to verify claims and check files as needed.
Then call either submit(confirmed=True) if the answer is satisfactory, or restart_orchestration(reason, instructions) if improvements are needed."""

        # Get all answers for context
        all_answers = {aid: s.answer for aid, s in self.agent_states.items() if s.answer}

        # Build post-evaluation system message using section architecture
        base_system_message = self._get_system_message_builder().build_post_evaluation_message(
            agent=agent,
            all_answers=all_answers,
            previous_turns=self._previous_turns,
        )

        # Create evaluation messages
        evaluation_messages = [
            {"role": "system", "content": base_system_message},
            {"role": "user", "content": evaluation_content},
        ]

        # Get post-evaluation tools
        api_format = "chat_completions"  # Default format
        if hasattr(agent.backend, "api_format"):
            api_format = agent.backend.api_format
        post_eval_tools = get_post_evaluation_tools(api_format=api_format)

        log_stream_chunk(
            "orchestrator",
            "status",
            "ðŸ” Post-evaluation: Reviewing final answer\n",
        )
        yield StreamChunk(
            type="status",
            content="ðŸ” Post-evaluation: Reviewing final answer\n",
            source="orchestrator",
        )

        # Start round token tracking for post-evaluation
        post_eval_round = self.coordination_tracker.get_agent_round(selected_agent_id) + 1
        if hasattr(agent.backend, "start_round_tracking"):
            agent.backend.start_round_tracking(
                round_number=post_eval_round,
                round_type="post_evaluation",
                agent_id=selected_agent_id,
            )

        # Stream evaluation with tools (with timeout protection)
        evaluation_complete = False
        tool_call_detected = False
        accumulated_content = ""  # Buffer to detect inline JSON across chunks

        try:
            timeout_seconds = self.config.timeout_config.orchestrator_timeout_seconds
            async with asyncio.timeout(timeout_seconds):
                async for chunk in agent.chat(
                    messages=evaluation_messages,
                    tools=post_eval_tools,
                    reset_chat=True,  # Reset conversation history for clean evaluation
                    current_stage=CoordinationStage.POST_EVALUATION,
                    orchestrator_turn=self._current_turn,
                    previous_winners=self._winning_agents_history.copy(),
                ):
                    chunk_type = self._get_chunk_type_value(chunk)

                    if chunk_type == "content" and chunk.content:
                        log_stream_chunk(
                            "orchestrator",
                            "content",
                            chunk.content,
                            selected_agent_id,
                        )
                        yield StreamChunk(
                            type="content",
                            content=chunk.content,
                            source=selected_agent_id,
                        )

                        # Accumulate content for JSON parsing across chunks
                        accumulated_content += chunk.content

                        # Fallback: parse inline JSON tool calls from accumulated content
                        # Some backends output submit/restart as JSON text instead of tool_calls
                        if not evaluation_complete and not tool_call_detected:
                            # Try to extract and parse JSON from accumulated content
                            import json
                            import re

                            # Find JSON objects in the content (handle nested braces)
                            # Look for { ... "action_type" ... } allowing nested braces
                            json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*"action_type"[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', accumulated_content, re.DOTALL)
                            for json_str in json_matches:
                                try:
                                    data = json.loads(json_str)
                                    action_type = data.get("action_type")

                                    if action_type == "submit":
                                        tool_call_detected = True
                                        log_stream_chunk(
                                            "orchestrator",
                                            "status",
                                            "âœ… Evaluation complete - answer approved\n",
                                        )
                                        yield StreamChunk(
                                            type="status",
                                            content="âœ… Evaluation complete - answer approved\n",
                                            source="orchestrator",
                                        )
                                        evaluation_complete = True
                                        break
                                    elif action_type == "restart_orchestration":
                                        tool_call_detected = True
                                        restart_data = data.get("restart_data", {})
                                        self.restart_reason = restart_data.get("reason", data.get("reason", "Answer needs improvement"))
                                        self.restart_instructions = restart_data.get("instructions", data.get("instructions", ""))
                                        self.restart_pending = True

                                        log_stream_chunk(
                                            "orchestrator",
                                            "status",
                                            "ðŸ”„ Restart requested\n",
                                        )
                                        yield StreamChunk(
                                            type="status",
                                            content="ðŸ”„ Restart requested\n",
                                            source="orchestrator",
                                        )
                                        evaluation_complete = True
                                        break
                                except json.JSONDecodeError:
                                    # Not valid JSON yet, keep accumulating
                                    pass
                    elif chunk_type in [
                        "reasoning",
                        "reasoning_done",
                        "reasoning_summary",
                        "reasoning_summary_done",
                    ]:
                        reasoning_chunk = StreamChunk(
                            type=chunk_type,
                            content=chunk.content,
                            source=selected_agent_id,
                            reasoning_delta=getattr(chunk, "reasoning_delta", None),
                            reasoning_text=getattr(chunk, "reasoning_text", None),
                            reasoning_summary_delta=getattr(
                                chunk,
                                "reasoning_summary_delta",
                                None,
                            ),
                            reasoning_summary_text=getattr(
                                chunk,
                                "reasoning_summary_text",
                                None,
                            ),
                            item_id=getattr(chunk, "item_id", None),
                            content_index=getattr(chunk, "content_index", None),
                            summary_index=getattr(chunk, "summary_index", None),
                        )
                        log_stream_chunk(
                            "orchestrator",
                            chunk.type,
                            chunk.content,
                            selected_agent_id,
                        )
                        yield reasoning_chunk
                    elif chunk_type == "tool_calls":
                        # Post-evaluation tool call detected - only set flag if valid tool found
                        if hasattr(chunk, "tool_calls") and chunk.tool_calls:
                            for tool_call in chunk.tool_calls:
                                # Use backend's tool extraction (same as regular coordination)
                                tool_name = agent.backend.extract_tool_name(tool_call)
                                tool_args = agent.backend.extract_tool_arguments(
                                    tool_call,
                                )

                                # Only set tool_call_detected if we got a valid tool name
                                if tool_name:
                                    tool_call_detected = True

                                if tool_name == "submit":
                                    log_stream_chunk(
                                        "orchestrator",
                                        "status",
                                        "âœ… Evaluation complete - answer approved\n",
                                    )
                                    yield StreamChunk(
                                        type="status",
                                        content="âœ… Evaluation complete - answer approved\n",
                                        source="orchestrator",
                                    )
                                    evaluation_complete = True
                                elif tool_name == "restart_orchestration":
                                    # Parse restart parameters from extracted args
                                    self.restart_reason = tool_args.get(
                                        "reason",
                                        "No reason provided",
                                    )
                                    self.restart_instructions = tool_args.get(
                                        "instructions",
                                        "No instructions provided",
                                    )
                                    self.restart_pending = True

                                    # Save the current winning answer for next attempt's context
                                    if self._selected_agent and self._selected_agent in self.agent_states:
                                        self.previous_attempt_answer = self.agent_states[self._selected_agent].answer
                                        logger.info(
                                            f"Saved previous attempt answer from {self._selected_agent} for restart context",
                                        )

                                    log_stream_chunk(
                                        "orchestrator",
                                        "status",
                                        "ðŸ”„ Restart requested\n",
                                    )
                                    yield StreamChunk(
                                        type="status",
                                        content="ðŸ”„ Restart requested\n",
                                        source="orchestrator",
                                    )
                                    evaluation_complete = True
                    elif chunk_type == "done":
                        log_stream_chunk(
                            "orchestrator",
                            "done",
                            None,
                            selected_agent_id,
                        )
                        yield StreamChunk(type="done", source=selected_agent_id)
                    elif chunk_type == "error":
                        log_stream_chunk(
                            "orchestrator",
                            "error",
                            chunk.error,
                            selected_agent_id,
                        )
                        yield StreamChunk(
                            type="error",
                            error=chunk.error,
                            source=selected_agent_id,
                        )
                    else:
                        # Pass through other chunk types
                        log_stream_chunk(
                            "orchestrator",
                            chunk_type,
                            getattr(chunk, "content", ""),
                            selected_agent_id,
                        )
                        yield StreamChunk(
                            type=chunk_type,
                            content=getattr(chunk, "content", ""),
                            source=selected_agent_id,
                            **{
                                k: v
                                for k, v in chunk.__dict__.items()
                                if k
                                not in [
                                    "type",
                                    "content",
                                    "source",
                                    "timestamp",
                                    "sequence_number",
                                ]
                            },
                        )
        except asyncio.TimeoutError:
            log_stream_chunk(
                "orchestrator",
                "status",
                "â±ï¸ Post-evaluation timed out - auto-submitting answer\n",
            )
            yield StreamChunk(
                type="status",
                content="â±ï¸ Post-evaluation timed out - auto-submitting answer\n",
                source="orchestrator",
            )
            evaluation_complete = True
            # Don't set restart_pending - let it default to False (auto-submit)
        finally:
            # Note: end_round_tracking for post_evaluation is called from _present_final_answer
            # after the async for loop completes, to ensure reliable timing before save_coordination_logs

            # If evaluation didn't complete (no submit/restart called), auto-submit
            # This handles cases where:
            # 1. No tool was called at all
            # 2. Tools were called but not submit/restart (e.g., read_file for verification)
            if not evaluation_complete:
                log_stream_chunk(
                    "orchestrator",
                    "status",
                    "âœ… Evaluation complete - answer approved\n",
                )
                yield StreamChunk(
                    type="status",
                    content="âœ… Evaluation complete - answer approved\n",
                    source="orchestrator",
                )

    def handle_restart(self):
        """Reset orchestration state for restart attempt.

        Clears agent states and coordination messages while preserving
        restart reason and instructions for the next attempt.
        """
        log_orchestrator_activity(
            "handle_restart",
            f"Resetting state for restart attempt {self.current_attempt + 1}",
        )

        # Reset agent states
        for agent_id in self.agent_states:
            self.agent_states[agent_id] = AgentState()

        # Clear coordination messages
        self._coordination_messages = []
        self._selected_agent = None
        self._final_presentation_content = None

        # Reset coordination tracker for new attempt (MAS-199: includes log_path)
        self.coordination_tracker = CoordinationTracker()
        log_dir = get_log_session_dir()
        log_path = str(log_dir) if log_dir else None
        self.coordination_tracker.initialize_session(
            list(self.agents.keys()),
            log_path=log_path,
        )

        # Reset MCP initialization flag to force tool re-setup on next agent.chat()
        # This ensures agents get full tool set after restart (not limited set from timeout)
        for agent_key, agent in self.agents.items():
            if hasattr(agent.backend, "_mcp_initialized"):
                agent.backend._mcp_initialized = False
                logger.info(
                    f"[Orchestrator] Reset MCP initialized flag for agent {agent_key}",
                )

        # Reset workflow phase to idle so next coordinate() call starts fresh
        self.workflow_phase = "idle"

        # Increment attempt counter
        self.current_attempt += 1

        log_orchestrator_activity(
            "handle_restart",
            f"State reset complete - starting attempt {self.current_attempt + 1}",
        )

    def _should_skip_injection_due_to_timeout(self, agent_id: str) -> bool:
        """Check if mid-stream injection should be skipped due to approaching timeout.

        If the agent doesn't have enough time remaining before soft timeout to properly
        consider a new answer, it's better to skip injection and let the agent restart
        with fresh context so they get a full round to think.

        Args:
            agent_id: The agent to check

        Returns:
            True if injection should be skipped, False otherwise
        """
        timeout_config = self.config.timeout_config
        round_start = self.agent_states[agent_id].round_start_time

        if round_start is None:
            return False

        current_round = self.coordination_tracker.get_agent_round(agent_id)
        if current_round == 0:
            soft_timeout = timeout_config.initial_round_timeout_seconds
        else:
            soft_timeout = timeout_config.subsequent_round_timeout_seconds

        if soft_timeout is None:
            return False

        elapsed = time.time() - round_start
        min_thinking_time = timeout_config.round_timeout_grace_seconds
        remaining = soft_timeout - elapsed

        if remaining < min_thinking_time:
            logger.info(
                f"[Orchestrator] Skipping mid-stream injection for {agent_id} - " f"only {remaining:.0f}s until soft timeout (need {min_thinking_time}s to think)",
            )
            return True

        return False

    def _get_vote_results(self) -> Dict[str, Any]:
        """Get current vote results and statistics."""
        agent_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}
        votes = {aid: state.votes for aid, state in self.agent_states.items() if state.votes}

        # Count votes for each agent
        vote_counts = {}
        voter_details = {}

        for voter_id, vote_data in votes.items():
            voted_for = vote_data.get("agent_id")
            if voted_for:
                vote_counts[voted_for] = vote_counts.get(voted_for, 0) + 1
                if voted_for not in voter_details:
                    voter_details[voted_for] = []
                voter_details[voted_for].append(
                    {
                        "voter": voter_id,
                        "reason": vote_data.get("reason", "No reason provided"),
                    },
                )

        # Determine winner
        winner = None
        is_tie = False
        if vote_counts:
            max_votes = max(vote_counts.values())
            tied_agents = [agent_id for agent_id, count in vote_counts.items() if count == max_votes]
            is_tie = len(tied_agents) > 1

            # Break ties by agent registration order
            for agent_id in agent_answers.keys():
                if agent_id in tied_agents:
                    winner = agent_id
                    break

            if not winner:
                winner = tied_agents[0] if tied_agents else None

        # Create agent mapping for anonymous display
        # Use global mapping (all agents) for consistency with vote tool and injections
        agent_mapping = self.coordination_tracker.get_anonymous_agent_mapping()

        return {
            "vote_counts": vote_counts,
            "voter_details": voter_details,
            "winner": winner,
            "is_tie": is_tie,
            "total_votes": len(votes),
            "agents_with_answers": len(agent_answers),
            "agents_voted": len([v for v in votes.values() if v.get("agent_id")]),
            "agent_mapping": agent_mapping,
        }

    def _determine_final_agent_from_states(self) -> Optional[str]:
        """Determine final agent based on current agent states."""
        # Find agents with answers
        agents_with_answers = {aid: state.answer for aid, state in self.agent_states.items() if state.answer}

        if not agents_with_answers:
            return None

        # Return the first agent with an answer (by order in agent_states)
        return next(iter(agents_with_answers))

    async def _handle_followup(
        self,
        user_message: str,
        conversation_context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Handle follow-up questions after presenting final answer with conversation context."""
        # Analyze the follow-up question for irreversibility before re-coordinating
        has_irreversible = await self._analyze_question_irreversibility(
            user_message,
            conversation_context or {},
        )

        # Set planning mode for all agents based on analysis
        for agent_id, agent in self.agents.items():
            if hasattr(agent.backend, "set_planning_mode"):
                agent.backend.set_planning_mode(has_irreversible)
                log_orchestrator_activity(
                    self.orchestrator_id,
                    f"Set planning mode for {agent_id} (follow-up)",
                    {
                        "planning_mode_enabled": has_irreversible,
                        "reason": "follow-up irreversibility analysis",
                    },
                )

        # For now, acknowledge with context awareness
        # Future: implement full re-coordination with follow-up context

        if conversation_context and len(conversation_context.get("conversation_history", [])) > 0:
            log_stream_chunk(
                "orchestrator",
                "content",
                f"ðŸ¤” Thank you for your follow-up question in our ongoing conversation. I understand you're asking: "
                f"'{user_message}'. Currently, the coordination is complete, but I can help clarify the answer or "
                f"coordinate a new task that takes our conversation history into account.",
            )
            yield StreamChunk(
                type="content",
                content=f"ðŸ¤” Thank you for your follow-up question in our ongoing conversation. I understand you're "
                f"asking: '{user_message}'. Currently, the coordination is complete, but I can help clarify the answer "
                f"or coordinate a new task that takes our conversation history into account.",
            )
        else:
            log_stream_chunk(
                "orchestrator",
                "content",
                f"ðŸ¤” Thank you for your follow-up: '{user_message}'. The coordination is complete, but I can help clarify the answer or coordinate a new task if needed.",
            )
            yield StreamChunk(
                type="content",
                content=f"ðŸ¤” Thank you for your follow-up: '{user_message}'. The coordination is complete, but I can help clarify the answer or coordinate a new task if needed.",
            )

        log_stream_chunk("orchestrator", "done", None)
        yield StreamChunk(type="done")

    # =============================================================================
    # PUBLIC API METHODS
    # =============================================================================

    def add_agent(self, agent_id: str, agent: ChatAgent) -> None:
        """Add a new sub-agent to the orchestrator."""
        self.agents[agent_id] = agent
        self.agent_states[agent_id] = AgentState()

    def remove_agent(self, agent_id: str) -> None:
        """Remove a sub-agent from the orchestrator."""
        if agent_id in self.agents:
            del self.agents[agent_id]
        if agent_id in self.agent_states:
            del self.agent_states[agent_id]

    def get_final_result(self) -> Optional[Dict[str, Any]]:
        """
        Get final result for session persistence.

        Returns:
            Dict with final_answer, winning_agent_id, workspace_path, and winning_agents_history,
            or None if not available
        """
        if not self._selected_agent or not self._final_presentation_content:
            return None

        # Use the final log directory workspace which we just saved
        # This is guaranteed to have the correct content (from either workspace or snapshot_storage)
        from massgen.logger_config import get_log_session_dir

        workspace_path = None
        log_session_dir = get_log_session_dir()
        if log_session_dir:
            final_workspace = log_session_dir / "final" / self._selected_agent / "workspace"
            if final_workspace.exists():
                workspace_path = str(final_workspace)
                logger.info(f"[Orchestrator] Using final log workspace for session persistence: {workspace_path}")

        return {
            "final_answer": self._final_presentation_content,
            "winning_agent_id": self._selected_agent,
            "workspace_path": workspace_path,
            "winning_agents_history": self._winning_agents_history.copy(),  # For cross-turn memory sharing
        }

    def get_partial_result(self) -> Optional[Dict[str, Any]]:
        """Get partial coordination result for interrupted sessions.

        Captures whatever state is available mid-coordination, useful when
        a user cancels with Ctrl+C before coordination completes.

        Returns:
            Dict with partial state including:
            - status: Always "incomplete"
            - phase: Current workflow phase
            - current_task: The task being worked on
            - answers: Dict of agent_id -> answer data for agents with answers
            - workspaces: Dict of agent_id -> workspace path
            - selected_agent: Winning agent if voting completed, else None
            - coordination_tracker: Coordination state if available

            Returns None if no answers have been generated yet.

        Example:
            >>> partial = orchestrator.get_partial_result()
            >>> if partial:
            ...     save_partial_turn(session_id, turn, task, partial)
        """
        # Collect any answers that have been submitted
        answers = {}
        for agent_id, state in self.agent_states.items():
            if state.answer:
                answers[agent_id] = {
                    "answer": state.answer,
                    "has_voted": state.has_voted,
                    "votes": state.votes if state.has_voted else None,
                    "answer_count": state.answer_count,
                }

        # Get all agent workspaces (even those without answers)
        workspaces = self.get_all_agent_workspaces()

        def has_files_recursive(directory: Path) -> bool:
            """Check if directory contains any files (recursively)."""
            if not directory.is_dir():
                return False
            for item in directory.iterdir():
                if item.is_file():
                    return True
                if item.is_dir() and has_files_recursive(item):
                    return True
            return False

        # Check if any workspaces have content (actual files, not just empty dirs)
        workspaces_with_content = {}
        for agent_id, ws_path in workspaces.items():
            if ws_path and Path(ws_path).exists():
                ws = Path(ws_path)
                if has_files_recursive(ws):
                    workspaces_with_content[agent_id] = ws_path

        # If no answers AND no workspaces with content, nothing worth saving
        if not answers and not workspaces_with_content:
            return None

        # Check if voting is complete (all non-killed agents have voted)
        active_agents = [state for state in self.agent_states.values() if not state.is_killed]
        voting_complete = all(state.has_voted for state in active_agents) if active_agents else False

        # Build partial result
        result = {
            "status": "incomplete",
            "phase": self.workflow_phase,
            "current_task": self.current_task,
            "answers": answers,
            "workspaces": workspaces_with_content,  # Only include workspaces with content
            "selected_agent": self._selected_agent,  # May be None if voting incomplete
            "voting_complete": voting_complete,  # Whether all agents had voted before cancellation
        }

        # Include coordination tracker state if available
        if self.coordination_tracker:
            try:
                result["coordination_tracker"] = self.coordination_tracker.to_dict()
            except Exception:
                # Don't fail if tracker serialization fails
                pass

        return result

    def get_all_agent_workspaces(self) -> Dict[str, Optional[str]]:
        """Get workspace paths for all agents.

        Returns:
            Dict mapping agent_id to workspace path (or None if no workspace)
        """
        workspaces = {}
        for agent_id, agent in self.agents.items():
            if hasattr(agent, "backend") and hasattr(
                agent.backend,
                "filesystem_manager",
            ):
                fm = agent.backend.filesystem_manager
                if fm:
                    workspaces[agent_id] = str(fm.get_current_workspace())
                else:
                    workspaces[agent_id] = None
            else:
                workspaces[agent_id] = None
        return workspaces

    def get_coordination_result(self) -> Dict[str, Any]:
        """Get comprehensive coordination result for API consumption.

        Returns:
            Dict with all coordination metadata:
            - final_answer: The final presented answer
            - selected_agent: ID of the winning agent
            - log_directory: Root log directory path
            - final_answer_path: Path to final/ directory
            - answers: List of answers with labels (answerX.Y), paths, and content
            - vote_results: Full voting details
        """
        from pathlib import Path

        from .logger_config import get_log_session_dir, get_log_session_root

        # Get log paths
        log_root = None
        log_session_dir = None
        final_path = None
        try:
            log_root = get_log_session_root()
            log_session_dir = get_log_session_dir()
            final_path = log_session_dir / "final"
        except Exception:
            pass  # Log paths not available

        # Build answers list from snapshot_mappings with full log paths
        answers = []
        if self.coordination_tracker and self.coordination_tracker.snapshot_mappings:
            for label, mapping in self.coordination_tracker.snapshot_mappings.items():
                if mapping.get("type") == "answer":
                    # Build full path from log_session_dir + relative path
                    answer_dir = None
                    if log_session_dir and mapping.get("path"):
                        answer_dir = str(log_session_dir / Path(mapping["path"]).parent)

                    # Get answer content from agent state
                    agent_id = mapping.get("agent_id")
                    content = None
                    if agent_id and agent_id in self.agent_states:
                        content = self.agent_states[agent_id].answer

                    answers.append(
                        {
                            "label": label,  # e.g., "agent1.1"
                            "agent_id": agent_id,
                            "answer_path": answer_dir,
                            "content": content,
                        },
                    )

        # Get vote results
        vote_results = self._get_vote_results()

        # Aggregate token usage across all agents
        total_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
        for agent in self.agents.values():
            backend = getattr(agent, "backend", None)
            if backend and hasattr(backend, "token_usage") and backend.token_usage:
                # Finalize tracking if available
                if hasattr(backend, "finalize_token_tracking"):
                    try:
                        backend.finalize_token_tracking()
                    except Exception:
                        pass
                tu = backend.token_usage
                prompt = tu.input_tokens + tu.cached_input_tokens + tu.cache_creation_tokens
                completion = tu.output_tokens + tu.reasoning_tokens
                total_usage["prompt_tokens"] += prompt
                total_usage["completion_tokens"] += completion
                total_usage["total_tokens"] += prompt + completion

        return {
            "final_answer": self._final_presentation_content or "",
            "selected_agent": self._selected_agent,
            "log_directory": str(log_root) if log_root else None,
            "final_answer_path": str(final_path) if final_path else None,
            "answers": answers,
            "vote_results": vote_results,
            "usage": total_usage,
        }

    def get_status(self) -> Dict[str, Any]:
        """Get current orchestrator status."""
        # Calculate vote results
        vote_results = self._get_vote_results()

        return {
            "session_id": self.session_id,
            "workflow_phase": self.workflow_phase,
            "current_task": self.current_task,
            "selected_agent": self._selected_agent,
            "final_presentation_content": self._final_presentation_content,
            "vote_results": vote_results,
            "agents": {
                aid: {
                    "agent_status": agent.get_status(),
                    "coordination_state": {
                        "answer": state.answer,
                        "has_voted": state.has_voted,
                    },
                }
                for aid, (agent, state) in zip(
                    self.agents.keys(),
                    zip(self.agents.values(), self.agent_states.values()),
                )
            },
            "conversation_length": len(self.conversation_history),
        }

    def get_agent_timeout_state(self, agent_id: str) -> Optional[Dict[str, Any]]:
        """Get timeout state for display purposes.

        Returns timeout countdown and status information for a specific agent,
        used by TUI and WebUI to show per-agent timeout progress.

        Args:
            agent_id: The agent identifier

        Returns:
            Dictionary with timeout state, or None if agent not found.
            Contains:
                - round_number: Current coordination round
                - round_start_time: When current round started
                - active_timeout: Soft timeout for current round type (initial/subsequent)
                - grace_seconds: Grace period before hard block
                - elapsed: Seconds elapsed since round start
                - remaining_soft: Seconds until soft timeout
                - remaining_hard: Seconds until hard block
                - soft_timeout_fired: Whether soft timeout warning was injected
                - is_hard_blocked: Whether hard timeout is active (tools blocked)
        """
        state = self.agent_states.get(agent_id)
        if not state:
            return None

        timeout_config = self.config.timeout_config
        round_num = self.coordination_tracker.get_agent_round(agent_id)

        # Determine active timeout based on round
        if round_num == 0:
            active_timeout = timeout_config.initial_round_timeout_seconds
        else:
            active_timeout = timeout_config.subsequent_round_timeout_seconds

        # Calculate elapsed and remaining
        elapsed: Optional[float] = None
        remaining_soft: Optional[float] = None
        remaining_hard: Optional[float] = None

        if state.round_start_time and active_timeout:
            elapsed = time.time() - state.round_start_time
            remaining_soft = max(0, active_timeout - elapsed)
            grace = timeout_config.round_timeout_grace_seconds or 0
            remaining_hard = max(0, active_timeout + grace - elapsed)

        # Get soft timeout fired status from hook
        soft_timeout_fired = False
        if state.round_timeout_hooks:
            post_hook, _ = state.round_timeout_hooks
            # Access the private attribute that tracks if soft timeout fired
            soft_timeout_fired = getattr(post_hook, "_soft_timeout_fired", False)

        return {
            "round_number": round_num,
            "round_start_time": state.round_start_time,
            "active_timeout": active_timeout,
            "grace_seconds": timeout_config.round_timeout_grace_seconds or 0,
            "elapsed": elapsed,
            "remaining_soft": remaining_soft,
            "remaining_hard": remaining_hard,
            "soft_timeout_fired": soft_timeout_fired,
            "is_hard_blocked": remaining_hard == 0 if remaining_hard is not None else False,
        }

    def get_configurable_system_message(self) -> Optional[str]:
        """
        Get the configurable system message for the orchestrator.

        This can define how the orchestrator should coordinate agents, construct messages,
        handle conflicts, make decisions, etc. For example:
        - Custom voting strategies
        - Message construction templates
        - Conflict resolution approaches
        - Coordination workflow preferences

        Returns:
            Orchestrator's configurable system message if available, None otherwise
        """
        if self.config and hasattr(self.config, "get_configurable_system_message"):
            return self.config.get_configurable_system_message()
        elif self.config and hasattr(self.config, "_custom_system_instruction"):
            # Access private attribute to avoid deprecation warning
            return self.config._custom_system_instruction
        elif self.config and self.config.backend_params:
            # Check for backend-specific system prompts
            backend_params = self.config.backend_params
            if "system_prompt" in backend_params:
                return backend_params["system_prompt"]
            elif "append_system_prompt" in backend_params:
                return backend_params["append_system_prompt"]
        return None

    def _get_system_message_builder(self) -> SystemMessageBuilder:
        """Get or create the SystemMessageBuilder instance.

        Returns:
            SystemMessageBuilder instance initialized with orchestrator's config and state
        """
        if self._system_message_builder is None:
            self._system_message_builder = SystemMessageBuilder(
                config=self.config,
                message_templates=self.message_templates,
                agents=self.agents,
                snapshot_storage=self._snapshot_storage,
                session_id=self.session_id,
                agent_temporary_workspace=self._agent_temporary_workspace,
            )
        return self._system_message_builder

    def _clear_agent_workspaces(self) -> None:
        """
        Clear all agent workspaces and pre-populate with previous turn's results.

        This creates a WRITABLE copy of turn n-1 in each agent's workspace.
        Note: CLI separately provides turn n-1 as a READ-ONLY context path, allowing
        agents to both modify files (in workspace) and reference originals (via context path).
        """
        # Get previous turn (n-1) workspace for pre-population
        previous_turn_workspace = None
        if self._previous_turns:
            # Get the most recent turn (last in list)
            latest_turn = self._previous_turns[-1]
            previous_turn_workspace = Path(latest_turn["path"])

        for agent_id, agent in self.agents.items():
            if agent.backend.filesystem_manager:
                workspace_path = agent.backend.filesystem_manager.get_current_workspace()
                if workspace_path and Path(workspace_path).exists():
                    # Archive memories BEFORE clearing workspace
                    self._archive_agent_memories(agent_id, Path(workspace_path))

                    # Clear workspace contents but keep the directory
                    for item in Path(workspace_path).iterdir():
                        if item.is_symlink():
                            # Remove symlinks directly (don't follow them)
                            item.unlink()
                        elif item.is_file():
                            item.unlink()
                        elif item.is_dir():
                            shutil.rmtree(item)
                    logger.info(
                        f"[Orchestrator] Cleared workspace for {agent_id}: {workspace_path}",
                    )

                    # Check if this is a planâ†’execute transition
                    # For plan execution, we want a clean workspace (planning artifacts in context only)
                    skip_workspace_copy = False
                    if self._plan_session_id:
                        skip_workspace_copy = True
                        logger.info(
                            f"[Orchestrator] Skipping workspace pre-population for plan execution (plan_session: {self._plan_session_id})",
                        )

                    # Pre-populate with previous turn's results if available (creates writable copy)
                    if not skip_workspace_copy and previous_turn_workspace and previous_turn_workspace.exists():
                        logger.info(
                            f"[Orchestrator] Pre-populating {agent_id} workspace with writable copy of turn n-1 from {previous_turn_workspace}",
                        )
                        for item in previous_turn_workspace.iterdir():
                            dest = Path(workspace_path) / item.name
                            if item.is_file():
                                shutil.copy2(item, dest)
                            elif item.is_dir():
                                shutil.copytree(
                                    item,
                                    dest,
                                    dirs_exist_ok=True,
                                    symlinks=True,
                                    ignore_dangling_symlinks=True,
                                )
                        logger.info(
                            f"[Orchestrator] Pre-populated {agent_id} workspace with writable copy of turn n-1",
                        )

    def _archive_agent_memories(self, agent_id: str, workspace_path: Path) -> None:
        """
        Archive memories from agent workspace before clearing.

        Copies all memory files from workspace/memory/ to archived_memories/{agent_id}_answer_{n}/
        This preserves memories from discarded answers so they're not lost.

        Args:
            agent_id: ID of the agent whose memories to archive
            workspace_path: Path to the agent's current workspace
        """
        memory_dir = workspace_path / "memory"
        if not memory_dir.exists():
            logger.info(
                f"[Orchestrator] No memory directory for {agent_id}, skipping archive",
            )
            return

        # Get current answer count for this agent
        answer_num = self.agent_states[agent_id].answer_count

        # Archive path: .massgen/sessions/{session_id}/archived_memories/agent_id_answer_n/
        # Use hardcoded session storage path (not snapshot_storage which gets cleared)
        if not self.session_id:
            logger.warning("[Orchestrator] Cannot archive memories: no session_id")
            return

        # Archives must be in sessions/ directory for persistence, not snapshots/
        archive_base = Path(".massgen/sessions") / self.session_id / "archived_memories"
        archive_path = archive_base / f"{agent_id}_answer_{answer_num}"

        # Copy entire memory/ directory to archive
        try:
            archive_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copytree(
                memory_dir,
                archive_path,
                dirs_exist_ok=True,
                symlinks=True,
                ignore_dangling_symlinks=True,
            )
            logger.info(
                f"[Orchestrator] Archived memories for {agent_id} answer {answer_num} to {archive_path}",
            )
        except Exception as e:
            logger.error(
                f"[Orchestrator] Failed to archive memories for {agent_id}: {e}",
            )

        # Increment answer count for next answer
        self.agent_states[agent_id].answer_count += 1

    def _get_previous_turns_context_paths(self) -> List[Dict[str, Any]]:
        """
        Get previous turns as context paths for current turn's agents.

        Returns:
            List of previous turn information with path, turn number, and task
        """
        return self._previous_turns

    async def reset(self) -> None:
        """Reset orchestrator state for new task."""
        self.conversation_history.clear()
        self.current_task = None
        self.workflow_phase = "idle"
        self._coordination_messages.clear()
        self._selected_agent = None
        self._final_presentation_content = None

        # Reset agent states
        for state in self.agent_states.values():
            state.answer = None
            state.has_voted = False
            state.votes = {}  # Clear stale vote data
            state.restart_pending = False
            state.is_killed = False
            state.timeout_reason = None
            state.answer_count = 0
            state.injection_count = 0
            state.restart_count = 0
            state.known_answer_ids = set()

        # Reset orchestrator timeout tracking
        self.total_tokens = 0
        self.coordination_start_time = 0
        self.is_orchestrator_timeout = False
        self.timeout_reason = None

        # Clear coordination state
        self._active_streams = {}
        self._active_tasks = {}

        if self.dspy_paraphraser:
            self.dspy_paraphraser.clear_cache()


# =============================================================================
# CONVENIENCE FUNCTIONS
# =============================================================================


def create_orchestrator(
    agents: List[tuple],
    orchestrator_id: str = "orchestrator",
    session_id: Optional[str] = None,
    config: Optional[AgentConfig] = None,
    snapshot_storage: Optional[str] = None,
    agent_temporary_workspace: Optional[str] = None,
) -> Orchestrator:
    """
    Create a MassGen orchestrator with sub-agents.

    Args:
        agents: List of (agent_id, ChatAgent) tuples
        orchestrator_id: Unique identifier for this orchestrator (default: "orchestrator")
        session_id: Optional session ID
        config: Optional AgentConfig for orchestrator customization
        snapshot_storage: Optional path to store agent workspace snapshots
        agent_temporary_workspace: Optional path for agent temporary workspaces (for Claude Code context sharing)

    Returns:
        Configured Orchestrator
    """
    agents_dict = {agent_id: agent for agent_id, agent in agents}

    return Orchestrator(
        agents=agents_dict,
        orchestrator_id=orchestrator_id,
        session_id=session_id,
        config=config,
        snapshot_storage=snapshot_storage,
        agent_temporary_workspace=agent_temporary_workspace,
    )
