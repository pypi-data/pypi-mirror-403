import os
from agents import Agent, Runner, function_tool
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from mcp_client.client import get_streamable_http_mcp_client
from model.load import load_model

if os.getenv("LOCAL_DEV") == "1":
    from contextlib import nullcontext
    mcp_server = nullcontext(None)
else:
    # Import AgentCore Gateway as Streamable HTTP MCP Server
    mcp_server = get_streamable_http_mcp_client()

# Set environment variables for model authentication
load_model()

# Define a simple function tool
@function_tool
def add_numbers(a: int, b: int) -> int:
    """Return the sum of two numbers"""
    return a+b

# Integrate with Bedrock AgentCore
app = BedrockAgentCoreApp()
logger = app.logger

# Define an Agent with tools
async def main(query):
    try:
        async with mcp_server as server:
            active_servers = [server] if server else []
            # Currently defaults to GPT-4.1
            # https://openai.github.io/openai-agents-python/models/
            agent = Agent(
                name="{{ agent_name }}",
                mcp_servers=active_servers,
                tools=[add_numbers]
            )
            result = await Runner.run(agent, query)
            return result
    except Exception as e:
        logger.error(f"Error during agent execution: {e}", exc_info=True)
        raise e

@app.entrypoint
async def agent_invocation(payload, context):
    # assume payload input is structured as { "prompt": "<user input>" }

    # Process the user prompt
    prompt = payload.get("prompt", "What is Agentic AI?")

    # Run the agent
    result = await main(prompt)

    # Return result
    return {"result": result.final_output}


if __name__== "__main__":
    app.run()
