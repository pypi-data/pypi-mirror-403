{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e06d0c6d",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
        "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
        "<br/>\n",
        "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
        "<br/>\n",
        "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644fc36b",
      "metadata": {},
      "source": [
        "# RapidFire AI Tutorial: Optimizing RAG Pipelines with Trackio Experiment Tracking \n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a practical way to make an AI assistant **answer using your documents**:\n",
        "\n",
        "- **Retrieve**: find the most relevant passages for a question.\n",
        "- **Generate**: give those passages to a language model so it can answer *grounded in evidence*.\n",
        "\n",
        "In this tutorial, we'll build and evaluate a RAG pipeline for a **financial opinion Q&A** assistant using the [FiQA dataset](https://huggingface.co/datasets/explodinggradients/fiqa).\n",
        "\n",
        "Examples of the kind of questions we're targeting:\n",
        "\n",
        "- \"Should I invest in index funds or individual stocks?\"\n",
        "- \"What's a good way to save for retirement in my 30s?\"\n",
        "- \"Is it worth refinancing my mortgage right now?\"\n",
        "\n",
        "## What We're Building\n",
        "\n",
        "A concrete RAG pipeline that looks like this:\n",
        "\n",
        "1. **Load a financial corpus** (documents + posts).\n",
        "2. **Split documents into chunks** (so we can search smaller, more relevant pieces).\n",
        "3. **Embed the chunks** (turn text into vectors) and store them in a vector index (FAISS).\n",
        "4. **Retrieve top‚ÄëK chunks** for each question using similarity search.\n",
        "5. *(Optional)* **Rerank** the retrieved chunks with a stronger model to keep only the best evidence.\n",
        "6. **Build a prompt** that includes the question + retrieved context.\n",
        "7. **Generate an answer** with a vLLM model.\n",
        "8. **Evaluate retrieval quality** (Precision, Recall, NDCG@5, MRR) so we can tell which settings find better evidence.\n",
        "\n",
        "## Our Approach\n",
        "\n",
        "RAG has a lot of \"knobs\", and it's easy to lose track of what helped. In this notebook we'll systematically vary both **retrieval settings** and **generator models** to find the best combination.\n",
        "\n",
        "We'll use [RapidFireAI](https://github.com/RapidFireAI/rapidfireai) to:\n",
        "\n",
        "- **Define a full experiment grid**: 2 chunking strategies √ó 2 reranking `top_n` values √ó 2 generator models = **8 total configs**.\n",
        "- **Run all configs the same way** on the same dataset.\n",
        "- **Compare retrieval metrics side-by-side** as they update (Precision/Recall/NDCG/MRR) to pick the best evidence-finding setup.\n",
        "\n",
        "We use **Trackio** for experiment tracking and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5184c3b",
      "metadata": {},
      "source": [
        "### Configure Trackio and Import RapidFire Components\n",
        "\n",
        "First, we enable Trackio as the experiment tracking backend and disable MLflow and TensorBoard. These environment variables must be set **before** importing RapidFire components.\n",
        "\n",
        "Then we import RapidFire's core classes for defining the RAG pipeline and running a grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8598e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Enable Trackio as the tracking backend\n",
        "os.environ[\"RF_TRACKIO_ENABLED\"] = \"true\"\n",
        "\n",
        "# Disable other tracking backends for standalone Trackio usage\n",
        "os.environ[\"RF_MLFLOW_ENABLED\"] = \"false\"\n",
        "os.environ[\"RF_TENSORBOARD_ENABLED\"] = \"false\"\n",
        "\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
        "import re, json\n",
        "from typing import List as listtype, Dict, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16327b",
      "metadata": {},
      "source": [
        "### Loading the Data\n",
        "\n",
        "We load the FiQA **queries** and **relevance labels (qrels)**. The qrels file contains ground truth information about which documents are relevant to which queries, which we'll use to evaluate retrieval quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee571098",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Dataset directory (relative to this notebook's location)\n",
        "dataset_dir = Path(\"../datasets\")\n",
        "\n",
        "fiqa_dataset = load_dataset(\"json\", data_files=str(dataset_dir / \"fiqa\" / \"queries.jsonl\"), split=\"train\")\n",
        "fiqa_dataset = fiqa_dataset.rename_columns({\"text\": \"query\", \"_id\": \"query_id\"})\n",
        "qrels = pd.read_csv(str(dataset_dir / \"fiqa\" / \"qrels.tsv\"), sep=\"\\t\")\n",
        "qrels = qrels.rename(\n",
        "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28399289",
      "metadata": {},
      "source": [
        "### Create Experiment\n",
        "\n",
        "An `Experiment` is RapidFire's top-level container for this notebook run: it groups configs/runs, saves artifacts, and tracks metrics under a unique name. We set `mode=\"evals\"` because we're running evaluation (not training). See the [Experiment API docs](https://oss-docs.rapidfire.ai/en/latest/experiment.html#api-experiment) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70816920",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment = Experiment(experiment_name=\"exp1-fiqa-rag-trackio\", mode=\"evals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73a21ee",
      "metadata": {},
      "source": [
        "### Defining the RAG Search Space\n",
        "\n",
        "This is where RapidFireAI shines. Instead of hardcoding a single RAG configuration, we define a search space using `RFLangChainRagSpec`.\n",
        "\n",
        "We will test:\n",
        "\n",
        "* **2 Chunking Strategies**: Different chunk sizes (256 vs 128 tokens).\n",
        "* **2 Reranking Strategies**: Different `top_n` values (2 vs 5).\n",
        "\n",
        "This gives us 4 retrieval combinations to evaluate. Combined with 2 generator models, we get **8 total configurations**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b73586",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "# Per-Actor batch size for hardware efficiency\n",
        "batch_size = 128\n",
        "\n",
        "# 2 chunk sizes x 2 reranking top-n = 4 combinations in total\n",
        "rag_gpu = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        path=str(dataset_dir / \"fiqa\"),\n",
        "        glob=\"corpus.jsonl\",\n",
        "        loader_cls=JSONLoader,\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"content_key\": \"text\",\n",
        "            \"metadata_func\": lambda record, metadata: {\n",
        "                \"corpus_id\": int(record.get(\"_id\"))\n",
        "            },  # store the document id\n",
        "            \"json_lines\": True,\n",
        "            \"text_content\": False,\n",
        "        },\n",
        "        sample_seed=42,\n",
        "    ),\n",
        "    # 2 chunking strategies with different chunk sizes\n",
        "    text_splitter=List([\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=256, chunk_overlap=32\n",
        "            ),\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "    },\n",
        "    vector_store=None,  # uses FAISS by default\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 15},\n",
        "    # 2 reranking strategies with different top-n values\n",
        "    reranker_cls=CrossEncoderReranker,\n",
        "    reranker_kwargs={\n",
        "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"top_n\": List([2, 5]),\n",
        "    },\n",
        "    enable_gpu_search=True,  # GPU-based exact search instead of ANN index\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6fb0a8",
      "metadata": {},
      "source": [
        "### Define Data Processing and Postprocessing Functions\n",
        "\n",
        "We retrieve context for each question and turn it into LLM-ready prompts. The preprocessing function:\n",
        "1. Performs batched retrieval over all queries\n",
        "2. Extracts retrieved document IDs for evaluation\n",
        "3. Serializes context into prompts for the generator\n",
        "\n",
        "The postprocessing function attaches \"ground truth\" relevant documents from FiQA (`qrels`) so we can score retrieval quality later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc17276",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_preprocess_fn(\n",
        "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
        ") -> Dict[str, listtype]:\n",
        "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
        "\n",
        "    INSTRUCTIONS = \"Utilize your financial knowledge, give your answer or opinion to the input question or subject matter.\"\n",
        "\n",
        "    # Perform batched retrieval over all queries; returns a list of lists of k documents per query\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "    # Extract the retrieved document ids from the context\n",
        "    retrieved_documents = [\n",
        "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
        "    ]\n",
        "\n",
        "    # Serialize the retrieved documents into a single string per query using the default template\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
        "\n",
        "    # Each batch to contain conversational prompt, retrieved documents, and original 'query_id', 'query', 'metadata'\n",
        "    return {\n",
        "        \"prompts\": [\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Here is some relevant context:\\n{context}. \\nNow answer the following question using the context provided earlier:\\n{question}\",\n",
        "                },\n",
        "            ]\n",
        "            for question, context in zip(batch[\"query\"], serialized_context)\n",
        "        ],\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "        **batch,\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
        "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
        "    # Get ground truth documents for each query; can be done in preprocess_fn too but done here for clarity\n",
        "    batch[\"ground_truth_documents\"] = [\n",
        "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
        "        for query_id in batch[\"query_id\"]\n",
        "    ]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39eb16c3",
      "metadata": {},
      "source": [
        "### Define Custom Eval Metrics Functions\n",
        "\n",
        "The following helper methods compute standard retrieval metrics from the retrieved vs. ground-truth document IDs:\n",
        "\n",
        "- **Precision**: What fraction of retrieved documents are relevant?\n",
        "- **Recall**: What fraction of relevant documents were retrieved?\n",
        "- **F1 Score**: Harmonic mean of Precision and Recall.\n",
        "- **NDCG@5**: Normalized Discounted Cumulative Gain at rank 5 (measures ranking quality).\n",
        "- **MRR**: Mean Reciprocal Rank (how early does the first relevant doc appear?).\n",
        "\n",
        "We compute metrics per batch and then combine them across batches so each config gets one consistent score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22773d53",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
        "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
        "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
        "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
        "\n",
        "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
        "    ideal_length = min(k, len(expected_docs))\n",
        "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
        "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
        "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
        "    rr = 0\n",
        "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
        "        if retrieved_doc in expected_docs:\n",
        "            rr = 1 / (i + 1)\n",
        "            break\n",
        "    return rr\n",
        "\n",
        "\n",
        "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
        "\n",
        "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs = 0, [], [], [], [], []\n",
        "    total_queries = len(batch[\"query\"])\n",
        "\n",
        "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
        "        expected_set = set(gt)\n",
        "        retrieved_set = set(pred)\n",
        "\n",
        "        true_positives = len(expected_set.intersection(retrieved_set))\n",
        "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
        "        f1 = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision + recall) > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
        "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
        "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
        "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
        "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
        "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_accumulate_metrics_fn(\n",
        "    aggregated_metrics: Dict[str, listtype],\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
        "\n",
        "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
        "    total_queries = sum(num_queries_per_batch)\n",
        "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        **{\n",
        "            metric: {\n",
        "                \"value\": sum(\n",
        "                    m[\"value\"] * queries\n",
        "                    for m, queries in zip(\n",
        "                        aggregated_metrics[metric], num_queries_per_batch\n",
        "                    )\n",
        "                )\n",
        "                / total_queries,\n",
        "                \"is_algebraic\": True,\n",
        "                \"value_range\": (0, 1),\n",
        "            }\n",
        "            for metric in algebraic_metrics\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57887bc",
      "metadata": {},
      "source": [
        "### Define vLLM Generator Configurations\n",
        "\n",
        "We define two vLLM generator configurations with different model sizes:\n",
        "\n",
        "1. **Qwen2.5-0.5B-Instruct**: A lightweight model for faster inference.\n",
        "2. **Qwen2.5-3B-Instruct**: A larger model for potentially better generation quality.\n",
        "\n",
        "Each generator config will be combined with the 4 retrieval configs (2 chunking √ó 2 reranking), giving us 8 total configurations to evaluate.\n",
        "\n",
        "We bundle the generators with our preprocessing/metrics functions into `config_set`, which RapidFire will run across all configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5d0824",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2 vLLM generator configs with different sizes of generator models\n",
        "\n",
        "vllm_config1 = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.7,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "        \"distributed_executor_backend\": \"mp\",\n",
        "        \"enable_chunked_prefill\": False,\n",
        "        \"enable_prefix_caching\": True,\n",
        "        \"max_model_len\": 4096,\n",
        "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.8,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 512,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        "    prompt_manager=None,\n",
        ")\n",
        "\n",
        "vllm_config2 = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.7,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "        \"distributed_executor_backend\": \"mp\",\n",
        "        \"enable_chunked_prefill\": False,\n",
        "        \"enable_prefix_caching\": True,\n",
        "        \"max_model_len\": 4096,\n",
        "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.8,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 512,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        "    prompt_manager=None,\n",
        ")\n",
        "\n",
        "config_set = {\n",
        "    \"vllm_config\": List([vllm_config1, vllm_config2]),  # Each represents 4 configs\n",
        "    \"batch_size\": batch_size,\n",
        "    \"preprocess_fn\": sample_preprocess_fn,\n",
        "    \"postprocess_fn\": sample_postprocess_fn,\n",
        "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
        "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
        "    \"online_strategy_kwargs\": {\n",
        "        \"strategy_name\": \"normal\",\n",
        "        \"confidence_level\": 0.95,\n",
        "        \"use_fpc\": True,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7dd280",
      "metadata": {},
      "source": [
        "### Create Config Group\n",
        "\n",
        "We create an `RFGridSearch` over `config_set`, producing **8 total configs** (2 generators √ó 2 chunkers √ó 2 rerankers) to run and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f26d9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple grid search across all sets of config knob values = 8 combinations in total\n",
        "config_group = RFGridSearch(config_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa186134",
      "metadata": {},
      "source": [
        "### Run Multi-Config Evals\n",
        "\n",
        "Now we run the main evaluation function. Two tables will appear below:\n",
        "\n",
        "1. **First table**: Lists all preprocessing/RAG sources (appears immediately).\n",
        "2. **Second table**: Lists all individual runs with their knobs and metrics, updated in real-time via online aggregation showing both estimates and confidence intervals.\n",
        "\n",
        "RapidFire AI provides an Interactive Controller that lets you manage executing runs dynamically:\n",
        "\n",
        "- ‚èπÔ∏è **Stop**: Gracefully stop a running config\n",
        "- ‚ñ∂Ô∏è **Resume**: Resume a stopped run\n",
        "- üóëÔ∏è **Delete**: Remove a run from this experiment\n",
        "- üìã **Clone**: Create a new run by editing the config dictionary of a parent run\n",
        "- üîÑ **Refresh**: Update run status and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e07274a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
        "# NB: If your machine has only 1 GPU, set num_actors=1\n",
        "results = experiment.run_evals(\n",
        "    config_group=config_group,\n",
        "    dataset=fiqa_dataset,\n",
        "    num_actors=2,\n",
        "    num_shards=4,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5489898b",
      "metadata": {},
      "source": [
        "### View Trackio Dashboard\n",
        "\n",
        "To visualize your experiment metrics in the Trackio dashboard, open a terminal and run:\n",
        "\n",
        "```bash\n",
        "trackio show --project \"exp1-fiqa-rag-trackio\"\n",
        "```\n",
        "\n",
        "This will launch the Trackio dashboard in your browser where you can:\n",
        "- View real-time training curves\n",
        "- Compare metrics across different configurations\n",
        "- Analyze hyperparameter impacts on performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "656ce33b",
      "metadata": {},
      "source": [
        "### View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127e7b4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results dict to DataFrame\n",
        "results_df = pd.DataFrame([\n",
        "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
        "    for run_id, (_, metrics_dict) in results.items()\n",
        "])\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9135d951",
      "metadata": {},
      "source": [
        "### End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ab038d",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09265e66",
      "metadata": {},
      "source": [
        "### View RapidFire AI Log Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05379a93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the experiment-specific log file\n",
        "log_file = experiment.get_log_file_path()\n",
        "\n",
        "print(f\"üìÑ Log File: {log_file}\")\n",
        "print()\n",
        "\n",
        "if log_file.exists():\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Last 30 lines of {log_file.name}:\")\n",
        "    print(\"=\" * 80)\n",
        "    with open(log_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-30:]:\n",
        "            print(line.rstrip())\n",
        "else:\n",
        "    print(f\"‚ùå Log file not found: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d5816f",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "We built a Financial Q&A RAG pipeline and compared **8 configurations** (2 generators √ó 2 chunking strategies √ó 2 reranking settings) using standard retrieval metrics, all tracked with Trackio.\n",
        "\n",
        "**What we covered:**\n",
        "- Enabling Trackio as the sole experiment tracking backend\n",
        "- Defining a search space with `RFLangChainRagSpec` and `RFGridSearch`\n",
        "- Computing retrieval metrics (Precision, Recall, F1, NDCG@5, MRR)\n",
        "- Visualizing results in the Trackio dashboard\n",
        "\n",
        "**Ideas to explore next:**\n",
        "- Try additional retrieval knobs (e.g., different embedding models, varying `k`, chunk overlap settings)\n",
        "- Add generation quality metrics alongside retrieval metrics\n",
        "- Scale to larger datasets or more generator model comparisons"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
