# Trinity Score: 90.0 (Established by Chancellor)
# packages/afo-core/alignment/basic_dpo.py
# (Í∏∞Î≥∏ DPO Íµ¨ÌòÑ - ÎÖºÎ¨∏ Rafailov et al. 2023 Ïû¨ÌòÑ ÏÉÅÏÑ∏)
# üß≠ Trinity Score: Áúû95% ÂñÑ99% Áæé90% Â≠ù95%

import logging
import os

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer

# Configure Logger
logger = logging.getLogger(__name__)


def run_basic_dpo(
    model_name: str = "gpt2",
    data_path: str = "data/alignment/preference_dataset.json",
    output_dir: str = "artifacts/models/dpo_aligned",
) -> None:
    """Executes Basic DPO Training with Full Configuration.

    Args:
        model_name: HuggingFace model ID (Default: gpt2 for testing)
        data_path: Path to preference dataset (JSON format with 'prompt', 'chosen', 'rejected')
        output_dir: Directory to save the aligned model

    """
    logger.info(f"üöÄ [DPO] Initializing Basic DPO for model: {model_name}")

    # 1. Load Model & Tokenizer
    # In a real scenario, use device_map="auto" for GPU
    model = AutoModelForCausalLM.from_pretrained(model_name, revision="main")  # nosec B615
    tokenizer = AutoTokenizer.from_pretrained(model_name, revision="main")  # nosec B615

    # Fix for models without pad token (common in Llama/GPT)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 2. Load Preference Dataset
    # Expected format: {"prompt": "...", "chosen": "...", "rejected": "..."}
    if not os.path.exists(data_path):
        logger.warning(f"‚ö†Ô∏è Data file {data_path} not found. Using dummy data for DRY_RUN.")
        # Create dummy data creation logic or fail gracefully
        return

    dataset = load_dataset("json", data_files=data_path, revision="main")  # nosec B615

    # Split dataset if no test split exists
    if "test" not in dataset:
        dataset = dataset["train"].train_test_split(test_size=0.1)

    # 3. Define Full Training Arguments
    # Optimized for stability and convergence based on Rafailov et al. 2023
    training_args = TrainingArguments(
        output_dir=output_dir,
        # Training Strategy
        num_train_epochs=3,  # Standard DPO stability range (1-3)
        per_device_train_batch_size=4,  # Low batch size for VRAM efficiency
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,  # Effective batch size = 16
        # Optimization
        learning_rate=1e-5,  # Lower IR for alignment than pre-training
        weight_decay=0.01,  # Regularization
        max_grad_norm=1.0,  # Gradient clipping for stability
        optim="adamw_torch",  # Standard robust optimizer
        warmup_ratio=0.1,  # 10% warmup
        lr_scheduler_type="linear",  # Linear decay usually works best for DPO
        # Logistics
        logging_steps=10,
        save_strategy="steps",
        save_steps=100,
        evaluation_strategy="steps",
        eval_steps=50,
        load_best_model_at_end=True,
        metric_for_best_model="loss",  # DPO Loss minimization
        greater_is_better=False,
        # System
        report_to="tensorboard",  # Visibility (Truth)
        remove_unused_columns=False,  # Required for DPO dataset format
        run_name="afo_dpo_basic",
        no_cuda=False,  # Set True for Mac M-series MPS availability check logic needed
    )

    # 4. Initialize Trainer
    trainer = DPOTrainer(
        model=model,
        ref_model=None,  # None = create a copy of model as reference (Implicit)
        args=training_args,
        beta=0.1,  # KL Penalty Coefficient (Reference Paper Value)
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=tokenizer,
        max_length=512,  # Initial context window
        max_prompt_length=256,
    )

    # 5. Execute Training
    logger.info("‚öîÔ∏è [DPO] Starting Direct Preference Optimization...")
    trainer.train()

    # 6. Save Artifacts
    trainer.save_model(output_dir)
    logger.info(f"‚úÖ [DPO] Model aligned and saved to {output_dir}")


if __name__ == "__main__":
    # DRY_RUN Example
    logging.basicConfig(level=logging.INFO)
    print("Running DPO Module in CLI Mode...")
    # run_basic_dpo() # Commented out to prevent accidental execution
