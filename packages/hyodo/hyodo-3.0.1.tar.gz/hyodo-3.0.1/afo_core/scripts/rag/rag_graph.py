from __future__ import annotations

import sys
from pathlib import Path
from typing import Literal

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_qdrant import Qdrant
from langgraph.graph import END, StateGraph
from qdrant_client import QdrantClient
from typing_extensions import TypedDict

from config import EMBEDDING_MODEL, LLM_MODEL, QDRANT_COLLECTION_NAME, QDRANT_URL

# Trinity Score: 90.0 (Established by Chancellor)
#!/usr/bin/env python3
# mypy: ignore-errors
"""LangGraphë¥¼ ì‚¬ìš©í•œ RAG íŒŒì´í”„ë¼ì¸
ì˜µì‹œë””ì–¸ vault ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""


# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))


class GraphState(TypedDict):
    """Graph ìƒíƒœ"""

    question: str
    context: list[str]
    answer: str
    generation: str


def retrieve_documents(state: GraphState) -> GraphState:
    """ë¬¸ì„œ ê²€ìƒ‰"""
    question = state["question"]

    # Qdrantì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    client = QdrantClient(url=QDRANT_URL)
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Qdrant(
        client=client,
        collection_name=QDRANT_COLLECTION_NAME,
        embeddings=embeddings,
    )

    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ)
    docs = vectorstore.similarity_search(question, k=5)

    # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
    context = [doc.page_content for doc in docs]
    metadata = [doc.metadata for doc in docs]

    return {
        **state,
        "context": context,
        "metadata": metadata,
    }


def generate_answer(state: GraphState) -> GraphState:
    """ë‹µë³€ ìƒì„±"""
    question = state["question"]
    context = "\n\n".join(state["context"])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    # LLM í˜¸ì¶œ
    llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
    response = llm.invoke([HumanMessage(content=prompt)])

    return {
        **state,
        "answer": response.content,
        "generation": response.content,
    }


def should_continue(state: GraphState) -> Literal["generate", "end"]:
    """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    if state.get("context"):
        return "generate"
    return "end"


def create_rag_graph() -> StateGraph:
    """RAG Graph ìƒì„±"""
    workflow = StateGraph(GraphState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("retrieve")
    workflow.add_conditional_edges(
        "retrieve",
        should_continue,
        {
            "generate": "generate",
            "end": END,
        },
    )
    workflow.add_edge("generate", END)

    return workflow.compile()


def query_obsidian_vault(question: str) -> dict:
    """ì˜µì‹œë””ì–¸ vaultì— ì§ˆì˜

    Args:
        question: ì§ˆë¬¸

    Returns:
        ë‹µë³€ ë° ë©”íƒ€ë°ì´í„°

    """
    graph = create_rag_graph()

    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "question": question,
        "context": [],
        "answer": "",
        "generation": "",
    }

    # Graph ì‹¤í–‰
    result = graph.invoke(initial_state)

    return {
        "question": result["question"],
        "answer": result["answer"],
        "context_sources": result.get("metadata", []),
    }


def main() -> None:
    """í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ì˜µì‹œë””ì–¸ vault RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n")

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    questions = [
        "ì˜µì‹œë””ì–¸ í”ŒëŸ¬ê·¸ì¸ ìµœì í™” ê²°ê³¼ëŠ”?",
        "Trinity ì ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€?",
        "ì„œë¹„ìŠ¤ ìµœì í™”ì—ì„œ ì œê±°ëœ í”ŒëŸ¬ê·¸ì¸ì€?",
    ]

    for question in questions:
        print(f"â“ ì§ˆë¬¸: {question}")
        result = query_obsidian_vault(question)
        print(f"ğŸ’¡ ë‹µë³€: {result['answer']}")
        print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['context_sources'])}ê°œ")
        print()


if __name__ == "__main__":
    main()
