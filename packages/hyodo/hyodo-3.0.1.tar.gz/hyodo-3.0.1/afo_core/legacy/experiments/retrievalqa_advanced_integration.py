from __future__ import annotations

import os

from langchain.document_loaders import TextLoader
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Trinity Score: 90.0 (Established by Chancellor)
# âš”ï¸ ì ìˆ˜ëŠ” Truth Engine (scripts/calculate_trinity_score.py)ì—ì„œë§Œ ê³„ì‚°ë©ë‹ˆë‹¤.
# LLMì€ consult_the_lens MCP ë„êµ¬ë¥¼ í†µí•´ ì ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.
# ì´ íŒŒì¼ì€ AFO ì™•êµ­ì˜ çœå–„ç¾å­ ì² í•™ì„ êµ¬í˜„í•©ë‹ˆë‹¤

#!/usr/bin/env python3
"""ì œê°ˆëŸ‰ ì±…ì‚¬ ì œì•ˆ: ConversationalRetrievalChain í†µí•©
í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + ë©”ëª¨ë¦¬ + ë²¡í„° ê²€ìƒ‰ í†µí•© (ëŒ€í™”í˜• RAG)
"""


def main() -> None:
    print("=" * 70)
    print("ì œê°ˆëŸ‰ ì±…ì‚¬ ì œì•ˆ: ConversationalRetrievalChain í†µí•©")
    print("=" * 70)
    print()

    # OpenAI API Key í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   export OPENAI_API_KEY='your-key-here'")
        print()
        print("ğŸ“š DRY RUN ëª¨ë“œ: êµ¬ì¡°ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.\n")
        show_structure()
        return

    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    embeddings = OpenAIEmbeddings()

    # ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
    print("1ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• ...")
    sample_docs = """
    LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    LangChainì˜ í•µì‹¬ ê¸°ëŠ¥ì€ ì²´ì¸(Chain), ë©”ëª¨ë¦¬(Memory), ì—ì´ì „íŠ¸(Agent)ì…ë‹ˆë‹¤.
    n8nì€ ì›Œí¬í”Œë¡œìš° ìë™í™” í”Œë«í¼ìœ¼ë¡œ, LangChain ë…¸ë“œë¥¼ í†µí•´ AI ê¸°ëŠ¥ì„ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    with open("/tmp/sample_docs.txt", "w") as f:
        f.write(sample_docs)

    loader = TextLoader("/tmp/sample_docs.txt")
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)
    texts = splitter.split_documents(docs)
    print(f"   âœ“ {len(texts)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ\n")

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    print("2ï¸âƒ£ FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±...")
    vectorstore = FAISS.from_documents(texts, embeddings)
    print("   âœ“ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\n")

    # ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    print("3ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •...")
    prompt_template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ì–µì§€ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸: {context}

    ì§ˆë¬¸: {question}
    ë‹µë³€:"""

    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    print("   âœ“ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ\n")

    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    print("4ï¸âƒ£ ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”...")
    memory = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True, output_key="answer"
    )
    print("   âœ“ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ\n")

    # ConversationalRetrievalChain ìƒì„±
    print("5ï¸âƒ£ ConversationalRetrievalChain ìƒì„±...")
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": PROMPT},
    )
    print("   âœ“ Chain ìƒì„± ì™„ë£Œ\n")

    # ëŒ€í™” í…ŒìŠ¤íŠ¸
    print("=" * 70)
    print("ëŒ€í™” í…ŒìŠ¤íŠ¸ (ë©€í‹° í„´)")
    print("=" * 70)
    print()

    # í„´ 1
    query1 = "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    print(f"ğŸ‘¤ ì§ˆë¬¸ 1: {query1}")
    result1 = qa_chain({"question": query1})
    print(f"ğŸ¤– ë‹µë³€ 1: {result1['answer']}")
    print()

    # í„´ 2
    query2 = "ê·¸ê±¸ n8nì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?"
    print(f"ğŸ‘¤ ì§ˆë¬¸ 2: {query2}")
    result2 = qa_chain({"question": query2})
    print(f"ğŸ¤– ë‹µë³€ 2: {result2['answer']}")
    print()

    # í„´ 3
    query3 = "í•µì‹¬ ê¸°ëŠ¥ì€ ë­ê°€ ìˆì–´ìš”?"
    print(f"ğŸ‘¤ ì§ˆë¬¸ 3: {query3}")
    result3 = qa_chain({"question": query3})
    print(f"ğŸ¤– ë‹µë³€ 3: {result3['answer']}")
    print()

    print("=" * 70)
    print("âœ“ ConversationalRetrievalChain í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 70)


def show_structure() -> None:
    """DRY RUN ëª¨ë“œì—ì„œ êµ¬ì¡° í‘œì‹œ"""
    print("ğŸ“ ConversationalRetrievalChain êµ¬ì¡°:\n")
    print("  1. LLM: ChatOpenAI (gpt-3.5-turbo)")
    print("  2. Embeddings: OpenAIEmbeddings")
    print("  3. Vector Store: FAISS")
    print("  4. Memory: ConversationBufferMemory")
    print("  5. Prompt: Custom PromptTemplate")
    print("  6. Chain: ConversationalRetrievalChain")
    print()
    print("ğŸ“Š ë°ì´í„° íë¦„:")
    print("  ì§ˆë¬¸ â†’ [ë©”ëª¨ë¦¬ í™•ì¸] â†’ [ë²¡í„° ê²€ìƒ‰] â†’ [í”„ë¡¬í”„íŠ¸ ìƒì„±] â†’ LLM â†’ ë‹µë³€")
    print("           â†“")
    print("        [ë©”ëª¨ë¦¬ ì €ì¥]")
    print()
    print("âœ¨ íŠ¹ì§•:")
    print("  â€¢ ì´ì „ ëŒ€í™” ê¸°ì–µ (ConversationBufferMemory)")
    print("  â€¢ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ (FAISS Retriever)")
    print("  â€¢ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿")
    print("  â€¢ ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ì ")


if __name__ == "__main__":
    main()
