from __future__ import annotations

import os
import traceback
from typing import Any, Literal, Optional, TypedDict, Union

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph

# Trinity Score: 90.0 (Established by Chancellor)
# âš”ï¸ ì ìˆ˜ëŠ” Truth Engine (scripts/calculate_trinity_score.py)ì—ì„œë§Œ ê³„ì‚°ë©ë‹ˆë‹¤.
# LLMì€ consult_the_lens MCP ë„êµ¬ë¥¼ í†µí•´ ì ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.
# ì´ íŒŒì¼ì€ AFO ì™•êµ­ì˜ çœå–„ç¾å­ ì² í•™ì„ êµ¬í˜„í•©ë‹ˆë‹¤

"""AFO Kingdom - CRAG LangGraph Implementation
ì œê°ˆëŸ‰ì˜ ì „ëµ, ì˜ë•ì˜ ì‹¤í–‰

CRAG = LangGraph ìƒíƒœ ê·¸ë˜í”„ë¡œ êµ¬í˜„í•œ ë²„ì „
- ì‹œê°í™” ê°€ëŠ¥
- ì›Œí¬í”Œë¡œìš° ëª…í™•
- ë‹¨ê³„ë³„ ì¶”ì  ê°€ëŠ¥

êµ¬ì¡°:
START â†’ retrieve â†’ grade â†’ [route] â†’ (web_search) â†’ generate â†’ END
                              â†“
                            generate

ì² í•™: çœå–„ç¾å­
- çœ (Truth): ìƒíƒœ ê·¸ë˜í”„ë¡œ íë¦„ ì¦ëª…
- å–„ (Goodness): ì‹œê°í™”ë¡œ ì´í•´ ì‰¬ì›€
- ç¾ (Beauty): ê·¸ë˜í”„ êµ¬ì¡°ì˜ ì•„ë¦„ë‹¤ì›€
- å­ (Serenity): í˜•ë‹˜ì´ ì›Œí¬í”Œë¡œìš° í•œëˆˆì— íŒŒì•…
"""


# í™˜ê²½ ë³€ìˆ˜
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")


class GraphState(TypedDict):
    """CRAG ê·¸ë˜í”„ ìƒíƒœ

    ì´ˆë“±í•™ìƒ: ë¡œë´‡ ë†€ì´ì—ì„œ ê¸°ì–µí•  ê²ƒë“¤
    - question: ì§ˆë¬¸
    - documents: ì°¾ì€ ì±…ë“¤
    - web_search: ì›¹ ê²€ìƒ‰ í•„ìš”? ("Yes" or "No")
    - generation: ìµœì¢… ë‹µë³€
    """

    question: str
    documents: list[Document]
    web_search: str
    generation: str


class CRAGLangGraph:
    """LangGraph ê¸°ë°˜ CRAG ì—”ì§„

    ì œê°ˆëŸ‰ì˜ ì „ëµ: ìƒíƒœ ê·¸ë˜í”„ë¡œ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
    ì˜ë•ì˜ ì‹¤í–‰: ì‹¤ì œ ì‘ë™í•˜ëŠ” ì½”ë“œ

    ì‚¬ìš©ë²•:
        crag = CRAGLangGraph()
        result = crag.query("What is CRAG?")
    """

    def __init__(
        self,
        vectorstore=None,
        llm_model: str = "gpt-3.5-turbo",
        grade_threshold: float = 0.5,
    ):
        """ì´ˆê¸°í™”

        Args:
            vectorstore: ë²¡í„° DB (ì—†ìœ¼ë©´ ë”ë¯¸ ì‚¬ìš©)
            llm_model: LLM ëª¨ë¸ ì´ë¦„
            grade_threshold: í‰ê°€ ì„ê³„ê°’ (0.5 = 50%)

        """
        self.vectorstore = vectorstore
        self.llm = ChatOpenAI(model=llm_model, temperature=0, api_key=OPENAI_API_KEY)
        self.grade_threshold = grade_threshold

        # Tavily ì›¹ ê²€ìƒ‰ (API í‚¤ ìˆì„ ë•Œë§Œ)
        self.web_search_tool: TavilySearchResults | None
        if TAVILY_API_KEY:
            self.web_search_tool = TavilySearchResults(k=3)
        else:
            self.web_search_tool = None

        # ê·¸ë˜í”„ ë¹Œë“œ
        self.app = self._build_graph()

        # í†µê³„
        self.stats = {
            "total_queries": 0,
            "web_searches": 0,
            "local_only": 0,
        }

    def _build_graph(self) -> Any:
        """CRAG ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ë¹Œë“œ

        ì´ˆë“±í•™ìƒ: ë¡œë´‡ ë†€ì´ ìˆœì„œ ì •í•˜ê¸°
        """
        workflow = StateGraph(GraphState)

        # ë…¸ë“œ ì¶”ê°€ (ê° ë‹¨ê³„)
        workflow.add_node("retrieve", self._retrieve)
        workflow.add_node("grade_documents", self._grade_documents)
        workflow.add_node("web_search", self._web_search)
        workflow.add_node("generate", self._generate)

        # ì—£ì§€ ì¶”ê°€ (íë¦„)
        workflow.add_edge(START, "retrieve")
        workflow.add_edge("retrieve", "grade_documents")

        # ì¡°ê±´ë¶€ ì—£ì§€ (ë¼ìš°íŒ…)
        workflow.add_conditional_edges(
            "grade_documents",
            self._route_decision,
            {"web_search": "web_search", "generate": "generate"},
        )

        workflow.add_edge("web_search", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    def _retrieve(self, state: GraphState) -> GraphState:
        """1ë‹¨ê³„: ê²€ìƒ‰ (Retrieve)

        ì´ˆë“±í•™ìƒ: ì±…ì¥ì—ì„œ ì±… ì°¾ì•„ì™€
        """
        question = state["question"]

        print(f"ğŸ“š [Retrieve] ì§ˆë¬¸: {question}")

        if self.vectorstore:
            # ì‹¤ì œ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰
            retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
            documents = retriever.invoke(question)
        else:
            # ë”ë¯¸ ë¬¸ì„œ (í…ŒìŠ¤íŠ¸ìš©)
            documents = [
                Document(
                    page_content="CRAG (Corrective RAG) is an improved RAG system that evaluates retrieved documents.",
                    metadata={"source": "mock_db"},
                ),
                Document(
                    page_content="RAG retrieves documents to improve LLM answers.",
                    metadata={"source": "mock_db"},
                ),
                Document(
                    page_content="Unrelated content about Python programming.",
                    metadata={"source": "mock_db"},
                ),
            ]

        print(f"  âœ… ì°¾ì€ ë¬¸ì„œ: {len(documents)}ê°œ")

        return {
            "question": question,
            "documents": documents,
            "web_search": "No",
            "generation": "",
        }

    def _grade_documents(self, state: GraphState) -> GraphState:
        """2ë‹¨ê³„: í‰ê°€ (Grade)

        ì´ˆë“±í•™ìƒ: ê° ì±… ì½ê³  "ê´€ë ¨ ìˆì–´?" í™•ì¸
        """
        question = state["question"]
        documents = state["documents"]

        print("ğŸ“– [Grade] ë¬¸ì„œ í‰ê°€ ì‹œì‘...")

        # í‰ê°€ í”„ë¡¬í”„íŠ¸
        grade_prompt = ChatPromptTemplate.from_template(
            """ì§ˆë¬¸: {question}
ë¬¸ì„œ: {document}

ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ?

"yes" ë˜ëŠ” "no"ë¡œë§Œ ë‹µí•˜ì„¸ìš”."""
        )

        filtered_docs = []
        relevant_count = 0

        for i, doc in enumerate(documents):
            # LLMìœ¼ë¡œ ê´€ë ¨ì„± í‰ê°€
            chain = grade_prompt | self.llm | StrOutputParser()
            score = chain.invoke({"question": question, "document": doc.page_content[:500]})

            is_relevant = "yes" in score.lower()

            if is_relevant:
                filtered_docs.append(doc)
                relevant_count += 1
                print(f"  ğŸ“– ë¬¸ì„œ {i + 1}: âœ… ê´€ë ¨ ìˆìŒ")
            else:
                print(f"  ğŸ“– ë¬¸ì„œ {i + 1}: âŒ ê´€ë ¨ ì—†ìŒ")

        # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨
        relevance_ratio = relevant_count / len(documents) if documents else 0
        web_search = "Yes" if relevance_ratio < self.grade_threshold else "No"

        print(f"  ğŸ“Š ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨: {relevance_ratio:.2%} (ì„ê³„ê°’: {self.grade_threshold:.0%})")
        print(f"  ğŸ” ì›¹ ê²€ìƒ‰ í•„ìš”: {web_search}")

        return {
            "question": question,
            "documents": filtered_docs,
            "web_search": web_search,
            "generation": "",
        }

    def _route_decision(self, state: GraphState) -> Literal["web_search", "generate"]:
        """3ë‹¨ê³„: ë¼ìš°íŒ… ê²°ì • (Route)

        ì´ˆë“±í•™ìƒ: ì±… ì¶©ë¶„í•´? â†’ ë°”ë¡œ ë‹µ / ë¶€ì¡±í•´? â†’ ì¸í„°ë„· ê²€ìƒ‰
        """
        web_search = state["web_search"]

        if web_search == "Yes":
            print("ğŸŒ [Route] ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™")
            return "web_search"
        else:
            print("âœ… [Route] ë°”ë¡œ ë‹µë³€ ìƒì„±")
            return "generate"

    def _web_search(self, state: GraphState) -> GraphState:
        """4ë‹¨ê³„: ì›¹ ê²€ìƒ‰ (Web Search)

        ì´ˆë“±í•™ìƒ: ì¸í„°ë„·ì—ì„œ ìµœì‹  ì •ë³´ ì°¾ì•„ì™€
        """
        question = state["question"]
        documents = state["documents"]

        print("ğŸŒ [Web Search] Tavily ê²€ìƒ‰ ì¤‘...")

        if self.web_search_tool:
            try:
                # Tavily ê²€ìƒ‰
                search_results = self.web_search_tool.invoke(question)

                # ê²°ê³¼ë¥¼ Documentë¡œ ë³€í™˜
                for result in search_results:
                    web_doc = Document(
                        page_content=result["content"],
                        metadata={"source": "web_search", "url": result.get("url", "")},
                    )
                    documents.append(web_doc)

                print(f"  âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼ ì¶”ê°€")
                self.stats["web_searches"] += 1

            except Exception as e:
                print(f"  âŒ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        else:
            # Tavily API í‚¤ ì—†ìœ¼ë©´ ë”ë¯¸ ê²°ê³¼
            dummy_doc = Document(
                page_content=f"Mock web search result for: {question}. CRAG uses web search to correct retrieval errors.",
                metadata={"source": "mock_web"},
            )
            documents.append(dummy_doc)
            print("  âš ï¸  Tavily API í‚¤ ì—†ìŒ - ë”ë¯¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")

        return {
            "question": question,
            "documents": documents,
            "web_search": "Yes",
            "generation": "",
        }

    def _generate(self, state: GraphState) -> GraphState:
        """5ë‹¨ê³„: ë‹µë³€ ìƒì„± (Generate)

        ì´ˆë“±í•™ìƒ: ì¢‹ì€ ì±…ìœ¼ë¡œ ë‹µ ë§Œë“¤ì–´
        """
        question = state["question"]
        documents = state["documents"]

        print("ğŸ’¡ [Generate] ë‹µë³€ ìƒì„± ì¤‘...")

        if not documents:
            generation = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
            context = "\n\n".join(
                [f"ë¬¸ì„œ {i + 1}: {doc.page_content}" for i, doc in enumerate(documents)]
            )

            # ìƒì„± í”„ë¡¬í”„íŠ¸
            generation_prompt = ChatPromptTemplate.from_template(
                """ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            )

            chain = generation_prompt | self.llm | StrOutputParser()
            generation = chain.invoke({"context": context, "question": question})

        print(f"  âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ ({len(generation)} ê¸€ì)")

        return {
            "question": question,
            "documents": documents,
            "web_search": state["web_search"],
            "generation": generation,
        }

    def query(self, question: str) -> dict:
        """CRAG ì¿¼ë¦¬ ì‹¤í–‰

        ì´ˆë“±í•™ìƒ: ì§ˆë¬¸ ë°›ìœ¼ë©´ ë¡œë´‡ ë†€ì´ ì‹œì‘!

        Args:
            question: ì§ˆë¬¸

        Returns:
            {
                "answer": "ë‹µë³€...",
                "web_search_used": True/False,
                "num_documents": 3,
                "stats": {...}
            }

        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ¤– CRAG (LangGraph) ì§ˆë¬¸: {question}")
        print(f"{'=' * 60}")

        # ê·¸ë˜í”„ ì‹¤í–‰
        inputs = {"question": question}
        result = self.app.invoke(inputs)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["total_queries"] += 1
        if result["web_search"] == "Yes":
            self.stats["web_searches"] += 1
        else:
            self.stats["local_only"] += 1

        print("\nâœ… CRAG ì™„ë£Œ!")
        print(f"{'=' * 60}\n")

        return {
            "answer": result["generation"],
            "web_search_used": result["web_search"] == "Yes",
            "num_documents": len(result["documents"]),
            "stats": self.stats.copy(),
        }


def test_crag_langgraph() -> None:
    """LangGraph CRAG í…ŒìŠ¤íŠ¸

    ì´ˆë“±í•™ìƒ: ë¡œë´‡ ë†€ì´ í…ŒìŠ¤íŠ¸í•´ë´
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª CRAG (LangGraph) í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    # API í‚¤ ì²´í¬
    if not OPENAI_API_KEY:
        print("âš ï¸  ê²½ê³ : OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEY=your-key-here ì¶”ê°€í•˜ì„¸ìš”.\n")
        return False

    try:
        # CRAG ì—”ì§„ ìƒì„±
        crag = CRAGLangGraph(
            vectorstore=None,  # ë”ë¯¸ ëª¨ë“œ
            grade_threshold=0.5,  # 50% ì´í•˜ë©´ ì›¹ ê²€ìƒ‰
        )

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_queries = [
            "What is CRAG?",  # ì¢‹ì€ ì§ˆë¬¸ (ë¡œì»¬ ë¬¸ì„œ ì¶©ë¶„)
            "Who invented quantum computing?",  # ë‚˜ìœ ì§ˆë¬¸ (ë¡œì»¬ ë¬¸ì„œ ë¶€ì¡±)
        ]

        for query in test_queries:
            result = crag.query(query)

            print("\nê²°ê³¼:")
            print(f"  ë‹µë³€: {result['answer'][:100]}...")
            print(f"  ì›¹ ê²€ìƒ‰ ì‚¬ìš©: {'âœ… Yes' if result['web_search_used'] else 'âŒ No'}")
            print(f"  ì‚¬ìš© ë¬¸ì„œ ìˆ˜: {result['num_documents']}")

        print("\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  ì´ ì§ˆë¬¸: {crag.stats['total_queries']}")
        print(f"  ë¡œì»¬ë§Œ ì‚¬ìš©: {crag.stats['local_only']}")
        print(f"  ì›¹ ê²€ìƒ‰ ì‚¬ìš©: {crag.stats['web_searches']}")

        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_crag_langgraph()

    if success:
        print("\nâœ… CRAG (LangGraph) ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ì œê°ˆëŸ‰ì˜ ì „ëµ(LangGraph)ì„ ì˜ë•ì´ ì½”ë“œë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ CRAG (LangGraph) ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("í™˜ê²½ ì„¤ì •(API í‚¤, ë¼ì´ë¸ŒëŸ¬ë¦¬)ì„ í™•ì¸í•˜ì„¸ìš”.")
