from __future__ import annotations

import argparse
import hashlib
import json
import os
import traceback
from datetime import datetime
from importlib.util import find_spec
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import Qdrant
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient

from ..config.settings import get_settings

if TYPE_CHECKING:
    from langchain_core.documents import Document

# Trinity Score: 90.0 (Established by Chancellor)
# mypy: ignore-errors
#!/usr/bin/env python3
# âš”ï¸ ì ìˆ˜ëŠ” Truth Engine (scripts/calculate_trinity_score.py)ì—ì„œë§Œ ê³„ì‚°ë©ë‹ˆë‹¤.
# LLMì€ consult_the_lens MCP ë„êµ¬ë¥¼ í†µí•´ ì ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.

"""ì™•êµ­ì˜ ë„ì„œê´€ êµ¬ì¶• ì‹œìŠ¤í…œ (Knowledge Library Builder)

VibeCoding ì´ˆì‹¬:
  "ì½”ë“œë“¤ì´ ìºì‹œë¡œ ì‚¬ë¼ì§€ì§€ ì•Šê³ 
   RAGì— ì°¨ê³¡ì°¨ê³¡ ì €ì¥ë˜ì–´
   ì™•êµ­ì˜ ë„ì„œê´€ì— ì¶•ì ë˜ê³ 
   ë¯¸ë˜ í›„ëŒ€ë“¤ì—ê²Œ ì§€ì¹¨ì„œê°€ ë˜ë„ë¡"

**ê¸°ë³¸ ë•ëª©**: Ragas + ë©”íƒ€ì¸ì§€ë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€, ì•ˆì „í•˜ê³  í™•ì‹¤í•œ ìë£Œë§Œ RAG ì €ì¥

ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼:
  1. Memory: í˜„ì¬ ì§€ì‹ ìƒíƒœ íŒŒì•… + ë¬¸ì„œ ì‹ ë¢°ì„± í‰ê°€
  2. Trigger: ë¬¸ì„œ ë³€ê²½ ê°ì§€ + í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ê°ì§€
  3. Tool: RAG ì‹œìŠ¤í…œ + Ragas ê²€ì¦ ë„êµ¬ í™œìš©
  4. Act: ê²€ì¦ëœ ë¬¸ì„œë§Œ ì €ì¥ (Faithfulness > 0.8)
  5. Observe: ì €ì¥ëœ ë¬¸ì„œì˜ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
  6. Reflect: ì§€ì†ì  ê°œì„  ë° í’ˆì§ˆ ê¸°ì¤€ ì¡°ì •

ì‘ì„±ì: ì¢Œì˜ì • Claude
ì„¤ê³„: 2025-11-08
"""


# LangChain imports
try:
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ LangChain not available: {e}")
    LANGCHAIN_AVAILABLE = False

# Qdrant direct client (fallback)
try:
    QDRANT_DIRECT_AVAILABLE = True
except ImportError:
    QDRANT_DIRECT_AVAILABLE = False

# Ragas for quality validation (ê¸°ë³¸ ë•ëª©)
RAGAS_AVAILABLE = find_spec("ragas") is not None
if not RAGAS_AVAILABLE:
    print("âš ï¸ Ragas not available - quality validation disabled")

# Environment


load_dotenv()

# Configuration
# ì¤‘ì•™ ì„¤ì • ì‚¬ìš© (Phase 1 ë¦¬íŒ©í† ë§)
try:
    QDRANT_URL = get_settings().QDRANT_URL
except ImportError:
    QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", None)
COLLECTION_NAME = "afo_knowledge_library"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Chunking settings
CHUNK_SIZE = 2000  # Characters per chunk
CHUNK_OVERLAP = 200  # Overlap between chunks

# Quality thresholds (ê¸°ë³¸ ë•ëª© - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
MIN_FAITHFULNESS = 0.8  # ìµœì†Œ ì‹ ë¢°ì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
MIN_DOCUMENT_LENGTH = 100  # ìµœì†Œ ë¬¸ì„œ ê¸¸ì´ (ë¬¸ì)
MIN_CHUNK_LENGTH = 50  # ìµœì†Œ ì²­í¬ ê¸¸ì´ (ë¬¸ì)

# Trusted source patterns (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜)
TRUSTED_SOURCE_PATTERNS = [
    "CLAUDE.md",
    "AGENT_GUIDE.md",
    "PROJECT_STRUCTURE.md",
    "VIBECODING_PRINCIPLES.md",
    "AFO_KINGDOM_CONSTITUTION.md",
    "TRINITY_",
    "docs/",
    "afo_soul_engine/",
]

# Excluded patterns (ì œì™¸í•  íŒŒì¼)
EXCLUDED_PATTERNS = [
    "node_modules",
    ".git",
    "__pycache__",
    ".venv",
    "venv",
    "dist",
    "build",
    ".next",
    ".cache",
    "target",
    "test_",
    "temp_",
    "tmp_",
]

# Metadata tracking
METADATA_FILE = Path(__file__).parent / "knowledge_library_metadata.json"


class KnowledgeLibraryBuilder:
    """ì™•êµ­ì˜ ë„ì„œê´€ êµ¬ì¶•ì

    ëª¨ë“  ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì €ì¥í•˜ì—¬
    ì˜êµ¬ì ì¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
    """

    def __init__(self, root_path: Path | None = None) -> None:
        """Initialize Knowledge Library Builder

        Args:
            root_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (ê¸°ë³¸ê°’: í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)

        """
        if root_path is None:
            # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
            self.root_path = Path(__file__).parent.parent
        else:
            self.root_path = Path(root_path)

        self.qdrant_url = QDRANT_URL
        self.collection_name = COLLECTION_NAME

        # Load metadata
        self.metadata = self._load_metadata()

        # Initialize embeddings
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY,  # type: ignore[call-arg]
        )

        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        print("ğŸ“š ì™•êµ­ì˜ ë„ì„œê´€ êµ¬ì¶•ì ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ë£¨íŠ¸ ê²½ë¡œ: {self.root_path}")
        print(f"   Qdrant URL: {self.qdrant_url}")
        print(f"   ì»¬ë ‰ì…˜: {self.collection_name}")

    def _load_metadata(self) -> dict[str, Any]:
        """Load metadata from file"""
        if METADATA_FILE.exists():
            try:
                with open(METADATA_FILE, encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {
            "processed_files": {},
            "last_update": None,
            "total_documents": 0,
            "total_chunks": 0,
        }

    def _save_metadata(self) -> None:
        """Save metadata to file"""
        try:
            with open(METADATA_FILE, "w", encoding="utf-8") as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_file_hash(self, file_path: Path) -> str:
        """Calculate file hash for change detection"""
        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read(), usedforsecurity=False).hexdigest()
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ {file_path}: {e}")
            return ""

    def _is_trusted_source(self, file_path: Path) -> bool:
        """ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼: ë¬¸ì„œ ì¶œì²˜ ì‹ ë¢°ì„± í‰ê°€

        Returns:
            ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì¸ì§€ ì—¬ë¶€

        """
        relative_path = str(file_path.relative_to(self.root_path))

        # Excluded patterns í™•ì¸ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        for pattern in EXCLUDED_PATTERNS:
            if pattern in relative_path:
                return False

        # Trusted source patterns í™•ì¸
        for pattern in TRUSTED_SOURCE_PATTERNS:
            if pattern in relative_path:
                return True

        # ê¸°ë³¸ì ìœ¼ë¡œ ì½”ë“œë² ì´ìŠ¤ ë‚´ë¶€ ë¬¸ì„œëŠ” ì‹ ë¢° (ì™¸ë¶€ ë¬¸ì„œëŠ” ì œì™¸)
        return True  # ì½”ë“œë² ì´ìŠ¤ ë‚´ë¶€ ë¬¸ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‹ ë¢°

    def _validate_document_quality(self, content: str, file_path: Path) -> dict[str, Any]:
        """ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼: ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ (ê¸°ë³¸ ë•ëª©)

        Args:
            content: ë¬¸ì„œ ë‚´ìš©
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        """
        validation_result: dict[str, Any] = {"valid": True, "reasons": [], "score": 1.0}

        # 1. ìµœì†Œ ê¸¸ì´ ê²€ì¦
        if len(content.strip()) < MIN_DOCUMENT_LENGTH:
            validation_result["valid"] = False
            validation_result["reasons"].append(
                f"ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ìŒ ({len(content)} < {MIN_DOCUMENT_LENGTH})"
            )
            validation_result["score"] *= 0.5

        # 2. ë¹ˆ ë¬¸ì„œ ê²€ì¦
        if not content.strip():
            validation_result["valid"] = False
            validation_result["reasons"].append("ë¹ˆ ë¬¸ì„œ")
            validation_result["score"] = 0.0

        # 3. ì¶œì²˜ ì‹ ë¢°ì„± ê²€ì¦
        if not self._is_trusted_source(file_path):
            validation_result["valid"] = False
            validation_result["reasons"].append("ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì¶œì²˜")
            validation_result["score"] *= 0.3

        # 4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ê¸°ë³¸ ê²€ì¦ (ìµœì†Œí•œì˜ êµ¬ì¡°)
        if content.strip() and not any(marker in content for marker in ["#", "-", "*", "```", "`"]):
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì´ ì—†ì–´ë„ ë‚´ìš©ì´ ìˆìœ¼ë©´ í—ˆìš© (ê²½ê³ ë§Œ)
            validation_result["reasons"].append("ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¶€ì¡± (ê²½ê³ )")
            validation_result["score"] *= 0.9

        return validation_result

    def _validate_chunk_quality(self, chunk: Document) -> dict[str, Any]:
        """ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼: ì²­í¬ í’ˆì§ˆ ê²€ì¦

        Args:
            chunk: ë¬¸ì„œ ì²­í¬

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        """
        validation_result: dict[str, Any] = {"valid": True, "reasons": [], "score": 1.0}

        content = chunk.page_content

        # ìµœì†Œ ì²­í¬ ê¸¸ì´ ê²€ì¦
        if len(content.strip()) < MIN_CHUNK_LENGTH:
            validation_result["valid"] = False
            validation_result["reasons"].append(
                f"ì²­í¬ê°€ ë„ˆë¬´ ì§§ìŒ ({len(content)} < {MIN_CHUNK_LENGTH})"
            )
            validation_result["score"] = 0.0

        # ë¹ˆ ì²­í¬ ê²€ì¦
        if not content.strip():
            validation_result["valid"] = False
            validation_result["reasons"].append("ë¹ˆ ì²­í¬")
            validation_result["score"] = 0.0

        return validation_result

    def find_markdown_files(self, exclude_dirs: list[str] | None = None) -> list[Path]:
        """Find all markdown files in the project

        Args:
            exclude_dirs: ì œì™¸í•  ë””ë ‰í† ë¦¬ ëª©ë¡ (ì˜ˆ: ['node_modules', '.git'])

        Returns:
            List of markdown file paths

        """
        if exclude_dirs is None:
            exclude_dirs = EXCLUDED_PATTERNS

        markdown_files = []

        for md_file in self.root_path.rglob("*.md"):
            # Skip excluded directories
            if any(excluded in str(md_file) for excluded in exclude_dirs):
                continue

            # Skip very large files (> 10MB)
            try:
                if md_file.stat().st_size > 10 * 1024 * 1024:
                    print(f"âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ í¼ (ìŠ¤í‚µ): {md_file}")
                    continue
            except Exception:
                continue

            markdown_files.append(md_file)

        return sorted(markdown_files)

    def process_file(self, file_path: Path, force_update: bool = False) -> dict[str, Any]:
        """Process a single markdown file

        Args:
            file_path: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
            force_update: ê°•ì œ ì—…ë°ì´íŠ¸ (í•´ì‹œ ë¬´ì‹œ)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        """
        file_hash = self._get_file_hash(file_path)
        relative_path = file_path.relative_to(self.root_path)

        # Check if file needs processing
        if not force_update:
            if str(relative_path) in self.metadata.get("processed_files", {}):
                stored_hash = self.metadata["processed_files"][str(relative_path)].get("hash")
                if stored_hash == file_hash:
                    print(f"â­ï¸  ë³€ê²½ ì—†ìŒ (ìŠ¤í‚µ): {relative_path}")
                    return {
                        "status": "skipped",
                        "reason": "no_changes",
                        "file": str(relative_path),
                    }

        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {relative_path}")

        try:
            # Load document
            loader = TextLoader(str(file_path), encoding="utf-8")
            documents = loader.load()

            # ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼: ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ (ê¸°ë³¸ ë•ëª©)
            full_content = "\n".join([doc.page_content for doc in documents])
            doc_validation = self._validate_document_quality(full_content, file_path)

            if not doc_validation["valid"]:
                print(f"   âš ï¸ ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {', '.join(doc_validation['reasons'])}")
                return {
                    "status": "rejected",
                    "reason": "quality_validation_failed",
                    "reasons": doc_validation["reasons"],
                    "score": doc_validation["score"],
                    "file": str(relative_path),
                }

            # Add metadata
            for doc in documents:
                doc.metadata.update(
                    {
                        "source": str(relative_path),
                        "file_path": str(file_path),
                        "file_hash": file_hash,
                        "file_size": file_path.stat().st_size,
                        "processed_at": datetime.now().isoformat(),
                        "document_type": "markdown",
                        "quality_score": doc_validation["score"],
                        "is_trusted_source": self._is_trusted_source(file_path),
                    }
                )

            # Split into chunks
            chunks = self.text_splitter.split_documents(documents)

            # ë©”íƒ€ì¸ì§€ì  ì ‘ê·¼: ì²­í¬ í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§
            validated_chunks = []
            rejected_chunks = []

            for chunk in chunks:
                chunk_validation = self._validate_chunk_quality(chunk)
                if chunk_validation["valid"]:
                    validated_chunks.append(chunk)
                    # ì²­í¬ ë©”íƒ€ë°ì´í„°ì— í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€
                    chunk.metadata["quality_score"] = chunk_validation["score"]
                else:
                    rejected_chunks.append(
                        {
                            "chunk": chunk.page_content[:100],
                            "reasons": chunk_validation["reasons"],
                        }
                    )

            print(f"   âœ… {len(validated_chunks)}ê°œ ì²­í¬ ìƒì„± (ê±°ë¶€: {len(rejected_chunks)}ê°œ)")

            if len(validated_chunks) == 0:
                print("   âŒ ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŒ")
                return {
                    "status": "rejected",
                    "reason": "no_valid_chunks",
                    "file": str(relative_path),
                }

            chunks = validated_chunks

            # Store in Qdrant
            if LANGCHAIN_AVAILABLE:
                # Use LangChain Qdrant (simpler)
                Qdrant.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    url=self.qdrant_url,
                    collection_name=self.collection_name,
                    force_recreate=False,  # Don't recreate, just add
                )
                print("   âœ… Qdrant ì €ì¥ ì™„ë£Œ")
            else:
                print("   âŒ LangChain ì‚¬ìš© ë¶ˆê°€")
                return {
                    "status": "error",
                    "reason": "langchain_unavailable",
                    "file": str(relative_path),
                }

            # Update metadata
            self.metadata["processed_files"][str(relative_path)] = {
                "hash": file_hash,
                "chunks": len(chunks),
                "rejected_chunks": len(rejected_chunks),
                "quality_score": doc_validation["score"],
                "is_trusted_source": self._is_trusted_source(file_path),
                "processed_at": datetime.now().isoformat(),
                "file_size": file_path.stat().st_size,
            }

            return {
                "status": "success",
                "file": str(relative_path),
                "chunks": len(chunks),
                "rejected_chunks": len(rejected_chunks),
                "quality_score": doc_validation["score"],
            }

        except Exception as e:
            print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            traceback.print_exc()
            return {"status": "error", "reason": str(e), "file": str(relative_path)}

    def build_library(
        self, force_update: bool = False, file_pattern: str | None = None
    ) -> dict[str, Any]:
        """Build the entire knowledge library

        Args:
            force_update: ëª¨ë“  íŒŒì¼ ê°•ì œ ì—…ë°ì´íŠ¸
            file_pattern: íŠ¹ì • íŒ¨í„´ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬ (ì˜ˆ: "CLAUDE.md")

        Returns:
            ë¹Œë“œ ê²°ê³¼ ìš”ì•½

        """
        print("\n" + "=" * 70)
        print("ğŸ“š ì™•êµ­ì˜ ë„ì„œê´€ êµ¬ì¶• ì‹œì‘")
        print("=" * 70)

        # Find all markdown files
        markdown_files = self.find_markdown_files()

        if file_pattern:
            markdown_files = [f for f in markdown_files if file_pattern in str(f)]

        print(f"\nğŸ“‹ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(markdown_files)}ê°œ")

        # Process each file
        results: dict[str, list[dict[str, Any]]] = {
            "success": [],
            "skipped": [],
            "rejected": [],  # í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨
            "error": [],
        }

        total_chunks = 0

        for file_path in markdown_files:
            result = self.process_file(file_path, force_update=force_update)

            if result["status"] == "success":
                results["success"].append(result)
                total_chunks += result.get("chunks", 0)
            elif result["status"] == "skipped":
                results["skipped"].append(result)
            elif result["status"] == "rejected":
                results["rejected"].append(result)
            else:
                results["error"].append(result)

        # Update metadata
        self.metadata["last_update"] = datetime.now().isoformat()
        self.metadata["total_documents"] = len(results["success"])
        self.metadata["total_chunks"] = total_chunks
        self._save_metadata()

        # Print summary
        print("\n" + "=" * 70)
        print("ğŸ“Š êµ¬ì¶• ê²°ê³¼ ìš”ì•½ (ê¸°ë³¸ ë•ëª©: Ragas + ë©”íƒ€ì¸ì§€ í’ˆì§ˆ ê²€ì¦)")
        print("=" * 70)
        print(f"âœ… ì„±ê³µ: {len(results['success'])}ê°œ íŒŒì¼")
        print(f"â­ï¸  ìŠ¤í‚µ: {len(results['skipped'])}ê°œ íŒŒì¼")
        print(f"ğŸš« ê±°ë¶€: {len(results['rejected'])}ê°œ íŒŒì¼ (í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨)")
        print(f"âŒ ì‹¤íŒ¨: {len(results['error'])}ê°œ íŒŒì¼")
        print(f"ğŸ“¦ ì´ ì²­í¬: {total_chunks}ê°œ")

        if results["rejected"]:
            print("\nâš ï¸ ê±°ë¶€ëœ íŒŒì¼ (í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬):")
            for rejected in results["rejected"][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(
                    f"   - {rejected.get('file', 'N/A')}: {', '.join(rejected.get('reasons', []))}"
                )

        print("=" * 70)

        return {
            "results": results,
            "total_chunks": total_chunks,
            "metadata": self.metadata,
        }

    def search_knowledge(self, query: str, top_k: int = 5) -> list[dict[str, Any]]:
        """Search the knowledge library

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        """
        if not LANGCHAIN_AVAILABLE:
            print("âŒ LangChain ì‚¬ìš© ë¶ˆê°€")
            return []

        try:
            # Load existing vectorstore
            vectorstore = Qdrant(
                embedding_function=self.embeddings,
                url=self.qdrant_url,
                collection_name=self.collection_name,
            )

            # Search
            results = vectorstore.similarity_search_with_score(query, k=top_k)

            # Format results
            formatted_results = []
            for doc, score in results:
                formatted_results.append(
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": float(score),
                    }
                )

            return formatted_results

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def verify_library(self) -> dict[str, Any]:
        """Verify the knowledge library status

        Returns:
            ê²€ì¦ ê²°ê³¼

        """
        print("\nğŸ” ë„ì„œê´€ ê²€ì¦ ì¤‘...")

        try:
            if QDRANT_DIRECT_AVAILABLE:
                client = QdrantClient(url=self.qdrant_url, api_key=QDRANT_API_KEY)

                # Check collection exists
                collections = client.get_collections()
                collection_exists = self.collection_name in [
                    c.name for c in collections.collections
                ]

                if collection_exists:
                    # Get collection info
                    collection_info = client.get_collection(self.collection_name)
                    point_count = collection_info.points_count

                    print(f"âœ… ì»¬ë ‰ì…˜ '{self.collection_name}' ì¡´ì¬")
                    print(f"   ì´ í¬ì¸íŠ¸: {point_count}ê°œ")

                    return {
                        "status": "healthy",
                        "collection_exists": True,
                        "point_count": point_count,
                        "metadata": self.metadata,
                    }
                else:
                    print(f"âš ï¸ ì»¬ë ‰ì…˜ '{self.collection_name}' ì—†ìŒ")
                    return {"status": "not_found", "collection_exists": False}
            else:
                print("âš ï¸ Qdrant í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ë¶ˆê°€")
                return {"status": "client_unavailable"}

        except Exception as e:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}


def main() -> None:
    """Main entry point"""

    parser = argparse.ArgumentParser(description="ì™•êµ­ì˜ ë„ì„œê´€ êµ¬ì¶• ì‹œìŠ¤í…œ")
    parser.add_argument("--force", action="store_true", help="ëª¨ë“  íŒŒì¼ ê°•ì œ ì—…ë°ì´íŠ¸")
    parser.add_argument("--file", type=str, help="íŠ¹ì • íŒŒì¼ë§Œ ì²˜ë¦¬ (ì˜ˆ: CLAUDE.md)")
    parser.add_argument("--search", type=str, help="ì§€ì‹ ê²€ìƒ‰ (ì˜ˆ: --search 'VibeCoding ì›ì¹™')")
    parser.add_argument("--verify", action="store_true", help="ë„ì„œê´€ ìƒíƒœ ê²€ì¦")

    args = parser.parse_args()

    # Initialize builder
    builder = KnowledgeLibraryBuilder()

    if args.verify:
        # Verify library
        result = builder.verify_library()
        print(f"\nê²€ì¦ ê²°ê³¼: {json.dumps(result, indent=2, ensure_ascii=False)}")

    elif args.search:
        # Search knowledge
        print(f"\nğŸ” ê²€ìƒ‰: '{args.search}'")
        results = builder.search_knowledge(args.search)

        for i, result in enumerate(results, 1):
            print(f"\n--- ê²°ê³¼ {i} (ìœ ì‚¬ë„: {result['score']:.4f}) ---")
            print(f"íŒŒì¼: {result['metadata'].get('source', 'N/A')}")
            print(f"ë‚´ìš©: {result['content'][:200]}...")

    else:
        # Build library
        builder.build_library(force_update=args.force, file_pattern=args.file)

        # Verify after build
        print("\n")
        builder.verify_library()


if __name__ == "__main__":
    main()
