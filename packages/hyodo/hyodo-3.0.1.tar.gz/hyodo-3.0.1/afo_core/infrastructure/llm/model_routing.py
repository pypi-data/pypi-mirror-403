from __future__ import annotations

import logging
import re
from typing import Any

# Trinity Score: 95.0 (Established by Chancellor)
"""
AFO Model Routing - Task-Type Based Model Selection

ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ (M4 Pro 24GB ìµœì í™”)
- ê¸°ë³¸ ëŒ€í™”/ì´ë¯¸ì§€: qwen3-vl (í™”íƒ€) [Ollama - Vision ì „ìš©]
- ì½”ë”©: Qwen2.5-Coder-32B (ì‚¬ë§ˆíœ˜) [MLX - ë„¤ì´í‹°ë¸Œ 2ë°° ë¹ ë¦„]
- ì¶”ë¡ : DeepSeek-R1-14B (ì¢Œì) [MLX - ë„¤ì´í‹°ë¸Œ 2ë°° ë¹ ë¦„]

MLX ìš°ì„  ì •ì±…: Apple Silicon í†µí•© ë©”ëª¨ë¦¬ í™œìš©ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥
"""

# Import core config from model_config (SSOT)
from .model_config import ModelConfig, TaskType

# Import scholar utilities (extracted to reduce file size)
from .scholar_utils import (
    AutoOptimizationEngine,
    ScholarCollaboration,
    ScholarMetrics,
    auto_optimizer,
    scholar_collaboration,
    scholar_metrics,
)

__all__ = [
    "TaskType",
    "ModelConfig",
    "TASK_PATTERNS",
    "classify_task",
    "get_model_for_task",
    "get_routing_info",
    "get_advanced_routing_info",
    "get_model_with_escalation",
    "should_escalate",
    "get_escalation_model",
    "get_vision_model",
    "scholar_collaboration",
    "scholar_metrics",
    "auto_optimizer",
    # Re-export classes for backward compatibility
    "ScholarCollaboration",
    "ScholarMetrics",
    "AutoOptimizationEngine",
]

logger = logging.getLogger(__name__)


# í‚¤ì›Œë“œ íŒ¨í„´ ì •ì˜ (ì •ê·œì‹)
# OPTIMIZATION: í™”íƒ€ Beauty Score ê°œì„ ì„ ìœ„í•œ UX íŒ¨í„´ (ê°„ì†Œí™”)
TASK_PATTERNS: dict[TaskType, list[str]] = {
    TaskType.CHAT: [
        # ëŒ€í™”/ê°ì„± í‘œí˜„ (í•µì‹¬)
        r"ì–´ë–»ê²Œ.*(?:ìƒê°|í•˜ë©´|í• ê¹Œ)",
        r"(?:ì¡°ì–¸|ì˜ê²¬|ì¶”ì²œ|ì œì•ˆ|ì•„ì´ë””ì–´)",
        r"(?:ëŠë‚Œ|ê°ì •|ë§ˆìŒ)",
        # UX/ë””ìì¸ (í•µì‹¬)
        r"(?:UX|UI|ì‚¬ìš©ì.*ê²½í—˜|ì¸í„°í˜ì´ìŠ¤)",
        r"(?:ë””ìì¸|ìŠ¤íƒ€ì¼|í…Œë§ˆ|ì»¬ëŸ¬|í°íŠ¸|ë ˆì´ì•„ì›ƒ)",
        r"(?:ì‚¬ìš©ì„±|í¸ë¦¬|ì§ê´€|ê¹”ë”|ì˜ˆì˜|ì•„ë¦„ë‹µ)",
        # ê°ì„±ì  í‘œí˜„ (í†µí•©)
        r"(?:ì¢‹ì•„|ì‹«ì–´|ë§ˆìŒì—.*ë“¤)",
        r"(?:ë¶€íƒ|ë„ì™€ì¤˜|ê°ì‚¬|ê³ ë§ˆì›Œ|ë¯¸ì•ˆ|ì£„ì†¡)",
        r"(?:ì¶•í•˜|ê¸°ë…|ì´ë²¤íŠ¸|íŒŒí‹°|ì—¬í–‰|íœ´ê°€)",
        # ê°ì •/ì‹¬ë¦¬ (í†µí•©)
        r"(?:íë§|íœ´ì‹|ìŠ¤íŠ¸ë ˆìŠ¤|í”¼ë¡œ|í˜ë“¤)",
        r"(?:ì„±ê³µ|ì‹¤íŒ¨|ë…¸ë ¥|í¬ë§|ê¿ˆ|ëª©í‘œ)",
        r"(?:ì‚¬ë‘|ìš°ì •|ê°€ì¡±|ì¹œêµ¬|ê´€ê³„|ì†Œí†µ)",
        # ë§ˆìŒ ê´€ë ¨ (í†µí•© íŒ¨í„´)
        r"ë§ˆìŒ.*(?:í‘œí˜„|ì „ë‹¬|ì´í•´|ê³µê°|ì¹˜ìœ |ì•ˆì •|í–‰ë³µ)",
        r"ê°ì •.*(?:í‘œí˜„|ì „ë‹¬|ê³µìœ |ì´í•´|ê³µê°|ì¸ì‹|ê´€ë¦¬)",
        # UX ì „ë¬¸ íŒ¨í„´ (í†µí•©)
        r"ì‚¬ìš©ì.*(?:ê²½í—˜|ë§Œì¡±|ì‹¬ë¦¬|ë§¥ë½|ë§ì¶¤|ê°œì¸í™”)",
        r"(?:ì ‘ê·¼ì„±|í¬ìš©ì„±|ê°ì„±.*ì§€ëŠ¥|ì¸ì§€.*ë¶€í•˜)",
    ],
    TaskType.VISION: [
        r"(?:image|ì´ë¯¸ì§€|ì‚¬ì§„|ê·¸ë¦¼|vision)",
        r"(?:screenshot|ìŠ¤í¬ë¦°ìƒ·|í™”ë©´)",
        r"(?:ë³´ì—¬|ë´ë´)",
    ],
    TaskType.CODE_GENERATE: [
        r"ì½”ë“œ.*(?:ì‘ì„±|ìƒì„±|ë§Œë“¤|ì§œ|ì§¤)",
        r"(?:implement|êµ¬í˜„|write.*code)",
        r"(?:í•¨ìˆ˜|í´ë˜ìŠ¤|ì»´í¬ë„ŒíŠ¸).*(?:ë§Œë“¤|ìƒì„±|ì •ì˜|êµ¬í˜„)",
        r"(?:def|class|import)\s+\w+",
        r"(?:ì•Œê³ ë¦¬ì¦˜|í”„ë¡œê·¸ë˜ë°|ì½”ë”©|ê°œë°œ)",
        r"(?:ìŠ¤í¬ë¦½íŠ¸|í”„ë¡œê·¸ë¨|ì†ŒìŠ¤ì½”ë“œ)",
        r"(?:ëª¨ë“ˆ|ë¼ì´ë¸ŒëŸ¬ë¦¬|íŒ¨í‚¤ì§€)",
        r"(?:API|ë°±ì—”ë“œ|í”„ë¡ íŠ¸ì—”ë“œ).*ê°œë°œ",
        r"(?:ë°ì´í„°ë² ì´ìŠ¤|SQL|ì¿¼ë¦¬)",
        r"(?:ì„œë²„|í´ë¼ì´ì–¸íŠ¸)",
        r"(?:ì›¹|ëª¨ë°”ì¼|ë°ìŠ¤í¬í†±).*ì•±",
        r"(?:AI|ì¸ê³µì§€ëŠ¥|ë¨¸ì‹ ëŸ¬ë‹|ë”¥ëŸ¬ë‹)",
        r"(?:í…ŒìŠ¤íŠ¸|ìë™í™”|ë¹Œë“œ|ë°°í¬|CI/CD)",
        r"(?:ë„ì»¤|ì¿ ë²„ë„¤í‹°ìŠ¤|í´ë¼ìš°ë“œ|AWS|Azure|GCP)",
    ],
    TaskType.CODE_REVIEW: [
        r"ì½”ë“œ.*(?:ë¦¬ë·°|ë¶„ì„|ê²€í† |í™•ì¸|ë´)",
        r"(?:review|refactor|ë¦¬íŒ©í„°)",
        r"(?:debug|ë””ë²„ê·¸|ë²„ê·¸.*ì°¾|ì˜¤ë¥˜.*í™•ì¸)",
    ],
    TaskType.REASONING: [
        r"(?:ë‹¨ê³„.*ë¶„ì„|step.*by.*step)",
        r"(?:ê¹Šì´.*ìƒê°|ì¶”ë¡ |reasoning)",
        r"(?:ì™œ.*(?:ê·¸ëŸ°|ì´ëŸ°)|ë…¼ë¦¬|ë¶„ì„.*í•´)",
        r"(?:think.*through|ë³µì¡.*ë¬¸ì œ)",
    ],
    TaskType.EMBED: [
        r"(?:embed|ì„ë² ë”©|ë²¡í„°|vector)",
        r"(?:similarity|ìœ ì‚¬ë„)",
    ],
    TaskType.DOCUMENT: [
        r"(?:ë¬¸ì„œí™”|document|docstring)",
        r"(?:ì£¼ì„|comment|readme)",
        r"ì„¤ëª….*ì‘ì„±",
    ],
}

# ì»´íŒŒì¼ëœ íŒ¨í„´ ìºì‹œ
_COMPILED_PATTERNS: dict[TaskType, list[re.Pattern[str]]] = {}


def _get_compiled_patterns() -> dict[TaskType, list[re.Pattern[str]]]:
    """ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ (ìºì‹œ)"""
    global _COMPILED_PATTERNS
    if not _COMPILED_PATTERNS:
        for task_type, patterns in TASK_PATTERNS.items():
            _COMPILED_PATTERNS[task_type] = [re.compile(p, re.IGNORECASE) for p in patterns]
    return _COMPILED_PATTERNS


def classify_task(query: str, context: dict[str, Any] | None = None) -> TaskType:
    """
    ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì‘ì—… ìœ í˜• ë¶„ë¥˜

    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€ ë“±)

    Returns:
        ë¶„ë¥˜ëœ TaskType
    """
    ctx = context or {}

    # 1. ì»¨í…ìŠ¤íŠ¸ì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ VISION
    if ctx.get("has_image") or ctx.get("image_url") or ctx.get("images"):
        logger.debug("ğŸ–¼ï¸ Task classified as VISION (image context detected)")
        return TaskType.VISION

    # 2. ëª…ì‹œì  task_type ì§€ì •
    if explicit_type := ctx.get("task_type"):
        try:
            task = TaskType(explicit_type)
            logger.debug(f"ğŸ“‹ Task explicitly set: {task.value}")
            return task
        except ValueError:
            pass

    # 3. íŒ¨í„´ ë§¤ì¹­
    compiled = _get_compiled_patterns()
    query_lower = query.lower()

    # ìš°ì„ ìˆœìœ„: VISION > CODE > REASONING > DOCUMENT > EMBED > CHAT
    priority_order = [
        TaskType.VISION,
        TaskType.CODE_GENERATE,
        TaskType.CODE_REVIEW,
        TaskType.REASONING,
        TaskType.DOCUMENT,
        TaskType.EMBED,
    ]

    for task_type in priority_order:
        patterns = compiled.get(task_type, [])
        for pattern in patterns:
            if pattern.search(query_lower):
                logger.debug(f"ğŸ¯ Task classified as {task_type.value} (pattern match)")
                return task_type

    # 4. ê¸°ë³¸ê°’: CHAT
    logger.debug("ğŸ’¬ Task classified as CHAT (default)")
    return TaskType.CHAT


def get_model_for_task(task_type: TaskType) -> str:
    """
    ì‘ì—… ìœ í˜•ì— ë§ëŠ” ëª¨ë¸ ë°˜í™˜

    Args:
        task_type: ì‘ì—… ìœ í˜•

    Returns:
        Ollama ëª¨ë¸ ID
    """
    model = ModelConfig.TASK_MODEL_MAP.get(task_type, ModelConfig.HEO_JUN)
    scholar = ModelConfig.TASK_SCHOLAR_MAP.get(task_type, "Unknown")
    logger.info(f"ğŸ§‘â€ğŸ“ Selected scholar: {scholar} â†’ model: {model}")
    return model


def get_routing_info(query: str, context: dict[str, Any] | None = None) -> dict[str, Any]:
    """
    ë¼ìš°íŒ… ì •ë³´ ì¢…í•© ë°˜í™˜

    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        {task_type, model, scholar, reasoning}
    """
    task_type = classify_task(query, context)
    model = get_model_for_task(task_type)
    scholar = ModelConfig.TASK_SCHOLAR_MAP.get(task_type, "Unknown")

    return {
        "task_type": task_type.value,
        "model": model,
        "scholar": scholar,
        "reasoning": f"Query classified as '{task_type.value}' â†’ {scholar}",
    }


# ============================================================================
# Escalation Pattern (Bottom-Up LLM ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)
# ============================================================================

# OPTIMIZATION: Task-specific escalation thresholds (Trinity Score ìµœì í™”)
# Higher threshold = stricter quality requirement for that task type
# Lower threshold = more lenient (task is simpler or model is already optimal)
# í™”íƒ€ Beauty Score ê°œì„ ì„ ìœ„í•œ CHAT ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
TASK_THRESHOLD_MAP: dict[TaskType, float] = {
    TaskType.CHAT: 92.0,  # Beauty Score ê°œì„ : ë” ë†’ì€ í’ˆì§ˆ ìš”êµ¬ (was 88.0)
    TaskType.VISION: 90.0,  # Vision tasks need precision
    TaskType.CODE_GENERATE: 90.0,  # ì‚¬ë§ˆíœ˜ í’ˆì§ˆ ê°œì„ : ì¢Œì ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì„ê³„ê°’ ì¡°ì • (was 92.0)
    TaskType.CODE_REVIEW: 0.0,  # Bypass - already uses top model
    TaskType.REASONING: 0.0,  # Bypass - already uses top model
    TaskType.EMBED: 0.0,  # Bypass - deterministic
    TaskType.DOCUMENT: 85.0,  # Documentation - more lenient
}

# Default threshold for unknown task types
DEFAULT_ESCALATION_THRESHOLD = 90.0

# Backward compatibility alias (deprecated - use get_escalation_threshold())
ESCALATION_THRESHOLD = DEFAULT_ESCALATION_THRESHOLD


def get_escalation_threshold(task_type: TaskType) -> float:
    """Get task-specific escalation threshold.

    OPTIMIZATION: Different task types have different quality requirements.
    Returns 0.0 for tasks that bypass escalation (already on top models).
    """
    return TASK_THRESHOLD_MAP.get(task_type, DEFAULT_ESCALATION_THRESHOLD)


def should_escalate(trinity_score: float, task_type: TaskType) -> bool:
    """
    Trinity Score ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€ íŒë‹¨

    OPTIMIZATION: Task-specific thresholds for better routing.
    - CHAT: 88.0 (more lenient)
    - CODE_GENERATE: 92.0 (stricter)
    - DOCUMENT: 85.0 (documentation is lenient)

    Args:
        trinity_score: Trinity Score (0-100)
        task_type: ì‘ì—… ìœ í˜•

    Returns:
        True if escalation needed
    """
    # Get task-specific threshold
    threshold = get_escalation_threshold(task_type)

    # Threshold 0.0 means bypass (already using optimal model)
    if threshold == 0.0:
        return False

    # Trinity Score ê¸°ë°˜ íŒë‹¨
    if trinity_score < threshold:
        logger.info(
            f"âš¡ Escalation triggered: Trinity Score {trinity_score:.1f} < {threshold} ({task_type.value})"
        )
        return True

    return False


def get_escalation_model(task_type: TaskType) -> str:
    """
    ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‹œ ì‚¬ìš©í•  ì •ë°€ ëª¨ë¸ ë°˜í™˜

    Args:
        task_type: ì‘ì—… ìœ í˜•

    Returns:
        ì—ìŠ¤ì»¬ë ˆì´ì…˜ ëª¨ë¸ ID
    """
    model = ModelConfig.ESCALATION_MODEL_MAP.get(task_type, ModelConfig.HEO_JUN)
    logger.info(f"ğŸ“ˆ Escalation model selected: {model} for {task_type.value}")
    return model


def get_model_with_escalation(
    task_type: TaskType,
    trinity_score: float | None = None,
) -> tuple[str, bool]:
    """
    ì—ìŠ¤ì»¬ë ˆì´ì…˜ì„ ê³ ë ¤í•œ ëª¨ë¸ ì„ íƒ

    Bottom-Up ì „ëµ:
    1. ê¸°ë³¸ ëª¨ë¸(ë¹ ë¦„)ë¡œ ì‹œì‘
    2. Trinity Score í™•ì¸
    3. í•„ìš”ì‹œ ì •ë°€ ëª¨ë¸ë¡œ ì—ìŠ¤ì»¬ë ˆì´ì…˜

    Args:
        task_type: ì‘ì—… ìœ í˜•
        trinity_score: Trinity Score (Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜)

    Returns:
        (model_id, is_escalated)
    """
    base_model = ModelConfig.TASK_MODEL_MAP.get(task_type, ModelConfig.HEO_JUN_FAST)

    # Trinity Score ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ (ë¹ ë¥¸ ì‘ë‹µ)
    if trinity_score is None:
        return base_model, False

    # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€ íŒë‹¨
    if should_escalate(trinity_score, task_type):
        escalation_model = get_escalation_model(task_type)
        return escalation_model, True

    return base_model, False


# ============================================================================
# Vision 2ë‹¨ê³„ ì„ íƒ (Trinity Score ê¸°ë°˜)
# ============================================================================


def get_vision_model(trinity_score: float | None = None) -> tuple[str, str]:
    """
    Vision ì‘ì—…ìš© ëª¨ë¸ ì„ íƒ (2ë‹¨ê³„ Bottom-Up)

    ì „ëµ:
    - 1ë‹¨ê³„: qwen3-vl:2b (12ì´ˆ, ë¹ ë¥¸ ì´ˆì•ˆ) - 62% ì¼€ì´ìŠ¤ ì»¤ë²„
    - 2ë‹¨ê³„: qwen3-vl:latest (ì •ë°€ ê²€ì¦) - Trinity < 90ì¼ ë•Œ

    Args:
        trinity_score: Trinity Score (Noneì´ë©´ 1ë‹¨ê³„ ëª¨ë¸)

    Returns:
        (model_id, stage_description)
    """
    if trinity_score is None:
        logger.info("ğŸ–¼ï¸ Vision Stage 1: Fast model (qwen3-vl:2b)")
        return ModelConfig.HEO_JUN_FAST, "Stage 1 (Fast)"

    vision_threshold = get_escalation_threshold(TaskType.VISION)
    if trinity_score >= vision_threshold:
        logger.info(f"ğŸ–¼ï¸ Vision complete: Trinity {trinity_score:.1f} â‰¥ {vision_threshold}")
        return ModelConfig.HEO_JUN_FAST, "Stage 1 (Sufficient)"

    logger.info(
        f"ğŸ–¼ï¸ Vision Stage 2: Precise model needed (Trinity {trinity_score:.1f} < {vision_threshold})"
    )
    return ModelConfig.HEO_JUN, "Stage 2 (Precise)"


def get_advanced_routing_info(
    query: str,
    context: dict[str, Any] | None = None,
    trinity_score: float | None = None,
) -> dict[str, Any]:
    """
    ê³ ê¸‰ ë¼ìš°íŒ… ì •ë³´ (ì—ìŠ¤ì»¬ë ˆì´ì…˜ í¬í•¨)

    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        trinity_score: Trinity Score (ì—ìŠ¤ì»¬ë ˆì´ì…˜ íŒë‹¨ìš©)

    Returns:
        í™•ì¥ëœ ë¼ìš°íŒ… ì •ë³´
    """
    task_type = classify_task(query, context)

    # Vision íŠ¹ë³„ ì²˜ë¦¬
    if task_type == TaskType.VISION:
        model, stage = get_vision_model(trinity_score)
        is_escalated = stage == "Stage 2 (Precise)"
    else:
        model, is_escalated = get_model_with_escalation(task_type, trinity_score)
        stage = "Escalated" if is_escalated else "Default"

    scholar = ModelConfig.TASK_SCHOLAR_MAP.get(task_type, "Unknown")

    return {
        "task_type": task_type.value,
        "model": model,
        "scholar": scholar,
        "stage": stage,
        "is_escalated": is_escalated,
        "trinity_score": trinity_score,
        "reasoning": f"Query '{task_type.value}' â†’ {model} ({stage})",
    }


# ============================================================================
# Monitoring Functions (using imported scholar_metrics)
# ============================================================================


def get_routing_info_with_monitoring(
    query: str, context: dict[str, Any] | None = None
) -> dict[str, Any]:
    """
    ëª¨ë‹ˆí„°ë§ì´ í¬í•¨ëœ ë¼ìš°íŒ… ì •ë³´ ë°˜í™˜

    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ë¼ìš°íŒ… ì •ë³´ + ëª¨ë‹ˆí„°ë§ ë°ì´í„°
    """
    import time

    start_time = time.time()

    # ê¸°ì¡´ ë¼ìš°íŒ… ë¡œì§
    routing_info = get_routing_info(query, context)

    # ëª¨ë‹ˆí„°ë§ ê¸°ë¡ (ê¸°ë³¸ ë©”íŠ¸ë¦­)
    end_time = time.time()
    response_time = end_time - start_time

    # ê°„ë‹¨í•œ ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ ì¶”ì • (ì„ì‹œ)
    estimated_quality = min(100.0, len(query) * 0.5)  # ì¿¼ë¦¬ ê¸¸ì´ì— ê¸°ë°˜í•œ ì¶”ì •

    scholar_metrics.record_response(
        scholar=routing_info["scholar"],
        response_time=response_time,
        response_length=len(query),  # ì¿¼ë¦¬ ê¸¸ì´ë¡œ ì„ì‹œ ì‚¬ìš©
        quality_score=estimated_quality,
    )

    # ëª¨ë‹ˆí„°ë§ ì •ë³´ ì¶”ê°€
    routing_info["monitoring"] = {
        "response_time": response_time,
        "estimated_quality": estimated_quality,
        "performance_report": scholar_metrics.get_performance_report(routing_info["scholar"]),
    }

    return routing_info


def get_scholar_performance_report() -> dict[str, Any]:
    """í•™ì ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸"""
    return {
        "timestamp": "2026-01-22T00:23:00Z",
        "monitoring_active": True,
        "scholars": scholar_metrics.get_all_scholars_report(),
        "summary": {
            "total_tasks": sum(scholar_metrics.task_counts.values()),
            "total_errors": sum(scholar_metrics.error_counts.values()),
            "error_rate": sum(scholar_metrics.error_counts.values())
            / max(sum(scholar_metrics.task_counts.values()), 1),
        },
    }


# ============================================================================
# Dynamic Escalation Optimization (ë™ì  ì„ê³„ê°’ ì¡°ì •)
# ============================================================================


def optimize_escalation_thresholds(current_trinity_score: float) -> dict[TaskType, float]:
    """
    Trinity Score ê¸°ë°˜ ë™ì  ì„ê³„ê°’ ìµœì í™”

    Args:
        current_trinity_score: í˜„ì¬ Trinity Score

    Returns:
        ìµœì í™”ëœ ì„ê³„ê°’ ë§µ
    """
    # ê¸°ë³¸ ì„ê³„ê°’
    base_thresholds = TASK_THRESHOLD_MAP.copy()

    # Trinity Score ê¸°ë°˜ ë™ì  ì¡°ì •
    if current_trinity_score >= 90.0:
        # ê³ í’ˆì§ˆ ìƒíƒœ: ì„ê³„ê°’ ë‚®ì¶°ì„œ ë” ë§ì€ ì‘ì—…ì— ì •ë°€ ëª¨ë¸ ì‚¬ìš©
        adjustment = -2.0  # ì„ê³„ê°’ 2ì  ë‚®ì¶¤
    elif current_trinity_score >= 75.0:
        # ì¤‘ê°„ ìƒíƒœ: ê¸°ë³¸ ì„ê³„ê°’ ìœ ì§€
        adjustment = 0.0
    else:
        # ì €í’ˆì§ˆ ìƒíƒœ: ì„ê³„ê°’ ë†’ì—¬ì„œ ì •ë°€ ëª¨ë¸ ì‚¬ìš© ì œí•œ
        adjustment = +3.0  # ì„ê³„ê°’ 3ì  ë†’ì„

    # Beauty Score ê°œì„ ì„ ìœ„í•œ í™”íƒ€ íŠ¹í™” ì¡°ì •
    optimized_thresholds = {}
    for task_type, threshold in base_thresholds.items():
        if threshold == 0.0:
            # ì´ë¯¸ ìµœì  ëª¨ë¸ì¸ ê²½ìš° ë³€ê²½í•˜ì§€ ì•ŠìŒ
            optimized_thresholds[task_type] = threshold
        else:
            new_threshold = threshold + adjustment

            # í™”íƒ€ CHAT ì‘ì—… íŠ¹ë³„ ìµœì í™”
            if task_type == TaskType.CHAT:
                if current_trinity_score < 75.0:
                    # Beauty Score ë‚®ì„ ë•Œ ë” ì—„ê²©í•˜ê²Œ
                    new_threshold = min(95.0, new_threshold + 2.0)
                else:
                    # Beauty Score ë†’ì„ ë•Œ ë” ê´€ëŒ€í•˜ê²Œ
                    new_threshold = max(85.0, new_threshold - 1.0)

            # ë²”ìœ„ ì œí•œ (70-95)
            optimized_thresholds[task_type] = max(70.0, min(95.0, new_threshold))

    return optimized_thresholds


def get_dynamic_escalation_threshold(
    task_type: TaskType, current_trinity_score: float | None = None
) -> float:
    """
    ë™ì  ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì„ê³„ê°’ ê³„ì‚°

    Args:
        task_type: ì‘ì—… ìœ í˜•
        current_trinity_score: í˜„ì¬ Trinity Score

    Returns:
        ìµœì í™”ëœ ì„ê³„ê°’
    """
    if current_trinity_score is None:
        return get_escalation_threshold(task_type)

    optimized_thresholds = optimize_escalation_thresholds(current_trinity_score)
    return optimized_thresholds.get(task_type, get_escalation_threshold(task_type))


def should_escalate_dynamic(
    trinity_score: float, task_type: TaskType, current_overall_score: float | None = None
) -> bool:
    """
    ë™ì  ì„ê³„ê°’ ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜ íŒë‹¨

    Args:
        trinity_score: ì‘ì—…ë³„ Trinity Score
        task_type: ì‘ì—… ìœ í˜•
        current_overall_score: í˜„ì¬ ì „ì²´ Trinity Score

    Returns:
        ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€
    """
    dynamic_threshold = get_dynamic_escalation_threshold(task_type, current_overall_score)

    if dynamic_threshold == 0.0:
        return False

    if trinity_score < dynamic_threshold:
        logger.info(
            f"ğŸ¯ Dynamic escalation: {trinity_score:.1f} < {dynamic_threshold:.1f} ({task_type.value})"
        )
        return True

    return False


# ============================================================================
# Auto-Optimization Functions (auto_optimizer imported from scholar_utils)
# ============================================================================


def get_auto_optimized_routing(
    query: str,
    context: dict[str, Any] | None = None,
    trinity_score: float | None = None,
    task_complexity: str = "medium",
) -> dict[str, Any]:
    """
    ìë™ ìµœì í™”ëœ ë¼ìš°íŒ… ì •ë³´ ë°˜í™˜

    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        trinity_score: í˜„ì¬ Trinity Score
        task_complexity: ì‘ì—… ë³µì¡ë„ (simple, medium, complex)

    Returns:
        ìë™ ìµœì í™”ëœ ë¼ìš°íŒ… ì •ë³´
    """
    # ê¸°ë³¸ ë¼ìš°íŒ…
    routing_info = get_routing_info(query, context)

    # ìë™ ìµœì í™” ì ìš©
    scholar = routing_info["scholar"]
    task_type = routing_info["task_type"]

    optimization = auto_optimizer.predict_optimal_strategy(
        scholar=scholar,
        task_type=task_type,
        current_trinity=trinity_score or 85.0,
        task_complexity=task_complexity,
    )

    # ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ
    task_enum = TaskType(task_type)
    base_model = ModelConfig.TASK_MODEL_MAP.get(task_enum, ModelConfig.HEO_JUN)

    if optimization["should_escalate"]:
        final_model = get_escalation_model(task_enum)
        stage = "Auto-Escalated"
    else:
        final_model = base_model
        stage = "Auto-Optimized"

    # ê²°ê³¼ ì—…ë°ì´íŠ¸
    routing_info["model"] = final_model
    routing_info["stage"] = stage
    routing_info["optimization"] = optimization
    routing_info["reasoning"] = f"Auto-optimized: {optimization['reasoning']}"

    return routing_info


def get_auto_optimization_report() -> dict[str, Any]:
    """ìë™ ìµœì í™” ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸"""
    return {
        "timestamp": "2026-01-22T00:32:00Z",
        "system_active": True,
        "insights": auto_optimizer.get_pattern_insights(),
        "learning_progress": auto_optimizer.get_pattern_insights()["learning_progress"],
        "recommendations": {
            "ready_for_production": len(
                [
                    k
                    for k, v in auto_optimizer.get_pattern_insights()["learning_progress"].items()
                    if v["ready_for_learning"]
                ]
            ),
            "needs_more_data": len(
                [
                    k
                    for k, v in auto_optimizer.get_pattern_insights()["learning_progress"].items()
                    if not v["ready_for_learning"]
                ]
            ),
        },
    }
