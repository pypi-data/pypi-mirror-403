from __future__ import annotations

import logging
import os
import re
import time
from typing import TYPE_CHECKING

from fastapi import APIRouter, HTTPException
from langchain_community.tools import TavilySearchResults
from langsmith import traceable
from pydantic import BaseModel

from AFO.config.settings import get_settings
from AFO.llm_router import LLMRouter

if TYPE_CHECKING:
    from collections.abc import Callable

# Trinity Score: 90.0 (Established by Chancellor)
# ğŸ›¡ï¸ CRAG Self-Correction API Router
# çœå–„ç¾å­: Truth 95%, Goodness 90%, Beauty 85%, Serenity 100%
# CRAG = Corrective RAG: ë¬¸ì„œ ì±„ì  + í•„ìš”ì‹œ ì›¹ ê²€ìƒ‰ fallback
# Phase 5: LangSmith Tracing í†µí•©


logger = logging.getLogger("crag")

# LangSmith tracing (optional)
try:
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False

    # Fallback: no-op decorator
    def traceable(name: str | None = None) -> Callable[[Callable], Callable]:  # type: ignore[no-redef]
        def decorator(func: Callable) -> Callable:
            return func

        return decorator


# AFO LLM Router ì‚¬ìš© (Ollama â†’ Gemini â†’ Claude â†’ OpenAI fallback)
try:
    LLM_ROUTER_AVAILABLE = True
    llm_router: LLMRouter | None = LLMRouter()
    print("âœ… CRAG: AFO LLM Router ì´ˆê¸°í™” ì™„ë£Œ (Ollama â†’ Gemini â†’ Claude â†’ OpenAI)")
except ImportError as e:
    LLM_ROUTER_AVAILABLE = False
    llm_router = None
    print(f"âš ï¸  CRAG: LLM Router ì‚¬ìš© ë¶ˆê°€ ({e})")

# Tavily ì›¹ ê²€ìƒ‰ (ì„ íƒì‚¬í•­)
try:
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    print("âš ï¸  tavily-python not available, web fallback disabled")

router = APIRouter(prefix="/api/crag", tags=["CRAG"])

# Web search ì„¤ì • (Phase 2-4: settings ì‚¬ìš©)
web_search = None

if TAVILY_AVAILABLE:
    # Phase 2-4: settings ì‚¬ìš©
    try:
        settings = get_settings()
        tavily_key = settings.TAVILY_API_KEY
    except ImportError:
        tavily_key = os.getenv("TAVILY_API_KEY")

    if tavily_key:
        try:
            web_search = TavilySearchResults(max_results=3, api_key=tavily_key)
            print("âœ… CRAG: Tavily ì›¹ ê²€ìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  CRAG: Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸  CRAG: TAVILY_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ (ì›¹ fallback ë¹„í™œì„±í™”)")


class CragRequest(BaseModel):
    """CRAG ìš”ì²­ ëª¨ë¸"""

    question: str
    documents: list[str] = []  # n8nì—ì„œ RAGë¡œ ê°€ì ¸ì˜¨ ë¬¸ì„œë“¤ (MVPì—ì„œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)


class CragResponse(BaseModel):
    """CRAG ì‘ë‹µ ëª¨ë¸"""

    answer: str
    graded_docs: dict[str, float]  # {doc: score (0-1)}
    used_web_fallback: bool


@traceable(name="crag_grade_documents")
async def grade_documents(question: str, documents: list[str]) -> dict[str, float]:
    """
    ê°„ë‹¨í•œ CRAG ë¬¸ì„œ ì ìˆ˜ ë§¤ê¸°ê¸° (0~1).

    LangSmith tracing: crag_grade_documents
    AFO LLM Router ì‚¬ìš© (Ollama â†’ Gemini â†’ Claude â†’ OpenAI)

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        {doc: score} ë”•ì…”ë„ˆë¦¬
    """
    scores: dict[str, float] = {}

    if not documents or not llm_router:
        return scores

    for doc in documents:
        prompt = (
            "You are a strict relevance grader.\n"
            "Return ONLY one decimal number between 0 and 1.\n\n"
            f"Question: {question}\n"
            f"Document:\n{doc}\n"
        )
        try:
            # AFO LLM Routerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            result = await llm_router.execute_with_routing(
                query=prompt,
                context={
                    "task": "relevance_grading",
                    "question": question,
                    "document": doc[:500],
                },
            )

            # ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ
            response_text = result.get("response", "").strip()
            if not response_text:
                scores[doc] = 0.0
                continue

            # ì‘ë‹µì—ì„œ ì²« ë²ˆì§¸ ìˆ«ìë§Œ floatìœ¼ë¡œ íŒŒì‹±
            try:
                # ìˆ«ì íŒ¨í„´ ì°¾ê¸° (0.0 ~ 1.0)

                numbers = re.findall(r"\b0?\.\d+|\b1\.0|\b0\.0|\b1\b", response_text)
                value = float(numbers[0]) if numbers else float(response_text.split()[0])
            except (ValueError, IndexError):
                value = 0.0
            value = max(0.0, min(1.0, value))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            scores[doc] = value
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ ì±„ì  ì‹¤íŒ¨: {e}")
            scores[doc] = 0.0

    return scores


@traceable(name="crag_web_fallback")
def perform_web_fallback(question: str) -> list[str]:
    """
    Tavily ì›¹ ê²€ìƒ‰ fallback ìˆ˜í–‰.

    LangSmith tracing: crag_web_fallback

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if web_search is None:
        return []

    try:
        search_results = web_search.run(question)
        docs: list[str] = []

        if isinstance(search_results, list):
            for item in search_results:
                content = item.get("content") or item.get("text")
                if content:
                    docs.append(content)
        elif isinstance(search_results, str):
            docs.append(search_results)

        return docs
    except Exception as e:
        logger.warning(f"ì›¹ ê²€ìƒ‰ fallback ì‹¤íŒ¨: {e}")
        return []


@traceable(name="crag_generate_answer")
async def generate_answer(question: str, context: str) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ë‹µë³€ ìƒì„±.

    LangSmith tracing: crag_generate_answer
    AFO LLM Router ì‚¬ìš© (Ollama â†’ Gemini â†’ Claude â†’ OpenAI)

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ

    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    if not llm_router:
        return "CRAG LLM Router not available. Please check AFO LLM Router configuration."

    answer_prompt = (
        "You are an assistant that answers using the context when possible.\n"
        "If the context does not contain the answer, say you do not know instead of guessing.\n\n"
        f"Question: {question}\n\n"
    )

    if context:
        answer_prompt += f"Context:\n{context}\n\n"
    else:
        answer_prompt += "Context: No context available.\n\n"

    answer_prompt += "Answer:"

    try:
        # AFO LLM Routerë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
        result = await llm_router.execute_with_routing(
            query=answer_prompt,
            context={
                "task": "answer_generation",
                "question": question,
                "has_context": bool(context),
                "context_length": len(context) if context else 0,
            },
        )

        answer_text = str(result.get("response", "")).strip()

        if not answer_text:
            answer_text = "I could not generate an answer. Please try rephrasing your question."

        return answer_text
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        return f"Error generating answer: {e!s}"


@traceable(name="crag_pipeline_entry")
@router.post("", response_model=CragResponse)
async def crag_endpoint(payload: CragRequest) -> CragResponse:
    """
    CRAG ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸: ë¬¸ì„œ ì±„ì  + í•„ìš”í•˜ë©´ ì›¹ ê²€ìƒ‰ fallback + ìµœì¢… ë‹µë³€.

    LangSmith tracing: crag_pipeline_entry (ì „ì²´ íŒŒì´í”„ë¼ì¸)

    Flow:
    1. ë¬¸ì„œ ì±„ì  (grade_documents) â†’ crag_grade_documents
    2. ìµœê³  ì ìˆ˜ < 0.7ì´ë©´ Tavily ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°• â†’ crag_web_fallback
    3. ì»¨í…ìŠ¤íŠ¸ í•©ì³ì„œ LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± â†’ crag_generate_answer

    KPI ë¡œê¹…:
    - best_score: ìµœê³  ë¬¸ì„œ ì ìˆ˜
    - used_web_fallback: ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    - latency_ms: ì „ì²´ ì²˜ë¦¬ ì‹œê°„
    """
    start_time = time.time()
    question = payload.question.strip()
    if not question:
        raise HTTPException(status_code=400, detail="question is required")

    if not llm_router:
        raise HTTPException(
            status_code=503,
            detail="CRAG LLM Router not available. Please check AFO LLM Router configuration.",
        )

    # 1) ë¬¸ì„œ ì±„ì  (traced: crag_grade_documents) - asyncë¡œ ë³€ê²½
    graded_docs = await grade_documents(question, payload.documents)
    best_score = max(graded_docs.values()) if graded_docs else 0.0

    # 2) í•„ìš”í•˜ë©´ Tavily ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°• (traced: crag_web_fallback)
    used_web_fallback = False
    combined_docs: list[str] = list(payload.documents)

    if (not graded_docs or best_score < 0.7) and web_search is not None:
        used_web_fallback = True
        fallback_docs = perform_web_fallback(question)
        combined_docs.extend(fallback_docs)

    context = "\n\n---\n\n".join(combined_docs) if combined_docs else ""

    # 3) ìµœì¢… ë‹µë³€ ìƒì„± (traced: crag_generate_answer) - asyncë¡œ ë³€ê²½
    answer_text = await generate_answer(question, context)

    # KPI ë¡œê¹… (çœå–„ç¾å­: Truth ì¸¡ì •)
    elapsed_ms = int((time.time() - start_time) * 1000)
    avg_score = sum(graded_docs.values()) / len(graded_docs) if graded_docs else 0.0

    logger.info(
        "CRAG question=%r best_score=%.2f avg_score=%.2f used_web_fallback=%s latency_ms=%d doc_count=%d",
        question[:80],  # ì§ˆë¬¸ ì• 80ìë§Œ ë¡œê·¸
        best_score,
        avg_score,
        used_web_fallback,
        elapsed_ms,
        len(payload.documents),
    )

    return CragResponse(
        answer=answer_text,
        graded_docs=graded_docs,
        used_web_fallback=used_web_fallback,
    )
