"""
Chancellor Router - í—¬í¼ í•¨ìˆ˜ë“¤
ì‹¤í–‰ ëª¨ë“œ ê²°ì •, LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•, í´ë°± í…ìŠ¤íŠ¸ ìƒì„±
V2 ë¼ìš°íŒ… ë¡œì§ (Phase 23-24)
"""

from __future__ import annotations

import logging
import random
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Literal, cast

if TYPE_CHECKING:
    from AFO.api.compat import ChancellorInvokeRequest

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹œìŠ¤í…œ ì¿¼ë¦¬ í‚¤ì›Œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SYSTEM_QUERY_KEYWORDS = [
    "ìƒíƒœ",
    "health",
    "í—¬ìŠ¤",
    "metrics",
    "ë©”íŠ¸ë¦­",
    "redis",
    "postgres",
    "postgresql",
    "db",
    "ë°ì´í„°ë² ì´ìŠ¤",
    "í¬íŠ¸",
    "ì„œë²„",
    "ë©”ëª¨ë¦¬",
    "ìŠ¤ì™‘",
    "ë””ìŠ¤í¬",
    "ì˜¤ì¥ìœ¡ë¶€",
    "langgraph",
    "ì—”ë“œí¬ì¸íŠ¸",
]


def _looks_like_system_query(query: str) -> bool:
    """ì‹œìŠ¤í…œ ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ í™•ì¸"""
    q = query.lower()
    return any(k in q for k in SYSTEM_QUERY_KEYWORDS)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹¤í–‰ ëª¨ë“œ ê²°ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def determine_execution_mode(
    request: ChancellorInvokeRequest,
) -> Literal["offline", "fast", "lite", "full"]:
    """
    ì‹¤í–‰ ëª¨ë“œ ê²°ì • (ç¾: ìˆœìˆ˜ í•¨ìˆ˜ - ë™ì¼ ì…ë ¥ì— ë™ì¼ ì¶œë ¥)

    Args:
        request: Chancellor ìš”ì²­

    Returns:
        ê²°ì •ëœ ì‹¤í–‰ ëª¨ë“œ
    """
    query_text = request.query or request.input

    if request.mode == "auto":
        if _looks_like_system_query(query_text):
            return "offline"
        elif request.timeout_seconds <= 12:
            return "fast"
        elif request.timeout_seconds <= 45:
            return "lite"
        else:
            return "full"
    else:
        mode = request.mode
        if mode in ["offline", "fast", "lite", "full"]:
            return cast("Literal['offline', 'fast', 'lite', 'full']", mode)
        return "full"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def build_llm_context(request: ChancellorInvokeRequest) -> dict[str, Any]:
    """
    LLM ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• (ç¾: ìˆœìˆ˜ í•¨ìˆ˜)

    Args:
        request: Chancellor ìš”ì²­

    Returns:
        LLM ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    llm_context: dict[str, Any] = {}

    if request.provider != "auto":
        llm_context["provider"] = request.provider
    if request.ollama_model:
        llm_context["ollama_model"] = request.ollama_model
    if request.ollama_timeout_seconds is not None:
        llm_context["ollama_timeout_seconds"] = request.ollama_timeout_seconds
    if request.ollama_num_ctx is not None:
        llm_context["ollama_num_ctx"] = request.ollama_num_ctx
    if request.ollama_num_thread is not None:
        llm_context["ollama_num_thread"] = request.ollama_num_thread
    if request.max_tokens is not None:
        llm_context["max_tokens"] = request.max_tokens
    if request.temperature is not None:
        llm_context["temperature"] = request.temperature

    return llm_context


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í´ë°± í…ìŠ¤íŠ¸ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def build_fallback_text(query: str, metrics: dict[str, Any]) -> str:
    """Build fallback text for offline mode responses."""
    is_system = _looks_like_system_query(query)

    mem = metrics.get("memory_percent")
    swap = metrics.get("swap_percent")
    disk = metrics.get("disk_percent")
    redis_ok = metrics.get("redis_connected")
    langgraph_ok = metrics.get("langgraph_active")
    containers = metrics.get("containers_running")

    lines = [
        "ìŠ¹ìƒ ë³´ê³ (í´ë°± ëª¨ë“œ): LLM ì‘ë‹µì´ ì§€ì—°ë˜ì–´ í˜„ì¬ëŠ” ì œí•œëœ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.",
        "",
        f"- ìš”ì²­: {query}",
    ]

    if is_system:
        lines.extend(
            [
                f"- ë©”ëª¨ë¦¬: {mem}%",
                f"- ìŠ¤ì™‘: {swap}%",
                f"- ë””ìŠ¤í¬: {disk}%",
                f"- Redis: {'ì—°ê²°ë¨' if redis_ok else 'ë¯¸ì—°ê²°'}",
                f"- LangGraph: {'í™œì„±' if langgraph_ok else 'ë¹„í™œì„±'}",
                f"- ê°ì§€ëœ ì„œë¹„ìŠ¤(ì¶”ì •): {containers}",
            ]
        )
    else:
        q_lower = query.lower()
        if any(k in q_lower for k in ["ìê¸°ì†Œê°œ", "who are you", "ë„ˆëŠ” ëˆ„êµ¬", "ë‹¹ì‹ ì€ ëˆ„êµ¬"]):
            lines.append(
                "- ì˜¤í”„ë¼ì¸ ì‘ë‹µ: ì €ëŠ” AFO Kingdomì˜ ìŠ¹ìƒ(Chancellor)ì´ë©°, "
                "ì‹œìŠ¤í…œ ìƒíƒœ/ì „ëµ/ì‹¤í–‰ì„ ì •ë¦¬í•´ ì‚¬ë ¹ê´€ì˜ ê²°ì •ì„ ë•ìŠµë‹ˆë‹¤."
            )
        else:
            lines.append(
                "- ì˜¤í”„ë¼ì¸ ì‘ë‹µ: í˜„ì¬ LLMì´ ì œì‹œê°„ì— ì‘ë‹µí•˜ì§€ ëª»í•´ "
                "ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„±í˜• ë‹µë³€ì„ í™•ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

    lines.extend(
        [
            "",
            "504ë¥¼ ì¤„ì´ê³  ì‹¤ì œ LLM ë‹µë³€ í™•ë¥ ì„ ì˜¬ë¦¬ë ¤ë©´:",
            "- `mode=fast` ë˜ëŠ” `mode=lite`ë¡œ LLM í˜¸ì¶œ ìˆ˜ë¥¼ 1íšŒë¡œ ì œí•œ",
            "- ë” ì‘ì€(ë¹ ë¥¸) ëª¨ë¸ ì‚¬ìš©: `ollama_model` (ì˜ˆ: `llama3.2:3b`, `qwen2.5:3b`)",
            "- Ollama ì„±ëŠ¥ ì˜µì…˜: `ollama_num_ctx`(ì˜ˆ: 2048~4096), `ollama_num_thread`",
            "- ì‹œê°„ ì˜ˆì‚° í™•ëŒ€: `timeout_seconds` (ì˜ˆ: 20~60ì´ˆ)",
        ]
    )
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‘ë‹µ ê²€ì¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def is_real_answer(answer: str, routing: dict[str, Any] | None) -> bool:
    """Check if the answer is a real LLM response (not fallback)."""
    text = (answer or "").strip()
    if not text:
        return False
    if routing and routing.get("is_fallback") is True:
        return False
    lowered = text.lower()
    return not (
        text.lstrip().startswith("[")
        and (
            " error" in lowered
            or lowered.startswith("[fallback")
            or "unavailable" in lowered
            or "api wrapper unavailable" in lowered
        )
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# V2 ë¼ìš°íŒ… ë¡œì§ (Phase 23-24)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class V2RoutingDecision:
    """V2 ë¼ìš°íŒ… ê²°ì • ê²°ê³¼."""

    use_v2: bool
    shadow_mode: bool
    reason: str


def get_v2_settings() -> dict[str, Any]:
    """V2 ë¼ìš°íŒ… ì„¤ì • ë¡œë“œ."""
    try:
        from config.settings import get_settings

        settings = get_settings()
        return {
            "enabled": settings.CHANCELLOR_V2_ENABLED,
            "header_routing": settings.CHANCELLOR_V2_HEADER_ROUTING,
            "canary_percent": settings.CHANCELLOR_V2_CANARY_PERCENT,
            "shadow_mode": settings.CHANCELLOR_V2_SHADOW_MODE,
            "fallback_to_v1": settings.CHANCELLOR_V2_FALLBACK_TO_V1,
        }
    except Exception as e:
        logger.warning(f"V2 ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        return {
            "enabled": True,
            "header_routing": True,
            "canary_percent": 0,
            "shadow_mode": False,
            "fallback_to_v1": True,
        }


def determine_v2_routing(
    headers: dict[str, str] | None = None,
    v2_available: bool = True,
) -> V2RoutingDecision:
    """
    V2 ë¼ìš°íŒ… ê²°ì • (Phase 23-24 Strangler Fig Pattern).

    ë¼ìš°íŒ… ìš°ì„ ìˆœìœ„:
    1. X-AFO-Engine í—¤ë” (v2, v1, shadow)
    2. Canary í¼ì„¼íŠ¸ (ëœë¤ íŠ¸ë˜í”½ ë¶„ë°°)
    3. Shadow ëª¨ë“œ (V2 ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)

    Args:
        headers: HTTP ìš”ì²­ í—¤ë”
        v2_available: V2 runner ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€

    Returns:
        V2RoutingDecision ê°ì²´
    """
    settings = get_v2_settings()

    # V2 ë¹„í™œì„±í™” ë˜ëŠ” ë¶ˆê°€ëŠ¥
    if not settings["enabled"] or not v2_available:
        return V2RoutingDecision(
            use_v2=False,
            shadow_mode=False,
            reason="V2 disabled or unavailable",
        )

    # 1. í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ… (ìµœìš°ì„ )
    if settings["header_routing"] and headers:
        engine_header = headers.get("x-afo-engine", "").lower()

        if engine_header == "v2":
            logger.info("ğŸ¯ V2 ë¼ìš°íŒ…: X-AFO-Engine í—¤ë”")
            return V2RoutingDecision(
                use_v2=True,
                shadow_mode=False,
                reason="header: X-AFO-Engine=v2",
            )
        elif engine_header == "v1":
            logger.info("ğŸ¯ V1 ë¼ìš°íŒ…: X-AFO-Engine í—¤ë”")
            return V2RoutingDecision(
                use_v2=False,
                shadow_mode=False,
                reason="header: X-AFO-Engine=v1",
            )
        elif engine_header == "shadow":
            logger.info("ğŸ¯ Shadow ë¼ìš°íŒ…: X-AFO-Engine í—¤ë”")
            return V2RoutingDecision(
                use_v2=False,
                shadow_mode=True,
                reason="header: X-AFO-Engine=shadow",
            )

    # 2. Canary ëª¨ë“œ (í¼ì„¼íŠ¸ ê¸°ë°˜)
    canary_percent = settings["canary_percent"]
    if canary_percent > 0:
        roll = random.randint(1, 100)
        if roll <= canary_percent:
            logger.info(f"ğŸ¯ V2 Canary ë¼ìš°íŒ…: roll={roll} <= {canary_percent}%")
            return V2RoutingDecision(
                use_v2=True,
                shadow_mode=False,
                reason=f"canary: {canary_percent}% (roll={roll})",
            )

    # 3. Shadow ëª¨ë“œ (ì „ì—­ ì„¤ì •)
    if settings["shadow_mode"]:
        logger.info("ğŸ¯ Shadow ëª¨ë“œ í™œì„±í™” (ì „ì—­ ì„¤ì •)")
        return V2RoutingDecision(
            use_v2=False,
            shadow_mode=True,
            reason="config: shadow_mode=True",
        )

    # ê¸°ë³¸: V1 ì‚¬ìš©
    return V2RoutingDecision(
        use_v2=False,
        shadow_mode=False,
        reason="default: V1",
    )
