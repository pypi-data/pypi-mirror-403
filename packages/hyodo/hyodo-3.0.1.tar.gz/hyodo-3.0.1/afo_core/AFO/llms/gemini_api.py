from __future__ import annotations

import asyncio
import logging
import sys
from typing import Any

import httpx

from AFO.api_wallet import create_wallet
from AFO.config.settings import settings

# Trinity Score: 90.0 (Established by Chancellor)
"""
Gemini API Wrapper
í•˜ì´ë¸Œë¦¬ë“œ LLM ì „ëµì„ ìœ„í•œ Google Gemini REST API ì—°ë™

CLI ì—†ì´ ì§ì ‘ API í˜¸ì¶œë¡œ ì›” êµ¬ë…ì œ LLM í†µí•©
"""

logger = logging.getLogger(__name__)


class GeminiAPIWrapper:
    """
    Google Gemini API ì§ì ‘ ì—°ë™
    ì›” êµ¬ë…ì œ CLI ëŒ€ì‹  REST API ì‚¬ìš©
    """

    def __init__(self) -> None:
        """Gemini API Wrapper ì´ˆê¸°í™” (çœ - Centralized Config)"""
        # 1ìˆœìœ„: settings.GEMINI_API_KEY
        # 2ìˆœìœ„: API Wallet (ì•”í˜¸í™” ì €ì¥ì†Œ)
        self.api_key = settings.GEMINI_API_KEY

        # API Wallet ì—°ë™ (í´ë°±)
        if not self.api_key:
            try:
                wallet = create_wallet()
                self.api_key = wallet.get("gemini", decrypt=True) or wallet.get(
                    "google", decrypt=True
                )
                if self.api_key:
                    logger.info("âœ… API Walletì—ì„œ Gemini/Google í‚¤ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.debug(f"API Wallet ì—°ë™ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

        self.base_url = "https://generativelanguage.googleapis.com"
        self.available: bool = bool(self.api_key)
        self.client: httpx.AsyncClient | None = None

        if self.available:
            self.client = httpx.AsyncClient(timeout=30.0)
            logger.info("âœ… Gemini API Wrapper ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            # ì„œë²„ ì‹œì‘ ì‹œì ì—ì„œëŠ” ì¡°ìš©íˆ ì²˜ë¦¬ (optional dependency)

            if hasattr(sys, "_getframe") and "api_server" in str(
                sys._getframe(0).f_code.co_filename
            ):
                # ì„œë²„ ì‹œì‘ ì‹œì—ëŠ” ë¡œê¹…í•˜ì§€ ì•ŠìŒ
                pass
            else:
                # CLI ì •ê¸°êµ¬ë… ì‚¬ìš© ì‹œ API í‚¤ ë¶ˆí•„ìš”
                logger.debug("GEMINI_API_KEY ì—†ìŒ - Gemini API ë¹„í™œì„±í™” (CLI ì‚¬ìš© ì‹œ ë¬´ì‹œ)")

    async def generate(self, prompt: str, **kwargs: Any) -> dict[str, Any]:
        """
        Gemini APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        """
        if not self.available or not self.client:
            return {
                "error": "Gemini API not available",
                "fallback": f"[Gemini Unavailable] {prompt[:50]}...",
            }

        try:
            model = kwargs.get("model", "gemini-1.5-pro")
            url = f"{self.base_url}/v1beta/models/{model}:generateContent"

            request_data = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "maxOutputTokens": kwargs.get("max_tokens", 1024),
                    "topP": 0.95,
                    "topK": 40,
                },
            }

            params = {"key": self.api_key}

            response = await self.client.post(url, json=request_data, params=params)

            if response.status_code == 200:
                result = response.json()
                candidates = result.get("candidates", [])

                if candidates:
                    content = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    finish_reason = candidates[0].get("finishReason", "unknown")

                    return {
                        "success": True,
                        "content": content,
                        "model": model,
                        "finish_reason": finish_reason,
                        "usage": result.get("usageMetadata", {}),
                    }
                else:
                    return {
                        "error": "No candidates in Gemini response",
                        "full_response": result,
                    }
            else:
                error_msg = response.text
                try:
                    error_json = response.json()
                    error_msg = error_json.get("error", {}).get("message", error_msg)
                except Exception:
                    pass

                logger.error(f"Gemini API error: {error_msg}")

                return {
                    "error": f"Gemini API error: {error_msg}",
                    "status_code": response.status_code,
                }

        except Exception as e:
            logger.error(f"Gemini API exception: {e}")
            return {"error": f"Gemini API exception: {e!s}"}

    async def generate_with_system_instruction(
        self,
        prompt: str,
        system_instruction: str,
        conversation_history: list[dict[str, str]] | None = None,
        **kwargs: Any,
    ) -> dict[str, Any]:
        """
        Gemini API í˜¸ì¶œ with system_instruction (Gem ì—ë®¬ë ˆì´ì…˜ìš©)

        Args:
            prompt: ì‚¬ìš©ì ë©”ì‹œì§€
            system_instruction: ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ (Gem ë™ì‘ ì •ì˜)
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ [{"role": "user/model", "content": "..."}]
            **kwargs: ì¶”ê°€ ì„¤ì • (model, temperature, max_tokens ë“±)

        Returns:
            dict with success, content, model, finish_reason, usage
        """
        if not self.available or not self.client:
            return {
                "error": "Gemini API not available",
                "fallback": "[Gemini Unavailable] System instruction response",
            }

        try:
            model = kwargs.get("model", "gemini-1.5-flash")
            url = f"{self.base_url}/v1beta/models/{model}:generateContent"

            # Build contents array with conversation history
            contents: list[dict[str, Any]] = []

            # Add conversation history if provided
            if conversation_history:
                for msg in conversation_history:
                    role = "user" if msg.get("role") == "user" else "model"
                    contents.append({"role": role, "parts": [{"text": msg.get("content", "")}]})

            # Add current user message
            contents.append({"role": "user", "parts": [{"text": prompt}]})

            request_data: dict[str, Any] = {
                "contents": contents,
                "systemInstruction": {"parts": [{"text": system_instruction}]},
                "generationConfig": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "maxOutputTokens": kwargs.get("max_tokens", 2048),
                    "topP": 0.95,
                    "topK": 40,
                },
            }

            params = {"key": self.api_key}

            response = await self.client.post(url, json=request_data, params=params)

            if response.status_code == 200:
                result = response.json()
                candidates = result.get("candidates", [])

                if candidates:
                    content = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    finish_reason = candidates[0].get("finishReason", "unknown")

                    return {
                        "success": True,
                        "content": content,
                        "model": model,
                        "finish_reason": finish_reason,
                        "usage": result.get("usageMetadata", {}),
                    }
                else:
                    return {"error": "No candidates in Gemini response"}
            else:
                error_msg = response.text
                try:
                    error_json = response.json()
                    error_msg = error_json.get("error", {}).get("message", error_msg)
                except Exception:
                    pass

                return {
                    "error": f"Gemini API error: {error_msg}",
                    "status_code": response.status_code,
                }

        except Exception as e:
            logger.error(f"Gemini API system instruction exception: {e}")
            return {"error": f"Gemini API exception: {e!s}"}

    async def generate_with_context(
        self, messages: list[dict[str, str]], **kwargs: Any
    ) -> dict[str, Any]:
        """
        ëŒ€í™” ë§¥ë½ì„ í¬í•¨í•œ ìƒì„±
        """
        if not self.available or not self.client:
            return {
                "error": "Gemini API not available",
                "fallback": "[Gemini Unavailable] Context-based response",
            }

        try:
            model = kwargs.get("model", "gemini-1.5-pro")
            url = f"{self.base_url}/v1beta/models/{model}:generateContent"

            # ë©”ì‹œì§€ ë³€í™˜ (OpenAI ìŠ¤íƒ€ì¼ â†’ Gemini ìŠ¤íƒ€ì¼)
            contents: list[dict[str, Any]] = []
            current_content: dict[str, list[dict[str, str]]] = {"parts": []}

            for msg in messages:
                role = msg["role"]
                content = msg["content"]

                if role == "system":
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì²« ì‚¬ìš©ì ë©”ì‹œì§€ì— ì¶”ê°€
                    if not contents:
                        current_content["parts"].append({"text": f"System: {content}\n\n"})
                    else:
                        current_content["parts"][-1]["text"] += f"\n\nSystem: {content}"
                elif role == "user" or role == "assistant":
                    if current_content["parts"]:
                        contents.append(current_content)
                        current_content = {"parts": []}
                    current_content["parts"].append({"text": content})

            if current_content["parts"]:
                contents.append(current_content)

            request_data = {
                "contents": contents,
                "generationConfig": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "maxOutputTokens": kwargs.get("max_tokens", 2048),
                    "topP": 0.95,
                    "topK": 40,
                },
            }

            params = {"key": self.api_key}

            response = await self.client.post(url, json=request_data, params=params)

            if response.status_code == 200:
                result = response.json()
                candidates = result.get("candidates", [])

                if candidates:
                    content = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    finish_reason = candidates[0].get("finishReason", "unknown")

                    return {
                        "success": True,
                        "content": content,
                        "model": model,
                        "finish_reason": finish_reason,
                        "usage": result.get("usageMetadata", {}),
                    }
                else:
                    return {"error": "No candidates in Gemini response"}
            else:
                error_msg = response.text
                return {
                    "error": f"Gemini API error: {error_msg}",
                    "status_code": response.status_code,
                }

        except Exception as e:
            logger.error(f"Gemini API context exception: {e}")
            return {"error": f"Gemini API exception: {e!s}"}

    async def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.client:
            await self.client.aclose()

    def is_available(self) -> bool:
        """API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.available

    def get_models(self) -> list[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        return [
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-2.0-flash-exp",
            "gemini-pro",
            "gemini-pro-vision",
        ]

    def get_cost_estimate(self, tokens: int) -> float:
        """ë¹„ìš© ì¶”ì • (ë‹¬ëŸ¬)"""
        # Gemini ë¹„ìš© (2024ë…„ ê¸°ì¤€)
        # Gemini 1.5 Pro: $0.00125/M tokens input, $0.005/M tokens output
        input_cost_per_million = 1.25
        output_cost_per_million = 5.0

        # ëŒ€ëµì ì¸ ì¶”ì •: ì…ë ¥ í† í°ì˜ 20%ê°€ ì¶œë ¥ í† í°
        input_tokens = tokens * 0.8
        output_tokens = tokens * 0.2

        cost = (input_tokens / 1_000_000) * input_cost_per_million + (
            output_tokens / 1_000_000
        ) * output_cost_per_million

        return cost


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
gemini_api = GeminiAPIWrapper()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    async def test_gemini_api() -> None:
        print("ğŸ¤– Gemini API Wrapper í…ŒìŠ¤íŠ¸")
        print("=" * 50)

        if not gemini_api.is_available():
            print("âŒ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            print("   í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
            return

        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?"

        print(f"ğŸ” í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {test_prompt}")

        try:
            result = await gemini_api.generate(test_prompt, max_tokens=200)

            if result.get("success"):
                print("âœ… ì„±ê³µ!")
                print(f"ğŸ“ ì‘ë‹µ: {result['content'][:100]}...")
                print(f"ğŸ¤– ëª¨ë¸: {result['model']}")
                if "usage" in result:
                    print(f"ğŸ“Š ì‚¬ìš©ëŸ‰: {result['usage']}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")

        except Exception as e:
            print(f"âŒ ì˜ˆì™¸: {e}")

        await gemini_api.close()

    asyncio.run(test_gemini_api())
