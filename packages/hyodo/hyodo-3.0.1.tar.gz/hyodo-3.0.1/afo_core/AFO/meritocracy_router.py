# Trinity Score: 95.0 (Established by Chancellor)
"""
ğŸ¯ AFO Kingdom Meritocracy Router

ì„¸ì¢…ëŒ€ì™•ì˜ í†µì¹˜ìˆ ì„ ë³¸ë°›ì•„ ëŠ¥ë ¥ì£¼ì˜ ê¸°ë°˜ Agent ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ

ğŸ“– ê°œìš”
- ê° Agent ì—…ë¬´ì— ëŒ€í•´ Trinity Score ê¸°ë°˜ ìµœì  AI ëª¨ë¸ ì„ íƒ
- í˜„ëª…í•œ í†µì¹˜ìì²˜ëŸ¼ ê°€ì¥ ì í•©í•œ AIë¥¼ ê°€ì¥ ì¤‘ìš”í•œ ì—…ë¬´ì— ë°°ì¹˜
- íˆ¬ëª…í•œ ì„ íƒ ê·¼ê±°(Evidence Bundle) ì œê³µ

ğŸ‘‘ ì² í•™ (Philosophy)
- çœå–„ç¾å­æ°¸ 5ê¸°ë‘¥ ê· í˜• ìœ ì§€
- í˜„ëª…í•œ ì„ íƒ, ì ì¬ì ì†Œ ë°°ì¹˜, íˆ¬ëª…í•œ ê±°ë²„ë„ŒìŠ¤
- ì„¸ì¢…ëŒ€ì™•ì˜ í†µì¹˜ìˆ : ì í•©í•œ ì¸ì¬ë¥¼ ì í•©í•œ ìë¦¬ì—

ğŸ“… ë©”íƒ€ë°ì´í„°
- ì‘ì„±ì: ìŠ¹ìƒ (Chancellor)
- ë²„ì „: 1.0.1
- ìƒì„±ì¼: 2026-01-22
- SSOT: AFO_FINAL_SSOT.md
"""

from __future__ import annotations

import json
import logging
import time
from datetime import datetime
from typing import Any

from AFO.background_agents import BackgroundAgent
from AFO.meritocracy_models import (
    AgentRole,
    ModelCandidate,
    ModelProvider,
    SelectionEvidence,
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MeritocracyRouter(BackgroundAgent):
    """
    Meritocracy Router: ëŠ¥ë ¥ì£¼ì˜ ê¸°ë°˜ Agent ëª¨ë¸ ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ

    ì„¸ì¢…ëŒ€ì™•ì˜ í†µì¹˜ìˆ  ì ìš©:
    - í˜„ëª…í•œ ì„ íƒ: Trinity Score ê¸°ë°˜ ìµœì  ëª¨ë¸ ì„ ë°œ
    - ì ì¬ì ì†Œ ë°°ì¹˜: ê° ì—…ë¬´ì— ê°€ì¥ ì í•©í•œ AI ë°°ì¹˜
    - íˆ¬ëª…í•œ ê±°ë²„ë„ŒìŠ¤: ëª¨ë“  ì„ íƒì˜ Evidence Bundle ì œê³µ
    """

    def __init__(self):
        super().__init__("meritocracy_router", "Meritocracy Router")

        # ëª¨ë¸ í›„ë³´êµ° ì´ˆê¸°í™”
        self.model_candidates: dict[str, ModelCandidate] = {}
        self._initialize_model_candidates()

        # Agent ì—­í•  ì •ì˜
        self.agent_roles: dict[str, AgentRole] = {}
        self._initialize_agent_roles()

        # ì„ íƒ íˆìŠ¤í† ë¦¬ ë° ì¦ê±°
        self.selection_history: list[SelectionEvidence] = []
        self.evidence_bundles: dict[str, SelectionEvidence] = {}

        # ë¡œí…Œì´ì…˜ ì„¤ì •
        self.rotation_enabled = True
        self.confidence_threshold = 0.8
        self.performance_history_days = 7

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.selection_stats = {
            "total_selections": 0,
            "model_distribution": {},
            "success_rate": 0.0,
            "average_confidence": 0.0,
        }

        logger.info("ğŸ›ï¸ Meritocracy Router initialized - ì„¸ì¢…ëŒ€ì™•ì˜ í†µì¹˜ìˆ  êµ¬í˜„")

    def _initialize_model_candidates(self):
        """ëª¨ë¸ í›„ë³´êµ° ì´ˆê¸°í™”"""
        candidates_data = [
            {
                "model_id": "claude-opus-4.5",
                "provider": ModelProvider.ANTHROPIC,
                "model_name": "Claude Opus 4.5",
                "base_model": "claude-3-opus-20240229",
                "context_window": 200000,
                "max_tokens": 4096,
                "cost_per_token": 0.015,
                "specialty_tags": ["reasoning", "analysis", "strategy", "complexity", "wisdom"],
            },
            {
                "model_id": "claude-sonnet-4.5",
                "provider": ModelProvider.ANTHROPIC,
                "model_name": "Claude Sonnet 4.5",
                "base_model": "claude-3-5-sonnet-20241022",
                "context_window": 200000,
                "max_tokens": 8192,
                "cost_per_token": 0.003,
                "specialty_tags": ["balanced", "efficiency", "ux", "optimization", "versatility"],
            },
            {
                "model_id": "grok-2",
                "provider": ModelProvider.XAI,
                "model_name": "Grok 2",
                "base_model": "grok-2-1212",
                "context_window": 131072,
                "max_tokens": 8192,
                "cost_per_token": 0.002,
                "specialty_tags": ["exploration", "discovery", "search", "efficiency", "curiosity"],
            },
            {
                "model_id": "gemini-pro",
                "provider": ModelProvider.GOOGLE,
                "model_name": "Gemini Pro",
                "base_model": "gemini-pro",
                "context_window": 32768,
                "max_tokens": 8192,
                "cost_per_token": 0.00025,
                "specialty_tags": ["speed", "scalability", "multimodal", "cost_effective"],
            },
            {
                "model_id": "claude-haiku",
                "provider": ModelProvider.ANTHROPIC,
                "model_name": "Claude Haiku",
                "base_model": "claude-3-haiku-20240307",
                "context_window": 200000,
                "max_tokens": 4096,
                "cost_per_token": 0.00025,
                "specialty_tags": ["speed", "simplicity", "cost_effective", "basic_tasks"],
            },
            {
                "model_id": "qwen3-vl",
                "provider": ModelProvider.LOCAL,
                "model_name": "Qwen3-VL",
                "base_model": "qwen3-vl:latest",
                "context_window": 32768,
                "max_tokens": 4096,
                "cost_per_token": 0.0,
                "specialty_tags": ["vision", "local", "multimodal", "free"],
            },
        ]

        for data in candidates_data:
            candidate = ModelCandidate(**data)
            self.model_candidates[candidate.model_id] = candidate

        logger.info(f"ğŸ“‹ Initialized {len(self.model_candidates)} model candidates")

    def _initialize_agent_roles(self):
        """Agent ì—­í•  ì •ì˜ ì´ˆê¸°í™”"""
        roles_data = [
            {
                "agent_name": "librarian_agent",
                "role_description": "ë¬¸ì„œÂ·ì—°êµ¬ íŠ¹í™” ì—ì´ì „íŠ¸",
                "primary_tasks": ["document_search", "knowledge_synthesis", "research_assistance"],
                "trinity_weights": {
                    "truth": 0.40,  # ì •í™•í•œ ì •ë³´ ê²€ìƒ‰
                    "goodness": 0.30,  # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜
                    "beauty": 0.15,  # ëª…í™•í•œ ì •ë³´ ì „ë‹¬
                    "serenity": 0.10,  # ì•ˆì •ì ì¸ ê²€ìƒ‰
                    "eternity": 0.05,  # ì§€ì†ì ì¸ ì§€ì‹ ê´€ë¦¬
                },
                "model_candidates": [
                    "claude-opus-4.5",
                    "claude-sonnet-4.5",
                    "grok-2",
                    "gemini-pro",
                ],
                "min_trinity_threshold": 85.0,
            },
            {
                "agent_name": "explorer_agent",
                "role_description": "ì½”ë“œ íƒìƒ‰Â·ë¶„ì„ íŠ¹í™” ì—ì´ì „íŠ¸",
                "primary_tasks": ["code_analysis", "pattern_discovery", "dependency_mapping"],
                "trinity_weights": {
                    "truth": 0.35,  # ì •í™•í•œ ì½”ë“œ ë¶„ì„
                    "goodness": 0.35,  # ì•ˆì „í•œ íƒìƒ‰
                    "beauty": 0.10,  # ê¹”ë”í•œ êµ¬ì¡°í™”
                    "serenity": 0.15,  # ì•ˆì •ì ì¸ ë¶„ì„
                    "eternity": 0.05,  # ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§
                },
                "model_candidates": ["grok-2", "claude-sonnet-4.5", "gemini-pro", "claude-haiku"],
                "min_trinity_threshold": 82.0,
            },
            {
                "agent_name": "prometheus_agent",
                "role_description": "ì „ëµÂ·ë¦¬ìŠ¤í¬ ê´€ë¦¬ íŠ¹í™” ì—ì´ì „íŠ¸",
                "primary_tasks": ["strategy_planning", "risk_assessment", "resource_optimization"],
                "trinity_weights": {
                    "truth": 0.30,  # ì •í™•í•œ ë¶„ì„
                    "goodness": 0.40,  # ìœ¤ë¦¬ì  íŒë‹¨
                    "beauty": 0.10,  # ì „ëµì  ëª…í™•ì„±
                    "serenity": 0.15,  # ì•ˆì •ì ì¸ ê³„íš
                    "eternity": 0.05,  # ì¥ê¸°ì  ì§€ì†
                },
                "model_candidates": [
                    "claude-opus-4.5",
                    "claude-sonnet-4.5",
                    "grok-2",
                    "gemini-pro",
                ],
                "min_trinity_threshold": 88.0,
            },
            {
                "agent_name": "metis_agent",
                "role_description": "ê³„íš ê²€í† Â·ìµœì í™” íŠ¹í™” ì—ì´ì „íŠ¸",
                "primary_tasks": [
                    "plan_evaluation",
                    "optimization_analysis",
                    "feasibility_assessment",
                ],
                "trinity_weights": {
                    "truth": 0.25,  # ì •í™•í•œ í‰ê°€
                    "goodness": 0.25,  # ê³µì •í•œ íŒë‹¨
                    "beauty": 0.25,  # ëª…í™•í•œ ìµœì í™”
                    "serenity": 0.15,  # ì•ˆì •ì ì¸ ê²€í† 
                    "eternity": 0.10,  # ì§€ì†ì ì¸ ê°œì„ 
                },
                "model_candidates": [
                    "claude-sonnet-4.5",
                    "claude-opus-4.5",
                    "grok-2",
                    "gemini-pro",
                ],
                "min_trinity_threshold": 85.0,
            },
            {
                "agent_name": "sage_agent",
                "role_description": "ê³ ì „ ì§€ì‹ê³¼ í˜„ëŒ€ AIì˜ ì „ëµì  ê²°í•© íŠ¹í™” ì—ì´ì „íŠ¸",
                "primary_tasks": [
                    "strategic_advice",
                    "wisdom_accumulation",
                    "principle_application",
                ],
                "trinity_weights": {
                    "truth": 0.30,  # ì „ëµì  ì •í™•ì„±
                    "goodness": 0.25,  # ìœ¤ë¦¬ì  íŒë‹¨
                    "beauty": 0.20,  # ì „ëµì  ìš°ì•„í•¨
                    "serenity": 0.15,  # í‰ì˜¨í•œ ì „ëµ
                    "eternity": 0.10,  # ì˜ì†ì  ì§€í˜œ
                },
                "model_candidates": [
                    "claude-opus-4.5",  # ì „ëµì  ì‚¬ê³ ìš©
                    "claude-sonnet-4.5",
                    "grok-2",
                ],
                "min_trinity_threshold": 88.0,
            },
        ]

        for data in roles_data:
            role = AgentRole(**data)
            self.agent_roles[role.agent_name] = role

        logger.info(f"ğŸ‘¥ Initialized {len(self.agent_roles)} agent roles")

    async def execute_cycle(self) -> None:
        """
        Meritocracy Routerì˜ ì£¼ìš” ì‹¤í–‰ ë¡œì§

        ìˆ˜í–‰ ì‘ì—…:
        1. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—…ë°ì´íŠ¸
        2. ë¡œí…Œì´ì…˜ ì •ì±… ìµœì í™”
        3. ì„ íƒ íˆìŠ¤í† ë¦¬ ì •ë¦¬ ë° ë¶„ì„
        """

        try:
            # 1. ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            await self._update_model_performance()

            # 2. ë¡œí…Œì´ì…˜ ì •ì±… ìµœì í™”
            await self._optimize_rotation_policy()

            # 3. ì„ íƒ íˆìŠ¤í† ë¦¬ ë¶„ì„
            await self._analyze_selection_history()

            logger.info("ğŸ›ï¸ Meritocracy cycle completed - í˜„ëª…í•œ ì„ íƒì˜ ì—°ì†")

        except Exception as e:
            logger.error(f"Meritocracy cycle error: {e}")
            self.status.error_count += 1

    async def select_best_model(
        self, agent_name: str, task_context: dict[str, Any]
    ) -> tuple[str, SelectionEvidence]:
        """
        Agentì™€ ì—…ë¬´ì— ëŒ€í•œ ìµœì  ëª¨ë¸ ì„ íƒ

        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            task_context: ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸

        Returns:
            (ì„ íƒëœ ëª¨ë¸ ID, ì„ íƒ ê·¼ê±° ì¦ê±°)
        """
        if not self.rotation_enabled:
            # ë¡œí…Œì´ì…˜ ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜
            default_model = self._get_default_model(agent_name)
            evidence = self._create_default_evidence(agent_name, default_model, task_context)
            return default_model, evidence

        # Agent ì—­í•  í™•ì¸
        if agent_name not in self.agent_roles:
            raise ValueError(f"Unknown agent: {agent_name}")

        role = self.agent_roles[agent_name]
        task_type = task_context.get("task_type", "general")
        task_complexity = task_context.get("complexity", "medium")

        # ê° í›„ë³´ ëª¨ë¸ì˜ Trinity Score ê³„ì‚°
        model_scores = {}
        for model_id in role.model_candidates:
            if model_id in self.model_candidates:
                candidate = self.model_candidates[model_id]
                trinity_score = candidate.calculate_trinity_score(
                    task_type, self.performance_history_days
                )

                # ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
                complexity_multiplier = self._get_complexity_multiplier(task_complexity)
                adjusted_score = trinity_score * complexity_multiplier

                # ê°€ì¤‘ì¹˜ ì ìš©
                weighted_score = self._apply_role_weights(adjusted_score, role.trinity_weights)

                model_scores[model_id] = {
                    "raw_score": trinity_score,
                    "adjusted_score": adjusted_score,
                    "weighted_score": weighted_score,
                    "performance_records": len(candidate.get_recent_performance(task_type)),
                }

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        if not model_scores:
            # ì ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
            selected_model = role.model_candidates[0]
            confidence = 0.5
        else:
            # ê°€ì¤‘ ì ìˆ˜ ê¸°ì¤€ ìµœê³  ëª¨ë¸ ì„ íƒ
            sorted_models = sorted(
                model_scores.items(), key=lambda x: x[1]["weighted_score"], reverse=True
            )
            selected_model = sorted_models[0][0]
            best_score = sorted_models[0][1]["weighted_score"]
            confidence = min(1.0, best_score / 100.0)

        # ì„ íƒ ê·¼ê±° ì¦ê±° ìƒì„±
        evidence = self._create_selection_evidence(
            agent_name, selected_model, model_scores, task_context, confidence
        )

        # ì„ íƒ ê¸°ë¡ ì €ì¥
        self.selection_history.append(evidence)
        self.evidence_bundles[evidence.selection_id] = evidence

        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_selection_stats(selected_model, confidence)

        logger.info(f"ğŸ† {agent_name} â†’ {selected_model} (confidence: {confidence:.2f})")

        return selected_model, evidence

    def _get_default_model(self, agent_name: str) -> str:
        """ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜"""
        role = self.agent_roles.get(agent_name)
        if role and role.model_candidates:
            return role.model_candidates[0]
        return "claude-sonnet-4.5"  # ì•ˆì „í•œ ê¸°ë³¸ê°’

    def _create_default_evidence(
        self, agent_name: str, model: str, task_context: dict[str, Any]
    ) -> SelectionEvidence:
        """ê¸°ë³¸ ì„ íƒ ì¦ê±° ìƒì„±"""
        return SelectionEvidence(
            selection_id=f"default_{int(time.time())}_{agent_name}",
            agent_name=agent_name,
            task_description=task_context.get("description", "Default task"),
            selected_model=model,
            trinity_scores={model: 75.0},
            selection_reasoning="Rotation disabled - using default model",
            confidence_level=0.5,
            alternatives_considered=[],
            selection_timestamp=time.time(),
            selection_criteria={"rotation_enabled": False},
        )

    def _get_complexity_multiplier(self, complexity: str) -> float:
        """ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ìŠ¹ìˆ˜"""
        multipliers = {"low": 1.0, "medium": 1.0, "high": 1.1, "critical": 1.2}
        return multipliers.get(complexity, 1.0)

    def _apply_role_weights(self, score: float, weights: dict[str, float]) -> float:
        """ì—­í• ë³„ ê°€ì¤‘ì¹˜ ì ìš© (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        # ê°€ì¤‘ì¹˜ ì´í•© ê³„ì‚° (ì •ê·œí™”ìš©)
        weight_sum = sum(weights.values()) if weights else 1.0
        # ê°€ì¤‘ì¹˜ ì´í•©ì´ 1.0ì— ê°€ê¹Œìš°ë©´ ì ìˆ˜ ìœ ì§€, ì•„ë‹ˆë©´ ì •ê·œí™” ì ìš©
        multiplier = 1.0 if abs(weight_sum - 1.0) < 0.01 else weight_sum
        return score * multiplier

    def _create_selection_evidence(
        self,
        agent_name: str,
        selected_model: str,
        model_scores: dict[str, dict[str, Any]],
        task_context: dict[str, Any],
        confidence: float,
    ) -> SelectionEvidence:
        """ì„ íƒ ê·¼ê±° ì¦ê±° ìƒì„±"""
        selection_id = f"sel_{int(time.time())}_{agent_name}_{hash(selected_model) % 1000}"

        # Trinity ì ìˆ˜ ì¶”ì¶œ
        trinity_scores = {model: scores["weighted_score"] for model, scores in model_scores.items()}

        # ì„ íƒ ê·¼ê±° ìƒì„±
        reasoning = self._generate_selection_reasoning(selected_model, model_scores, confidence)

        # ê³ ë ¤ëœ ëŒ€ì•ˆë“¤
        alternatives = [model for model in model_scores.keys() if model != selected_model]

        evidence = SelectionEvidence(
            selection_id=selection_id,
            agent_name=agent_name,
            task_description=task_context.get("description", "Task execution"),
            selected_model=selected_model,
            trinity_scores=trinity_scores,
            selection_reasoning=reasoning,
            confidence_level=confidence,
            alternatives_considered=alternatives,
            selection_timestamp=time.time(),
            selection_criteria={
                "rotation_enabled": self.rotation_enabled,
                "performance_history_days": self.performance_history_days,
                "task_complexity": task_context.get("complexity", "medium"),
                "model_candidates_count": len(model_scores),
            },
        )

        return evidence

    def _generate_selection_reasoning(
        self, selected_model: str, model_scores: dict[str, dict[str, Any]], confidence: float
    ) -> str:
        """ì„ íƒ ê·¼ê±° ìƒì„±"""
        if not model_scores:
            return "ì ìˆ˜ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ ì„ íƒ"

        selected_score = model_scores[selected_model]["weighted_score"]
        best_alternative = None
        best_alt_score = 0

        for model, scores in model_scores.items():
            if model != selected_model and scores["weighted_score"] > best_alt_score:
                best_alternative = model
                best_alt_score = scores["weighted_score"]

        margin = selected_score - best_alt_score if best_alternative else 0

        reasoning = f"Trinity Score {selected_score:.1f}ì ìœ¼ë¡œ ì„ íƒ"
        if best_alternative:
            reasoning += f" (ì°¨ì´: {margin:.1f}ì , ëŒ€ì•ˆ: {best_alternative})"
        reasoning += f" - ì‹ ë¢°ë„: {confidence:.2f}"

        return reasoning

    def _update_selection_stats(self, selected_model: str, confidence: float):
        """ì„ íƒ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.selection_stats["total_selections"] += 1

        # ëª¨ë¸ ë¶„í¬ ì—…ë°ì´íŠ¸
        if selected_model not in self.selection_stats["model_distribution"]:
            self.selection_stats["model_distribution"][selected_model] = 0
        self.selection_stats["model_distribution"][selected_model] += 1

        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        current_avg = self.selection_stats["average_confidence"]
        total = self.selection_stats["total_selections"]
        self.selection_stats["average_confidence"] = (
            current_avg * (total - 1) + confidence
        ) / total

    async def _update_model_performance(self) -> None:
        """ëª¨ë¸ ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” ì‹¤ì œ API í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜

        for candidate in self.model_candidates.values():
            # ê°€ìƒì˜ ì„±ëŠ¥ ë°ì´í„° ì¶”ê°€
            if len(candidate.performance_history) < 10:
                mock_score = 75.0 + (hash(candidate.model_id) % 20)  # 75-95 ì‚¬ì´
                candidate.add_performance_record(
                    task_type="general",
                    trinity_score=mock_score,
                    response_time=1.0 + (hash(candidate.model_id) % 2),
                    cost=candidate.cost_per_token * 100,
                    quality_metrics={"accuracy": 0.9, "relevance": 0.85},
                )

    async def _optimize_rotation_policy(self) -> None:
        """ë¡œí…Œì´ì…˜ ì •ì±… ìµœì í™”"""
        # ì„±ê³µë¥  ë¶„ì„
        recent_selections = list(self.selection_history[-50:])  # ìµœê·¼ 50ê°œ

        if recent_selections:
            high_confidence_selections = [s for s in recent_selections if s.confidence_level > 0.8]
            success_rate = len(high_confidence_selections) / len(recent_selections)
            self.selection_stats["success_rate"] = success_rate

            # ì„±ê³µë¥ ì´ ë‚®ìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ
            if success_rate < 0.7:
                self.confidence_threshold = min(0.9, self.confidence_threshold + 0.05)
            elif success_rate > 0.9:
                self.confidence_threshold = max(0.7, self.confidence_threshold - 0.02)

    async def _analyze_selection_history(self) -> None:
        """ì„ íƒ íˆìŠ¤í† ë¦¬ ë¶„ì„"""
        # ì˜¤ë˜ëœ ê¸°ë¡ ì •ë¦¬ (1000ê°œ ì´ìƒì´ë©´)
        if len(self.selection_history) > 1000:
            # ìµœê·¼ 500ê°œë§Œ ìœ ì§€
            self.selection_history = self.selection_history[-500:]
            self.evidence_bundles = {
                k: v
                for k, v in self.evidence_bundles.items()
                if v.selection_timestamp > time.time() - (30 * 24 * 60 * 60)  # 30ì¼ ì´ë‚´
            }

    async def get_metrics(self) -> dict[str, Any]:
        """Meritocracy Router ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return await self.get_meritocracy_report()

    async def get_meritocracy_report(self) -> dict[str, Any]:
        """ëŠ¥ë ¥ì£¼ì˜ ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸"""
        total_selections = len(self.selection_history)
        recent_selections = list(self.selection_history[-100:])  # ìµœê·¼ 100ê°œ

        # ëª¨ë¸ë³„ ì„ íƒ í†µê³„
        model_stats = {}
        for selection in recent_selections:
            model = selection.selected_model
            if model not in model_stats:
                model_stats[model] = {"count": 0, "avg_confidence": 0, "total_confidence": 0}
            model_stats[model]["count"] += 1
            model_stats[model]["total_confidence"] += selection.confidence_level

        for model in model_stats:
            model_stats[model]["avg_confidence"] = (
                model_stats[model]["total_confidence"] / model_stats[model]["count"]
            )

        # Agentë³„ ì„±ëŠ¥
        agent_performance = {}
        for agent_name in self.agent_roles.keys():
            agent_selections = [s for s in recent_selections if s.agent_name == agent_name]
            if agent_selections:
                avg_confidence = sum(s.confidence_level for s in agent_selections) / len(
                    agent_selections
                )
                agent_performance[agent_name] = {
                    "selections": len(agent_selections),
                    "avg_confidence": avg_confidence,
                    "success_rate": len([s for s in agent_selections if s.confidence_level > 0.8])
                    / len(agent_selections),
                }

        return {
            "timestamp": datetime.now().isoformat(),
            "system_status": "active" if self.rotation_enabled else "disabled",
            "total_selections": total_selections,
            "recent_selections": len(recent_selections),
            "model_performance": model_stats,
            "agent_performance": agent_performance,
            "selection_stats": self.selection_stats,
            "philosophy": "ì„¸ì¢…ëŒ€ì™•ì˜ í†µì¹˜ìˆ  - í˜„ëª…í•œ ì„ íƒ, ì ì¬ì ì†Œ ë°°ì¹˜",
            "evidence_bundle_count": len(self.evidence_bundles),
        }

    def get_selection_evidence(self, selection_id: str) -> SelectionEvidence | None:
        """ì„ íƒ ì¦ê±° ì¡°íšŒ"""
        return self.evidence_bundles.get(selection_id)

    def export_evidence_bundle(self, selection_id: str) -> str | None:
        """ì¦ê±° ë²ˆë“¤ JSONå¯¼å‡º"""
        evidence = self.get_selection_evidence(selection_id)
        if evidence:
            return json.dumps(evidence.to_dict(), indent=2, ensure_ascii=False)
        return None


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
meritocracy_router = MeritocracyRouter()


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
async def select_best_model_for_agent(
    agent_name: str, task_context: dict[str, Any]
) -> tuple[str, SelectionEvidence]:
    """Agent ìµœì  ëª¨ë¸ ì„ íƒ ìœ í‹¸ë¦¬í‹°"""
    return await meritocracy_router.select_best_model(agent_name, task_context)


async def get_meritocracy_report() -> dict[str, Any]:
    """ëŠ¥ë ¥ì£¼ì˜ ë¦¬í¬íŠ¸ ìœ í‹¸ë¦¬í‹°"""
    return await meritocracy_router.get_meritocracy_report()


async def get_selection_evidence(selection_id: str) -> SelectionEvidence | None:
    """ì„ íƒ ì¦ê±° ì¡°íšŒ ìœ í‹¸ë¦¬í‹°"""
    return meritocracy_router.get_selection_evidence(selection_id)


__all__ = [
    # Models (re-export for convenience)
    "ModelProvider",
    "ModelCandidate",
    "AgentRole",
    "SelectionEvidence",
    # Router
    "MeritocracyRouter",
    "meritocracy_router",
    # Utility functions
    "select_best_model_for_agent",
    "get_meritocracy_report",
    "get_selection_evidence",
]
