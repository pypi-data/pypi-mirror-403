# Trinity Score: 90.0 (Established by Chancellor)
"""
Trinity Integration - Antigravity â†” Trinity Evidence ì—°ê²°
ëª¨ë“ˆì‹ Antigravity ì—”ì§„ì„ Trinity Evidence ì‹œìŠ¤í…œì— í†µí•©
"""

import logging
from datetime import datetime
from typing import Any, cast

from .modular_engine import ModularAntigravityEngine, create_simple_engine

logger = logging.getLogger(__name__)


class TrinityAntigravityIntegration:
    """
    Trinity Evidenceì™€ Antigravityì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    1. Trinity Evidenceì˜ ì ìˆ˜ë¥¼ Antigravity í’ˆì§ˆ ê²Œì´íŠ¸ì— ì—°ê²°
    2. Antigravity ê²°ê³¼ë¥¼ Trinity Evidenceì— í”¼ë“œë°±
    3. ìë™í™”ëœ í’ˆì§ˆ ê²Œì´íŠ¸ ì›Œí¬í”Œë¡œìš°
    """

    def __init__(self, engine: ModularAntigravityEngine | None = None) -> None:
        """
        í†µí•© ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”

        Args:
            engine: ì‚¬ìš©í•  Antigravity ì—”ì§„ (ê¸°ë³¸: simple_engine)
        """
        self.engine = engine or create_simple_engine()
        self.last_evaluation: dict[str, Any] | None = None
        self.evaluation_history: list[dict[str, Any]] = []

        logger.info("âœ… Trinity-Antigravity í†µí•© ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ì‚¬ìš© ì—”ì§„: {self.engine._get_active_modules()}")

    async def evaluate_from_trinity_evidence(
        self, trinity_evidence_path: str, context: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        Trinity Evidence íŒŒì¼ì—ì„œ ì§ì ‘ í’ˆì§ˆ ê²Œì´íŠ¸ í‰ê°€

        Args:
            trinity_evidence_path: Trinity Evidence JSON íŒŒì¼ ê²½ë¡œ
            context: ì¶”ê°€ ë§¥ë½ ì •ë³´

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        try:
            # Trinity Evidence ë¡œë“œ
            evidence_data = self._load_trinity_evidence(trinity_evidence_path)

            # ì ìˆ˜ ì¶”ì¶œ
            trinity_score = evidence_data.get("calculation", {}).get("total", 0.0)

            # Risk Score ê³„ì‚° (Trinity Score ê¸°ë°˜)
            risk_score = self._calculate_risk_from_evidence(evidence_data)

            # ë§¥ë½ ì •ë³´ êµ¬ì„±
            evaluation_context = self._build_evaluation_context(evidence_data, context or {})

            # í’ˆì§ˆ ê²Œì´íŠ¸ í‰ê°€
            result = await self.engine.evaluate_quality_gate(
                trinity_score, risk_score, evaluation_context
            )

            # ê²°ê³¼ ê¸°ë¡
            self.last_evaluation = result
            self.evaluation_history.append(
                {
                    "timestamp": datetime.now(),
                    "evidence_path": trinity_evidence_path,
                    "trinity_score": trinity_score,
                    "risk_score": risk_score,
                    "decision": result["decision"],
                    "confidence": result.get("confidence", 0.0),
                }
            )

            # Trinity Evidenceì— í”¼ë“œë°±
            await self._feedback_to_trinity_evidence(trinity_evidence_path, result)

            logger.info(f"âœ… Trinity Evidence ê¸°ë°˜ í‰ê°€ ì™„ë£Œ: {result['decision']}")
            return result

        except Exception as e:
            logger.error(f"Trinity Evidence í‰ê°€ ì‹¤íŒ¨: {e}")
            raise

    def _load_trinity_evidence(self, evidence_path: str) -> dict[str, Any]:
        """
        Trinity Evidence íŒŒì¼ ë¡œë“œ

        Args:
            evidence_path: JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            íŒŒì‹±ëœ Evidence ë°ì´í„°
        """
        import json
        from pathlib import Path

        evidence_file = Path(evidence_path)
        if not evidence_file.exists():
            raise FileNotFoundError(f"Trinity Evidence íŒŒì¼ ì—†ìŒ: {evidence_path}")

        with open(evidence_file, encoding="utf-8") as f:
            return cast("dict[str, Any]", json.load(f))

    def _calculate_risk_from_evidence(self, evidence_data: dict[str, Any]) -> float:
        """
        Trinity Evidenceì—ì„œ Risk Score ê³„ì‚°

        Risk Score ê³„ì‚° ê¸°ì¤€:
        - í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ìœ¨
        - ë¦°íŠ¸ ìœ„ë°˜ ìˆ˜
        - ë³´ì•ˆ ì·¨ì•½ì  ìˆ˜
        - ì½”ë“œ ë³µì¡ë„

        Args:
            evidence_data: Evidence ë°ì´í„°

        Returns:
            ê³„ì‚°ëœ Risk Score (0-100)
        """
        risk_factors = []

        # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ìœ¨ ê¸°ë°˜ ë¦¬ìŠ¤í¬
        test_results = evidence_data.get("test_results", {})
        if test_results:
            total_tests = test_results.get("total", 0)
            failed_tests = test_results.get("failed", 0)
            if total_tests > 0:
                failure_rate = failed_tests / total_tests
                risk_factors.append(failure_rate * 30)  # ìµœëŒ€ 30ì 

        # ë¦°íŠ¸ ìœ„ë°˜ ê¸°ë°˜ ë¦¬ìŠ¤í¬
        lint_results = evidence_data.get("lint_results", {})
        if lint_results:
            violations = lint_results.get("violations", 0)
            risk_factors.append(min(violations * 2, 20))  # ìµœëŒ€ 20ì 

        # ë³´ì•ˆ ì·¨ì•½ì  ê¸°ë°˜ ë¦¬ìŠ¤í¬
        security_results = evidence_data.get("security_results", {})
        if security_results:
            vulnerabilities = security_results.get("vulnerabilities", 0)
            risk_factors.append(min(vulnerabilities * 10, 30))  # ìµœëŒ€ 30ì 

        # ì½”ë“œ ë³µì¡ë„ ê¸°ë°˜ ë¦¬ìŠ¤í¬
        complexity = evidence_data.get("complexity_score", 0)
        if complexity > 50:  # ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ ë¦¬ìŠ¤í¬ ì¦ê°€
            risk_factors.append(min((complexity - 50) * 0.5, 20))

        # ì¢…í•© ë¦¬ìŠ¤í¬ ê³„ì‚°
        total_risk = sum(risk_factors)
        return float(min(total_risk, 100.0))  # ìµœëŒ€ 100ì 

    def _build_evaluation_context(
        self, evidence_data: dict[str, Any], additional_context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        í‰ê°€ ë§¥ë½ ì •ë³´ êµ¬ì„±

        Args:
            evidence_data: Evidence ë°ì´í„°
            additional_context: ì¶”ê°€ ë§¥ë½

        Returns:
            ì™„ì„±ëœ í‰ê°€ ë§¥ë½
        """
        context = {
            "evidence_type": "trinity_evidence",
            "timestamp": evidence_data.get("generated_at"),
            "project_info": evidence_data.get("project_info", {}),
            "test_coverage": evidence_data.get("test_coverage", 80.0),
            "ci_status": evidence_data.get("ci_status", "unknown"),
            "change_scope": self._infer_change_scope(evidence_data),
            "team_experience": additional_context.get("team_experience", "intermediate"),
            "time_pressure": additional_context.get("time_pressure", "normal"),
        }

        # ì¶”ê°€ ë§¥ë½ ë³‘í•©
        context.update(additional_context)
        return context

    def _infer_change_scope(self, evidence_data: dict[str, Any]) -> str:
        """
        Evidence ë°ì´í„°ì—ì„œ ë³€ê²½ ë²”ìœ„ ì¶”ë¡ 

        Args:
            evidence_data: Evidence ë°ì´í„°

        Returns:
            ì¶”ë¡ ëœ ë³€ê²½ ë²”ìœ„ ("small", "medium", "large", "breaking")
        """
        # ë³€ê²½ëœ íŒŒì¼ ìˆ˜ ê¸°ë°˜ ì¶”ë¡ 
        changed_files = evidence_data.get("changed_files", [])
        if len(changed_files) <= 3:
            return "small"
        elif len(changed_files) <= 10:
            return "medium"
        elif len(changed_files) <= 50:
            return "large"
        else:
            return "breaking"

    async def _feedback_to_trinity_evidence(
        self, evidence_path: str, evaluation_result: dict[str, Any]
    ) -> None:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ Trinity Evidenceì— í”¼ë“œë°±

        Args:
            evidence_path: Evidence íŒŒì¼ ê²½ë¡œ
            evaluation_result: í‰ê°€ ê²°ê³¼
        """
        try:
            # Evidence íŒŒì¼ì— í‰ê°€ ê²°ê³¼ ì¶”ê°€
            import json
            from pathlib import Path

            evidence_file = Path(evidence_path)
            if not evidence_file.exists():
                return

            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            with open(evidence_file, encoding="utf-8") as f:
                data = json.load(f)

            # í‰ê°€ ê²°ê³¼ ì¶”ê°€
            data["antigravity_evaluation"] = {
                "timestamp": datetime.now().isoformat(),
                "decision": evaluation_result["decision"],
                "confidence": evaluation_result.get("confidence", 0.0),
                "active_modules": evaluation_result.get("active_modules", []),
                "trinity_score_used": evaluation_result["trinity_score"],
                "risk_score_calculated": evaluation_result["risk_score"],
            }

            # íŒŒì¼ ì €ì¥
            with open(evidence_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… Trinity Evidenceì— í‰ê°€ ê²°ê³¼ í”¼ë“œë°±: {evidence_path}")

        except Exception as e:
            logger.warning(f"Trinity Evidence í”¼ë“œë°± ì‹¤íŒ¨: {e}")

    async def run_automated_workflow(
        self, evidence_path: str, auto_apply: bool = False
    ) -> dict[str, Any]:
        """
        ìë™í™”ëœ í’ˆì§ˆ ê²Œì´íŠ¸ ì›Œí¬í”Œë¡œìš°

        Args:
            evidence_path: Trinity Evidence íŒŒì¼ ê²½ë¡œ
            auto_apply: AUTO_RUN ì‹œ ìë™ ì ìš© ì—¬ë¶€

        Returns:
            ì›Œí¬í”Œë¡œìš° ê²°ê³¼
        """
        logger.info(f"ğŸš€ ìë™ í’ˆì§ˆ ê²Œì´íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {evidence_path}")

        # 1. í‰ê°€ ì‹¤í–‰
        evaluation = await self.evaluate_from_trinity_evidence(evidence_path)

        # 2. ê²°ì •ì— ë”°ë¥¸ ì•¡ì…˜
        decision = evaluation["decision"]
        actions_taken = []

        if decision == "AUTO_RUN":
            if auto_apply:
                actions_taken.append("ìë™ ë°°í¬ ì‹¤í–‰")
                # ì‹¤ì œ ë°°í¬ ë¡œì§ì€ ì—¬ê¸°ì„œ í˜¸ì¶œ
                logger.info("âœ… AUTO_RUN: ìë™ ë°°í¬ ì§„í–‰")
            else:
                actions_taken.append("ìë™ ë°°í¬ ì¤€ë¹„ë¨")
                logger.info("âœ… AUTO_RUN: ìˆ˜ë™ í™•ì¸ ëŒ€ê¸°")

        elif decision == "ASK_COMMANDER":
            actions_taken.append("í˜•ë‹˜ íŒë‹¨ ìš”ì²­")
            # ì•Œë¦¼ ì‹œìŠ¤í…œ í˜¸ì¶œ
            logger.info("âš ï¸ ASK_COMMANDER: í˜•ë‹˜ íŒë‹¨ ìš”ì²­")

        else:  # BLOCK
            actions_taken.append("ë°°í¬ ì°¨ë‹¨")
            # ì°¨ë‹¨ ì¡°ì¹˜ ì‹¤í–‰
            logger.warning("ğŸ›‘ BLOCK: ë°°í¬ ì°¨ë‹¨")

        # 3. ë³´ê³ ì„œ ìƒì„± (í™œì„±í™”ëœ ê²½ìš°)
        report_path = None
        if self.engine.config.get("use_reporting", False):
            try:
                report = await self.engine.generate_report(
                    "analysis",
                    {
                        "title": f"í’ˆì§ˆ ê²Œì´íŠ¸ í‰ê°€ - {decision}",
                        "evidence_path": evidence_path,
                    },
                    {
                        "decision": decision,
                        "confidence": evaluation.get("confidence", 0.0),
                    },
                    {"evaluation": evaluation},
                    actions_taken,
                )

                if report:
                    report_path = self.engine.save_report(
                        report,
                        f"quality_gate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    )
                    actions_taken.append(f"ë³´ê³ ì„œ ìƒì„±: {report_path}")

            except Exception as e:
                logger.warning(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

        result = {
            "evaluation": evaluation,
            "decision": decision,
            "actions_taken": actions_taken,
            "report_path": report_path,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info(f"âœ… ìë™ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {decision}")
        return result

    def get_evaluation_history(self, limit: int = 10) -> list[dict[str, Any]]:
        """
        í‰ê°€ íˆìŠ¤í† ë¦¬ ì¡°íšŒ

        Args:
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜

        Returns:
            ìµœê·¼ í‰ê°€ íˆìŠ¤í† ë¦¬
        """
        return self.evaluation_history[-limit:]

    def get_statistics(self) -> dict[str, Any]:
        """
        í‰ê°€ í†µê³„ ì¡°íšŒ

        Returns:
            í‰ê°€ í†µê³„
        """
        if not self.evaluation_history:
            return {"total_evaluations": 0}

        decisions = [h["decision"] for h in self.evaluation_history]
        total = len(decisions)

        return {
            "total_evaluations": total,
            "auto_run_rate": decisions.count("AUTO_RUN") / total * 100,
            "ask_commander_rate": decisions.count("ASK_COMMANDER") / total * 100,
            "block_rate": (decisions.count("BLOCK") if "BLOCK" in decisions else 0) / total * 100,
            "average_confidence": sum(h.get("confidence", 0.0) for h in self.evaluation_history)
            / total,
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
trinity_integration = TrinityAntigravityIntegration()
