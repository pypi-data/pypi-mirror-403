#!/usr/bin/env python3
"""
SSOT Document Drift Detector (ë¬¸ì„œ í•´ì‹œ ê¸°ë°˜ ë“œë¦¬í”„íŠ¸ ê°ì§€)

í•µì‹¬ SSOT ë¬¸ì„œì˜ í•´ì‹œë¥¼ ì¶”ì í•˜ì—¬ ë¬´ë‹¨ ë³€ê²½ì„ ê°ì§€í•©ë‹ˆë‹¤.
ë“œë¦¬í”„íŠ¸ ë°œê²¬ ì‹œ GitHub Issueë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

ì›ì¹™: "ë¬¸ì„œëŠ” ì½”ë“œì˜ ê³„ì•½ì´ë‹¤ - ê³„ì•½ ë³€ê²½ì€ ì¶”ì ë˜ì–´ì•¼ í•œë‹¤"
"""

import hashlib
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, TypedDict

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SSOT: ì¶”ì í•  í•µì‹¬ ë¬¸ì„œ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DocumentSpec(TypedDict):
    path: str
    category: str  # "governance" | "architecture" | "specification"
    criticality: str  # "critical" | "high" | "medium"
    description: str


TRACKED_DOCUMENTS: List[DocumentSpec] = [
    # Governance (í†µì¹˜ ê·œì¹™)
    {
        "path": "CLAUDE.md",
        "category": "governance",
        "criticality": "critical",
        "description": "Claude Code ì‘ì—… ì§€ì¹¨ ë° ê·œì¹™",
    },
    {
        "path": "AGENTS.md",
        "category": "governance",
        "criticality": "critical",
        "description": "ì—ì´ì „íŠ¸ ê±°ë²„ë„ŒìŠ¤ ë° Trinity Score",
    },
    {
        "path": "packages/afo-core/CLAUDE.md",
        "category": "governance",
        "criticality": "high",
        "description": "Backend ì „ìš© Claude ì§€ì¹¨",
    },
    {
        "path": "packages/dashboard/CLAUDE.md",
        "category": "governance",
        "criticality": "high",
        "description": "Frontend ì „ìš© Claude ì§€ì¹¨",
    },
    # Architecture (ì•„í‚¤í…ì²˜)
    {
        "path": "docs/AFO_FINAL_SSOT.md",
        "category": "architecture",
        "criticality": "critical",
        "description": "ìµœì¢… SSOT ì •ì˜ì„œ",
    },
    {
        "path": "docs/AFO_ROYAL_LIBRARY.md",
        "category": "architecture",
        "criticality": "critical",
        "description": "ì™•ë¦½ ë„ì„œê´€ - í•µì‹¬ ì›ì¹™",
    },
    {
        "path": "docs/AFO_CHANCELLOR_GRAPH_SPEC.md",
        "category": "specification",
        "criticality": "high",
        "description": "Chancellor Graph ìŠ¤í™",
    },
    # Configuration (ì„¤ì •)
    {
        "path": "packages/trinity-os/TRINITY_OS_PERSONAS.yaml",
        "category": "specification",
        "criticality": "critical",
        "description": "Trinity Score ê°€ì¤‘ì¹˜ SSOT",
    },
]

BASELINE_FILE = "artifacts/ssot_document_baseline.json"
DRIFT_REPORT_DIR = "artifacts/document_drift"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•´ì‹œ ê³„ì‚° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def get_repo_root() -> Path:
    """ì €ì¥ì†Œ ë£¨íŠ¸ ê²½ë¡œ ë°˜í™˜"""
    result = subprocess.run(
        ["git", "rev-parse", "--show-toplevel"],
        capture_output=True,
        text=True,
        check=True,
    )
    return Path(result.stdout.strip())


def compute_file_hash(filepath: Path) -> Optional[str]:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œ ê³„ì‚°"""
    if not filepath.exists():
        return None
    try:
        content = filepath.read_bytes()
        return hashlib.sha256(content).hexdigest()[:16]  # 16ìë¡œ ì¶•ì•½
    except Exception:
        return None


def compute_all_hashes(repo: Path) -> Dict[str, dict]:
    """ëª¨ë“  ì¶”ì  ë¬¸ì„œì˜ í•´ì‹œ ê³„ì‚°"""
    results = {}
    for doc in TRACKED_DOCUMENTS:
        filepath = repo / doc["path"]
        file_hash = compute_file_hash(filepath)
        mtime = None
        if filepath.exists():
            mtime = datetime.fromtimestamp(filepath.stat().st_mtime).isoformat()

        results[doc["path"]] = {
            "hash": file_hash,
            "exists": filepath.exists(),
            "category": doc["category"],
            "criticality": doc["criticality"],
            "description": doc["description"],
            "mtime": mtime,
        }
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë² ì´ìŠ¤ë¼ì¸ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def load_baseline(repo: Path) -> Optional[dict]:
    """ì €ì¥ëœ ë² ì´ìŠ¤ë¼ì¸ ë¡œë“œ"""
    baseline_path = repo / BASELINE_FILE
    if not baseline_path.exists():
        return None
    try:
        return json.loads(baseline_path.read_text())
    except Exception:
        return None


def save_baseline(repo: Path, hashes: Dict[str, dict]) -> None:
    """í˜„ì¬ í•´ì‹œë¥¼ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì €ì¥"""
    baseline_path = repo / BASELINE_FILE
    baseline_path.parent.mkdir(parents=True, exist_ok=True)

    baseline = {
        "created_at": datetime.now().isoformat(),
        "documents": hashes,
    }
    baseline_path.write_text(json.dumps(baseline, indent=2, ensure_ascii=False))
    print(f"âœ… Baseline saved: {baseline_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë“œë¦¬í”„íŠ¸ ê°ì§€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class DriftResult(TypedDict):
    path: str
    status: str  # "changed" | "added" | "deleted" | "unchanged"
    criticality: str
    old_hash: Optional[str]
    new_hash: Optional[str]
    description: str


def detect_drift(baseline: dict, current: Dict[str, dict]) -> tuple[List[DriftResult], dict]:
    """ë² ì´ìŠ¤ë¼ì¸ê³¼ í˜„ì¬ ìƒíƒœ ë¹„êµí•˜ì—¬ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
    drifts: List[DriftResult] = []
    baseline_docs = baseline.get("documents", {})

    summary = {
        "total": len(current),
        "changed": 0,
        "added": 0,
        "deleted": 0,
        "unchanged": 0,
        "critical_drift": 0,
    }

    for path, curr_info in current.items():
        base_info = baseline_docs.get(path, {})
        old_hash = base_info.get("hash")
        new_hash = curr_info.get("hash")

        if old_hash is None and new_hash is not None:
            status = "added"
            summary["added"] += 1
        elif old_hash is not None and new_hash is None:
            status = "deleted"
            summary["deleted"] += 1
        elif old_hash != new_hash:
            status = "changed"
            summary["changed"] += 1
        else:
            status = "unchanged"
            summary["unchanged"] += 1

        if status != "unchanged":
            drift: DriftResult = {
                "path": path,
                "status": status,
                "criticality": curr_info.get("criticality", "medium"),
                "old_hash": old_hash,
                "new_hash": new_hash,
                "description": curr_info.get("description", ""),
            }
            drifts.append(drift)

            if curr_info.get("criticality") == "critical":
                summary["critical_drift"] += 1

    return drifts, summary


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GitHub Issue ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def create_github_issue(repo: Path, drifts: List[DriftResult], summary: dict) -> bool:
    """ë“œë¦¬í”„íŠ¸ ë°œê²¬ ì‹œ GitHub Issue ìë™ ìƒì„±"""
    if not drifts:
        return False

    # Issue ë³¸ë¬¸ ìƒì„±
    today = datetime.now().strftime("%Y-%m-%d")
    critical_count = summary.get("critical_drift", 0)

    title = f"ğŸ“„ SSOT Document Drift Detected - {today}"
    if critical_count > 0:
        title = f"ğŸš¨ CRITICAL: {title}"

    body_lines = [
        "## SSOT Document Drift Report",
        "",
        f"**Date**: {today}",
        f"**Total Documents**: {summary['total']}",
        f"**Changed**: {summary['changed']}",
        f"**Added**: {summary['added']}",
        f"**Deleted**: {summary['deleted']}",
        f"**Critical Drifts**: {critical_count}",
        "",
        "---",
        "",
        "## Detected Changes",
        "",
    ]

    for drift in drifts:
        icon = {"changed": "ğŸ“", "added": "â•", "deleted": "âŒ"}.get(drift["status"], "â“")
        crit_badge = "ğŸ”´" if drift["criticality"] == "critical" else "ğŸŸ¡"

        body_lines.append(f"### {icon} {drift['path']}")
        body_lines.append(f"- **Status**: {drift['status'].upper()}")
        body_lines.append(f"- **Criticality**: {crit_badge} {drift['criticality']}")
        body_lines.append(f"- **Description**: {drift['description']}")
        if drift["old_hash"]:
            body_lines.append(f"- **Old Hash**: `{drift['old_hash']}`")
        if drift["new_hash"]:
            body_lines.append(f"- **New Hash**: `{drift['new_hash']}`")
        body_lines.append("")

    body_lines.extend(
        [
            "---",
            "",
            "## Required Actions",
            "",
            "1. Review the changed documents",
            "2. Verify changes are intentional and approved",
            "3. Update baseline if changes are approved: `python scripts/ssot_document_drift.py --update-baseline`",
            "4. Close this issue after verification",
            "",
            "---",
            "",
            "ğŸ¤– Auto-generated by SSOT Document Drift Detector",
        ]
    )

    body = "\n".join(body_lines)

    # GitHub CLIë¡œ Issue ìƒì„±
    labels = ["ssot", "document-drift", "automated"]
    if critical_count > 0:
        labels.append("critical")

    try:
        result = subprocess.run(
            [
                "gh",
                "issue",
                "create",
                "--title",
                title,
                "--body",
                body,
                "--label",
                ",".join(labels),
            ],
            capture_output=True,
            text=True,
            cwd=repo,
        )
        if result.returncode == 0:
            issue_url = result.stdout.strip()
            print(f"âœ… GitHub Issue created: {issue_url}")
            return True
        else:
            print(f"âš ï¸  Failed to create GitHub Issue: {result.stderr}")
            return False
    except FileNotFoundError:
        print("âš ï¸  GitHub CLI (gh) not found - skipping issue creation")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¦¬í¬íŠ¸ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def save_drift_report(
    repo: Path, drifts: List[DriftResult], summary: dict, current: Dict[str, dict]
) -> Path:
    """ë“œë¦¬í”„íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥"""
    report_dir = repo / DRIFT_REPORT_DIR
    report_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    report_file = report_dir / f"drift_{timestamp}.json"

    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": summary,
        "drifts": drifts,
        "current_state": current,
    }

    report_file.write_text(json.dumps(report, indent=2, ensure_ascii=False))
    return report_file


def print_report(drifts: List[DriftResult], summary: dict) -> None:
    """ì½˜ì†”ì— ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("=" * 60)
    print("  SSOT DOCUMENT DRIFT REPORT")
    print("=" * 60)
    print(f"  Timestamp: {datetime.now().isoformat()}")
    print("-" * 60)
    print(f"  Total Documents: {summary['total']}")
    print(f"  Changed: {summary['changed']}")
    print(f"  Added: {summary['added']}")
    print(f"  Deleted: {summary['deleted']}")
    print(f"  Unchanged: {summary['unchanged']}")
    print(f"  Critical Drifts: {summary['critical_drift']}")
    print("-" * 60)

    if not drifts:
        print("  âœ… No drift detected - all documents match baseline")
    else:
        print("  âš ï¸  DRIFT DETECTED:")
        print()
        for drift in drifts:
            icon = {"changed": "ğŸ“", "added": "â•", "deleted": "âŒ"}.get(drift["status"], "â“")
            crit = "ğŸ”´" if drift["criticality"] == "critical" else "ğŸŸ¡"
            print(f"  {icon} {crit} {drift['path']}")
            print(f"       Status: {drift['status']} | {drift['description']}")
            print()

    print("=" * 60)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Artifact ì •ë¦¬ (ë³´ì¡´ ì •ì±…)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RETENTION_DAYS = 30


def cleanup_old_reports(repo: Path, dry_run: bool = False) -> int:
    """ì˜¤ë˜ëœ ë“œë¦¬í”„íŠ¸ ë¦¬í¬íŠ¸ ì •ë¦¬ (ê¸°ë³¸ 30ì¼ ë³´ì¡´)"""
    report_dir = repo / DRIFT_REPORT_DIR
    if not report_dir.exists():
        return 0

    cutoff = datetime.now().timestamp() - (RETENTION_DAYS * 24 * 3600)
    removed = 0

    for report_file in report_dir.glob("drift_*.json"):
        if report_file.stat().st_mtime < cutoff:
            if dry_run:
                print(f"  Would remove: {report_file.name}")
            else:
                report_file.unlink()
            removed += 1

    return removed


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="SSOT Document Drift Detector")
    parser.add_argument(
        "--update-baseline",
        action="store_true",
        help="Update baseline with current state",
    )
    parser.add_argument(
        "--create-issue",
        action="store_true",
        help="Create GitHub issue on drift detection",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress console output",
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="Clean up old reports (older than 30 days)",
    )
    parser.add_argument(
        "--no-auto-cleanup",
        action="store_true",
        help="Disable automatic cleanup after each run",
    )
    args = parser.parse_args()

    repo = get_repo_root()

    # ì •ë¦¬ ì „ìš© ëª¨ë“œ
    if args.cleanup:
        removed = cleanup_old_reports(repo, dry_run=False)
        print(f"ğŸ§¹ Cleaned up {removed} old reports (>{RETENTION_DAYS} days)")
        return 0

    current = compute_all_hashes(repo)

    # ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸ ëª¨ë“œ
    if args.update_baseline:
        save_baseline(repo, current)
        return 0

    # ë² ì´ìŠ¤ë¼ì¸ ë¡œë“œ
    baseline = load_baseline(repo)
    if baseline is None:
        print("âš ï¸  No baseline found. Creating initial baseline...")
        save_baseline(repo, current)
        print("   Run again to detect drift against this baseline.")
        return 0

    # ë“œë¦¬í”„íŠ¸ ê°ì§€
    drifts, summary = detect_drift(baseline, current)

    # ë¦¬í¬íŠ¸ ì¶œë ¥
    if not args.quiet:
        print_report(drifts, summary)

    # ë¦¬í¬íŠ¸ ì €ì¥
    report_file = save_drift_report(repo, drifts, summary, current)
    if not args.quiet:
        print(f"\nğŸ“„ Report saved: {report_file}")

    # GitHub Issue ìƒì„±
    if args.create_issue and drifts:
        create_github_issue(repo, drifts, summary)

    # ìë™ ì •ë¦¬ (ê¸°ë³¸ í™œì„±í™”)
    if not args.no_auto_cleanup:
        removed = cleanup_old_reports(repo)
        if removed > 0 and not args.quiet:
            print(f"ğŸ§¹ Auto-cleaned {removed} old reports")

    # ì¢…ë£Œ ì½”ë“œ
    if summary["critical_drift"] > 0:
        return 2  # Critical drift
    if drifts:
        return 1  # Non-critical drift
    return 0  # No drift


if __name__ == "__main__":
    sys.exit(main())
