#!/usr/bin/env python3
"""
AFO ì™•êµ­ ì‹œìŠ¤í…œ ìµœì í™” ë„êµ¬
ìºì‹œ, ë„¤íŠ¸ì›Œí¬, ë©”ëª¨ë¦¬ ìµœì í™”
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path

import httpx
import psutil
import redis


class SystemOptimizer:
    def __init__(self) -> None:
        self.redis_client = redis.Redis(host="localhost", port=6379, db=0)

    async def optimize_redis_cache(self) -> dict:
        """Redis ìºì‹œ ìµœì í™”"""
        result = {"status": "success", "optimizations": []}

        try:
            # í˜„ì¬ ìºì‹œ ì •ë³´ ìˆ˜ì§‘
            info = self.redis_client.info()
            used_memory = info["used_memory"]
            total_keys = self.redis_client.dbsize()

            # ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ (TTLì´ ì—†ëŠ” í‚¤ë“¤)
            keys_without_ttl = []
            for key in self.redis_client.scan_iter():
                ttl = self.redis_client.ttl(key)
                if ttl == -1:  # TTL ì—†ìŒ
                    keys_without_ttl.append(key)

            # ê¸°ë³¸ TTL ì„¤ì • (7ì¼)
            default_ttl = 7 * 24 * 60 * 60
            for key in keys_without_ttl[:100]:  # ìµœëŒ€ 100ê°œë§Œ ì²˜ë¦¬
                try:
                    self.redis_client.expire(key, default_ttl)
                    result["optimizations"].append(f"Set TTL for key: {key[:50]}...")
                except:
                    pass

            result["optimizations"].append(
                f"Redis memory: {used_memory / 1024 / 1024:.1f}MB, Keys: {total_keys}"
            )
            result["optimizations"].append(f"Set TTL for {len(keys_without_ttl)} keys")

        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)

        return result

    async def optimize_network_connections(self) -> dict:
        """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìµœì í™”"""
        result = {"status": "success", "optimizations": []}

        services = [
            {
                "name": "ollama",
                "url": "http://localhost:11434/api/tags",
                "timeout": 5.0,
            },
            {
                "name": "api_server",
                "url": "http://localhost:8010/health",
                "timeout": 3.0,
            },
            {
                "name": "dashboard",
                "url": "http://localhost:3000/api/health",
                "timeout": 5.0,
            },
        ]

        for service in services:
            try:
                start_time = asyncio.get_event_loop().time()
                async with httpx.AsyncClient(timeout=service["timeout"]) as client:
                    response = await client.get(service["url"])
                    response_time = asyncio.get_event_loop().time() - start_time

                    if response.status_code == 200:
                        result["optimizations"].append(
                            f"{service['name']}: {response_time:.3f}s (OK)"
                        )
                    else:
                        result["optimizations"].append(
                            f"{service['name']}: {response_time:.3f}s (HTTP {response.status_code})"
                        )

            except Exception as e:
                result["optimizations"].append(f"{service['name']}: ERROR - {str(e)[:50]}")

        return result

    def optimize_memory_usage(self) -> dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        result = {"status": "success", "optimizations": []}

        # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ
        memory = psutil.virtual_memory()
        result["optimizations"].append(
            f"System Memory: {memory.percent:.1f}% used ({memory.available / 1024 / 1024 / 1024:.1f}GB free)"
        )

        # í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        processes = []
        for proc in psutil.process_iter(["pid", "name", "memory_percent"]):
            try:
                if proc.info["memory_percent"] > 0.1:  # 0.1% ì´ìƒ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤
                    processes.append(proc.info)
            except:
                continue

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ ì •ë ¬
        processes.sort(key=lambda x: x["memory_percent"], reverse=True)

        for proc in processes[:5]:  # ìƒìœ„ 5ê°œ
            result["optimizations"].append(
                f"Process {proc['name']} ({proc['pid']}): {proc['memory_percent']:.1f}%"
            )

        return result

    async def optimize_file_system(self) -> dict:
        """íŒŒì¼ ì‹œìŠ¤í…œ ìµœì í™”"""
        result = {"status": "success", "optimizations": []}

        # í° ë¡œê·¸ íŒŒì¼ ì •ë¦¬
        log_dir = Path("logs")
        if log_dir.exists():
            log_files = list(log_dir.glob("*.log"))
            for log_file in log_files:
                size_mb = log_file.stat().st_size / 1024 / 1024
                if size_mb > 50:  # 50MB ì´ìƒ ë¡œê·¸ íŒŒì¼
                    result["optimizations"].append(
                        f"Large log file: {log_file.name} ({size_mb:.1f}MB)"
                    )

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_patterns = ["*.tmp", "*.temp", "*.bak", "*~"]
        temp_files = []
        for pattern in temp_patterns:
            temp_files.extend(Path(".").rglob(pattern))

        if temp_files:
            result["optimizations"].append(f"Found {len(temp_files)} temporary files")

        # ìºì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ê¶Œì¥
        cache_dirs = ["__pycache__", ".pytest_cache", "node_modules/.cache"]
        for cache_dir in cache_dirs:
            if Path(cache_dir).exists():
                size = sum(f.stat().st_size for f in Path(cache_dir).rglob("*") if f.is_file())
                size_mb = size / 1024 / 1024
                if size_mb > 100:
                    result["optimizations"].append(f"Large cache: {cache_dir} ({size_mb:.1f}MB)")

        return result

    async def run_full_optimization(self) -> dict:
        """ì „ì²´ ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ AFO ì™•êµ­ ì‹œìŠ¤í…œ ìµœì í™” ì‹œì‘...")

        results = await asyncio.gather(
            self.optimize_redis_cache(),
            self.optimize_network_connections(),
            self.optimize_file_system(),
        )

        memory_result = self.optimize_memory_usage()

        # ê²°ê³¼ ì¢…í•©
        optimization_report = {
            "timestamp": datetime.now().isoformat(),
            "redis_cache": results[0],
            "network": results[1],
            "file_system": results[2],
            "memory": memory_result,
            "summary": {
                "total_optimizations": sum(
                    len(r.get("optimizations", [])) for r in results + [memory_result]
                ),
                "status": "completed",
            },
        }

        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ“Š ìµœì í™” ê²°ê³¼:")
        for category, result in [
            ("Redis ìºì‹œ", results[0]),
            ("ë„¤íŠ¸ì›Œí¬", results[1]),
            ("íŒŒì¼ ì‹œìŠ¤í…œ", results[2]),
            ("ë©”ëª¨ë¦¬", memory_result),
        ]:
            print(f"  ğŸ”§ {category}:")
            for opt in result.get("optimizations", []):
                print(f"    {opt}")
            print()

        # ê²°ê³¼ ì €ì¥
        os.makedirs("reports", exist_ok=True)
        report_file = f"reports/optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(optimization_report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ ìµœì í™” ë³´ê³ ì„œ ì €ì¥: {report_file}")

        return optimization_report


async def main():
    optimizer = SystemOptimizer()
    await optimizer.run_full_optimization()


if __name__ == "__main__":
    asyncio.run(main())
