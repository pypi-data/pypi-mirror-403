#!/usr/bin/env python3
"""
AFO ì™•êµ­ ë¡œê·¸ ì²­í‚¹ ì‹œìŠ¤í…œ (Log Chunker)
ëŒ€ìš©ëŸ‰ ë¡œê·¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ ì²­í¬ë¡œ ë¶„í• 

Context7 ê¸°ë°˜ ë¶„ë¥˜:
- SYNTAX: Python êµ¬ë¬¸ ì˜¤ë¥˜
- IMPORT: ëª¨ë“ˆ/í•¨ìˆ˜ import ëˆ„ë½
- COMPATIBILITY: Python ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ
- TYPE: íƒ€ì… íŒíŒ… ì˜¤ë¥˜
- UNKNOWN: ê¸°íƒ€ ì˜¤ë¥˜
"""

import json
import os
import re
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, List, Tuple


@dataclass
class LogChunk:
    """ë¡œê·¸ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""

    chunk_id: str
    lines: List[str]
    start_line: int
    end_line: int
    error_count: int
    primary_category: str
    file_path: str = ""


@dataclass
class ErrorPattern:
    """ì—ëŸ¬ íŒ¨í„´ ì •ì˜"""

    category: str
    patterns: List[str]
    priority: str  # CRITICAL, HIGH, MEDIUM, LOW
    description: str


class Context7Classifier:
    """Context7 ê¸°ë°˜ ì—ëŸ¬ ë¶„ë¥˜ê¸°"""

    ERROR_PATTERNS = [
        ErrorPattern(
            category="SYNTAX",
            patterns=[
                r"Expected.*indented.*block",
                r"invalid-syntax",
                r"Simple statements must be separated",
                r"Expected.*newline",
                r"Expected.*semicolon",
            ],
            priority="CRITICAL",
            description="Python êµ¬ë¬¸ ë° ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜",
        ),
        ErrorPattern(
            category="IMPORT",
            patterns=[
                r".*is not defined",
                r"ImportError",
                r"Module level import not at top",
                r"reportMissingImports",
                r"reportUndefinedVariable",
            ],
            priority="HIGH",
            description="ëª¨ë“ˆ/í•¨ìˆ˜ import ëˆ„ë½ ë˜ëŠ” ì •ì˜ë˜ì§€ ì•ŠìŒ",
        ),
        ErrorPattern(
            category="COMPATIBILITY",
            patterns=[
                r"Union.*not defined",
                r"Optional.*not defined",
                r"TypeVar.*not defined",
                r"reportInvalidTypeForm",
            ],
            priority="CRITICAL",
            description="Python ë²„ì „ í˜¸í™˜ì„± ë° íƒ€ì… íŒíŒ… ë¬¸ì œ",
        ),
        ErrorPattern(
            category="TYPE",
            patterns=[
                r"reportAttributeAccessIssue",
                r"reportGeneralTypeIssues",
                r"Variable not allowed in type expression",
            ],
            priority="MEDIUM",
            description="íƒ€ì… ì‹œìŠ¤í…œ ê´€ë ¨ ì˜¤ë¥˜",
        ),
        ErrorPattern(
            category="UNKNOWN",
            patterns=[r".*"],
            priority="LOW",
            description="ë¶„ë¥˜ë˜ì§€ ì•Šì€ ê¸°íƒ€ ì˜¤ë¥˜",
        ),
    ]

    @classmethod
    def classify_error(cls, error_line: str) -> str:
        """ì—ëŸ¬ ë¼ì¸ì„ Context7 ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        for pattern in cls.ERROR_PATTERNS:
            for regex_pattern in pattern.patterns:
                if re.search(regex_pattern, error_line, re.IGNORECASE):
                    return pattern.category
        return "UNKNOWN"


class LogChunker:
    """ë¡œê·¸ ì²­í‚¹ ì‹œìŠ¤í…œ"""

    def __init__(self, log_path: str, chunk_size: int = 100) -> None:
        self.log_path = Path(log_path)
        self.chunk_size = chunk_size
        self.chunks: List[LogChunk] = []

    def load_log(self) -> List[str]:
        """ë¡œê·¸ íŒŒì¼ ë¡œë“œ"""
        if not self.log_path.exists():
            raise FileNotFoundError(f"Log file not found: {self.log_path}")

        with open(self.log_path, "r", encoding="utf-8") as f:
            return f.readlines()

    def split_by_error_type(self, lines: List[str]) -> List[LogChunk]:
        """ì—ëŸ¬ íƒ€ì…ë³„ë¡œ ì²­í¬ ë¶„í• """
        chunks = []
        current_chunk_lines = []
        current_category = None
        start_line = 0

        for i, line in enumerate(lines):
            category = Context7Classifier.classify_error(line)

            # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ê°€ ë‚˜ì˜¤ê±°ë‚˜ ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
            if (category != current_category and current_category is not None) or len(
                current_chunk_lines
            ) >= self.chunk_size:
                if current_chunk_lines:
                    chunk = LogChunk(
                        chunk_id=f"chunk_{len(chunks):03d}",
                        lines=current_chunk_lines.copy(),
                        start_line=start_line,
                        end_line=i - 1,
                        error_count=len(current_chunk_lines),
                        primary_category=current_category,
                    )
                    chunks.append(chunk)

                current_chunk_lines = []
                start_line = i
                current_category = category

            if line.strip():  # ë¹ˆ ë¼ì¸ ì œì™¸
                current_chunk_lines.append(line.rstrip())
                if current_category is None:
                    current_category = category

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk_lines:
            chunk = LogChunk(
                chunk_id=f"chunk_{len(chunks):03d}",
                lines=current_chunk_lines,
                start_line=start_line,
                end_line=len(lines) - 1,
                error_count=len(current_chunk_lines),
                primary_category=current_category or "UNKNOWN",
            )
            chunks.append(chunk)

        return chunks

    def split_by_file(self, lines: List[str]) -> List[LogChunk]:
        """íŒŒì¼ë³„ë¡œ ì²­í¬ ë¶„í• """
        file_chunks = {}
        current_file = None

        for i, line in enumerate(lines):
            # íŒŒì¼ ê²½ë¡œ íŒ¨í„´ ì°¾ê¸°
            file_match = re.search(r"./([^:]+):", line)
            if file_match:
                current_file = file_match.group(1)

                if current_file not in file_chunks:
                    file_chunks[current_file] = []

                file_chunks[current_file].append((i, line.rstrip()))

        # íŒŒì¼ë³„ ì²­í¬ ìƒì„±
        chunks = []
        for file_path, file_lines in file_chunks.items():
            if len(file_lines) > self.chunk_size:
                # í° íŒŒì¼ì€ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
                for i in range(0, len(file_lines), self.chunk_size):
                    chunk_lines = file_lines[i : i + self.chunk_size]
                    chunk = LogChunk(
                        chunk_id=f"file_{file_path.replace('/', '_')}_{i // self.chunk_size:02d}",
                        lines=[line for _, line in chunk_lines],
                        start_line=chunk_lines[0][0],
                        end_line=chunk_lines[-1][0],
                        error_count=len(chunk_lines),
                        primary_category=Context7Classifier.classify_error(chunk_lines[0][1]),
                        file_path=file_path,
                    )
                    chunks.append(chunk)
            else:
                # ì‘ì€ íŒŒì¼ì€ í•˜ë‚˜ì˜ ì²­í¬
                chunk = LogChunk(
                    chunk_id=f"file_{file_path.replace('/', '_')}",
                    lines=[line for _, line in file_lines],
                    start_line=file_lines[0][0],
                    end_line=file_lines[-1][0],
                    error_count=len(file_lines),
                    primary_category=Context7Classifier.classify_error(file_lines[0][1]),
                    file_path=file_path,
                )
                chunks.append(chunk)

        return chunks

    def process_log(self, method: str = "error_type") -> List[LogChunk]:
        """ë¡œê·¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        lines = self.load_log()

        if method == "error_type":
            self.chunks = self.split_by_error_type(lines)
        elif method == "file":
            self.chunks = self.split_by_file(lines)
        else:
            raise ValueError(f"Unknown method: {method}")

        return self.chunks

    def save_chunks(self, output_dir: str = "log_chunks") -> str:
        """ì²­í¬ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ì²­í¬ë³„ë¡œ ì €ì¥
        for chunk in self.chunks:
            chunk_file = output_path / f"{chunk.chunk_id}.json"
            with open(chunk_file, "w", encoding="utf-8") as f:
                json.dump(asdict(chunk), f, ensure_ascii=False, indent=2)

        # í†µê³„ íŒŒì¼ ìƒì„±
        stats = self.generate_statistics()
        stats_file = output_path / "statistics.json"
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        return str(output_path)

    def generate_statistics(self) -> Dict:
        """ì²­í¬ í†µê³„ ìƒì„±"""
        total_errors = sum(chunk.error_count for chunk in self.chunks)
        category_stats = {}

        for chunk in self.chunks:
            cat = chunk.primary_category
            if cat not in category_stats:
                category_stats[cat] = {"count": 0, "chunks": 0, "total_errors": 0}
            category_stats[cat]["count"] += 1
            category_stats[cat]["chunks"] += 1
            category_stats[cat]["total_errors"] += chunk.error_count

        return {
            "total_chunks": len(self.chunks),
            "total_errors": total_errors,
            "avg_errors_per_chunk": (total_errors / len(self.chunks) if self.chunks else 0),
            "category_breakdown": category_stats,
            "chunks_by_category": {
                cat: [chunk.chunk_id for chunk in self.chunks if chunk.primary_category == cat]
                for cat in set(chunk.primary_category for chunk in self.chunks)
            },
        }


def main() -> None:
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    import argparse

    parser = argparse.ArgumentParser(description="AFO ì™•êµ­ ë¡œê·¸ ì²­í‚¹ ì‹œìŠ¤í…œ")
    parser.add_argument("log_path", help="ë¶„ì„í•  ë¡œê·¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument(
        "--method",
        choices=["error_type", "file"],
        default="error_type",
        help="ì²­í‚¹ ë°©ë²•",
    )
    parser.add_argument("--chunk-size", type=int, default=100, help="ì²­í¬ í¬ê¸° (ë¼ì¸ ìˆ˜)")
    parser.add_argument("--output-dir", default="log_chunks", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")

    args = parser.parse_args()

    # ë¡œê·¸ ì²­ì»¤ ì´ˆê¸°í™”
    chunker = LogChunker(args.log_path, args.chunk_size)

    print(f"ğŸ” ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì¤‘: {args.log_path}")
    print(f"ğŸ“Š ì²­í‚¹ ë°©ë²•: {args.method}")
    print(f"ğŸ“ ì²­í¬ í¬ê¸°: {args.chunk_size} ë¼ì¸")

    # ë¡œê·¸ ì²˜ë¦¬
    chunks = chunker.process_log(args.method)

    print(f"âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

    # ì²­í¬ ì €ì¥
    output_dir = chunker.save_chunks(args.output_dir)
    print(f"ğŸ’¾ ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_dir}")

    # í†µê³„ ì¶œë ¥
    stats = chunker.generate_statistics()
    print("\nğŸ“ˆ ë¶„ì„ í†µê³„:")
    print(f"   ì´ ì—ëŸ¬ ìˆ˜: {stats['total_errors']}")
    print(f"   í‰ê·  ì—ëŸ¬/ì²­í¬: {stats['avg_errors_per_chunk']:.1f}")
    print("\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for cat, data in stats["category_breakdown"].items():
        print(f"   {cat}: {data['chunks']} ì²­í¬, {data['total_errors']} ì—ëŸ¬")


if __name__ == "__main__":
    main()
