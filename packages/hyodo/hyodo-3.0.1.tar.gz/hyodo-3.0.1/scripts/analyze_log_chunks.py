#!/usr/bin/env python3
"""
log_chunks ë¶„ì„ê¸° - ìºì‹œë¥¼ DNAë¡œ ë³€í™˜
103,739ê°œ ì—ëŸ¬ ë¡œê·¸ë¥¼ íŒ¨í„´ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì„ë² ë”© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
"""

import json
import re
from pathlib import Path
from collections import defaultdict
from datetime import datetime

LOG_CHUNKS_DIR = Path("log_chunks")
OUTPUT_DIR = Path("data/log_embeddings")

# ì—ëŸ¬ íŒ¨í„´ ë¶„ë¥˜
PATTERNS = {
    "module_not_found": r"ModuleNotFoundError|No module named",
    "import_error": r"ImportError|cannot import name",
    "sentry_error": r"Sentry|sentry_sdk",
    "redis_error": r"Redis|redis",
    "postgres_error": r"PostgreSQL|postgres|pg_",
    "playwright_error": r"playwright",
    "port_in_use": r"address already in use|Errno 48",
    "type_error": r"TypeError|type error",
    "attribute_error": r"AttributeError",
    "key_error": r"KeyError",
    "success": r"âœ…|SUCCESS|successfully|ì™„ë£Œ",
    "warning": r"âš ï¸|WARNING|ê²½ê³ ",
    "info": r"INFO|ì •ë³´",
}


def classify_line(line: str) -> str:
    """ë¡œê·¸ ë¼ì¸ì„ íŒ¨í„´ë³„ë¡œ ë¶„ë¥˜"""
    for pattern_name, pattern in PATTERNS.items():
        if re.search(pattern, line, re.IGNORECASE):
            return pattern_name
    return "unknown"


def extract_timestamp(line: str) -> str | None:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
    match = re.search(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", line)
    return match.group(1) if match else None


def analyze_chunks():
    """ëª¨ë“  ì²­í¬ ë¶„ì„"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    stats = {
        "total_lines": 0,
        "total_chunks": 0,
        "patterns": defaultdict(int),
        "timeline": defaultdict(list),
        "modules_failed": defaultdict(int),
        "services_status": defaultdict(lambda: {"success": 0, "error": 0}),
    }

    embeddings = []

    for chunk_file in sorted(LOG_CHUNKS_DIR.glob("chunk_*.json")):
        with open(chunk_file) as f:
            chunk = json.load(f)

        chunk_id = chunk["chunk_id"]
        lines = chunk.get("lines", [])
        stats["total_chunks"] += 1

        for line in lines:
            stats["total_lines"] += 1
            pattern = classify_line(line)
            stats["patterns"][pattern] += 1

            # ì‹¤íŒ¨í•œ ëª¨ë“ˆ ì¶”ì¶œ
            if pattern == "module_not_found":
                match = re.search(r"No module named ['\"]?([^'\"]+)", line)
                if match:
                    stats["modules_failed"][match.group(1)] += 1

            # ì„œë¹„ìŠ¤ ìƒíƒœ ì¶”ì 
            if "âœ…" in line or "successfully" in line.lower():
                service_match = re.search(r"(\w+(?:Service|Engine|Router|Cache))", line)
                if service_match:
                    stats["services_status"][service_match.group(1)]["success"] += 1
            elif "ERROR" in line or "âŒ" in line:
                service_match = re.search(r"(\w+(?:Service|Engine|Router|Cache))", line)
                if service_match:
                    stats["services_status"][service_match.group(1)]["error"] += 1

            # ì„ë² ë”©ìš© ë°ì´í„° ìƒì„±
            timestamp = extract_timestamp(line)
            if pattern in ["module_not_found", "import_error", "port_in_use", "sentry_error"]:
                embeddings.append(
                    {
                        "chunk_id": chunk_id,
                        "timestamp": timestamp,
                        "pattern": pattern,
                        "content": line[:500],  # 500ì ì œí•œ
                        "severity": "error" if pattern != "success" else "info",
                    }
                )

    # í†µê³„ ì €ì¥
    final_stats = {
        "analyzed_at": datetime.now().isoformat(),
        "total_lines": stats["total_lines"],
        "total_chunks": stats["total_chunks"],
        "pattern_distribution": dict(stats["patterns"]),
        "top_failed_modules": dict(
            sorted(stats["modules_failed"].items(), key=lambda x: -x[1])[:20]
        ),
        "services_status": {k: dict(v) for k, v in stats["services_status"].items()},
    }

    with open(OUTPUT_DIR / "log_analysis_stats.json", "w") as f:
        json.dump(final_stats, f, indent=2, ensure_ascii=False)

    # ì„ë² ë”© ë°ì´í„° ì €ì¥ (ì²­í¬ ë‹¨ìœ„)
    chunk_size = 100
    for i in range(0, len(embeddings), chunk_size):
        chunk_embeddings = embeddings[i : i + chunk_size]
        with open(OUTPUT_DIR / f"embeddings_{i // chunk_size:04d}.json", "w") as f:
            json.dump(chunk_embeddings, f, indent=2, ensure_ascii=False)

    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ì´ ë¼ì¸: {stats['total_lines']:,}")
    print(f"   - ì´ ì²­í¬: {stats['total_chunks']:,}")
    print(f"   - ì„ë² ë”© ë°ì´í„°: {len(embeddings):,}ê°œ")
    print(f"\nğŸ“Š íŒ¨í„´ ë¶„í¬:")
    for pattern, count in sorted(stats["patterns"].items(), key=lambda x: -x[1])[:10]:
        print(f"   - {pattern}: {count:,}")

    print(f"\nğŸ”¥ ìì£¼ ì‹¤íŒ¨í•œ ëª¨ë“ˆ (Top 10):")
    for module, count in list(stats["modules_failed"].items())[:10]:
        print(f"   - {module}: {count}")

    return final_stats


if __name__ == "__main__":
    analyze_chunks()
