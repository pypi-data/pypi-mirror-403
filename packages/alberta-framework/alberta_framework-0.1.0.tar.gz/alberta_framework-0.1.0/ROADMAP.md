To build a 10-year framework for the Alberta Plan, your architecture must move away from the "static model" paradigm of modern deep learning and embrace the "massive retreat" into online, continual, and meta-learning.I. The "Alberta Framework" ArchitectureThe core philosophy of this framework is Temporal Uniformity: every component must be able to update and provide an output at every single time step without batching or offline phases.Core Software Components (Python/JAX)Experience Stream (ExperienceStream): A standardized interface that wraps diverse "Big World" signals (e.g., your SSH logs, sensor telemetry, or game state) into a continuous sequence of observations ($x_t$) and rewards ($r_t$).Learning Unit (LearningUnit): The fundamental "brick." Each unit should represent a single prediction or control task. It must be linear-by-default to satisfy Step 1.Meta-Optimizer (MetaOptimizer): A decoupled module that manages step-sizes. This is where LMS (fixed $\alpha$) and IDBD (learned $\beta$) reside.Horde Layer (Horde): A collection of many learning units (Generalized Value Functions or GVFs) that learn in parallel about different sub-signals of the stream.FC-STOMP progression (AbstractionEngine): A specialized module to handle the five-step progression from Feature Construction to Planning.II. The 10-Year Roadmap (Step-by-Step)This roadmap follows the 12-stage "retreat and return" strategy.Phase 1: Foundation (Steps 1–3)Step 1: Representation I (Continual Supervised Learning): Implement the framework with a fixed feature vector and compare LMS vs. IDBD. The goal is to prove the meta-learner can track non-stationary targets better than a human-tuned rate.Step 2: Representation II (Supervised Feature Finding): Add "generate and test" mechanisms to discover new features (e.g., non-linear combinations of SSH signals) online.Step 3: Prediction I (Continual GVF Learning): Move from predicting just reward to predicting "anything" (e.g., "Will the next command be sudo?").Phase 2: Interaction (Steps 4–7)Step 4: Control I (Continual Actor-Critic): Introduce an action-selection policy. The agent now manages the SSH session (e.g., deciding to tarpit).Step 5 & 6: Continuing Control: Transition to Average Reward objectives, which are more natural for long-lived agents than discounted returns.Phase 3: Intelligence (Steps 8–12)Step 8: Planning I: Implement a transition model that can predict future states.Step 10: STOMP Progression: Automate the creation of Subtasks and Options based on learned features.Step 11: OaK (The Proto-AI): Integrate all components into a single, cohesive architecture where every weight has its own meta-learned step-size.



alberta-framework/
├── core/                # JAX-based TD(λ) and IDBD implementations
├── envs/                # Experience streams (SSH, Quake, etc.)
├── steps/               # Sub-packages for each Alberta Plan step
│   ├── step_1_rep/      # Linear supervised + IDBD
│   ├── step_3_gvf/      # Horde of GVFs
│   └── step_11_oak/     # Integrated STOMP/OaK
├── contrib/             # Community-solved modules
└── docs/                # Academic citations and 10-year vision