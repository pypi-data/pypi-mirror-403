import os
import uuid

import polars as pl
from fastapi import APIRouter, BackgroundTasks, HTTPException, Request, Response

from flowfile_worker import CACHE_DIR, PROCESS_MEMORY_USAGE, models, status_dict, status_dict_lock
from flowfile_worker.configs import logger
from flowfile_worker.create import FileType, table_creator_factory_method
from flowfile_worker.create.models import ReceivedTable
from flowfile_worker.external_sources.sql_source.main import read_sql_source
from flowfile_worker.external_sources.sql_source.models import DatabaseReadSettings
from flowfile_worker.spawner import process_manager, start_fuzzy_process, start_generic_process, start_process

router = APIRouter()


def create_and_get_default_cache_dir(flowfile_flow_id: int) -> str:
    default_cache_dir = CACHE_DIR / str(flowfile_flow_id)
    default_cache_dir.mkdir(parents=True, exist_ok=True)
    return str(default_cache_dir)


@router.post("/submit_query/")
async def submit_query(request: Request, background_tasks: BackgroundTasks) -> models.Status:
    """Accept raw binary data with metadata in headers for efficient transfer."""
    try:
        # Read raw bytes directly from request body - no base64 decoding needed
        polars_serializable_object = await request.body()

        # Get metadata from headers
        task_id = request.headers.get("X-Task-Id") or str(uuid.uuid4())
        operation_type = request.headers.get("X-Operation-Type", "store")
        flow_id = int(request.headers.get("X-Flow-Id", "1"))
        node_id = request.headers.get("X-Node-Id", "-1")
        # Try to parse node_id as int, fall back to string
        try:
            node_id = int(node_id)
        except ValueError:
            pass

        logger.info(f"Processing query with operation: {operation_type}")

        default_cache_dir = create_and_get_default_cache_dir(flow_id)
        file_path = os.path.join(default_cache_dir, f"{task_id}.arrow")
        result_type = "polars" if operation_type == "store" else "other"

        status = models.Status(
            background_task_id=task_id, status="Starting", file_ref=file_path, result_type=result_type
        )
        status_dict[task_id] = status

        background_tasks.add_task(
            start_process,
            polars_serializable_object=polars_serializable_object,
            task_id=task_id,
            operation=operation_type,
            file_ref=file_path,
            flowfile_flow_id=flow_id,
            flowfile_node_id=node_id,
            kwargs={},
        )
        logger.info(f"Started background task: {task_id}")
        return status

    except Exception as e:
        logger.error(f"Error processing query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/store_sample/")
async def store_sample(request: Request, background_tasks: BackgroundTasks) -> models.Status:
    """Accept raw binary data with metadata in headers for efficient transfer."""
    try:
        # Read raw bytes directly from request body - no base64 decoding needed
        polars_serializable_object = await request.body()

        # Get metadata from headers
        task_id = request.headers.get("X-Task-Id") or str(uuid.uuid4())
        sample_size = int(request.headers.get("X-Sample-Size", "100"))
        flow_id = int(request.headers.get("X-Flow-Id", "1"))
        node_id = request.headers.get("X-Node-Id", "-1")
        # Try to parse node_id as int, fall back to string
        try:
            node_id = int(node_id)
        except ValueError:
            pass

        logger.info(f"Processing sample storage with size: {sample_size}")

        default_cache_dir = create_and_get_default_cache_dir(flow_id)
        file_path = os.path.join(default_cache_dir, f"{task_id}.arrow")

        status = models.Status(
            background_task_id=task_id, status="Starting", file_ref=file_path, result_type="other"
        )
        status_dict[task_id] = status

        background_tasks.add_task(
            start_process,
            polars_serializable_object=polars_serializable_object,
            task_id=task_id,
            operation="store_sample",
            file_ref=file_path,
            flowfile_flow_id=flow_id,
            flowfile_node_id=node_id,
            kwargs={"sample_size": sample_size},
        )
        logger.info(f"Started sample storage task: {task_id}")

        return status

    except Exception as e:
        logger.error(f"Error storing sample: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/write_data_to_cloud/")
def write_data_to_cloud(
    cloud_storage_script_write: models.CloudStorageScriptWrite, background_tasks: BackgroundTasks
) -> models.Status:
    """
    Write polars dataframe to a file in cloud storage.
    Args:
        cloud_storage_script_write (): Contains dataframe and write options for cloud storage
        background_tasks (): FastAPI background tasks handler

    Returns:
        models.Status: Status object tracking the write operation
    """
    try:
        logger.info("Starting write operation to: cloud storage")
        task_id = str(uuid.uuid4())
        polars_serializable_object = cloud_storage_script_write.polars_serializable_object()
        status = models.Status(background_task_id=task_id, status="Starting", file_ref="", result_type="other")
        status_dict[task_id] = status
        background_tasks.add_task(
            start_process,
            polars_serializable_object=polars_serializable_object,
            task_id=task_id,
            operation="write_to_cloud_storage",
            file_ref="",
            flowfile_flow_id=cloud_storage_script_write.flowfile_flow_id,
            flowfile_node_id=cloud_storage_script_write.flowfile_node_id,
            kwargs=dict(cloud_write_settings=cloud_storage_script_write.get_cloud_storage_write_settings()),
        )
        logger.info(f"Started write task: {task_id} to database")
        return status
    except Exception as e:
        logger.error(f"Error in write operation: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/store_database_write_result/")
def store_in_database(
    database_script_write: models.DatabaseScriptWrite, background_tasks: BackgroundTasks
) -> models.Status:
    """
    Write polars dataframe to a file in specified format.

    Args:
        database_script_write (models.DatabaseScriptWrite): Contains dataframe and write options for database
        background_tasks (BackgroundTasks): FastAPI background tasks handler

    Returns:
        models.Status: Status object tracking the write operation
    """
    logger.info("Starting write operation to: database")
    try:
        task_id = str(uuid.uuid4())
        polars_serializable_object = database_script_write.polars_serializable_object()
        status = models.Status(background_task_id=task_id, status="Starting", file_ref="", result_type="other")
        status_dict[task_id] = status
        background_tasks.add_task(
            start_process,
            polars_serializable_object=polars_serializable_object,
            task_id=task_id,
            operation="write_to_database",
            file_ref="",
            flowfile_flow_id=database_script_write.flowfile_flow_id,
            flowfile_node_id=database_script_write.flowfile_node_id,
            kwargs=dict(database_write_settings=database_script_write.get_database_write_settings()),
        )

        logger.info(f"Started write task: {task_id} to database")

        return status

    except Exception as e:
        logger.error(f"Error in write operation: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/write_results/")
def write_results(polars_script_write: models.PolarsScriptWrite, background_tasks: BackgroundTasks) -> models.Status:
    """
    Write polars dataframe to a file in specified format.

    Args:
        polars_script_write (models.PolarsScriptWrite): Contains dataframe and write options
        background_tasks (BackgroundTasks): FastAPI background tasks handler

    Returns:
        models.Status: Status object tracking the write operation
    """
    logger.info(f"Starting write operation to: {polars_script_write.path}")
    try:
        task_id = str(uuid.uuid4())
        file_path = polars_script_write.path
        polars_serializable_object = polars_script_write.polars_serializable_object()
        result_type = "other"
        status = models.Status(
            background_task_id=task_id, status="Starting", file_ref=file_path, result_type=result_type
        )
        status_dict[task_id] = status
        background_tasks.add_task(
            start_process,
            polars_serializable_object=polars_serializable_object,
            task_id=task_id,
            operation="write_output",
            file_ref=file_path,
            flowfile_flow_id=polars_script_write.flowfile_flow_id,
            flowfile_node_id=polars_script_write.flowfile_node_id,
            kwargs=dict(
                data_type=polars_script_write.data_type,
                path=polars_script_write.path,
                write_mode=polars_script_write.write_mode,
                sheet_name=polars_script_write.sheet_name,
                delimiter=polars_script_write.delimiter,
            ),
        )
        logger.info(f"Started write task: {task_id} with type: {polars_script_write.data_type}")

        return status

    except Exception as e:
        logger.error(f"Error in write operation: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/store_database_read_result")
def store_sql_db_result(
    database_read_settings: DatabaseReadSettings, background_tasks: BackgroundTasks
) -> models.Status:
    """
    Store the result of an sql source operation.

    Args:
        database_read_settings (SQLSourceSettings): Settings for the SQL source operation
        background_tasks (BackgroundTasks): FastAPI background tasks handler

    Returns:
        models.Status: Status object tracking the Sql operation
    """
    logger.info("Processing Sql source operation")

    try:
        task_id = str(uuid.uuid4())
        file_path = os.path.join(
            create_and_get_default_cache_dir(database_read_settings.flowfile_flow_id), f"{task_id}.arrow"
        )
        status = models.Status(background_task_id=task_id, status="Starting", file_ref=file_path, result_type="polars")
        status_dict[task_id] = status
        logger.info(f"Starting reading from database source task: {task_id}")
        background_tasks.add_task(
            start_generic_process,
            func_ref=read_sql_source,
            file_ref=file_path,
            flowfile_flow_id=database_read_settings.flowfile_flow_id,
            flowfile_node_id=database_read_settings.flowfile_node_id,
            task_id=task_id,
            kwargs=dict(database_read_settings=database_read_settings),
        )
        return status

    except Exception as e:
        logger.error(f"Error processing sql source: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/create_table/{file_type}")
def create_table(
    file_type: FileType,
    received_table: ReceivedTable,
    background_tasks: BackgroundTasks,
    flowfile_flow_id: int = 1,
    flowfile_node_id: int | str = -1,
) -> models.Status:
    """
    Create a Polars table from received dictionary data based on specified file type.

    Args:
        file_type (FileType): Type of file/format for table creation
        received_table (Dict): Raw table data as dictionary
        background_tasks (BackgroundTasks): FastAPI background tasks handler
        flowfile_flow_id: Flowfile ID
        flowfile_node_id: Node ID

    Returns:
        models.Status: Status object tracking the table creation
    """
    logger.info(f"Creating table of type: {file_type}")
    try:
        task_id = str(uuid.uuid4())
        file_ref = os.path.join(create_and_get_default_cache_dir(flowfile_flow_id), f"{task_id}.arrow")
        status = models.Status(background_task_id=task_id, status="Starting", file_ref=file_ref, result_type="polars")
        status_dict[task_id] = status
        func_ref = table_creator_factory_method(file_type)
        background_tasks.add_task(
            start_generic_process,
            func_ref=func_ref,
            file_ref=file_ref,
            task_id=task_id,
            kwargs={"received_table": received_table},
            flowfile_flow_id=flowfile_flow_id,
            flowfile_node_id=flowfile_node_id,
        )
        logger.info(f"Started table creation task: {task_id}")

        return status

    except Exception as e:
        logger.error(f"Error creating table: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


def validate_result(task_id: str) -> bool | None:
    """
    Validate the result of a completed task by checking the IPC file.

    Args:
        task_id (str): ID of the task to validate

    Returns:
        bool | None: True if valid, False if error, None if not applicable
    """
    logger.debug(f"Validating result for task: {task_id}")
    status = status_dict.get(task_id)
    if status.status == "Completed" and status.result_type == "polars":
        try:
            pl.scan_ipc(status.file_ref)
            logger.debug(f"Validation successful for task: {task_id}")
            return True
        except Exception as e:
            logger.error(f"Validation failed for task {task_id}: {str(e)}")
            return False
    return True


@router.get("/status/{task_id}", response_model=models.Status)
def get_status(task_id: str) -> models.Status:
    """Get status of a task by ID and validate its result if completed.

    Args:
        task_id: Unique identifier of the task

    Returns:
        models.Status: Current status of the task

    Raises:
        HTTPException: If task not found or invalid result
    """
    logger.debug(f"Getting status for task: {task_id}")
    status = status_dict.get(task_id)
    if status is None:
        logger.warning(f"Task not found: {task_id}")
        raise HTTPException(status_code=404, detail="Task not found")
    result_valid = validate_result(task_id)
    if not result_valid:
        logger.error(f"Invalid result for task: {task_id}")
        raise HTTPException(status_code=404, detail="Task not found")
    return status


@router.get("/fetch_results/{task_id}")
async def fetch_results(task_id: str):
    """Fetch results for a completed task.

    Args:
        task_id: Unique identifier of the task

    Returns:
        dict: Task ID and serialized result data

    Raises:
        HTTPException: If result not found or error occurred
    """
    logger.debug(f"Fetching results for task: {task_id}")
    status = status_dict.get(task_id)
    if not status:
        logger.warning(f"Result not found: {task_id}")
        raise HTTPException(status_code=404, detail="Result not found")
    if status.status == "Processing":
        return Response(status_code=202, content="Result not ready yet")
    if status.status == "Error":
        logger.error(f"Task error: {status.error_message}")
        raise HTTPException(status_code=404, detail=f"An error occurred during processing: {status.error_message}")
    try:
        lf = pl.scan_parquet(status.file_ref)
        # Return raw bytes - Pydantic/FastAPI will handle serialization
        return {"task_id": task_id, "result": lf.serialize()}
    except Exception as e:
        logger.error(f"Error reading results: {str(e)}")
        raise HTTPException(status_code=500, detail="Error reading results")


@router.get("/memory_usage/{task_id}")
async def memory_usage(task_id: str):
    """Get memory usage for a specific task.

    Args:
        task_id: Unique identifier of the task

    Returns:
        dict: Task ID and memory usage data

    Raises:
        HTTPException: If memory usage data not found
    """
    logger.debug(f"Getting memory usage for task: {task_id}")
    memory_usage = PROCESS_MEMORY_USAGE.get(task_id)
    if memory_usage is None:
        logger.warning(f"Memory usage not found: {task_id}")
        raise HTTPException(status_code=404, detail="Memory usage data not found for this task ID")
    return {"task_id": task_id, "memory_usage": memory_usage}


@router.post("/add_fuzzy_join")
async def add_fuzzy_join(polars_script: models.FuzzyJoinInput, background_tasks: BackgroundTasks) -> models.Status:
    """Start a fuzzy join operation between two dataframes.

    Args:
        polars_script: Input containing left and right dataframes and fuzzy mapping config
        background_tasks: FastAPI background tasks handler

    Returns:
        models.Status: Status object for the fuzzy join task

    Raises:
        HTTPException: If error occurs during setup
    """
    logger.info("Starting fuzzy join operation")
    try:
        default_cache_dir = create_and_get_default_cache_dir(polars_script.flowfile_flow_id)
        polars_script.task_id = str(uuid.uuid4()) if polars_script.task_id is None else polars_script.task_id
        polars_script.cache_dir = polars_script.cache_dir if polars_script.cache_dir is not None else default_cache_dir
        left_serializable_object = polars_script.left_df_operation.polars_serializable_object()
        right_serializable_object = polars_script.right_df_operation.polars_serializable_object()

        file_path = os.path.join(polars_script.cache_dir, f"{polars_script.task_id}.arrow")
        status = models.Status(
            background_task_id=polars_script.task_id, status="Starting", file_ref=file_path, result_type="polars"
        )
        status_dict[polars_script.task_id] = status
        background_tasks.add_task(
            start_fuzzy_process,
            left_serializable_object=left_serializable_object,
            right_serializable_object=right_serializable_object,
            file_ref=file_path,
            fuzzy_maps=polars_script.fuzzy_maps,
            task_id=polars_script.task_id,
            flowfile_flow_id=polars_script.flowfile_flow_id,
            flowfile_node_id=polars_script.flowfile_node_id,
        )
        logger.info(f"Started fuzzy join task: {polars_script.task_id}")
        return status
    except Exception as e:
        logger.error(f"Error in fuzzy join: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/clear_task/{task_id}")
def clear_task(task_id: str):
    """
    Clear task data and status by ID.

    Args:
        task_id: Unique identifier of the task to clear
    Returns:
        dict: Success message
    Raises:
        HTTPException: If task not found
    """

    logger.info(f"Clearing task: {task_id}")
    status = status_dict.get(task_id)
    if not status:
        logger.warning(f"Task not found for clearing: {task_id}")
        raise HTTPException(status_code=404, detail="Task not found")
    try:
        if os.path.exists(status.file_ref):
            os.remove(status.file_ref)
            logger.debug(f"Removed file: {status.file_ref}")
    except Exception as e:
        logger.error(f"Error removing file {status.file_ref}: {str(e)}", exc_info=True)
    with status_dict_lock:
        status_dict.pop(task_id, None)
        PROCESS_MEMORY_USAGE.pop(task_id, None)
        logger.info(f"Successfully cleared task: {task_id}")
    return {"message": f"Task {task_id} has been cleared."}


@router.post("/cancel_task/{task_id}")
def cancel_task(task_id: str):
    """Cancel a running task by ID.

    Args:
        task_id: Unique identifier of the task to cancel

    Returns:
        dict: Success message

    Raises:
        HTTPException: If task cannot be cancelled
    """
    logger.info(f"Attempting to cancel task: {task_id}")
    if not process_manager.cancel_process(task_id):
        logger.warning(f"Cannot cancel task: {task_id}")
        raise HTTPException(status_code=404, detail="Task not found or already completed")
    with status_dict_lock:
        if task_id in status_dict:
            status_dict[task_id].status = "Cancelled"
            logger.info(f"Successfully cancelled task: {task_id}")
    return {"message": f"Task {task_id} has been cancelled."}


@router.get("/ids")
async def get_all_ids():
    """Get list of all task IDs in the system.

    Returns:
        list: List of all task IDs currently tracked
    """
    logger.debug("Fetching all task IDs")
    ids = [k for k in status_dict.keys()]
    logger.debug(f"Found {len(ids)} tasks")
    return ids
