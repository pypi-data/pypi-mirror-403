import io
import os
from collections.abc import Callable
from pathlib import Path
from typing import Any, Literal

import polars as pl
from polars._typing import IO, CsvEncoding, PolarsDataType, SchemaDict, Sequence

from flowfile_core.flowfile.flow_data_engine.flow_data_engine import FlowDataEngine
from flowfile_core.flowfile.flow_graph import FlowGraph
from flowfile_core.schemas import cloud_storage_schemas, input_schema, transform_schema
from flowfile_frame.cloud_storage.secret_manager import get_current_user_id
from flowfile_frame.config import logger
from flowfile_frame.expr import col
from flowfile_frame.flow_frame import FlowFrame
from flowfile_frame.utils import create_flow_graph, generate_node_id


def sum(expr):
    """Sum aggregation function."""
    if isinstance(expr, str):
        expr = col(expr)
    return expr.sum()


def mean(expr):
    """Mean aggregation function."""
    if isinstance(expr, str):
        expr = col(expr)
    return expr.mean()


def min(expr):
    """Min aggregation function."""
    if isinstance(expr, str):
        expr = col(expr)
    return expr.min()


def max(expr):
    """Max aggregation function."""
    if isinstance(expr, str):
        expr = col(expr)
    return expr.max()


def count(expr):
    """Count aggregation function."""
    if isinstance(expr, str):
        expr = col(expr)
    return expr.count()


def read_csv(
    source: str | Path | IO[bytes] | bytes | list[str | Path | IO[bytes] | bytes],
    *,
    flow_graph: Any | None = None,  # Using Any for FlowGraph placeholder
    separator: str = ",",
    convert_to_absolute_path: bool = True,
    description: str | None = None,
    has_header: bool = True,
    new_columns: list[str] | None = None,
    comment_prefix: str | None = None,
    quote_char: str | None = '"',
    skip_rows: int = 0,
    skip_lines: int = 0,
    schema: SchemaDict | None = None,
    schema_overrides: SchemaDict | Sequence[PolarsDataType] | None = None,
    null_values: str | list[str] | dict[str, str] | None = None,
    missing_utf8_is_empty_string: bool = False,
    ignore_errors: bool = False,
    try_parse_dates: bool = False,
    infer_schema: bool = True,
    infer_schema_length: int | None = 100,
    n_rows: int | None = None,
    encoding: CsvEncoding = "utf8",
    low_memory: bool = False,
    rechunk: bool = False,
    storage_options: dict[str, Any] | None = None,
    skip_rows_after_header: int = 0,
    row_index_name: str | None = None,
    row_index_offset: int = 0,
    eol_char: str = "\n",
    raise_if_empty: bool = True,
    truncate_ragged_lines: bool = False,
    decimal_comma: bool = False,
    glob: bool = True,
    cache: bool = True,
    with_column_names: Callable[[list[str]], list[str]] | None = None,
    **other_options: Any,
) -> FlowFrame:
    """
    Read a CSV file into a FlowFrame.

    This function uses the native FlowGraph implementation when the parameters
    fall within the supported range, and falls back to using Polars' scan_csv implementation
    for more advanced features.

    Args:
        source: Path(s) to CSV file(s), or a file-like object.
        flow_graph: if you want to add it to an existing graph
        separator: Single byte character to use as separator in the file.
        convert_to_absolute_path: If the path needs to be set to a fixed location
        description: if you want to add a readable name in the frontend (advised)

        # Polars.scan_csv aligned parameters
        has_header: Indicate if the first row of the dataset is a header or not.
        new_columns: Rename columns after selection.
        comment_prefix: String that indicates a comment line if found at beginning of line.
        quote_char: Character used for quoting. None to disable.
        skip_rows: Start reading after this many rows.
        skip_lines: Skip this many lines by newline char only.
        schema: Schema to use when reading the CSV.
        schema_overrides: Schema overrides for specific columns.
        null_values: Values to interpret as null.
        missing_utf8_is_empty_string: Treat missing utf8 values as empty strings.
        ignore_errors: Try to keep reading lines if some parsing errors occur.
        try_parse_dates: Try to automatically parse dates.
        infer_schema: Boolean flag. If False, `infer_schema_length` for Polars is set to 0.
        infer_schema_length: Number of rows to use for schema inference. Polars default is 100.
        n_rows: Stop reading after this many rows.
        encoding: Character encoding to use.
        low_memory: Reduce memory usage at the cost of performance.
        rechunk: Ensure data is in contiguous memory layout after parsing.
        storage_options: Options for fsspec for cloud storage.
        skip_rows_after_header: Skip rows after header.
        row_index_name: Name of the row index column.
        row_index_offset: Start value for the row index.
        eol_char: End of line character.
        raise_if_empty: Raise error if file is empty.
        truncate_ragged_lines: Truncate lines with too many values.
        decimal_comma: Parse floats with decimal comma.
        glob: Use glob pattern for file path (if source is a string).
        cache: Cache the result after reading (Polars default True).
        with_column_names: Apply a function over the column names.
        other_options: Any other options to pass to polars.scan_csv (e.g. retries, file_cache_ttl).

    Returns:
        A FlowFrame with the CSV data.
    """
    node_id = generate_node_id()
    if flow_graph is None:
        flow_graph = create_flow_graph()
    flow_id = flow_graph.flow_id
    current_source_path_for_native = None
    if isinstance(source, (str, os.PathLike)):
        current_source_path_for_native = str(source)
        if "~" in current_source_path_for_native:
            current_source_path_for_native = os.path.expanduser(current_source_path_for_native)
    elif isinstance(source, list) and all(isinstance(s, (str, os.PathLike)) for s in source):
        current_source_path_for_native = str(source[0]) if source else None
        if current_source_path_for_native and "~" in current_source_path_for_native:
            current_source_path_for_native = os.path.expanduser(current_source_path_for_native)
    elif isinstance(source, (io.BytesIO, io.StringIO)):
        logger.warning("Read from bytes io from csv not supported, converting data to raw data")
        return from_dict(pl.read_csv(source), flow_graph=flow_graph, description=description)
    actual_infer_schema_length: int | None
    if not infer_schema:
        actual_infer_schema_length = 0
    else:
        actual_infer_schema_length = infer_schema_length
    can_use_native = (
        current_source_path_for_native is not None
        and comment_prefix is None
        and skip_lines == 0
        and schema is None
        and schema_overrides is None
        and null_values is None
        and not missing_utf8_is_empty_string
        and not try_parse_dates
        and n_rows is None
        and not low_memory
        and not rechunk
        and storage_options is None
        and skip_rows_after_header == 0
        and row_index_name is None
        and row_index_offset == 0
        and eol_char == "\n"
        and not decimal_comma
        and new_columns is None
        and glob is True
    )
    if can_use_native and current_source_path_for_native:
        received_table = input_schema.ReceivedTable(
            file_type="csv",
            path=current_source_path_for_native,
            name=Path(current_source_path_for_native).name,
            table_settings=input_schema.InputCsvTable(
                delimiter=separator,
                has_headers=has_header,
                encoding=encoding,
                starting_from_line=skip_rows,
                quote_char=quote_char if quote_char is not None else '"',
                infer_schema_length=actual_infer_schema_length if actual_infer_schema_length is not None else 10000,
                truncate_ragged_lines=truncate_ragged_lines,
                ignore_errors=ignore_errors,
                row_delimiter=eol_char,
            ),
        )
        if convert_to_absolute_path:
            try:
                received_table.set_absolute_filepath()
                received_table.path = received_table.abs_file_path
            except Exception as e:
                logger.warning(f"Could not determine absolute path for {current_source_path_for_native}: {e}")

        read_node_description = description or f"Read CSV from {Path(current_source_path_for_native).name}"
        read_node = input_schema.NodeRead(
            flow_id=flow_id,
            node_id=node_id,
            received_file=received_table,
            pos_x=100,
            pos_y=100,
            is_setup=True,
            description=read_node_description,
        )
        flow_graph.add_read(read_node)
        flow_graph.get_node(1)

        result_frame = FlowFrame(
            data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
        )
        flow_graph.get_node(1)
        return result_frame
    else:
        polars_source_arg = source
        polars_code = _build_polars_code_args(
            source=polars_source_arg,
            separator=separator,
            has_header=has_header,
            new_columns=new_columns,
            comment_prefix=comment_prefix,
            quote_char=quote_char,
            skip_rows=skip_rows,
            skip_lines=skip_lines,
            schema=schema,
            schema_overrides=schema_overrides,
            null_values=null_values,
            missing_utf8_is_empty_string=missing_utf8_is_empty_string,
            ignore_errors=ignore_errors,
            try_parse_dates=try_parse_dates,
            infer_schema_length=actual_infer_schema_length,
            n_rows=n_rows,
            encoding=encoding,
            low_memory=low_memory,
            rechunk=rechunk,
            storage_options=storage_options,
            skip_rows_after_header=skip_rows_after_header,
            row_index_name=row_index_name,
            row_index_offset=row_index_offset,
            eol_char=eol_char,
            raise_if_empty=raise_if_empty,
            truncate_ragged_lines=truncate_ragged_lines,
            decimal_comma=decimal_comma,
            glob=glob,
            cache=cache,
            with_column_names=with_column_names,
            **other_options,
        )
        polars_code_node_description = description or "Read CSV with Polars scan_csv"
        if isinstance(source, (str, os.PathLike)):
            polars_code_node_description = description or f"Read CSV with Polars scan_csv from {Path(source).name}"
        elif isinstance(source, list) and source and isinstance(source[0], (str, os.PathLike)):
            polars_code_node_description = (
                description or f"Read CSV with Polars scan_csv from {Path(source[0]).name} (and possibly others)"
            )

        # Assuming input_schema.NodePolarsCode, transform_schema.PolarsCodeInput are defined
        polars_code_settings = input_schema.NodePolarsCode(
            flow_id=flow_id,
            node_id=node_id,
            polars_code_input=transform_schema.PolarsCodeInput(polars_code=polars_code),
            is_setup=True,
            description=polars_code_node_description,
        )
        flow_graph.add_polars_code(polars_code_settings)
        return FlowFrame(
            data=flow_graph.get_node(node_id).get_resulting_data().data_frame,
            flow_graph=flow_graph,
            node_id=node_id,
        )


def _build_polars_code_args(
    source: str | Path | IO[bytes] | bytes | list[str | Path | IO[bytes] | bytes],
    separator: str,
    has_header: bool,
    new_columns: list[str] | None,
    comment_prefix: str | None,
    quote_char: str | None,
    skip_rows: int,
    skip_lines: int,
    schema: SchemaDict | None,
    schema_overrides: SchemaDict | Sequence[PolarsDataType] | None,
    null_values: str | list[str] | dict[str, str] | None,
    missing_utf8_is_empty_string: bool,
    ignore_errors: bool,
    try_parse_dates: bool,
    infer_schema_length: int | None,
    n_rows: int | None,
    encoding: CsvEncoding,
    low_memory: bool,
    rechunk: bool,
    storage_options: dict[str, Any] | None,
    skip_rows_after_header: int,
    row_index_name: str | None,
    row_index_offset: int,
    eol_char: str,
    raise_if_empty: bool,
    truncate_ragged_lines: bool,
    decimal_comma: bool,
    glob: bool,
    cache: bool,
    with_column_names: Callable[[list[str]], list[str]] | None,
    **other_options: Any,
) -> str:
    source_repr: str
    if isinstance(source, (str, Path)):
        source_repr = repr(str(source))
    elif isinstance(source, list):
        source_repr = repr([str(p) for p in source])
    elif isinstance(source, bytes):
        source_repr = "source_bytes_obj"
    elif hasattr(source, "read"):
        source_repr = "source_file_like_obj"
    else:
        source_repr = repr(source)

    param_mapping = {
        "has_header": (True, lambda x: str(x)),
        "separator": (",", lambda x: repr(str(x))),
        "comment_prefix": (None, lambda x: repr(str(x)) if x is not None else "None"),
        "quote_char": ('"', lambda x: repr(str(x)) if x is not None else "None"),
        "skip_rows": (0, str),
        "skip_lines": (0, str),
        "schema": (None, lambda x: repr(x) if x is not None else "None"),
        "schema_overrides": (None, lambda x: repr(x) if x is not None else "None"),
        "null_values": (None, lambda x: repr(x) if x is not None else "None"),
        "missing_utf8_is_empty_string": (False, str),
        "ignore_errors": (False, str),
        "cache": (True, str),
        "with_column_names": (None, lambda x: repr(x) if x is not None else "None"),
        "infer_schema_length": (100, lambda x: str(x) if x is not None else "None"),
        "n_rows": (None, lambda x: str(x) if x is not None else "None"),
        "encoding": ("utf8", lambda x: repr(str(x))),
        "low_memory": (False, str),
        "rechunk": (False, str),
        "skip_rows_after_header": (0, str),
        "row_index_name": (None, lambda x: repr(str(x)) if x is not None else "None"),
        "row_index_offset": (0, str),
        "try_parse_dates": (False, str),
        "eol_char": ("\n", lambda x: repr(str(x))),
        "new_columns": (None, lambda x: repr(x) if x is not None else "None"),
        "raise_if_empty": (True, str),
        "truncate_ragged_lines": (False, str),
        "decimal_comma": (False, str),
        "glob": (True, str),
        "storage_options": (None, lambda x: repr(x) if x is not None else "None"),
    }

    all_vars = locals()
    kwargs_list = []

    for param_name_key, (default_value, format_func) in param_mapping.items():
        value = all_vars.get(param_name_key)
        formatted_value = format_func(value)
        kwargs_list.append(f"{param_name_key}={formatted_value}")

    if other_options:
        for k, v in other_options.items():
            kwargs_list.append(f"{k}={repr(v)}")

    kwargs_str = ",\n    ".join(kwargs_list)

    if kwargs_str:
        polars_code = f"output_df = pl.scan_csv(\n    {source_repr},\n    {kwargs_str}\n)"
    else:
        polars_code = f"output_df = pl.scan_csv({source_repr})"

    return polars_code


def read_parquet(
    source, *, flow_graph: FlowGraph = None, description: str = None, convert_to_absolute_path: bool = True, **options
) -> FlowFrame:
    """
    Read a Parquet file into a FlowFrame.

    Args:
        source: Path to Parquet file
        flow_graph: if you want to add it to an existing graph
        description: if you want to add a readable name in the frontend (advised)
        convert_to_absolute_path: If the path needs to be set to a fixed location
        **options: Options for polars.read_parquet

    Returns:
        A FlowFrame with the Parquet data
    """
    if "~" in source:
        file_path = os.path.expanduser(source)
    node_id = generate_node_id()

    if flow_graph is None:
        flow_graph = create_flow_graph()

    flow_id = flow_graph.flow_id

    received_table = input_schema.ReceivedTable(
        file_type="parquet", path=source, name=Path(source).name, table_settings=input_schema.InputParquetTable()
    )
    if convert_to_absolute_path:
        received_table.path = received_table.abs_file_path

    read_node = input_schema.NodeRead(
        flow_id=flow_id,
        node_id=node_id,
        received_file=received_table,
        pos_x=100,
        pos_y=100,
        is_setup=True,
        description=description,
    )

    flow_graph.add_read(read_node)

    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )


def from_dict(data, *, flow_graph: FlowGraph = None, description: str = None) -> FlowFrame:
    """
    Create a FlowFrame from a dictionary or list of dictionaries.

    Args:
        data: Dictionary of lists or list of dictionaries
        flow_graph: if you want to add it to an existing graph
        description: if you want to add a readable name in the frontend (advised)
    Returns:
        A FlowFrame with the data
    """
    # Create new node ID
    node_id = generate_node_id()

    if not flow_graph:
        flow_graph = create_flow_graph()
    flow_id = flow_graph.flow_id

    input_node = input_schema.NodeManualInput(
        flow_id=flow_id,
        node_id=node_id,
        raw_data_format=FlowDataEngine(data).to_raw_data(),
        pos_x=100,
        pos_y=100,
        is_setup=True,
        description=description,
    )

    # Add to graph
    flow_graph.add_manual_input(input_node)

    # Return new frame
    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )


def concat(
    frames: list["FlowFrame"],
    how: str = "vertical",
    rechunk: bool = False,
    parallel: bool = True,
    description: str = None,
) -> "FlowFrame":
    """
    Concatenate multiple FlowFrames into one.

    Parameters
    ----------
    frames : List[FlowFrame]
        List of FlowFrames to concatenate
    how : str, default 'vertical'
        How to combine the FlowFrames (see concat method documentation)
    rechunk : bool, default False
        Whether to ensure contiguous memory in result
    parallel : bool, default True
        Whether to use parallel processing for the operation
    description : str, optional
        Description of this operation

    Returns
    -------
    FlowFrame
        A new FlowFrame with the concatenated data
    """
    if not frames:
        raise ValueError("No frames provided to concat_frames")
    if len(frames) == 1:
        return frames[0]
    # Use first frame's concat method with remaining frames
    first_frame = frames[0]
    remaining_frames = frames[1:]

    return first_frame.concat(remaining_frames, how=how, rechunk=rechunk, parallel=parallel, description=description)


def scan_csv(
    source: str | Path | IO[bytes] | bytes | list[str | Path | IO[bytes] | bytes],
    *,
    flow_graph: Any | None = None,  # Using Any for FlowGraph placeholder
    separator: str = ",",
    convert_to_absolute_path: bool = True,
    description: str | None = None,
    has_header: bool = True,
    new_columns: list[str] | None = None,
    comment_prefix: str | None = None,
    quote_char: str | None = '"',
    skip_rows: int = 0,
    skip_lines: int = 0,
    schema: SchemaDict | None = None,
    schema_overrides: SchemaDict | Sequence[PolarsDataType] | None = None,
    null_values: str | list[str] | dict[str, str] | None = None,
    missing_utf8_is_empty_string: bool = False,
    ignore_errors: bool = False,
    try_parse_dates: bool = False,
    infer_schema: bool = True,
    infer_schema_length: int | None = 100,
    n_rows: int | None = None,
    encoding: CsvEncoding = "utf8",
    low_memory: bool = False,
    rechunk: bool = False,
    storage_options: dict[str, Any] | None = None,
    skip_rows_after_header: int = 0,
    row_index_name: str | None = None,
    row_index_offset: int = 0,
    eol_char: str = "\n",
    raise_if_empty: bool = True,
    truncate_ragged_lines: bool = False,
    decimal_comma: bool = False,
    glob: bool = True,
    cache: bool = True,
    with_column_names: Callable[[list[str]], list[str]] | None = None,
    **other_options: Any,
) -> FlowFrame:
    """
    Scan a CSV file into a FlowFrame. This function is an alias for read_csv.

    This method is the same as read_csv but is provided for compatibility with
    the polars API where scan_csv returns a LazyFrame.

    See read_csv for full documentation.
    """
    return read_csv(
        source=source,
        flow_graph=flow_graph,
        separator=separator,
        convert_to_absolute_path=convert_to_absolute_path,
        description=description,
        has_header=has_header,
        new_columns=new_columns,
        comment_prefix=comment_prefix,
        quote_char=quote_char,
        skip_rows=skip_rows,
        skip_lines=skip_lines,
        schema=schema,
        schema_overrides=schema_overrides,
        null_values=null_values,
        missing_utf8_is_empty_string=missing_utf8_is_empty_string,
        ignore_errors=ignore_errors,
        try_parse_dates=try_parse_dates,
        infer_schema=infer_schema,
        infer_schema_length=infer_schema_length,
        n_rows=n_rows,
        encoding=encoding,
        low_memory=low_memory,
        rechunk=rechunk,
        storage_options=storage_options,
        skip_rows_after_header=skip_rows_after_header,
        row_index_name=row_index_name,
        row_index_offset=row_index_offset,
        eol_char=eol_char,
        raise_if_empty=raise_if_empty,
        truncate_ragged_lines=truncate_ragged_lines,
        decimal_comma=decimal_comma,
        glob=glob,
        cache=cache,
        with_column_names=with_column_names,
        **other_options,
    )


def scan_parquet(
    source, *, flow_graph: FlowGraph = None, description: str = None, convert_to_absolute_path: bool = True, **options
) -> FlowFrame:
    """
    Scan a Parquet file into a FlowFrame. This function is an alias for read_parquet.

    This method is the same as read_parquet but is provided for compatibility with
    the polars API where scan_parquet returns a LazyFrame.

    See read_parquet for full documentation.
    """
    return read_parquet(
        source=source,
        flow_graph=flow_graph,
        description=description,
        convert_to_absolute_path=convert_to_absolute_path,
        **options,
    )


def scan_parquet_from_cloud_storage(
    source: str,
    *,
    flow_graph: FlowGraph | None = None,
    connection_name: str | None = None,
    scan_mode: Literal["single_file", "directory", None] = None,
    description: str | None = None,
) -> FlowFrame:
    node_id = generate_node_id()

    if scan_mode is None:
        if source[-1] in ("*", "/"):
            scan_mode: Literal["single_file", "directory"] = "directory"
        else:
            scan_mode: Literal["single_file", "directory"] = "single_file"

    if flow_graph is None:
        flow_graph = create_flow_graph()

    flow_id = flow_graph.flow_id
    settings = input_schema.NodeCloudStorageReader(
        flow_id=flow_id,
        node_id=node_id,
        cloud_storage_settings=cloud_storage_schemas.CloudStorageReadSettings(
            resource_path=source, scan_mode=scan_mode, connection_name=connection_name, file_format="parquet"
        ),
        user_id=get_current_user_id(),
        description=description,
    )
    flow_graph.add_cloud_storage_reader(settings)
    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )


def scan_csv_from_cloud_storage(
    source: str,
    *,
    flow_graph: FlowGraph | None = None,
    connection_name: str | None = None,
    scan_mode: Literal["single_file", "directory", None] = None,
    delimiter: str = ";",
    has_header: bool | None = True,
    encoding: CsvEncoding | None = "utf8",
) -> FlowFrame:
    node_id = generate_node_id()

    if scan_mode is None:
        if source[-1] in ("*", "/"):
            scan_mode: Literal["single_file", "directory"] = "directory"
        else:
            scan_mode: Literal["single_file", "directory"] = "single_file"

    if flow_graph is None:
        flow_graph = create_flow_graph()
    flow_id = flow_graph.flow_id
    settings = input_schema.NodeCloudStorageReader(
        flow_id=flow_id,
        node_id=node_id,
        cloud_storage_settings=cloud_storage_schemas.CloudStorageReadSettings(
            resource_path=source,
            scan_mode=scan_mode,
            connection_name=connection_name,
            csv_delimiter=delimiter,
            csv_encoding=encoding,
            csv_has_header=has_header,
            file_format="csv",
        ),
        user_id=get_current_user_id(),
    )
    flow_graph.add_cloud_storage_reader(settings)
    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )


def scan_delta(
    source: str, *, flow_graph: FlowGraph | None = None, connection_name: str | None = None, version: int = None
) -> FlowFrame:
    node_id = generate_node_id()
    if flow_graph is None:
        flow_graph = create_flow_graph()
    flow_id = flow_graph.flow_id
    settings = input_schema.NodeCloudStorageReader(
        flow_id=flow_id,
        node_id=node_id,
        cloud_storage_settings=cloud_storage_schemas.CloudStorageReadSettings(
            resource_path=source, connection_name=connection_name, file_format="delta", delta_version=version
        ),
        user_id=get_current_user_id(),
    )
    flow_graph.add_cloud_storage_reader(settings)
    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )


def scan_json_from_cloud_storage(
    source: str,
    *,
    flow_graph: FlowGraph | None = None,
    connection_name: str | None = None,
    scan_mode: Literal["single_file", "directory", None] = None,
) -> FlowFrame:
    node_id = generate_node_id()

    if scan_mode is None:
        if source[-1] in ("*", "/"):
            scan_mode: Literal["single_file", "directory"] = "directory"
        else:
            scan_mode: Literal["single_file", "directory"] = "single_file"

    if flow_graph is None:
        flow_graph = create_flow_graph()
    flow_id = flow_graph.flow_id
    settings = input_schema.NodeCloudStorageReader(
        flow_id=flow_id,
        node_id=node_id,
        cloud_storage_settings=cloud_storage_schemas.CloudStorageReadSettings(
            resource_path=source, scan_mode=scan_mode, connection_name=connection_name, file_format="json"
        ),
        user_id=get_current_user_id(),
    )
    flow_graph.add_cloud_storage_reader(settings)
    return FlowFrame(
        data=flow_graph.get_node(node_id).get_resulting_data().data_frame, flow_graph=flow_graph, node_id=node_id
    )
