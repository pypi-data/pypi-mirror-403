"""Task(s) for the transfer and publishing of L1 data from a production run of a processing pipeline."""

import logging
from abc import ABC
from pathlib import Path
from typing import Iterable

from dkist_processing_common.codecs.quality import quality_data_encoder
from dkist_processing_common.models.message import CatalogFrameMessage
from dkist_processing_common.models.message import CatalogFrameMessageBody
from dkist_processing_common.models.message import CatalogObjectMessage
from dkist_processing_common.models.message import CatalogObjectMessageBody
from dkist_processing_common.models.tags import Tag
from dkist_processing_common.tasks.mixin.globus import GlobusMixin
from dkist_processing_common.tasks.mixin.interservice_bus import InterserviceBusMixin
from dkist_processing_common.tasks.mixin.quality import QualityMixin
from dkist_processing_common.tasks.output_data_base import OutputDataBase
from dkist_processing_common.tasks.output_data_base import TransferDataBase

__all__ = [
    "L1OutputDataBase",
    "TransferL1Data",
    "AssembleQualityData",
    "SubmitDatasetMetadata",
    "PublishCatalogAndQualityMessages",
]


logger = logging.getLogger(__name__)


class L1OutputDataBase(OutputDataBase, ABC):
    """Subclass of OutputDataBase which encapsulates common level 1 output data methods."""

    @property
    def dataset_has_quality_data(self) -> bool:
        """Return True if the dataset has quality data."""
        path_count = self.count(tags=[Tag.output(), Tag.quality_data()])
        return path_count > 0

    def rollback(self):
        """Warn that the metadata-store and the interservice bus retain the effect of this tasks execution.  Rolling back this task may not be achievable without other action."""
        super().rollback()
        logger.warning(
            f"Modifications to the metadata store and the interservice bus were not rolled back."
        )


class TransferL1Data(TransferDataBase, GlobusMixin):
    """Task class for transferring Level 1 processed data to the object store."""

    def transfer_objects(self):
        """Transfer movie and L1 output frames."""
        with self.telemetry_span("Upload movie"):
            # Movie needs to be transferred separately as the movie headers need to go with it
            self.transfer_movie()

        with self.telemetry_span("Upload quality data"):
            self.transfer_quality_data()

        with self.telemetry_span("Upload output frames"):
            self.transfer_output_frames()

    def transfer_output_frames(self):
        """Create a Globus transfer for all output data, as well as any available dataset extras."""
        output_transfer_items = self.build_output_frame_transfer_list()
        dataset_extra_transfer_items = self.build_dataset_extra_transfer_list()
        transfer_items = output_transfer_items + dataset_extra_transfer_items

        logger.info(
            f"Preparing globus transfer {len(transfer_items)} items: "
            f"{len(output_transfer_items)} output frames. "
            f"{len(dataset_extra_transfer_items)} dataset extras. "
            f"recipe_run_id={self.recipe_run_id}. "
            f"transfer_items={transfer_items[:3]}..."
        )

        self.globus_transfer_scratch_to_object_store(
            transfer_items=transfer_items,
            label=f"Transfer science frames for recipe_run_id {self.recipe_run_id}",
        )

    def transfer_movie(self):
        """Transfer the movie to the object store."""
        paths = list(self.read(tags=[Tag.output(), Tag.movie()]))

        count = len(paths)
        if count != 1:
            raise RuntimeError(
                f"Expected exactly one movie to upload, found {count}. "
                f"recipe_run_id={self.recipe_run_id}"
            )
        movie = paths[0]
        logger.info(f"Uploading Movie: recipe_run_id={self.recipe_run_id}, {movie=}")
        movie_object_key = self.format_object_key(movie)
        self.object_store_upload_movie(
            movie=movie,
            bucket=self.destination_bucket,
            object_key=movie_object_key,
            content_type="video/mp4",
        )

    def transfer_quality_data(self):
        """Transfer quality data to the object store."""
        paths = list(self.read(tags=[Tag.output(), Tag.quality_data()]))
        if len(paths) == 0:
            logger.info(
                f"No quality data found to upload for dataset. recipe_run_id={self.recipe_run_id}"
            )
            return

        if count := len(paths) > 1:
            # dataset inventory does not support multiple quality data object keys
            raise RuntimeError(
                f"Found multiple quality data files to upload.  Not supported."
                f"{count=}, recipe_run_id={self.recipe_run_id}"
            )

        with self.telemetry_span(f"Uploading the trial quality data"):
            path = paths[0]
            logger.info(f"Uploading quality data: recipe_run_id={self.recipe_run_id}, {path=}")
            quality_data_object_key = self.format_object_key(path)
            self.object_store_upload_quality_data(
                quality_data=path,
                bucket=self.destination_bucket,
                object_key=quality_data_object_key,
                content_type="application/json",
            )


class AssembleQualityData(L1OutputDataBase, QualityMixin):
    """
    Assemble quality data from the various quality metrics.

    **NOTE:** Please set `~dkist_processing_common.tasks.mixin.quality.QualityMixin.quality_task_types` in any subclass
    to the same value that was used in a subclass of `~dkist_processing_common.tasks.quality_metrics.QualityL0Metrics`.
    """

    @property
    def polcal_label_list(self) -> list[str] | None:
        """Return the list of labels to look for when building polcal metrics.

        If no labels are specified then no polcal metrics will be built.
        """
        return None

    def run(self):
        """Run method for the task."""
        with self.telemetry_span("Assembling quality data"):
            quality_data = self.quality_assemble_data(polcal_label_list=self.polcal_label_list)

        with self.telemetry_span(
            f"Saving quality data with {len(quality_data)} metrics to the file system"
        ):
            self.write(
                quality_data,
                tags=[Tag.output(), Tag.quality_data()],
                encoder=quality_data_encoder,
                relative_path=f"{self.constants.dataset_id}_quality_data.json",
            )


class SubmitDatasetMetadata(L1OutputDataBase):
    """
    Add receipt account to the metadata store.

    Add a Dataset Receipt Account record to Processing Support for use by the Dataset Catalog Locker.
    Adds the number of files to be created during the calibration processing to the Processing Support table
    for use by the Dataset Catalog Locker.
    """

    def run(self) -> None:
        """Run method for this task."""
        with self.telemetry_span("Count Expected Outputs"):
            dataset_id = self.constants.dataset_id
            expected_object_count = self.count(tags=Tag.output())
        logger.info(
            f"Adding Dataset Receipt Account: "
            f"{dataset_id=}, {expected_object_count=}, recipe_run_id={self.recipe_run_id}"
        )
        with self.telemetry_span(
            f"Add Dataset Receipt Account: {dataset_id = }, {expected_object_count = }"
        ):
            self.metadata_store_add_dataset_receipt_account(
                dataset_id=dataset_id, expected_object_count=expected_object_count
            )


class PublishCatalogAndQualityMessages(L1OutputDataBase, InterserviceBusMixin):
    """Task class for publishing Catalog and Quality Messages."""

    def frame_messages(
        self, paths: Iterable[Path], folder_modifier: str | None = None
    ) -> list[CatalogFrameMessage]:
        """
        Create the frame messages.

        Parameters
        ----------
        paths
            The input paths for which to publish frame messages
        folder_modifier
            A subdirectory to use if the files in paths are not in the base directory

        Returns
        -------
        A list of frame messages
        """
        message_bodies = [
            CatalogFrameMessageBody(
                objectName=self.format_object_key(path=p, folder_modifier=folder_modifier),
                conversationId=str(self.recipe_run_id),
                bucket=self.destination_bucket,
            )
            for p in paths
        ]
        messages = [CatalogFrameMessage(body=body) for body in message_bodies]
        return messages

    def object_messages(
        self, paths: Iterable[Path], object_type: str
    ) -> list[CatalogObjectMessage]:
        """
        Create the object messages.

        Parameters
        ----------
        paths
            The input paths for which to publish object messages
        object_type
            The object type

        Returns
        -------
        A list of object messages
        """
        message_bodies = [
            CatalogObjectMessageBody(
                objectType=object_type,
                objectName=self.format_object_key(path=p),
                bucket=self.destination_bucket,
                conversationId=str(self.recipe_run_id),
                groupId=self.constants.dataset_id,
            )
            for p in paths
        ]
        messages = [CatalogObjectMessage(body=body) for body in message_bodies]
        return messages

    def run(self) -> None:
        """Run method for this task."""
        with self.telemetry_span("Gather output data"):
            frames = self.read(
                tags=self.output_frame_tags
            )  # frames is kept as a generator as it is much longer than the other file categories
            extras = list(self.read(tags=self.extra_frame_tags))
            movies = list(self.read(tags=[Tag.output(), Tag.movie()]))
            quality_data = self.read(tags=[Tag.output(), Tag.quality_data()])
        with self.telemetry_span("Create message objects"):
            messages = []
            messages += self.frame_messages(paths=frames)
            frame_message_count = len(messages)
            messages += self.frame_messages(paths=extras, folder_modifier="extra")
            extra_message_count = len(extras)
            messages += self.object_messages(paths=movies, object_type="MOVIE")
            object_message_count = len(movies)
            dataset_has_quality_data = self.dataset_has_quality_data
            if dataset_has_quality_data:
                messages += self.object_messages(paths=quality_data, object_type="QDATA")
        with self.telemetry_span(
            f"Publish messages: {frame_message_count = }, {extra_message_count = }, {object_message_count = }, {dataset_has_quality_data = }"
        ):
            self.interservice_bus_publish(messages=messages)
