import os
from collections import defaultdict
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import multiprocessing
import time
import matplotlib.ticker as ticker
from colorama import Fore
import re
import toml
import platform
from multiprocessing.pool import RUN

from crisscross.core_functions.megastructures import Megastructure
from crisscross.slat_handle_match_evolver.handle_mutation import mutate_handle_arrays
from crisscross.slat_handle_match_evolver import generate_random_slat_handles, generate_layer_split_handles
from crisscross.helper_functions import save_list_dict_to_file, create_dir_if_empty
from eqcorr2d.eqcorr2d_interface import comprehensive_score_analysis


class EvolveManager:
    def __init__(self, megastructure: Megastructure, random_seed=8, generational_survivors=3,
                 mutation_rate=5, mutation_type_probabilities=(0.425, 0.425, 0.15), unique_handle_sequences=32,
                 evolution_generations=200, evolution_population=30, split_sequence_handles=False, sequence_split_factor=2,
                 process_count=None, early_max_valency_stop=None, log_tracking_directory=None, progress_bar_update_iterations=2,
                 mutation_memory_system='off', memory_length=10, repeating_unit_constraints=None,
                 similarity_score_calculation_frequency=10, update_scope='interfaces'):
        """
        Prepares an evolution manager to optimize a handle array for the provided slat array.
        WARNING: Make sure to use the "if __name__ == '__main__':" block to run this class in a script.
        Otherwise, the spawned processes will cause a recursion error.
        :param megastructure: Megastructure object containing slat array and optionally a seed handle array for starting evolution
        :param random_seed: Random seed to use to ensure consistency
        :param generational_survivors: Number of surviving candidate arrays that persist through each generation
        :param mutation_rate: The expected number of mutations per iteration
        :param mutation_type_probabilities: Probability of selecting a specific mutation type for a target handle/antihandle
        (either handle, antihandle or mixed mutations)
        :param unique_handle_sequences: Handle library length
        :param evolution_generations: Number of generations to consider before stopping
        :param evolution_population: Number of handle arrays to mutate in each generation
        :param split_sequence_handles: Set to true to enforce the splitting of handle sequences between subsequent layers
        :param sequence_split_factor: Factor by which to split the handle sequences between layers (default is 2, which means that if handles are split, the first layer will have 1/2 of the handles, etc.)
        :param process_count: Number of threads to use for hamming multiprocessing (if set to default, will use 67% of available cores)
        :param early_max_valency_stop: If this worst match score  is achieved, the evolution will stop early
        :param log_tracking_directory: Set to a directory to export plots and metrics during the optimization process (optional)
        :param progress_bar_update_iterations: Number of iterations before progress bar is updated
        - useful for server output files, but does not seem to work consistently on every system (optional)
        :param mutation_memory_system: The type of memory system to use for the handle mutation process. Options are 'all', 'best', 'special', or 'off'.
        :param memory_length: Memory of previous 'worst' handle combinations to retain when selecting positions to mutate.
        :param repeating_unit_constraints: Dictionary to define handle link constraints on handle mutations (mainly for use with repeating unit designs).  This is largely depracted now in favour of the Megastructure recursive handle linking system.
        :param similarity_score_calculation_frequency: The duplication risk score will be calculated every x generations.
        This helps speed up the evolution process, since it isn't used for deciding on the best generations to retain.
        :param update_scope: Either 'interfaces' (default) to only place handles at layer interfaces, or 'all' to also place handles at positions with existing handles in the seed array.
        """

        # initial parameter setup
        np.random.seed(int(random_seed))
        self.seed = int(random_seed)
        self.metrics = defaultdict(list)
        self.mismatch_histograms = []

        # creates Megastructure objects to help compute arrays required for compensating match calculation histograms
        self.dummy_megastructure = megastructure
        self.slat_array = megastructure.generate_slat_occupancy_grid()
        self.repeating_unit_constraints = {} if repeating_unit_constraints is None else repeating_unit_constraints

        seed_handle_array = megastructure.generate_assembly_handle_grid()

        if np.sum(seed_handle_array) == 0:
            self.handle_array = None
        else:
            self.handle_array = seed_handle_array # this variable holds the current best handle array in the system (updated throughout evolution)

        # Store update_scope and extract additional positions from seed handle array if 'all'
        self.update_scope = update_scope
        self.additional_positions = None
        if update_scope == 'all' and self.handle_array is not None:
            # Extract all non-zero positions from the seed handle array as additional positions
            self.additional_positions = set(zip(*np.where(self.handle_array != 0)))

        self.generational_survivors = int(generational_survivors)
        self.mutation_rate = mutation_rate
        self.max_evolution_generations = int(evolution_generations)
        self.number_unique_handles = int(unique_handle_sequences)
        self.evolution_population = int(evolution_population)
        self.similarity_score_calculation_frequency = int(similarity_score_calculation_frequency)
        self.current_generation = 0

        self.log_tracking_directory = log_tracking_directory
        if self.log_tracking_directory is not None:
            create_dir_if_empty(self.log_tracking_directory)

        self.progress_bar_update_iterations = int(progress_bar_update_iterations)
        self.mutation_memory_system = mutation_memory_system
        self.hall_of_shame_memory = memory_length

        # converters for alternative parameter definitions
        if early_max_valency_stop is None:
            self.early_max_valency_stop = None
        else:
            self.early_max_valency_stop = int(early_max_valency_stop)

        if isinstance(mutation_type_probabilities, str):
            self.mutation_type_probabilities = tuple(map(float, mutation_type_probabilities.split(', ')))
        else:
            self.mutation_type_probabilities = mutation_type_probabilities

        if isinstance(split_sequence_handles, str):
            self.split_sequence_handles = eval(split_sequence_handles.capitalize())
        else:
            self.split_sequence_handles = split_sequence_handles
        self.sequence_split_factor = sequence_split_factor

        if isinstance(process_count, float) or isinstance(process_count, int):
            self.num_processes = int(process_count)
        else:
            # if no exact count specified, use 67 percent of the cores available on the computer as a reasonable load
            self.num_processes = max(1, int(multiprocessing.cpu_count() / 1.5))

        print(Fore.BLUE + f'Handle array evolution core count set to {self.num_processes}.' + Fore.RESET)

        self.next_candidates = self.initialize_evolution()
        self.initial_candidates = self.next_candidates.copy()

        if self.handle_array is None:
            self.dummy_megastructure.assign_assembly_handles(self.next_candidates[0])

        self.mutation_mask = self.dummy_megastructure.generate_assembly_handle_grid(create_mutation_mask=True, category='all_slats')

        self.slat_compensation_match_counts, self.slat_connection_graph = self.dummy_megastructure.get_slat_match_counts()

        self.memory_hallofshame = defaultdict(list)
        self.memory_best_parent_hallofshame = defaultdict(list)
        self.initial_hallofshame = defaultdict(list)

        self.excel_conditional_formatting = {'type': '3_color_scale',
                                             'criteria': '<>',
                                             'min_color': "#63BE7B",  # Green
                                             'mid_color': "#FFEB84",  # Yellow
                                             'max_color': "#F8696B",  # Red
                                             'value': 0}

        # finally, prepares multiprocessing pool to speed up computation
        if platform.system() == "Linux":
            self._mp_ctx = multiprocessing.get_context("spawn") # in an attempt to reduce memory leaks on linux systems
        else:
            self._mp_ctx = multiprocessing

        self.pool = self._mp_ctx.Pool(processes=self.num_processes)

    def initialize_evolution(self):
        """
        Initializes the pool of candidate handle arrays.
        """
        candidate_handle_arrays = []
        slat_array_with_phantoms = self.dummy_megastructure.generate_slat_occupancy_grid(category='all')
        if not self.split_sequence_handles or slat_array_with_phantoms.shape[2] < 3:
            for j in range(self.evolution_population):
                candidate_array = generate_random_slat_handles(slat_array_with_phantoms, self.number_unique_handles, self.repeating_unit_constraints, additional_positions=self.additional_positions)
                self.dummy_megastructure.enforce_phantom_links_on_assembly_handle_array(candidate_array)
                candidate_handle_arrays.append(candidate_array)
        else:
            for j in range(self.evolution_population):
                candidate_array = generate_layer_split_handles(slat_array_with_phantoms,  self.number_unique_handles, self.sequence_split_factor, self.repeating_unit_constraints, additional_positions=self.additional_positions)
                self.dummy_megastructure.enforce_phantom_links_on_assembly_handle_array(candidate_array)
                candidate_handle_arrays.append(candidate_array)
        if self.handle_array is not None:
            candidate_handle_arrays[0] = self.handle_array

            # Fill missing handle positions in the initial array
            # Use a random candidate as reference for valid interface positions
            reference_array = candidate_handle_arrays[1] if len(candidate_handle_arrays) > 1 else candidate_handle_arrays[0]
            missing_positions = (candidate_handle_arrays[0] == 0) & (reference_array != 0)

            if np.any(missing_positions):
                # Generate random values for missing positions
                if not self.split_sequence_handles or slat_array_with_phantoms.shape[2] < 3:
                    # Random mode: any value from 1 to number_unique_handles
                    random_fills = np.random.randint(1, self.number_unique_handles + 1, size=candidate_handle_arrays[0].shape, dtype=np.uint16)
                else:
                    # Split mode: layer-specific ranges
                    random_fills = np.zeros_like(candidate_handle_arrays[0], dtype=np.uint16)
                    handles_per_layer = self.number_unique_handles // self.sequence_split_factor
                    for i in range(random_fills.shape[2]):
                        layer_index = i % self.sequence_split_factor
                        h_start = 1 + layer_index * handles_per_layer
                        h_end = h_start + handles_per_layer
                        random_fills[..., i] = np.random.randint(h_start, h_end, size=(random_fills.shape[0], random_fills.shape[1]), dtype=np.uint16)

                candidate_handle_arrays[0][missing_positions] = random_fills[missing_positions]
                self.dummy_megastructure.enforce_phantom_links_on_assembly_handle_array(candidate_handle_arrays[0])

        return candidate_handle_arrays

    def single_evolution_step(self):
        """
        Performs a single evolution step, evaluating all candidate arrays and preparing new mutations for the next generation.
        :return:
        """

        self.ensure_pool() # recreates the pool if it's dead

        self.current_generation += 1

        request_sim_score = (self.current_generation % self.similarity_score_calculation_frequency == 0) or (self.current_generation == 1)

        mean_parasitic_valency = np.zeros(self.evolution_population)  # initialize the score variable which will be used as the phenotype for the selection.
        max_parasitic_valency = np.zeros(self.evolution_population)
        duplicate_risk_scores = np.zeros(self.evolution_population)

        hallofshame = defaultdict(list)

        # first step: analyze handle array population individual by individual and gather reports of the scores
        # and the bad handles of each
        # multiprocessing will be used to speed up overall computation and parallelize the hamming distance calculations
        # refer to the multirule_oneshot_hamming function for details on input arguments
        multiprocess_start = time.time()

        analysis_inputs = []
        for j in range(self.evolution_population):
            handles, antihandles = self.dummy_megastructure.get_bag_of_slat_handles(use_original_slat_array=True,
                                                                                    use_external_handle_array = self.next_candidates[j],
                                                                                    remove_blank_slats=True)

            analysis_inputs.append((handles, antihandles, self.slat_compensation_match_counts, self.slat_connection_graph, self.dummy_megastructure.connection_angle, True, 10, request_sim_score))

        results = self.pool.starmap(comprehensive_score_analysis, analysis_inputs)
        multiprocess_time = time.time() - multiprocess_start

        # Unpack and store results from multiprocessing
        for index, res in enumerate(results):
            mean_parasitic_valency[index] = res['mean_log_score']
            max_parasitic_valency[index] = res['worst_match_score']
            if request_sim_score:
                duplicate_risk_scores[index] = res['similarity_score']
            worst_handle_combos = [tuple(map(int, re.findall(r'\d+', r[0]))) for r in res['worst_slat_combos']]
            worst_antihandle_combos = [tuple(map(int, re.findall(r'\d+', r[1]))) for r in res['worst_slat_combos']]
            hallofshame['handles'].append(worst_handle_combos)
            hallofshame['antihandles'].append(worst_antihandle_combos)
            if self.current_generation == 1:
                self.initial_hallofshame['handles'].append(worst_handle_combos)
                self.initial_hallofshame['antihandles'].append(worst_antihandle_combos)

        best_idx = int(np.argmin(mean_parasitic_valency))

        # compare and find the individual with the best hamming distance i.e. the largest one. Note: there might be several
        max_physics_score_of_population = np.min(mean_parasitic_valency)

        # Get the indices of the top 'survivors' largest elements
        if self.generational_survivors == self.evolution_population:
            # if evolution pop = generational survivor count, just take all indices
            indices_of_largest_scores = np.arange(self.evolution_population)
        else:
            # give me the best generational survivors, in no particular order
            indices_of_largest_scores = np.argpartition(mean_parasitic_valency, self.generational_survivors)[:self.generational_survivors]

        # Get the largest elements using the sorted indices
        self.metrics['Best Effective Parasitic Valency'].append(max_physics_score_of_population)
        self.metrics['Corresponding Max Parasitic Valency'].append(max_parasitic_valency[best_idx])
        # All other metrics should match the specific handle array that has the best physics score
        if request_sim_score:
            self.metrics['Corresponding Duplicate Risk Score'].append(duplicate_risk_scores[best_idx])
        else:
            self.metrics['Corresponding Duplicate Risk Score'].append(np.nan) # not computed

        self.metrics['Compute Time'].append(multiprocess_time)
        self.metrics['Generation'].append(self.current_generation)

        similarity_scores = []
        for candidate in self.next_candidates:
            for initial_candidate in self.initial_candidates:
                similarity_scores.append(np.sum(candidate == initial_candidate))

        self.metrics['Similarity of Array to Initial Population'].append(sum(similarity_scores) / len(similarity_scores))

        # Select and store the current best handle array

        self.handle_array = self.next_candidates[best_idx]

        # Extracts precomputed match histogram from multirule_oneshot_hamming for the best candidate
        self.mismatch_histograms.append(results[best_idx]['match_histogram'])

        # apply mutations and select the next generation of candidates
        candidate_handle_arrays, _ = mutate_handle_arrays(self.slat_array, self.next_candidates,
                                                          hallofshame=hallofshame,
                                                          memory_hallofshame=self.memory_hallofshame,
                                                          memory_best_parent_hallofshame=self.memory_best_parent_hallofshame,
                                                          best_score_indices=indices_of_largest_scores,
                                                          unique_sequences=self.number_unique_handles,
                                                          mutation_rate=self.mutation_rate,
                                                          use_memory_type=self.mutation_memory_system,
                                                          special_hallofshame=self.initial_hallofshame,
                                                          mutation_type_probabilities=self.mutation_type_probabilities,
                                                          split_sequence_handles=self.split_sequence_handles,
                                                          sequence_split_factor=self.sequence_split_factor,
                                                          repeating_unit_constraints=self.repeating_unit_constraints,
                                                          mutation_mask=self.mutation_mask)

        candidate_handle_arrays = [self.dummy_megastructure.enforce_phantom_links_on_assembly_handle_array(arr) for arr in candidate_handle_arrays]

        for key, payload in hallofshame.items():
            self.memory_hallofshame[key].extend(payload)
            self.memory_best_parent_hallofshame[key].extend([payload[i] for i in indices_of_largest_scores])

        if len(self.memory_hallofshame['handles']) > self.hall_of_shame_memory * self.evolution_population:
            for key in self.memory_hallofshame.keys():
                self.memory_hallofshame[key] = self.memory_hallofshame[key][-self.hall_of_shame_memory * self.evolution_population:]
                self.memory_best_parent_hallofshame[key] = self.memory_best_parent_hallofshame[key][-self.hall_of_shame_memory * self.generational_survivors:]

        self.next_candidates = candidate_handle_arrays


    def export_results(self, main_folder_path=None, generate_unique_folder_name=True, parameter_export=False, suppress_handle_array_export=False):

        if main_folder_path:
            if generate_unique_folder_name:
                output_folder = os.path.join(main_folder_path, f"evolution_results_{time.strftime('%Y%m%d_%H%M%S')}")
            else:
                output_folder = os.path.join(main_folder_path, 'evolution_results')
        else:
            output_folder = self.log_tracking_directory

        create_dir_if_empty(output_folder)

        fig, ax = plt.subplots(5, 1, figsize=(10, 10))

        for ind, (name, data) in enumerate(zip(['Candidate with Best Effective Parasitic Valency',
                                                'Corresponding Max Parasitic Valency',
                                                'Corresponding Duplication Risk Score',
                                                'Similarity of Population to Initial Candidates',
                                                'Compute Time (s)'],
                                               [self.metrics['Best Effective Parasitic Valency'],
                                                self.metrics['Corresponding Max Parasitic Valency'],
                                                self.metrics['Corresponding Duplicate Risk Score'],
                                                self.metrics['Similarity of Array to Initial Population'],
                                                self.metrics['Compute Time']])):

            if len(data) > 5000:
                # turn off markers if too many data points
                ax[ind].plot(range(1, len(data)+1), data, linestyle='--')
            else:
                ax[ind].plot(range(1, len(data)+1), data, linestyle='--', marker='o')

            ax[ind].set_xlabel('Generation')
            ax[ind].set_ylabel('Measurement')
            ax[ind].set_title(name)
            ax[ind].xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

        plt.tight_layout()
        plt.savefig(os.path.join(output_folder, 'metrics_visualization.pdf'))
        plt.close(fig)

        # save the mismatch histograms to csv file
        df = pd.DataFrame(self.mismatch_histograms)
        # rewrite df headers  by appending ' Matches' to each column name
        df.columns = [f'{col} Matches' for col in df.columns]
        df.to_csv(os.path.join(output_folder, 'match_histograms.csv'), index=False, header=True)

        # produces a graph tracking each individual mismatch score throughout time
        fig, ax = plt.subplots(1, 1, figsize=(10, 5))
        for col in df.columns:
            if len(df[col].dropna()) > 5000:
                ax.plot(range(1, len(df[col].dropna()) + 1), df[col].dropna(), linestyle='--', label=col)
            else:
                ax.plot(range(1, len(df[col].dropna()) + 1), df[col].dropna(), linestyle='--', marker='o', label=col)

        ax.set_xlabel('Generation')
        ax.set_ylabel('Number of combinations')
        ax.set_title('Handle Match Tracking')
        ax.set_yscale('log')
        ax.set_xscale('log')
        ax.xaxis.set_major_locator(ticker.LogLocator(base=10.0, numticks=10))
        ax.xaxis.set_minor_locator(ticker.LogLocator(base=10.0, subs='auto', numticks=10))
        ax.xaxis.set_major_formatter(ticker.LogFormatter())

        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(output_folder, 'mismatch_tracking_visualization.pdf'), bbox_inches='tight')
        plt.close(fig)

        # save all other generic metrics to csv file
        save_list_dict_to_file(output_folder, 'metrics.csv', self.metrics, append=False, index_column='Generation')


        if not suppress_handle_array_export:
            # prepare a checkpoint of the current best handle array
            writer = pd.ExcelWriter(
                os.path.join(output_folder, f'best_handle_array_generation_{self.current_generation}.xlsx'),
                engine='xlsxwriter')

            # prints out handle dataframes in standard format
            for layer_index in range(self.handle_array.shape[-1]):
                df = pd.DataFrame(self.handle_array[..., layer_index])
                df.to_excel(writer, sheet_name=f'handle_interface_{layer_index + 1}', index=False, header=False)
                # Apply conditional formatting for easy color-based identification
                writer.sheets[f'handle_interface_{layer_index + 1}'].conditional_format(0, 0, df.shape[0],
                                                                                        df.shape[1] - 1,
                                                                                        self.excel_conditional_formatting)
            writer.close()

        if parameter_export:
            full_parameter_set = {
                'random_seed': self.seed,
                'generational_survivors': self.generational_survivors,
                'mutation_rate': self.mutation_rate,
                'mutation_type_probabilities': self.mutation_type_probabilities,
                'unique_handle_sequences': self.number_unique_handles,
                'evolution_generations': self.max_evolution_generations,
                'evolution_population': self.evolution_population,
                'split_sequence_handles': self.split_sequence_handles,
                'sequence_split_factor': self.sequence_split_factor,
                'process_count': self.num_processes,
                'early_max_valency_stop': self.early_max_valency_stop,
                'progress_bar_update_iterations': self.progress_bar_update_iterations,
                'mutation_memory_system': self.mutation_memory_system,
                'memory_length': self.hall_of_shame_memory,
            }

            toml.dump(full_parameter_set, open(os.path.join(output_folder, 'evolution_config.toml'), 'w'))

    def run_full_experiment(self, logging_interval=10, suppress_handle_array_export=False):
        """
        Runs a full evolution experiment.
        :param logging_interval: The frequency at which logs should be written to file (including the best hamming array file).
        :param suppress_handle_array_export: If true, the best handle array will not be exported at each logging interval, saving space.
        """

        if self.log_tracking_directory is None:
            raise ValueError('Log tracking directory must be specified to run an automatic full experiment.')

        try:
            with tqdm(total=self.max_evolution_generations - self.current_generation, desc='Evolution Progress', miniters=self.progress_bar_update_iterations) as pbar:
                for index, generation in enumerate(range(self.current_generation, self.max_evolution_generations)):
                    self.single_evolution_step()
                    if (index+1) % logging_interval == 0:
                        self.export_results(suppress_handle_array_export=suppress_handle_array_export)

                    pbar.update(1)
                    pbar.set_postfix({f'Latest max parasitic valency': self.metrics['Corresponding Max Parasitic Valency'][-1],
                                      'Time for calculations': self.metrics['Compute Time'][-1],
                                      'Latest effective parasitic valency': self.metrics['Best Effective Parasitic Valency'][-1]}, refresh=False)

                    if self.early_max_valency_stop and min(self.metrics['Corresponding Max Parasitic Valency']) <= self.early_max_valency_stop:
                        print('Early stopping criteria met, ending evolution process.')
                        break
            self.close_pool()

        except KeyboardInterrupt:
            # Optional: more friendly message/logging
            pass
        finally:
            # Ensure workers are cleaned up even on exceptions
            self.terminate_pool()

    def is_pool_alive(self) -> bool:
        try:
            return (self.pool is not None) and (getattr(self.pool, "_state", None) == RUN)
        except Exception:
            return False

    def ensure_pool(self):
        """Create a new Pool if none exists or it is not RUN.
        Safe to call at the beginning of each generation or on resume.
        """
        if not self.is_pool_alive():
            # Recreate with the same spawn-aware context decided in __init__
            self.pool = self._mp_ctx.Pool(processes=self.num_processes)

    def close_pool(self):
        if getattr(self, "pool", None) is not None:
            try:
                self.pool.close()
                self.pool.join()
            finally:
                self.pool = None

    def terminate_pool(self):
        if getattr(self, "pool", None) is not None:
            try:
                self.pool.terminate()
                self.pool.join()
            finally:
                self.pool = None

    def __del__(self):
        # Best-effort cleanup if object is GCâ€™d
        try:
            self.terminate_pool()
        except Exception:
            pass
