{
  "version": "0.1.1",
  "updated": "2024-12-30",
  "sources": {
    "ollama": {
      "name": "Ollama",
      "icon": "ðŸ¦™",
      "color": "#4a9eff",
      "url": "https://ollama.ai/library",
      "models": [
        {"name": "Llama 3.2 1B", "shortcut": "llama3.2:1b", "size_gb": 0.8, "params": "1B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Compact and fast", "tags": ["lightweight", "fast", "chat"]},
        {"name": "Llama 3.2 3B", "shortcut": "llama3.2:3b", "size_gb": 2.0, "params": "3B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Balanced performance", "tags": ["balanced", "chat", "code"]},
        {"name": "Llama 3.1 8B", "shortcut": "llama3.1:8b", "size_gb": 5.0, "params": "8B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Meta's flagship model", "tags": ["powerful", "chat", "reasoning"]},
        {"name": "Llama 3.1 70B", "shortcut": "llama3.1:70b", "size_gb": 40.0, "params": "70B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Largest Llama model", "tags": ["large", "powerful", "reasoning"]},
        {"name": "Qwen 2.5 0.5B", "shortcut": "qwen2.5:0.5b", "size_gb": 0.4, "params": "0.5B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Ultra-lightweight", "tags": ["tiny", "fast", "efficient"]},
        {"name": "Qwen 2.5 1.5B", "shortcut": "qwen2.5:1.5b", "size_gb": 1.0, "params": "1.5B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Compact Qwen", "tags": ["lightweight", "fast", "multilingual"]},
        {"name": "Qwen 2.5 3B", "shortcut": "qwen2.5:3b", "size_gb": 2.0, "params": "3B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Balanced Qwen", "tags": ["balanced", "multilingual", "code"]},
        {"name": "Qwen 2.5 7B", "shortcut": "qwen2.5:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Excellent reasoning", "tags": ["powerful", "reasoning", "multilingual"]},
        {"name": "Qwen 2.5 14B", "shortcut": "qwen2.5:14b", "size_gb": 9.0, "params": "14B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Large Qwen model", "tags": ["large", "reasoning", "multilingual"]},
        {"name": "Qwen 2.5 32B", "shortcut": "qwen2.5:32b", "size_gb": 20.0, "params": "32B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Very large Qwen", "tags": ["very-large", "reasoning", "multilingual"]},
        {"name": "Qwen 2.5 72B", "shortcut": "qwen2.5:72b", "size_gb": 45.0, "params": "72B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Largest Qwen model", "tags": ["largest", "powerful", "multilingual"]},
        {"name": "Qwen 2.5 Coder 1.5B", "shortcut": "qwen2.5-coder:1.5b", "size_gb": 1.0, "params": "1.5B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Small coding model", "tags": ["code", "fast", "programming"]},
        {"name": "Qwen 2.5 Coder 7B", "shortcut": "qwen2.5-coder:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Powerful coding model", "tags": ["code", "programming", "powerful"]},
        {"name": "Qwen 2.5 Coder 32B", "shortcut": "qwen2.5-coder:32b", "size_gb": 20.0, "params": "32B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Best coding model", "tags": ["code", "programming", "large"]},
        {"name": "Mistral 7B", "shortcut": "mistral:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Fast and capable", "tags": ["fast", "efficient", "chat"]},
        {"name": "Mistral Small 22B", "shortcut": "mistral-small:22b", "size_gb": 14.0, "params": "22B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Enterprise-grade", "tags": ["enterprise", "powerful", "reasoning"]},
        {"name": "Mistral Large 123B", "shortcut": "mistral-large:123b", "size_gb": 75.0, "params": "123B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Flagship Mistral", "tags": ["flagship", "very-large", "reasoning"]},
        {"name": "Mixtral 8x7B", "shortcut": "mixtral:8x7b", "size_gb": 26.0, "params": "8x7B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "MoE architecture", "tags": ["moe", "efficient", "powerful"]},
        {"name": "Mixtral 8x22B", "shortcut": "mixtral:8x22b", "size_gb": 80.0, "params": "8x22B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Large MoE model", "tags": ["moe", "very-large", "powerful"]},
        {"name": "Phi-3 Mini", "shortcut": "phi3:mini", "size_gb": 2.5, "params": "3.8B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Compact powerhouse", "tags": ["compact", "efficient", "reasoning"]},
        {"name": "Phi-3 Medium", "shortcut": "phi3:medium", "size_gb": 8.0, "params": "14B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Medium Phi model", "tags": ["balanced", "reasoning", "math"]},
        {"name": "Phi-3.5 Mini", "shortcut": "phi3.5:mini", "size_gb": 2.5, "params": "3.8B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Latest Phi mini", "tags": ["compact", "fast", "reasoning"]},
        {"name": "Gemma 2 2B", "shortcut": "gemma2:2b", "size_gb": 1.5, "params": "2B", "quantization": "Q4_K_M", "distributor": "Google", "description": "Lightweight Gemma", "tags": ["lightweight", "fast", "efficient"]},
        {"name": "Gemma 2 9B", "shortcut": "gemma2:9b", "size_gb": 5.5, "params": "9B", "quantization": "Q4_K_M", "distributor": "Google", "description": "Balanced Gemma", "tags": ["balanced", "reasoning", "chat"]},
        {"name": "Gemma 2 27B", "shortcut": "gemma2:27b", "size_gb": 16.0, "params": "27B", "quantization": "Q4_K_M", "distributor": "Google", "description": "Large Gemma model", "tags": ["large", "powerful", "reasoning"]},
        {"name": "DeepSeek R1 1.5B", "shortcut": "deepseek-r1:1.5b", "size_gb": 1.0, "params": "1.5B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Compact reasoning", "tags": ["reasoning", "compact", "efficient"]},
        {"name": "DeepSeek R1 7B", "shortcut": "deepseek-r1:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Strong reasoning", "tags": ["reasoning", "powerful", "math"]},
        {"name": "DeepSeek R1 14B", "shortcut": "deepseek-r1:14b", "size_gb": 9.0, "params": "14B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Large reasoning model", "tags": ["reasoning", "large", "math"]},
        {"name": "DeepSeek R1 32B", "shortcut": "deepseek-r1:32b", "size_gb": 20.0, "params": "32B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Very large reasoning", "tags": ["reasoning", "very-large", "math"]},
        {"name": "DeepSeek R1 70B", "shortcut": "deepseek-r1:70b", "size_gb": 45.0, "params": "70B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Largest DeepSeek R1", "tags": ["reasoning", "largest", "math"]},
        {"name": "DeepSeek Coder V2", "shortcut": "deepseek-coder-v2:16b", "size_gb": 10.0, "params": "16B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Advanced coder", "tags": ["code", "programming", "powerful"]},
        {"name": "CodeLlama 7B", "shortcut": "codellama:7b", "size_gb": 4.0, "params": "7B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Code-focused Llama", "tags": ["code", "programming", "balanced"]},
        {"name": "CodeLlama 13B", "shortcut": "codellama:13b", "size_gb": 7.5, "params": "13B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Larger code model", "tags": ["code", "programming", "powerful"]},
        {"name": "CodeLlama 34B", "shortcut": "codellama:34b", "size_gb": 20.0, "params": "34B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Best CodeLlama", "tags": ["code", "programming", "large"]},
        {"name": "StarCoder 2 3B", "shortcut": "starcoder2:3b", "size_gb": 2.0, "params": "3B", "quantization": "Q4_K_M", "distributor": "BigCode", "description": "Compact coder", "tags": ["code", "fast", "programming"]},
        {"name": "StarCoder 2 7B", "shortcut": "starcoder2:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "BigCode", "description": "Balanced coder", "tags": ["code", "balanced", "programming"]},
        {"name": "StarCoder 2 15B", "shortcut": "starcoder2:15b", "size_gb": 9.0, "params": "15B", "quantization": "Q4_K_M", "distributor": "BigCode", "description": "Large coder", "tags": ["code", "powerful", "programming"]},
        {"name": "Granite 3 2B", "shortcut": "granite3-dense:2b", "size_gb": 1.5, "params": "2B", "quantization": "Q4_K_M", "distributor": "IBM", "description": "Compact enterprise", "tags": ["enterprise", "compact", "efficient"]},
        {"name": "Granite 3 8B", "shortcut": "granite3-dense:8b", "size_gb": 5.0, "params": "8B", "quantization": "Q4_K_M", "distributor": "IBM", "description": "Enterprise model", "tags": ["enterprise", "balanced", "reasoning"]},
        {"name": "Granite Code 3B", "shortcut": "granite-code:3b", "size_gb": 2.0, "params": "3B", "quantization": "Q4_K_M", "distributor": "IBM", "description": "Enterprise coder", "tags": ["code", "enterprise", "programming"]},
        {"name": "Granite Code 8B", "shortcut": "granite-code:8b", "size_gb": 5.0, "params": "8B", "quantization": "Q4_K_M", "distributor": "IBM", "description": "Large enterprise coder", "tags": ["code", "enterprise", "powerful"]},
        {"name": "Orca 2 7B", "shortcut": "orca2:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Reasoning focused", "tags": ["reasoning", "balanced", "efficient"]},
        {"name": "Orca 2 13B", "shortcut": "orca2:13b", "size_gb": 7.5, "params": "13B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Large reasoning", "tags": ["reasoning", "large", "powerful"]},
        {"name": "Neural Chat 7B", "shortcut": "neural-chat:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Intel", "description": "Optimized for Intel", "tags": ["chat", "intel", "optimized"]},
        {"name": "Vicuna 7B", "shortcut": "vicuna:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "LMSYS", "description": "Fine-tuned chatbot", "tags": ["chat", "fine-tuned", "balanced"]},
        {"name": "Vicuna 13B", "shortcut": "vicuna:13b", "size_gb": 7.5, "params": "13B", "quantization": "Q4_K_M", "distributor": "LMSYS", "description": "Larger chatbot", "tags": ["chat", "fine-tuned", "powerful"]},
        {"name": "OpenHermes 2.5 7B", "shortcut": "openhermes:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Teknium", "description": "General assistant", "tags": ["chat", "general", "balanced"]},
        {"name": "Dolphin Mixtral 8x7B", "shortcut": "dolphin-mixtral:8x7b", "size_gb": 26.0, "params": "8x7B", "quantization": "Q4_K_M", "distributor": "Cognitive Computations", "description": "Uncensored MoE", "tags": ["uncensored", "moe", "powerful"]},
        {"name": "Wizard Coder 15B", "shortcut": "wizard-coder:15b", "size_gb": 9.0, "params": "15B", "quantization": "Q4_K_M", "distributor": "WizardLM", "description": "Wizard coding model", "tags": ["code", "wizard", "powerful"]},
        {"name": "MathStral 7B", "shortcut": "mathstral:7b", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Math specialized", "tags": ["math", "reasoning", "specialized"]},
        {"name": "Nomic Embed Text", "shortcut": "nomic-embed-text", "size_gb": 0.3, "params": "137M", "quantization": "FP16", "distributor": "Nomic AI", "description": "Text embeddings", "tags": ["embeddings", "text", "rag"]},
        {"name": "MxBAI Embed Large", "shortcut": "mxbai-embed-large", "size_gb": 0.7, "params": "334M", "quantization": "FP16", "distributor": "MixedBread", "description": "Large embeddings", "tags": ["embeddings", "large", "rag"]}
      ]
    },
    "huggingface": {
      "name": "HuggingFace",
      "icon": "ðŸ¤—",
      "color": "#ffcc00",
      "url": "https://huggingface.co/models",
      "models": [
        {"name": "Llama 3.2 1B Instruct", "shortcut": "bartowski/Llama-3.2-1B-Instruct-GGUF", "size_gb": 0.8, "params": "1B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Compact instruction model", "tags": ["lightweight", "instruct", "chat"]},
        {"name": "Llama 3.2 3B Instruct", "shortcut": "bartowski/Llama-3.2-3B-Instruct-GGUF", "size_gb": 2.0, "params": "3B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Balanced instruction model", "tags": ["balanced", "instruct", "chat"]},
        {"name": "Llama 3.1 8B Instruct", "shortcut": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF", "size_gb": 5.0, "params": "8B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Flagship instruction model", "tags": ["powerful", "instruct", "chat"]},
        {"name": "Qwen 2.5 7B Instruct", "shortcut": "Qwen/Qwen2.5-7B-Instruct-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Alibaba", "description": "Multilingual GGUF", "tags": ["multilingual", "instruct", "reasoning"]},
        {"name": "Mistral 7B Instruct v0.3", "shortcut": "bartowski/Mistral-7B-Instruct-v0.3-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "Mistral AI", "description": "Latest Mistral instruct", "tags": ["instruct", "fast", "efficient"]},
        {"name": "Phi-3 Mini 4K Instruct", "shortcut": "microsoft/Phi-3-mini-4k-instruct-gguf", "size_gb": 2.5, "params": "3.8B", "quantization": "Q4", "distributor": "Microsoft", "description": "Compact reasoning", "tags": ["compact", "reasoning", "math"]},
        {"name": "Phi-3.5 Mini Instruct", "shortcut": "bartowski/Phi-3.5-mini-instruct-GGUF", "size_gb": 2.5, "params": "3.8B", "quantization": "Q4_K_M", "distributor": "Microsoft", "description": "Latest Phi mini", "tags": ["compact", "fast", "reasoning"]},
        {"name": "Gemma 2 9B IT", "shortcut": "bartowski/gemma-2-9b-it-GGUF", "size_gb": 5.5, "params": "9B", "quantization": "Q4_K_M", "distributor": "Google", "description": "Instruction tuned Gemma", "tags": ["instruct", "balanced", "chat"]},
        {"name": "DeepSeek R1 Distill Qwen 7B", "shortcut": "bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Distilled reasoning", "tags": ["reasoning", "distilled", "efficient"]},
        {"name": "DeepSeek Coder V2 Lite", "shortcut": "bartowski/DeepSeek-Coder-V2-Lite-Instruct-GGUF", "size_gb": 10.0, "params": "16B", "quantization": "Q4_K_M", "distributor": "DeepSeek", "description": "Advanced coding", "tags": ["code", "programming", "powerful"]},
        {"name": "CodeLlama 7B Instruct", "shortcut": "TheBloke/CodeLlama-7B-Instruct-GGUF", "size_gb": 4.0, "params": "7B", "quantization": "Q4_K_M", "distributor": "Meta", "description": "Code instruction", "tags": ["code", "instruct", "programming"]},
        {"name": "StarCoder2 7B", "shortcut": "QuantFactory/starcoder2-7b-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "BigCode", "description": "Open source coder", "tags": ["code", "open-source", "programming"]},
        {"name": "Zephyr 7B Beta", "shortcut": "TheBloke/zephyr-7b-beta-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "HuggingFace", "description": "Fine-tuned Mistral", "tags": ["chat", "fine-tuned", "helpful"]},
        {"name": "OpenChat 3.5 7B", "shortcut": "TheBloke/openchat_3.5-GGUF", "size_gb": 4.5, "params": "7B", "quantization": "Q4_K_M", "distributor": "OpenChat", "description": "Top-tier chatbot", "tags": ["chat", "conversational", "helpful"]}
      ]
    },
    "lmstudio": {
      "name": "LM Studio",
      "icon": "ðŸŽ¨",
      "color": "#9966ff",
      "url": "https://lmstudio.ai/models",
      "models": [
        {"name": "Llama 3.2 1B Instruct Q4", "shortcut": "lmstudio-community/Llama-3.2-1B-Instruct-GGUF", "size_gb": 0.7, "params": "1B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Optimized for LM Studio", "tags": ["gguf", "optimized", "lightweight"]},
        {"name": "Llama 3.2 3B Instruct Q4", "shortcut": "lmstudio-community/Llama-3.2-3B-Instruct-GGUF", "size_gb": 1.8, "params": "3B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Balanced GGUF model", "tags": ["gguf", "balanced", "efficient"]},
        {"name": "Llama 3.1 8B Instruct Q4", "shortcut": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF", "size_gb": 4.5, "params": "8B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Powerful GGUF model", "tags": ["gguf", "powerful", "versatile"]},
        {"name": "Qwen 2.5 7B Instruct Q4", "shortcut": "lmstudio-community/Qwen2.5-7B-Instruct-GGUF", "size_gb": 4.3, "params": "7B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Multilingual GGUF", "tags": ["gguf", "multilingual", "reasoning"]},
        {"name": "Qwen 2.5 14B Instruct Q4", "shortcut": "lmstudio-community/Qwen2.5-14B-Instruct-GGUF", "size_gb": 8.5, "params": "14B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Large Qwen GGUF", "tags": ["gguf", "large", "multilingual"]},
        {"name": "Mistral 7B Instruct Q4", "shortcut": "lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF", "size_gb": 4.1, "params": "7B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Fast Mistral GGUF", "tags": ["gguf", "fast", "efficient"]},
        {"name": "Phi-3 Mini Q4", "shortcut": "lmstudio-community/Phi-3-mini-4k-instruct-GGUF", "size_gb": 2.2, "params": "3.8B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Compact Phi GGUF", "tags": ["gguf", "compact", "reasoning"]},
        {"name": "Phi-3.5 Mini Q4", "shortcut": "lmstudio-community/Phi-3.5-mini-instruct-GGUF", "size_gb": 2.2, "params": "3.8B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Latest Phi GGUF", "tags": ["gguf", "compact", "fast"]},
        {"name": "Gemma 2 9B IT Q4", "shortcut": "lmstudio-community/gemma-2-9b-it-GGUF", "size_gb": 5.2, "params": "9B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Gemma 2 GGUF", "tags": ["gguf", "balanced", "reasoning"]},
        {"name": "DeepSeek R1 Distill 7B Q4", "shortcut": "lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF", "size_gb": 4.3, "params": "7B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Reasoning GGUF", "tags": ["gguf", "reasoning", "distilled"]},
        {"name": "CodeLlama 7B Instruct Q4", "shortcut": "lmstudio-community/CodeLlama-7b-Instruct-GGUF", "size_gb": 3.8, "params": "7B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Coding GGUF", "tags": ["gguf", "code", "programming"]},
        {"name": "Qwen 2.5 Coder 7B Q4", "shortcut": "lmstudio-community/Qwen2.5-Coder-7B-Instruct-GGUF", "size_gb": 4.3, "params": "7B", "quantization": "Q4_K_M", "distributor": "LM Studio", "description": "Coding specialist GGUF", "tags": ["gguf", "code", "programming"]}
      ]
    }
  }
}
