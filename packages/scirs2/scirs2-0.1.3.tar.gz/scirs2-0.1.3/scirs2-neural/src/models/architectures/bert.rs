//! BERT implementation
//!
//! BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based
//! model designed to pretrain deep bidirectional representations from unlabeled text.
//! Reference: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", Devlin et al. (2018)
//! https://arxiv.org/abs/1810.04805

use crate::error::{NeuralError, Result};
use crate::layers::{Dense, Dropout, Embedding, Layer, LayerNorm, MultiHeadAttention};
use scirs2_core::ndarray::{Array, IxDyn, ScalarOperand};
use scirs2_core::numeric::Float;
use scirs2_core::random::SeedableRng;
use scirs2_core::simd_ops::SimdUnifiedOps;
use std::fmt::Debug;

/// Configuration for a BERT model
#[derive(Debug, Clone)]
pub struct BertConfig {
    /// Vocabulary size
    pub vocab_size: usize,
    /// Maximum sequence length
    pub max_position_embeddings: usize,
    /// Hidden size
    pub hidden_size: usize,
    /// Number of hidden layers
    pub num_hidden_layers: usize,
    /// Number of attention heads
    pub num_attention_heads: usize,
    /// Intermediate size in feed-forward networks
    pub intermediate_size: usize,
    /// Hidden activation function
    pub hidden_act: String,
    /// Hidden dropout probability
    pub hidden_dropout_prob: f64,
    /// Attention dropout probability
    pub attention_probs_dropout_prob: f64,
    /// Type vocabulary size (usually 2 for sentence pair tasks)
    pub type_vocab_size: usize,
    /// Layer norm epsilon
    pub layer_norm_eps: f64,
    /// Initializer range
    pub initializer_range: f64,
}

impl BertConfig {
    /// Create a BERT-Base configuration
    pub fn bert_base_uncased() -> Self {
        Self {
            vocab_size: 30522,
            max_position_embeddings: 512,
            hidden_size: 768,
            num_hidden_layers: 12,
            num_attention_heads: 12,
            intermediate_size: 3072,
            hidden_act: "gelu".to_string(),
            hidden_dropout_prob: 0.1,
            attention_probs_dropout_prob: 0.1,
            type_vocab_size: 2,
            layer_norm_eps: 1e-12,
            initializer_range: 0.02,
        }
    }

    /// Create a BERT-Large configuration
    pub fn bert_large_uncased() -> Self {
        Self {
            vocab_size: 30522,
            max_position_embeddings: 512,
            hidden_size: 1024,
            num_hidden_layers: 24,
            num_attention_heads: 16,
            intermediate_size: 4096,
            hidden_act: "gelu".to_string(),
            hidden_dropout_prob: 0.1,
            attention_probs_dropout_prob: 0.1,
            type_vocab_size: 2,
            layer_norm_eps: 1e-12,
            initializer_range: 0.02,
        }
    }

    /// Create a custom BERT configuration
    pub fn custom(
        vocab_size: usize,
        hidden_size: usize,
        num_hidden_layers: usize,
        num_attention_heads: usize,
    ) -> Self {
        Self {
            vocab_size,
            max_position_embeddings: 512,
            hidden_size,
            num_hidden_layers,
            num_attention_heads,
            intermediate_size: hidden_size * 4,
            hidden_act: "gelu".to_string(),
            hidden_dropout_prob: 0.1,
            attention_probs_dropout_prob: 0.1,
            type_vocab_size: 2,
            layer_norm_eps: 1e-12,
            initializer_range: 0.02,
        }
    }
}

/// BERT embeddings combining token, position, and token type embeddings
struct BertEmbeddings<F: Float + Debug + ScalarOperand + Send + Sync + 'static> {
    /// Token embeddings
    word_embeddings: Embedding<F>,
    /// Position embeddings
    position_embeddings: Embedding<F>,
    /// Token type embeddings
    token_type_embeddings: Embedding<F>,
    /// Layer normalization
    layer_norm: LayerNorm<F>,
    /// Dropout
    dropout: Dropout<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertEmbeddings<F>
{
    fn clone(&self) -> Self {
        Self {
            word_embeddings: self.word_embeddings.clone(),
            position_embeddings: self.position_embeddings.clone(),
            token_type_embeddings: self.token_type_embeddings.clone(),
            layer_norm: self.layer_norm.clone(),
            dropout: self.dropout.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertEmbeddings<F> {
    /// Create BERT embeddings
    pub fn new(config: &BertConfig) -> Result<Self> {
        let mut rng = scirs2_core::random::rngs::SmallRng::from_seed([42; 32]);

        let word_embeddings = Embedding::new(config.vocab_size, config.hidden_size, &mut rng)?;

        let mut rng2 = scirs2_core::random::rngs::SmallRng::from_seed([43; 32]);
        let position_embeddings =
            Embedding::new(config.max_position_embeddings, config.hidden_size, &mut rng2)?;

        let mut rng3 = scirs2_core::random::rngs::SmallRng::from_seed([44; 32]);
        let token_type_embeddings =
            Embedding::new(config.type_vocab_size, config.hidden_size, &mut rng3)?;

        let mut rng4 = scirs2_core::random::rngs::SmallRng::from_seed([45; 32]);
        let layer_norm = LayerNorm::new(config.hidden_size, config.layer_norm_eps, &mut rng4)?;

        let mut rng5 = scirs2_core::random::rngs::SmallRng::from_seed([46; 32]);
        let dropout = Dropout::new(config.hidden_dropout_prob, &mut rng5)?;

        Ok(Self {
            word_embeddings,
            position_embeddings,
            token_type_embeddings,
            layer_norm,
            dropout,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertEmbeddings<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Input should be of shape [batch_size, seq_len] and contain token IDs
        let shape = input.shape();
        if shape.len() != 2 {
            return Err(NeuralError::InferenceError(format!(
                "Expected input shape [batch_size, seq_len], got {:?}",
                shape
            )));
        }

        let batch_size = shape[0];
        let seq_len = shape[1];

        // Get word embeddings
        let inputs_embeds = self.word_embeddings.forward(input)?;

        // Create position IDs
        let mut position_ids = Array::zeros(IxDyn(&[batch_size, seq_len]));
        for b in 0..batch_size {
            for s in 0..seq_len {
                position_ids[[b, s]] = F::from(s).expect("Failed to convert to float");
            }
        }

        // Get position embeddings
        let position_embeds = self.position_embeddings.forward(&position_ids)?;

        // Create token type IDs (all zeros for single sequence)
        let token_type_ids = Array::zeros(IxDyn(&[batch_size, seq_len]));

        // Get token type embeddings
        let token_type_embeds = self.token_type_embeddings.forward(&token_type_ids)?;

        // Combine embeddings
        let embeddings = &inputs_embeds + &position_embeds + &token_type_embeds;

        // Apply layer normalization
        let embeddings = self.layer_norm.forward(&embeddings)?;

        // Apply dropout
        let embeddings = self.dropout.forward(&embeddings)?;

        Ok(embeddings)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.word_embeddings.update(learning_rate)?;
        self.position_embeddings.update(learning_rate)?;
        self.token_type_embeddings.update(learning_rate)?;
        self.layer_norm.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT self-attention layer
struct BertSelfAttention<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps> {
    /// Multi-head attention layer
    attention: MultiHeadAttention<F>,
    /// Output dropout
    dropout: Dropout<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertSelfAttention<F>
{
    fn clone(&self) -> Self {
        Self {
            attention: self.attention.clone(),
            dropout: self.dropout.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static>
    BertSelfAttention<F>
{
    /// Create BERT self-attention layer
    pub fn new(config: &BertConfig) -> Result<Self> {
        let head_dim = config.hidden_size / config.num_attention_heads;
        let attn_config = crate::layers::AttentionConfig {
            num_heads: config.num_attention_heads,
            head_dim,
            dropout_prob: config.attention_probs_dropout_prob,
            causal: false,
            scale: None,
        };

        let mut rng = scirs2_core::random::rngs::SmallRng::from_seed([47; 32]);
        let attention = MultiHeadAttention::new(config.hidden_size, attn_config, &mut rng)?;

        let mut rng2 = scirs2_core::random::rngs::SmallRng::from_seed([48; 32]);
        let dropout = Dropout::new(config.hidden_dropout_prob, &mut rng2)?;

        Ok(Self { attention, dropout })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertSelfAttention<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        let attention_output = self.attention.forward(input)?;
        let attention_output = self.dropout.forward(&attention_output)?;
        Ok(attention_output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.attention.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT feed-forward network (intermediate + output)
struct BertFeedForward<F: Float + Debug + ScalarOperand + Send + Sync + 'static> {
    /// Intermediate dense layer
    intermediate_dense: Dense<F>,
    /// Output dense layer
    output_dense: Dense<F>,
    /// Layer normalization
    layer_norm: LayerNorm<F>,
    /// Dropout
    dropout: Dropout<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertFeedForward<F>
{
    fn clone(&self) -> Self {
        Self {
            intermediate_dense: self.intermediate_dense.clone(),
            output_dense: self.output_dense.clone(),
            layer_norm: self.layer_norm.clone(),
            dropout: self.dropout.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertFeedForward<F> {
    /// Create BERT feed-forward layer
    pub fn new(config: &BertConfig) -> Result<Self> {
        let mut rng1 = scirs2_core::random::rngs::SmallRng::from_seed([49; 32]);
        let intermediate_dense =
            Dense::new(config.hidden_size, config.intermediate_size, None, &mut rng1)?;

        let mut rng2 = scirs2_core::random::rngs::SmallRng::from_seed([50; 32]);
        let output_dense =
            Dense::new(config.intermediate_size, config.hidden_size, None, &mut rng2)?;

        let mut rng3 = scirs2_core::random::rngs::SmallRng::from_seed([51; 32]);
        let layer_norm = LayerNorm::new(config.hidden_size, config.layer_norm_eps, &mut rng3)?;

        let mut rng4 = scirs2_core::random::rngs::SmallRng::from_seed([52; 32]);
        let dropout = Dropout::new(config.hidden_dropout_prob, &mut rng4)?;

        Ok(Self {
            intermediate_dense,
            output_dense,
            layer_norm,
            dropout,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertFeedForward<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Intermediate layer with GELU activation
        let hidden = self.intermediate_dense.forward(input)?;
        let hidden = hidden.mapv(|v| {
            // GELU approximation
            let x3 = v * v * v;
            v * F::from(0.5).expect("Failed to convert constant to float")
                * (F::one() + (v + F::from(0.044715).expect("Failed to convert constant to float") * x3).tanh())
        });

        // Output layer
        let output = self.output_dense.forward(&hidden)?;
        let output = self.dropout.forward(&output)?;

        // Add residual and layer norm
        let output = input + &output;
        let output = self.layer_norm.forward(&output)?;

        Ok(output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.intermediate_dense.update(learning_rate)?;
        self.output_dense.update(learning_rate)?;
        self.layer_norm.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT layer (attention + feed-forward)
struct BertLayer<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps> {
    /// Self attention
    attention: BertSelfAttention<F>,
    /// Attention output layer norm
    attention_layer_norm: LayerNorm<F>,
    /// Feed-forward network
    feed_forward: BertFeedForward<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertLayer<F>
{
    fn clone(&self) -> Self {
        Self {
            attention: self.attention.clone(),
            attention_layer_norm: self.attention_layer_norm.clone(),
            feed_forward: self.feed_forward.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertLayer<F> {
    /// Create BERT layer
    pub fn new(config: &BertConfig) -> Result<Self> {
        let attention = BertSelfAttention::new(config)?;

        let mut rng = scirs2_core::random::rngs::SmallRng::from_seed([53; 32]);
        let attention_layer_norm =
            LayerNorm::new(config.hidden_size, config.layer_norm_eps, &mut rng)?;

        let feed_forward = BertFeedForward::new(config)?;

        Ok(Self {
            attention,
            attention_layer_norm,
            feed_forward,
        })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertLayer<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Self-attention with residual and layer norm
        let attention_output = self.attention.forward(input)?;
        let attention_output = input + &attention_output;
        let attention_output = self.attention_layer_norm.forward(&attention_output)?;

        // Feed-forward with residual and layer norm
        let layer_output = self.feed_forward.forward(&attention_output)?;

        Ok(layer_output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.attention.update(learning_rate)?;
        self.attention_layer_norm.update(learning_rate)?;
        self.feed_forward.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT encoder
struct BertEncoder<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps> {
    /// BERT layers
    layers: Vec<BertLayer<F>>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertEncoder<F>
{
    fn clone(&self) -> Self {
        Self {
            layers: self.layers.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertEncoder<F> {
    /// Create BERT encoder
    pub fn new(config: &BertConfig) -> Result<Self> {
        let mut layers = Vec::with_capacity(config.num_hidden_layers);
        for _ in 0..config.num_hidden_layers {
            layers.push(BertLayer::new(config)?);
        }

        Ok(Self { layers })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertEncoder<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        let mut hidden_states = input.clone();
        for layer in &self.layers {
            hidden_states = layer.forward(&hidden_states)?;
        }
        Ok(hidden_states)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        for layer in &mut self.layers {
            layer.update(learning_rate)?;
        }
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT pooler (for classification tasks)
struct BertPooler<F: Float + Debug + ScalarOperand + Send + Sync + 'static> {
    /// Dense layer
    dense: Dense<F>,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertPooler<F>
{
    fn clone(&self) -> Self {
        Self {
            dense: self.dense.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertPooler<F> {
    /// Create BERT pooler
    pub fn new(config: &BertConfig) -> Result<Self> {
        let mut rng = scirs2_core::random::rngs::SmallRng::from_seed([54; 32]);
        let dense = Dense::new(config.hidden_size, config.hidden_size, None, &mut rng)?;

        Ok(Self { dense })
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertPooler<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // Take the first token ([CLS]) representation
        let shape = input.shape();
        if shape.len() != 3 {
            return Err(NeuralError::InferenceError(format!(
                "Expected input shape [batch_size, seq_len, hidden_size], got {:?}",
                shape
            )));
        }

        let batch_size = shape[0];
        let hidden_size = shape[2];

        // Extract [CLS] token (first token)
        let mut cls_tokens = Array::zeros(IxDyn(&[batch_size, hidden_size]));
        for b in 0..batch_size {
            for i in 0..hidden_size {
                cls_tokens[[b, i]] = input[[b, 0, i]];
            }
        }

        // Apply dense layer
        let pooled_output = self.dense.forward(&cls_tokens)?;

        // Apply tanh activation
        let pooled_output = pooled_output.mapv(|x| x.tanh());

        Ok(pooled_output)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.dense.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

/// BERT model implementation
pub struct BertModel<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps> {
    /// Embeddings layer
    embeddings: BertEmbeddings<F>,
    /// Encoder
    encoder: BertEncoder<F>,
    /// Pooler
    pooler: BertPooler<F>,
    /// Model configuration
    config: BertConfig,
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Clone
    for BertModel<F>
{
    fn clone(&self) -> Self {
        Self {
            embeddings: self.embeddings.clone(),
            encoder: self.encoder.clone(),
            pooler: self.pooler.clone(),
            config: self.config.clone(),
        }
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> BertModel<F> {
    /// Create a new BERT model
    pub fn new(config: BertConfig) -> Result<Self> {
        let embeddings = BertEmbeddings::new(&config)?;
        let encoder = BertEncoder::new(&config)?;
        let pooler = BertPooler::new(&config)?;

        Ok(Self {
            embeddings,
            encoder,
            pooler,
            config,
        })
    }

    /// Create a BERT-Base-Uncased model
    pub fn bert_base_uncased() -> Result<Self> {
        let config = BertConfig::bert_base_uncased();
        Self::new(config)
    }

    /// Create a BERT-Large-Uncased model
    pub fn bert_large_uncased() -> Result<Self> {
        let config = BertConfig::bert_large_uncased();
        Self::new(config)
    }

    /// Create a custom BERT model
    pub fn custom(
        vocab_size: usize,
        hidden_size: usize,
        num_hidden_layers: usize,
        num_attention_heads: usize,
    ) -> Result<Self> {
        let config = BertConfig::custom(vocab_size, hidden_size, num_hidden_layers, num_attention_heads);
        Self::new(config)
    }

    /// Get sequence output (last layer hidden states)
    pub fn get_sequence_output(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        let embedding_output = self.embeddings.forward(input)?;
        let sequence_output = self.encoder.forward(&embedding_output)?;
        Ok(sequence_output)
    }

    /// Get pooled output (for classification tasks)
    pub fn get_pooled_output(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        let sequence_output = self.get_sequence_output(input)?;
        let pooled_output = self.pooler.forward(&sequence_output)?;
        Ok(pooled_output)
    }

    /// Get the model configuration
    pub fn config(&self) -> &BertConfig {
        &self.config
    }
}

impl<F: Float + Debug + ScalarOperand + Send + Sync + SimdUnifiedOps + 'static> Layer<F>
    for BertModel<F>
{
    fn forward(&self, input: &Array<F, IxDyn>) -> Result<Array<F, IxDyn>> {
        // By default, return the full sequence output
        self.get_sequence_output(input)
    }

    fn backward(
        &self,
        _input: &Array<F, IxDyn>,
        grad_output: &Array<F, IxDyn>,
    ) -> Result<Array<F, IxDyn>> {
        Ok(grad_output.clone())
    }

    fn update(&mut self, learning_rate: F) -> Result<()> {
        self.embeddings.update(learning_rate)?;
        self.encoder.update(learning_rate)?;
        self.pooler.update(learning_rate)?;
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

    fn as_any_mut(&mut self) -> &mut dyn std::any::Any {
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_bert_config_base() {
        let config = BertConfig::bert_base_uncased();
        assert_eq!(config.vocab_size, 30522);
        assert_eq!(config.hidden_size, 768);
        assert_eq!(config.num_hidden_layers, 12);
        assert_eq!(config.num_attention_heads, 12);
    }

    #[test]
    fn test_bert_config_large() {
        let config = BertConfig::bert_large_uncased();
        assert_eq!(config.hidden_size, 1024);
        assert_eq!(config.num_hidden_layers, 24);
        assert_eq!(config.num_attention_heads, 16);
    }

    #[test]
    fn test_bert_config_custom() {
        let config = BertConfig::custom(10000, 256, 4, 4);
        assert_eq!(config.vocab_size, 10000);
        assert_eq!(config.hidden_size, 256);
        assert_eq!(config.num_hidden_layers, 4);
        assert_eq!(config.num_attention_heads, 4);
        assert_eq!(config.intermediate_size, 1024); // 256 * 4
    }
}
