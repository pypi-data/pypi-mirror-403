# langchain-maritaca

[![PyPI version](https://img.shields.io/pypi/v/langchain-maritaca.svg)](https://pypi.org/project/langchain-maritaca/)
[![Python](https://img.shields.io/pypi/pyversions/langchain-maritaca.svg)](https://pypi.org/project/langchain-maritaca/)
[![Downloads](https://img.shields.io/pypi/dm/langchain-maritaca.svg)](https://pypi.org/project/langchain-maritaca/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CI](https://github.com/anderson-ufrj/langchain-maritaca/actions/workflows/ci.yml/badge.svg)](https://github.com/anderson-ufrj/langchain-maritaca/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/anderson-ufrj/langchain-maritaca/graph/badge.svg)](https://codecov.io/gh/anderson-ufrj/langchain-maritaca)

[üá∫üá∏ Read in English](README.md)

Pacote de integra√ß√£o conectando [Maritaca AI](https://www.maritaca.ai/) e [LangChain](https://langchain.com/) para modelos de linguagem otimizados para Portugu√™s Brasileiro.

**Autor:** Anderson Henrique da Silva
**Localiza√ß√£o:** Minas Gerais, Brasil
**GitHub:** [anderson-ufrj](https://github.com/anderson-ufrj)

## Vis√£o Geral

A Maritaca AI oferece modelos de linguagem de √∫ltima gera√ß√£o para Portugu√™s Brasileiro, incluindo a fam√≠lia de modelos Sabi√°. Esta integra√ß√£o permite usar os modelos da Maritaca de forma transparente dentro do ecossistema LangChain.

### Modelos Dispon√≠veis

| Modelo | Contexto | Input (R$/1M) | Output (R$/1M) | Vision |
|--------|----------|---------------|----------------|--------|
| `sabia-3.1` | 128k | R$5,00 | R$10,00 | Sim |
| `sabiazinho-4` | 128k | R$1,00 | R$4,00 | Sim |
| `sabiazinho-3.1` | 32k | R$1,00 | R$3,00 | Sim |

> **Nota:** Todos os modelos suportam entradas multimodais (imagens).

## Instala√ß√£o

```bash
pip install langchain-maritaca
```

## Configura√ß√£o

Defina sua chave de API da Maritaca como vari√°vel de ambiente:

```bash
export MARITACA_API_KEY="sua-chave-api"
```

Ou passe diretamente para o modelo:

```python
from langchain_maritaca import ChatMaritaca

model = ChatMaritaca(api_key="sua-chave-api")
```

## Uso

### Uso B√°sico

```python
from langchain_maritaca import ChatMaritaca

model = ChatMaritaca(
    model="sabia-3.1",
    temperature=0.7,
)

messages = [
    ("system", "Voc√™ √© um assistente prestativo especializado em cultura brasileira."),
    ("human", "Quais s√£o as principais festas populares do Brasil?"),
]

response = model.invoke(messages)
print(response.content)
```

### Streaming

```python
from langchain_maritaca import ChatMaritaca

model = ChatMaritaca(model="sabia-3.1", streaming=True)

for chunk in model.stream("Conte uma hist√≥ria sobre o folclore brasileiro"):
    print(chunk.content, end="", flush=True)
```

### Uso Ass√≠ncrono

```python
import asyncio
from langchain_maritaca import ChatMaritaca

async def main():
    model = ChatMaritaca(model="sabia-3.1")
    response = await model.ainvoke("Qual √© a receita de p√£o de queijo?")
    print(response.content)

asyncio.run(main())
```

### Com LangChain Expression Language (LCEL)

```python
from langchain_maritaca import ChatMaritaca
from langchain_core.prompts import ChatPromptTemplate

model = ChatMaritaca(model="sabia-3.1")

prompt = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um especialista em {topic}."),
    ("human", "{question}"),
])

chain = prompt | model

response = chain.invoke({
    "topic": "hist√≥ria do Brasil",
    "question": "Quem foi Tiradentes?"
})
print(response.content)
```

### Com Tool Calling (Chamada de Fun√ß√µes)

```python
from langchain_maritaca import ChatMaritaca
from langchain_core.tools import tool

@tool
def get_weather(city: str) -> str:
    """Obt√©m o clima atual para uma cidade."""
    return f"O clima em {city} est√° ensolarado, 25¬∞C"

model = ChatMaritaca(model="sabia-3.1")
model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke("Como est√° o tempo em S√£o Paulo?")
print(response)
```

### Vision / Multimodal (Imagens)

Todos os modelos da Maritaca suportam entrada de imagens. Voc√™ pode enviar imagens via URL ou base64:

```python
from langchain_maritaca import ChatMaritaca
from langchain_core.messages import HumanMessage

model = ChatMaritaca(model="sabiazinho-4")

# Com URL da imagem
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "O que voc√™ v√™ nesta imagem?"},
        {"type": "image", "url": "https://example.com/imagem.jpg"}
    ])
])
print(response.content)

# Com imagem codificada em base64
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "Descreva esta imagem em detalhes"},
        {"type": "image", "base64": "iVBORw0KGgo...", "mime_type": "image/png"}
    ])
])
```

Tamb√©m compat√≠vel com o formato `image_url` da OpenAI:

```python
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "O que h√° nesta imagem?"},
        {"type": "image_url", "image_url": {"url": "https://example.com/foto.jpg"}}
    ])
])
```

### Com Sa√≠da Estruturada

```python
from langchain_maritaca import ChatMaritaca
from pydantic import BaseModel, Field

class Pessoa(BaseModel):
    """Informa√ß√µes sobre uma pessoa."""
    nome: str = Field(description="Nome da pessoa")
    idade: int = Field(description="Idade da pessoa")

model = ChatMaritaca(model="sabia-3.1")
structured_model = model.with_structured_output(Pessoa)

result = structured_model.invoke("Jo√£o tem 25 anos e mora em S√£o Paulo")
print(result)  # Pessoa(nome="Jo√£o", idade=25)
```

### Com Embeddings para RAG

```python
from langchain_maritaca import ChatMaritaca, DeepInfraEmbeddings

# Embeddings para recupera√ß√£o de documentos
embeddings = DeepInfraEmbeddings()
vectors = embeddings.embed_documents([
    "O Brasil foi descoberto em 1500",
    "A capital do Brasil √© Bras√≠lia"
])

# Chat para gera√ß√£o de respostas
chat = ChatMaritaca(model="sabia-3.1")
```

### Com Cache

```python
from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache
from langchain_maritaca import ChatMaritaca

# Habilitar cache globalmente
set_llm_cache(InMemoryCache())

model = ChatMaritaca(model="sabia-3.1")

# Primeira chamada - acessa a API
response1 = model.invoke("Qual √© a capital do Brasil?")

# Segunda chamada - usa cache (instant√¢neo, sem custo de API!)
response2 = model.invoke("Qual √© a capital do Brasil?")
```

### Com Callbacks para Observabilidade

```python
from langchain_maritaca import ChatMaritaca, CostTrackingCallback, LatencyTrackingCallback

# Criar callbacks para monitoramento
cost_cb = CostTrackingCallback()
latency_cb = LatencyTrackingCallback()

model = ChatMaritaca(callbacks=[cost_cb, latency_cb])

# Fazer algumas chamadas
model.invoke("Ol√°!")
model.invoke("Como voc√™ est√°?")

# Verificar m√©tricas
print(f"Custo total: ${cost_cb.total_cost:.6f}")
print(f"Tokens totais: {cost_cb.total_tokens}")
print(f"Lat√™ncia m√©dia: {latency_cb.average_latency:.2f}s")
print(f"P95: {latency_cb.p95_latency:.2f}s")
```

### Contagem de Tokens e Estimativa de Custos

```python
from langchain_maritaca import ChatMaritaca
from langchain_core.messages import HumanMessage

model = ChatMaritaca(model="sabia-3.1")

# Contar tokens no texto
tokens = model.get_num_tokens("Ol√°, como voc√™ est√°?")
print(f"Tokens: {tokens}")

# Estimar custo antes de fazer uma requisi√ß√£o
messages = [HumanMessage(content="Me conte sobre o Brasil")]
estimate = model.estimate_cost(messages, max_output_tokens=1000)
print(f"Custo estimado: ${estimate['total_cost']:.6f}")
```

> **Dica**: Instale com `pip install langchain-maritaca[tokenizer]` para contagem precisa de tokens usando tiktoken.

## Por que Maritaca AI?

Os modelos da Maritaca AI s√£o especificamente treinados para Portugu√™s Brasileiro, oferecendo:

- **Compreens√£o Nativa do Portugu√™s**: Melhor entendimento de express√µes idiom√°ticas, g√≠rias e contexto cultural brasileiro
- **Treinamento com Dados Locais**: Treinado em fontes diversas de dados em Portugu√™s Brasileiro
- **Custo-Benef√≠cio**: Pre√ßos competitivos para tarefas em portugu√™s
- **Baixa Lat√™ncia**: Servidores localizados no Brasil para respostas mais r√°pidas

## Usado em Produ√ß√£o

**[Cidad√£o.AI](https://cidadao-ai-frontend.vercel.app/pt)** - Plataforma brasileira de transpar√™ncia governamental alimentada por agentes de IA, processando mais de 331K requisi√ß√µes/m√™s.

- Frontend: [github.com/anderson-ufrj/cidadao.ai-frontend](https://github.com/anderson-ufrj/cidadao.ai-frontend)
- Backend: [github.com/anderson-ufrj/cidadao.ai-backend](https://github.com/anderson-ufrj/cidadao.ai-backend)

> *Usando este pacote em produ√ß√£o? [Abra uma issue](https://github.com/anderson-ufrj/langchain-maritaca/issues) para ser destacado!*

## Refer√™ncia da API

### ChatMaritaca

Classe principal para interagir com os modelos da Maritaca AI.

**Par√¢metros:**

| Par√¢metro | Tipo | Padr√£o | Descri√ß√£o |
|-----------|------|--------|-----------|
| `model` | str | `"sabia-3.1"` | Nome do modelo a usar |
| `temperature` | float | `0.7` | Temperatura de amostragem (0.0-2.0) |
| `max_tokens` | int | None | M√°ximo de tokens a gerar |
| `top_p` | float | `0.9` | Par√¢metro top-p de amostragem |
| `api_key` | str | None | Chave de API da Maritaca (ou use var de ambiente) |
| `base_url` | str | `"https://chat.maritaca.ai/api"` | URL base da API |
| `timeout` | float | `60.0` | Timeout da requisi√ß√£o em segundos |
| `max_retries` | int | `2` | M√°ximo de tentativas de retry |
| `retry_if_rate_limited` | bool | `True` | Auto-retry em rate limit (HTTP 429) |
| `retry_delay` | float | `1.0` | Delay inicial entre retries (segundos) |
| `retry_max_delay` | float | `60.0` | Delay m√°ximo entre retries (segundos) |
| `retry_multiplier` | float | `2.0` | Multiplicador para backoff exponencial |
| `streaming` | bool | `False` | Habilitar respostas em streaming |

### DeepInfraEmbeddings

Classe para gerar embeddings usando DeepInfra (recomendado pela Maritaca AI).

**Par√¢metros:**

| Par√¢metro | Tipo | Padr√£o | Descri√ß√£o |
|-----------|------|--------|-----------|
| `model` | str | `"intfloat/multilingual-e5-large"` | Modelo de embeddings |
| `api_key` | str | None | Chave de API DeepInfra (ou use var de ambiente) |
| `batch_size` | int | `32` | Tamanho do lote para processamento |

## Desenvolvimento

### Configura√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/anderson-ufrj/langchain-maritaca.git
cd langchain-maritaca

# Instale as depend√™ncias
pip install -e ".[dev]"

# Execute os testes
pytest

# Execute o linting
ruff check .
ruff format .

# Execute a verifica√ß√£o de tipos
mypy langchain_maritaca
```

### Executando Testes

```bash
# Apenas testes unit√°rios
pytest tests/unit_tests/

# Testes de integra√ß√£o (requer MARITACA_API_KEY)
pytest tests/integration_tests/

# Com cobertura
pytest --cov=langchain_maritaca --cov-report=html
```

## Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para enviar um Pull Request.

1. Fa√ßa um fork do reposit√≥rio
2. Crie sua branch de feature (`git checkout -b feature/feature-incrivel`)
3. Commit suas altera√ß√µes (`git commit -m 'feat: adiciona feature incr√≠vel'`)
4. Push para a branch (`git push origin feature/feature-incrivel`)
5. Abra um Pull Request

## Changelog

Veja [CHANGELOG.md](CHANGELOG.md) para a lista de altera√ß√µes.

## Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## Projetos Relacionados

- [LangChain](https://github.com/langchain-ai/langchain) - Construindo aplica√ß√µes com LLMs atrav√©s de composabilidade
- [Maritaca AI](https://www.maritaca.ai/) - Modelos de linguagem para Portugu√™s Brasileiro
