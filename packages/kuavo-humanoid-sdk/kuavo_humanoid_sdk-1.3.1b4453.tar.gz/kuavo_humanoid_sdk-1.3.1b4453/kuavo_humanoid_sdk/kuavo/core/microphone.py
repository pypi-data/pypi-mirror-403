import os
import numpy as np
import rospy
from kuavo_humanoid_sdk.kuavo.core.ros.microphone import Microphone
import contextlib, sys
from kuavo_humanoid_sdk.common.logger import SDKLogger

from funasr import AutoModel



class RobotMicrophoneCore:
    """
    The core logic for handling wake-up word detection using audio data provided by ROS nodes.
    """
    def __init__(self, subscribe_topic="/micphone_data"):
        self.microphone = Microphone(subscribe_topic)
    
        # åˆ›å»ºVADå’ŒASRä¸¤ä¸ªæ¨¡å‹
        self.vad_model = AutoModel(model="fsmn-vad", model_revision="v2.0.4", disable_update=True)
        self.asr_model = AutoModel(model="paraformer-zh-streaming", model_revision="v2.0.4",disable_update=True)
        
        # é…ç½®å‚æ•°
        self.CHUNK = 1024  # æ¯ä¸ªç¼“å†²åŒºçš„å¸§æ•°
        self.target_sample_rate = 16000  # ASRæ¨¡å‹çš„ç›®æ ‡é‡‡æ ·ç‡
        self.target_mic_keywords = ['å¤¸çˆ¶', 'é²ç­',]  # è¯†åˆ«éº¦å…‹é£è®¾å¤‡çš„å…³é”®è¯

        # æµå¼è¯†åˆ«ç›¸å…³å‚æ•°
        self.audio_buffer = []  # éŸ³é¢‘ç¼“å†²åŒº
        self.vad_cache = {}  # VADæ¨¡å‹ç¼“å­˜
        self.asr_cache = {}  # ASRæ¨¡å‹ç¼“å­˜
        self.silence_frames = 0  # é™éŸ³å¸§è®¡æ•°
        self.max_silence_frames = int(1.0 * self.target_sample_rate / self.CHUNK)  # æœ€å¤§é™éŸ³å¸§æ•°ï¼ˆ1ç§’ï¼‰


        # è¯­éŸ³æ®µçŠ¶æ€ç®¡ç†
        self.speech_segment_buffer = []  # è¯­éŸ³æ®µç¼“å†²åŒº
        self.is_in_speech = False  # æ˜¯å¦åœ¨è¯­éŸ³æ®µä¸­
        self.speech_start_time = 0  # è¯­éŸ³å¼€å§‹æ—¶é—´
        self.consecutive_speech_frames = 0  # è¿ç»­è¯­éŸ³å¸§è®¡æ•°
        self.min_speech_frames = 3  # æœ€å°è¿ç»­è¯­éŸ³å¸§æ•°ï¼Œé˜²æ­¢è¯¯è§¦å‘

        # VADé…ç½®
        self.vad_chunk_size = 300  # æ¯«ç§’ï¼ŒVADæ¨¡å‹çš„å—å¤§å°
        self.vad_chunk_stride = int(self.vad_chunk_size * self.target_sample_rate / 1000)  # 3200 é‡‡æ ·ç‚¹

        # ASRé…ç½®
        self.asr_chunk_size = [0, 10, 5]  # [0, 10, 5] è¡¨ç¤º600mså®æ—¶å‡ºå­—ç²’åº¦
        self.asr_encoder_chunk_look_back = 4  # ç¼–ç å™¨å›çœ‹çš„å—æ•°
        self.asr_decoder_chunk_look_back = 1  # è§£ç å™¨å›çœ‹çš„å—æ•°
        self.asr_chunk_stride = self.asr_chunk_size[1] * 960  # 9600 é‡‡æ ·ç‚¹ (600ms * 16kHz)

        SDKLogger.debug("The audio processor node is ready.")

    def wait_for_wake_word(self, timeout_sec=60, wake_word='é²ç­é²ç­'):
        """
        Actively pull audio data, process it and wait for wake-up word detection.
        Returns True if a wake-up word is detected within the timeout period, otherwise returns False.
        """
        hot_word = [wake_word] + self.target_mic_keywords
        start_time = rospy.get_time()
        
        # é‡ç½®æ‰€æœ‰çŠ¶æ€
        self.audio_buffer = []
        self.vad_cache = {}
        self.asr_cache = {}
        self.speech_segment_buffer = []
        self.is_in_speech = False
        self.silence_frames = 0
        
        while not rospy.is_shutdown():
            if rospy.get_time() - start_time > timeout_sec:
                SDKLogger.debug("Timeout has been reached. No wake-up word was detected.")
                return False

            new_data = self.microphone.get_data()
            if new_data:

                audio_np = np.frombuffer(new_data, dtype=np.int16).astype(np.float32) / 32768.0

                self.audio_buffer.extend(audio_np)
                # æ£€æµ‹æ˜¯å¦æœ‰å£°éŸ³ï¼ˆç®€å•çš„éŸ³é‡æ£€æµ‹ï¼‰
                current_volume = np.sqrt(np.mean(audio_np**2))
                
                # è°ƒè¯•ï¼šæ‰“å°éŸ³é‡ä¿¡æ¯
                # print(f"current_volume: {current_volume:.6f}")
                
                # è°ƒæ•´éŸ³é‡é˜ˆå€¼ - å¯¹äºå½’ä¸€åŒ–åˆ°[-1,1]çš„æ•°æ®ï¼Œé™éŸ³åº”è¯¥åœ¨0.01ä»¥ä¸‹
                is_speaking = current_volume > 0.01
                
                if is_speaking:
                    self.silence_frames = 0
                else:
                    self.silence_frames += 1
                
                # print(f"is_speaking: {is_speaking}")
                # å½“ç¼“å†²åŒºæœ‰è¶³å¤Ÿçš„æ•°æ®æ—¶è¿›è¡ŒVADæ£€æµ‹
                if len(self.audio_buffer) >= self.vad_chunk_stride:

                    # æå–ä¸€ä¸ªVADå—çš„æ•°æ®
                    vad_chunk = np.array(self.audio_buffer[:self.vad_chunk_stride], dtype=np.float64)
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªå—ï¼ˆé™éŸ³è¶…è¿‡é˜ˆå€¼ï¼‰
                    is_final = self.silence_frames >= self.max_silence_frames
                    
                    try:
                        # ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³æ´»åŠ¨
                        with self.suppress_output():
                            vad_res = self.vad_model.generate(input=vad_chunk, cache=self.vad_cache, is_final=is_final, chunk_size=self.vad_chunk_size)
                        
                        # print(vad_res)
                        # æ£€æŸ¥VADç»“æœ
                        has_speech = False
                        if len(vad_res) > 0 and "value" in vad_res[0] and vad_res[0]["value"]:
                            has_speech = True
                        
                        # è¯­éŸ³æ®µç®¡ç†
                        if has_speech and not self.is_in_speech:
                            # å¼€å§‹æ–°çš„è¯­éŸ³æ®µ
                            self.is_in_speech = True
                            self.speech_segment_buffer = []
                            SDKLogger.debug("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
                        
                        if self.is_in_speech:
                            # åœ¨è¯­éŸ³æ®µä¸­ï¼Œç´¯ç§¯éŸ³é¢‘æ•°æ®
                            self.speech_segment_buffer.extend(vad_chunk)
                        
                        if is_final and self.is_in_speech:
                            # è¯­éŸ³æ®µç»“æŸï¼Œè¿›è¡ŒASRè¯†åˆ«
                            SDKLogger.debug("ğŸ” è¯­éŸ³æ®µç»“æŸï¼Œå¼€å§‹è¯†åˆ«...")
                            
                            if len(self.speech_segment_buffer) > 0:
                                # å°†è¯­éŸ³æ®µæ•°æ®è½¬æ¢ä¸ºASRæ ¼å¼
                                speech_segment = np.array(self.speech_segment_buffer, dtype=np.float64)
                                
                                # ä½¿ç”¨ASRè¿›è¡Œè¯†åˆ«
                                with self.suppress_output():
                                    asr_res = self.asr_model.generate(input=speech_segment, cache=self.asr_cache, is_final=True, 
                                                            chunk_size=self.asr_chunk_size, 
                                                            encoder_chunk_look_back=self.asr_encoder_chunk_look_back, 
                                                            decoder_chunk_look_back=self.asr_decoder_chunk_look_back, hotword=hot_word)
                                
                                # æ£€æŸ¥ASRç»“æœ
                                if len(asr_res) > 0:
                                    # æ£€æŸ¥ä¸åŒçš„ç»“æœå­—æ®µ
                                    recognized_text = ""
                                    if "value" in asr_res[0] and asr_res[0]["value"]:
                                        recognized_text = " ".join(asr_res[0]["value"])
                                    elif "text" in asr_res[0] and asr_res[0]["text"]:
                                        recognized_text = asr_res[0]["text"]
                                    
                                    if recognized_text:
                                        SDKLogger.debug(f"ğŸ“ è¯†åˆ«ç»“æœ: {recognized_text}")
                                        if wake_word in recognized_text:
                                            return True
                                    else:
                                        SDKLogger.debug("âŒ æœªèƒ½è¯†åˆ«å‡ºå†…å®¹")
                                else:
                                    SDKLogger.debug("âŒ ASRè¯†åˆ«å¤±è´¥")
                            
                            # é‡ç½®è¯­éŸ³æ®µçŠ¶æ€
                            self.is_in_speech = False
                            self.speech_segment_buffer = []
                            self.vad_cache = {}
                            self.asr_cache = {}
                            SDKLogger.debug("--- è¯­éŸ³æ®µå¤„ç†å®Œæˆ ---")
                    
                        # ç§»é™¤å·²å¤„ç†çš„æ•°æ®ï¼Œä¿ç•™ä¸€äº›é‡å 
                        # overlap = self.vad_chunk_stride // 2  # 50% é‡å 
                        self.audio_buffer = self.audio_buffer[self.vad_chunk_stride:]
                            
                    except Exception as e:
                        SDKLogger.debug(f"å¤„ç†é”™è¯¯: {e}")
            rospy.sleep(0.01)  # å‡å°‘ç¡çœ æ—¶é—´ä»¥æé«˜å“åº”æ€§

    # åˆ›å»ºè¾“å‡ºæŠ‘åˆ¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    @contextlib.contextmanager
    def suppress_output(self):
        """ä¸´æ—¶æŠ‘åˆ¶æ‰€æœ‰æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º"""
        # ä¿å­˜åŸå§‹çš„æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        # é‡å®šå‘åˆ°ç©ºè®¾å¤‡
        with open(os.devnull, 'w') as devnull:
            sys.stdout = devnull
            sys.stderr = devnull
            try:
                yield
            finally:
                # æ¢å¤åŸå§‹è¾“å‡º
                sys.stdout = old_stdout
                sys.stderr = old_stderr
