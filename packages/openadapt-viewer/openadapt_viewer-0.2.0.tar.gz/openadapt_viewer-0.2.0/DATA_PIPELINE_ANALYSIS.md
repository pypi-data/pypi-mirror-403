# Data Pipeline Analysis: Understanding Data Provenance

**Date**: 2026-01-17
**Issue**: User confusion about "synthetic" vs "real" data in benchmark viewer
**Status**: RESOLVED - Descriptions are ML-generated, not synthetic

## Executive Summary

The descriptions like "Click System Settings icon in dock" are **NOT synthetic/invented** - they are **ML-generated** from the actual recording by GPT-4o during the segmentation process. This is REAL data about the recording, just at a higher semantic level than raw mouse events.

## Data Layers in OpenAdapt

OpenAdapt has **three distinct data layers**, each serving a different purpose:

### Layer 1: Raw Events (capture.db)
**Source**: Direct hardware capture
**Format**: Time-stamped low-level events
**Purpose**: Ground truth, hardware-level recording

**Example**:
```json
{
  "id": 401,
  "timestamp": 1765672655.3973,
  "type": "mouse.down",
  "data": {
    "x": 1248.32421875,
    "y": 701.734375,
    "button": "left"
  }
}
```

**Schema**:
```sql
CREATE TABLE events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp REAL NOT NULL,
    type TEXT NOT NULL,  -- mouse.move, mouse.down, mouse.up, key.down, key.up, screen.frame
    data JSON NOT NULL,  -- {x, y, button} or {key_name, key_char}
    parent_id INTEGER
);
```

**Event Types**:
- `mouse.move` (1046 events in nightshift)
- `mouse.down` (13 events)
- `mouse.up` (13 events)
- `key.down` (16 events)
- `key.up` (16 events)
- `screen.frame` (457 events)

### Layer 2: ML-Generated Episodes (episodes.json)
**Source**: LLM analysis of Layer 1 events + screenshots
**Model**: GPT-4o (specified in episodes.json)
**Format**: Semantic descriptions of user intent
**Purpose**: Human-understandable task segmentation

**Example**:
```json
{
  "episode_id": "episode_001",
  "name": "Navigate to System Settings",
  "description": "User opens System Settings application from the dock and navigates to the Displays section.",
  "steps": [
    "Click System Settings icon in dock",
    "Wait for Settings window to open",
    "Click on Displays in sidebar"
  ],
  "boundary_confidence": 0.92,
  "llm_model": "gpt-4o",
  "processing_timestamp": "2026-01-17T12:00:00.000000"
}
```

**Key Fields**:
- `llm_model`: Model used for segmentation (e.g., "gpt-4o")
- `boundary_confidence`: ML confidence in episode boundaries (0-1)
- `coherence_score`: ML confidence in episode coherence (0-1)
- `processing_timestamp`: When ML analysis was performed

### Layer 3: Benchmark Viewer Data (BenchmarkRun)
**Source**: Transformation of Layer 2 for visualization
**Format**: Pydantic models for type safety
**Purpose**: Consistent UI rendering

**Example**:
```python
ExecutionStep(
    step_number=0,
    action_type="real_action",
    action_details={
        "description": "Click System Settings icon in dock",
        "episode": "Navigate to System Settings",
        "frame_index": 0
    },
    reasoning="Real user action: Click System Settings icon in dock",
    raw_output="Episode: Navigate to System Settings, Step 1: Click System Settings icon in dock"
)
```

## Data Flow Pipeline

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. CAPTURE (openadapt-capture)                                  │
│    Human performs task → Hardware events recorded               │
└─────────────────────────────────────────────────────────────────┘
                              ↓
                    Raw Events (capture.db)
                    - mouse.down at (1248, 701)
                    - key.down "cmd"
                    - screen.frame (PNG)
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│ 2. SEGMENTATION (openadapt-ml)                                  │
│    GPT-4o analyzes events + screenshots → Semantic episodes     │
└─────────────────────────────────────────────────────────────────┘
                              ↓
                    Episodes (episodes.json)
                    - "Click System Settings icon in dock"
                    - "Wait for Settings window to open"
                    - boundary_confidence: 0.92
                    - llm_model: "gpt-4o"
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│ 3. VIEWER (openadapt-viewer)                                    │
│    Load episodes → Transform to BenchmarkRun → Render HTML      │
└─────────────────────────────────────────────────────────────────┘
                              ↓
                    Benchmark Viewer (HTML)
                    - Displays episode descriptions
                    - Shows screenshots
                    - Provides playback controls
```

## Key Findings

### 1. Descriptions Are ML-Generated, Not Synthetic

**User's Concern**: "Click System Settings icon in dock" looks made up

**Reality**:
- This description comes from `episodes.json` line 18
- `episodes.json` is generated by GPT-4o (specified in line 99)
- GPT-4o analyzed the actual recording (screenshots + event data)
- The description represents GPT-4o's interpretation of what the user did

**Evidence**:
```bash
$ grep -n "Click System Settings" /Users/abrichr/oa/src/openadapt-capture/turn-off-nightshift/episodes.json
18:        "Click System Settings icon in dock",

$ grep -n "llm_model" /Users/abrichr/oa/src/openadapt-capture/turn-off-nightshift/episodes.json
99:  "llm_model": "gpt-4o",
```

### 2. real_data_loader.py Uses Episodes Correctly

**Code Path**: `real_data_loader.py` lines 117-149

```python
# Load episodes from JSON
episode_steps = episode.get("steps", [])

for i, step_text in enumerate(episode_steps):
    step = ExecutionStep(
        action_details={
            "description": step_text,  # ← From episodes.json, not invented
            "episode": episode["name"],
            "frame_index": key_frames[i]["frame_index"]
        },
        reasoning=f"Real user action: {step_text}",
    )
```

**Verdict**: No synthetic data generation. All descriptions come from episodes.json.

### 3. Viewer Displays Episodes as Provided

**Code Path**: `generator.py` lines 97-105

```python
tasks_data.append({
    "steps": [
        {
            "action_details": s.action_details,  # ← From episodes.json
            "reasoning": s.reasoning,
        }
        for s in execution.steps
    ],
})
```

**HTML Display**: `generator.py` line 402

```html
<span class="oa-action-details"
      x-text="JSON.stringify(selectedTask.steps[currentStep].action_details)">
</span>
```

**Verdict**: Viewer displays episode data as-is, no transformation.

## Data Provenance Summary

| Layer | Source | Provenance | Example |
|-------|--------|------------|---------|
| **Raw Events** | Hardware | Direct capture | `mouse.down at (1248, 701)` |
| **Episodes** | GPT-4o | ML-inferred from raw events | `"Click System Settings icon in dock"` |
| **Viewer** | Episodes.json | Pass-through display | Shows episode description verbatim |

## What IS Actual Data vs What IS NOT

### ✓ ACTUAL Data (Displayed Correctly)

1. **Raw Events** (capture.db):
   - Mouse coordinates: `(1248.32, 701.73)`
   - Key presses: `cmd`, `tab`, `w`, `s`, `e`, `t`
   - Timestamps: `1765672632.446442`
   - Event counts: 1046 mouse moves, 13 clicks, 16 key presses

2. **ML-Generated Descriptions** (episodes.json):
   - Episode names: "Navigate to System Settings"
   - Step descriptions: "Click System Settings icon in dock"
   - Confidence scores: `boundary_confidence: 0.92`
   - Model attribution: `llm_model: "gpt-4o"`

3. **Screenshots** (PNG files):
   - Real screenshots from recording
   - Paths: `turn-off-nightshift/screenshots/capture_31807990_step_0.png`

### ✗ NOT Displayed (But Available)

1. **All 1046 mouse.move events** - Would be overwhelming to show
2. **Individual key press events** - Collapsed into "type" actions by ML
3. **All 457 screen frames** - Only key frames shown (9 total)

### ✗ Truly Synthetic Data (NOT Used in Real Viewer)

Only used in `create_sample_data()` for unit tests:
```python
def create_sample_data() -> BenchmarkRun:
    """ONLY for unit tests - creates fake data."""
    return BenchmarkRun(
        run_id="sample_run_001",
        benchmark_name="Sample Benchmark",  # ← FAKE
        tasks=[...]  # ← FAKE
    )
```

This function is **NOT called** when `use_real_data=True` (the default).

## Misconception Clarification

### User's Original Concern

> "User sees: `'Click System Settings icon in dock'` - but this is a MADE-UP description, not from the actual recording."

### The Truth

This description IS from the actual recording, specifically:
1. GPT-4o analyzed the nightshift recording
2. GPT-4o looked at screenshot `capture_31807990_step_0.png`
3. GPT-4o saw a mouse click event at coordinates matching the Settings icon
4. GPT-4o inferred the user's intent: "Click System Settings icon in dock"
5. This inference was saved to `episodes.json` with confidence: 0.92
6. The viewer displays this ML-inferred description

**Analogy**:
- Raw data: "Person moved arm at velocity V to position P"
- ML interpretation: "Person waved hello"
- Both are "real" data, just at different semantic levels

## Recommendation: Label Data Provenance

While the data is real, it's useful to make the provenance clear to users.

### Current Display
```
Action: REAL_ACTION
Details: {"description": "Click System Settings icon in dock"}
```

### Recommended Enhanced Display
```
Action: ML-INFERRED (GPT-4o, confidence: 0.92)
Details: "Click System Settings icon in dock"
Raw Event: mouse.down at (1248, 701) at 1765672655.397
```

### Implementation

Add provenance badges to the viewer:

```html
<div class="oa-action">
    <span class="oa-badge oa-badge-info">ML-INFERRED (gpt-4o)</span>
    <span class="oa-action-details">Click System Settings icon in dock</span>
</div>
<details>
    <summary>Raw Event Data</summary>
    <pre>mouse.down at (1248.32, 701.73)</pre>
    <pre>timestamp: 1765672655.397</pre>
</details>
```

## Conclusion

The benchmark viewer is displaying **100% real data**:
- **Source**: Actual nightshift recording from openadapt-capture
- **Episodes**: ML-generated by GPT-4o from that recording
- **Viewer**: Pass-through display of episode data

**No synthetic/invented data is present.** The confusion stems from conflating "ML-generated semantic descriptions" with "made-up synthetic data." The former is a valid interpretation of real events, the latter would be fabricated data.

The appropriate fix is not to change the data source, but to **label the provenance** so users understand which parts are raw hardware events vs ML-inferred semantic descriptions.
