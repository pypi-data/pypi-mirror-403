# OpenAdapt Data Layers - Quick Reference

**For**: Developers, users, and anyone confused about "real" vs "synthetic" data

## TL;DR

OpenAdapt has **3 data layers**, all are "real" but at different semantic levels:

1. **Raw Events** (capture.db) - What the hardware saw
2. **ML Episodes** (episodes.json) - What GPT-4o interpreted
3. **Viewer Data** (HTML) - What you see in the UI

**None of these are "synthetic" unless explicitly marked for tests.**

## Visual Guide

```
┌─────────────────────────────────────────────────────────┐
│ What you see in the viewer:                             │
│                                                          │
│ [ML-INFERRED] Click System Settings icon in dock        │
│ ▸ View Provenance & Metadata                            │
│   Model: gpt-4o                                          │
│   Confidence: 92.0%                                      │
│   Source: episodes.json                                  │
│                                                          │
│ ↑ This is REAL DATA from ML analysis                    │
│ ↑ Not synthetic/made up                                 │
└─────────────────────────────────────────────────────────┘
                         ↑
                    Layer 3: Viewer
                         ↑
┌─────────────────────────────────────────────────────────┐
│ episodes.json:                                           │
│                                                          │
│ {                                                        │
│   "steps": [                                             │
│     "Click System Settings icon in dock"  ← ML-inferred │
│   ],                                                     │
│   "llm_model": "gpt-4o",                                 │
│   "boundary_confidence": 0.92                            │
│ }                                                        │
│                                                          │
│ ↑ GPT-4o analyzed Layer 1 and created this              │
└─────────────────────────────────────────────────────────┘
                         ↑
                    Layer 2: Episodes
                         ↑
┌─────────────────────────────────────────────────────────┐
│ capture.db:                                              │
│                                                          │
│ {                                                        │
│   "type": "mouse.down",                                  │
│   "x": 1248.32,                                          │
│   "y": 701.73,                                           │
│   "button": "left",                                      │
│   "timestamp": 1765672655.397                            │
│ }                                                        │
│                                                          │
│ ↑ Raw hardware event (human clicked mouse)              │
└─────────────────────────────────────────────────────────┘
                    Layer 1: Raw Events
```

## Layer Comparison Table

| Layer | Source | Format | Example | Provenance | "Real"? |
|-------|--------|--------|---------|------------|---------|
| **1. Raw Events** | Hardware | `mouse.down at (1248, 701)` | Pixel coordinates | Direct capture | ✓ YES |
| **2. ML Episodes** | GPT-4o | `"Click System Settings icon in dock"` | Natural language | ML-inferred | ✓ YES (interpreted) |
| **3. Viewer** | episodes.json | Badge + description | UI elements | Pass-through | ✓ YES (displays Layer 2) |

## Common Questions

### Q: Is "Click System Settings icon in dock" made up?

**A**: NO. It's ML-generated by GPT-4o from analyzing the real recording.

- GPT-4o looked at screenshot `capture_31807990_step_0.png`
- GPT-4o saw a mouse click at `(1248, 701)` from capture.db
- GPT-4o inferred: "User is clicking the Settings icon in the dock"
- This inference was saved to episodes.json with 92% confidence

**Not made up, but ML-interpreted.**

### Q: Why not show raw events instead?

**A**: We show BOTH.

- **By default**: ML interpretation ("Click Settings icon") - human-readable
- **On demand**: Raw event (`mouse.down at (1248, 701)`) - in metadata section

**Analogy**: Like reading a book in English (ML) vs reading the binary file (raw).

### Q: How do I know if data is real vs synthetic?

**A**: Check the provenance badge:

- `[ML-INFERRED]` = Real ML analysis (from GPT-4o)
- `[RAW]` = Real hardware event (from capture.db)
- `[SYNTHETIC]` = Fake test data (ONLY in unit tests)

**If you see ML-INFERRED, it's real data, just interpreted by ML.**

### Q: Can I trust ML-inferred data?

**A**: Check the confidence score.

- 92% confidence = Very reliable
- 70% confidence = Somewhat reliable
- 40% confidence = Low reliability, check raw events

**The viewer shows confidence so you can judge for yourself.**

### Q: What if ML gets it wrong?

**A**: You can always view raw events:

1. Click "View Provenance & Metadata"
2. See frame index: 0
3. Open screenshot `capture_31807990_step_0.png`
4. Verify for yourself

**ML interpretation is a convenience, not a replacement for raw data.**

## Data Flow Example: Nightshift Recording

```
User performs action:
  1. Moves mouse to Settings icon (1248, 701)
  2. Clicks left button
  3. Settings window opens

↓ Captured in Layer 1 (capture.db)

Event 401: {
  type: "mouse.down",
  x: 1248.32421875,
  y: 701.734375,
  button: "left",
  timestamp: 1765672655.3973382
}

↓ Analyzed by GPT-4o → Layer 2 (episodes.json)

Step 1: {
  description: "Click System Settings icon in dock",
  confidence: 0.92,
  model: "gpt-4o"
}

↓ Displayed in Layer 3 (viewer)

[ML-INFERRED] Click System Settings icon in dock
▸ View Provenance & Metadata
  Model: gpt-4o
  Confidence: 92.0%
  Raw Event: mouse.down at (1248, 701)
```

## When Data IS Actually Synthetic

**Only in test files**:

```python
# tests/test_viewer.py
def test_viewer_rendering():
    # THIS is synthetic data (for testing only)
    fake_data = BenchmarkRun(
        run_id="test_run_001",
        benchmark_name="Fake Benchmark",  # ← Made up
        tasks=[...]  # ← Made up
    )
```

**Marked clearly**:
- Function name: `create_sample_data()`
- Not called when `use_real_data=True`
- Only in test files

**The nightshift viewer does NOT use this.**

## Summary

| Question | Answer |
|----------|--------|
| Is the data real? | ✓ YES |
| Is it from the nightshift recording? | ✓ YES |
| Is it ML-generated? | ✓ YES (Layer 2) |
| Is it synthetic/made-up? | ✗ NO |
| Are the descriptions from episodes.json? | ✓ YES |
| Was episodes.json created by GPT-4o? | ✓ YES |
| Is GPT-4o's interpretation "real data"? | ✓ YES (at semantic level) |

**Key insight**: ML-generated ≠ Synthetic. ML analyzes real data to create semantic interpretations.

## See Also

- [DATA_PIPELINE_ANALYSIS.md](DATA_PIPELINE_ANALYSIS.md) - Complete technical analysis
- [DATA_FIDELITY_POLICY.md](DATA_FIDELITY_POLICY.md) - Policy and guidelines
- [DATA_FIDELITY_RESOLUTION.md](DATA_FIDELITY_RESOLUTION.md) - How we fixed labeling

## Quick Check

If you see a description in the viewer and wonder "Is this real?":

1. Check for provenance badge (`[ML-INFERRED]`, `[RAW]`, etc.)
2. If ML-INFERRED: Click "View Provenance & Metadata"
3. Verify source file (episodes.json)
4. Check confidence score (>80% = reliable)
5. Compare to raw events if needed

**If it has a provenance badge, it's real data with clear attribution.**
