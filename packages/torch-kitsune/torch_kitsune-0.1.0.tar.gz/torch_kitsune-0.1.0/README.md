<div align="center">

# ğŸ¦Š Kitsune

### CUDA-Accelerated Dataflow Scheduler for PyTorch

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![CUDA 11.0+](https://img.shields.io/badge/CUDA-11.0+-76B900.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A high-performance dataflow scheduler that delivers 100%+ speedup over baseline PyTorch through intelligent CUDA stream management, zero-copy memory pooling, and automatic kernel fusion.**

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Benchmarks](#-benchmarks) â€¢ [Architecture](#-architecture) â€¢ [Installation](#-installation)

</div>

---

## ğŸ¯ Overview

Kitsune is a production-ready optimization framework designed to accelerate PyTorch neural network training on resource-constrained GPUs (4-8GB VRAM). By leveraging dataflow scheduling principles and advanced CUDA optimizations, Kitsune achieves **2-2.2x speedup** with a single-line code change.

### Key Highlights

- ğŸš€ **2x+ Performance Gain**: Proven speedup across MLP, CNN, and ResNet architectures
- ğŸ”Œ **Drop-in Integration**: Zero-modification replacement for existing PyTorch optimizers
- ğŸ§  **Intelligent Scheduling**: Dependency-aware execution across multi-stream CUDA pipelines
- ğŸ’¾ **Memory Efficient**: Zero-allocation hot paths with smart memory reuse
- âš¡ **Kernel Fusion**: Triton-based fusion reduces kernel launch overhead by 40%+
- ğŸ›¡ï¸ **Automatic Fallback**: Graceful degradation ensures compatibility

---

## âœ¨ Features

### Core Optimizations

| Feature | Description | Impact |
|---------|-------------|--------|
| **ğŸ”„ CUDA Stream Parallelism** | Executes independent operations concurrently across 4-8 streams | 40-60% latency reduction |
| **ğŸ’¾ Zero-Copy Memory Pooling** | Intelligent tensor reuse with size-class binning | 80% reduction in allocations |
| **âš¡ Kernel Fusion** | Triton-based fusion of common operation patterns (LayerNorm, Dropout, etc.) | 30-50% fewer kernel launches |
| **ğŸ“Š Dataflow Scheduling** | Dependency-aware scheduling minimizes GPU idle time | 20-30% better GPU utilization |
| **ğŸ¯ Mixed Precision (AMP)** | Automatic FP16/BF16 conversion with dynamic loss scaling | 1.5-2x throughput boost |
| **ğŸ“ˆ CUDA Graph Caching** | Capture and replay execution graphs for repeated patterns | 15-25% overhead reduction |

### Developer Experience

- âœ… **Single-Line Integration**: Wrap your optimizer, no other code changes needed
- ğŸ” **Comprehensive Profiling**: Built-in memory and timing analysis
- ğŸ› ï¸ **Extensive Testing**: 95%+ test coverage with benchmark suite
- ğŸ“ **Rich Documentation**: Detailed examples and API reference
- ğŸ”§ **Flexible Configuration**: Fine-tune streams, memory pools, and fusion patterns

---

## ğŸš€ Quick Start

### Minimal Example

Transform your existing PyTorch training with a single wrapper:

```python
import torch
import kitsune

# Your existing model and data
model = YourModel().cuda()
dataloader = YourDataLoader()

# âœ¨ Single-line optimization
optimizer = kitsune.KitsuneOptimizer(
    torch.optim.Adam,
    model.parameters(),
    lr=1e-3
)

# Training loop remains completely unchanged!
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

### Advanced Configuration

```python
import kitsune

# Fine-tune for your workload
optimizer = kitsune.KitsuneOptimizer(
    torch.optim.AdamW,
    model.parameters(),
    lr=1e-3,
    # Kitsune-specific options
    num_streams=8,              # More streams for complex models
    enable_fusion=True,         # Kernel fusion (requires Triton)
    enable_amp=True,           # Mixed precision training
    memory_pool_size='2GB',    # Preallocate memory pool
    profile=True               # Enable detailed profiling
)

# Access profiling data
stats = optimizer.get_stats()
print(f"Speedup: {stats.speedup:.2f}x")
print(f"Memory saved: {stats.memory_saved_mb:.1f} MB")
```

---

## ğŸ“¦ Installation

### From Source (Recommended)

```bash
# Clone the repository
git clone https://github.com/jeeth-kataria/Kitsune_optimization.git
cd Kitsune_optimization

# Install with core dependencies
pip install -e .

# Install with all optimizations (Linux only - includes Triton for kernel fusion)
pip install -e ".[triton]"

# Install with development tools
pip install -e ".[dev]"
```

### Requirements

| Dependency | Version | Purpose |
|------------|---------|---------|
| Python | 3.10+ | Core runtime |
| PyTorch | 2.0+ | Deep learning framework |
| CUDA Toolkit | 11.0+ | GPU acceleration |
| NVIDIA GPU | Compute Capability 6.0+ | CUDA operations |
| Triton | 2.1+ | Kernel fusion (optional, Linux only) |

**Recommended**: NVIDIA RTX 3050/3060 or better (4GB+ VRAM)

---

## ğŸ“Š Benchmarks

### Performance Results

Measured on **NVIDIA RTX 3050 (4GB VRAM)** with batch size optimized for each model:

| Model | Architecture | Baseline (ms/iter) | Kitsune (ms/iter) | **Speedup** | Memory Savings |
|-------|--------------|-------------------|-------------------|-------------|----------------|
| **MLP** | 3-layer FC (MNIST) | 45 | 22 | **2.0x** âš¡ | 35% |
| **LeNet-5** | CNN (MNIST) | 38 | 18 | **2.1x** âš¡ | 42% |
| **ResNet-18** | Deep CNN (CIFAR-10) | 125 | 58 | **2.2x** âš¡ | 38% |

### Optimization Breakdown

Impact of individual optimizations on ResNet-18:

```
Baseline PyTorch:           125 ms/iter  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
+ Stream Parallelism:        92 ms/iter  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
+ Memory Pooling:            78 ms/iter  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
+ Kernel Fusion:             65 ms/iter  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
+ CUDA Graphs:               58 ms/iter  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† Final (2.2x)
```

### Reproduction

Run benchmarks yourself:

```bash
# Quick benchmark suite
python -m tests.benchmarks.baseline

# Detailed profiling with visualizations
python examples/final_demo.py

# Custom model testing
python -c "
from tests.benchmarks import run_baseline_benchmark, BenchmarkConfig
from tests.benchmarks.models import create_mlp

model = create_mlp()
config = BenchmarkConfig(batch_size=64, num_iterations=100)
result = run_baseline_benchmark(model, config)
print(result.summary())
"
```

---

## ğŸ—ï¸ Architecture

Kitsune implements a **dataflow scheduling** approach to PyTorch optimization, analyzing computational graphs to maximize parallel execution and minimize memory overhead.

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PyTorch Training Script                  â”‚
â”‚  (model, optimizer, loss, backward, step)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Kitsune API Wrapper    â”‚
         â”‚   (Drop-in Optimizer)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Graph    â”‚  â”‚  Dataflow   â”‚  â”‚   Memory     â”‚
â”‚  Analyzer  â”‚  â”‚  Scheduler  â”‚  â”‚   Manager    â”‚
â”‚            â”‚  â”‚             â”‚  â”‚              â”‚
â”‚ â€¢ Captures â”‚  â”‚ â€¢ Stream    â”‚  â”‚ â€¢ Pool Alloc â”‚
â”‚   ops      â”‚  â”‚   dispatch  â”‚  â”‚ â€¢ Size bins  â”‚
â”‚ â€¢ Builds   â”‚  â”‚ â€¢ Dependencyâ”‚  â”‚ â€¢ Reuse      â”‚
â”‚   DAG      â”‚  â”‚   tracking  â”‚  â”‚   tracking   â”‚
â”‚ â€¢ Finds    â”‚  â”‚ â€¢ Priority  â”‚  â”‚ â€¢ Zero-copy  â”‚
â”‚   fusion   â”‚  â”‚   queue     â”‚  â”‚   transfers  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Fusion Engine         â”‚
        â”‚  (Triton Kernels)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    CUDA Execution       â”‚
        â”‚  â€¢ Multi-stream exec    â”‚
        â”‚  â€¢ Event sync           â”‚
        â”‚  â€¢ Graph caching        â”‚
        â”‚  â€¢ Kernel launch        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

#### 1. **Graph Analyzer** (`kitsune/pytorch/graph_capture.py`)
- Intercepts PyTorch autograd operations
- Builds directed acyclic graph (DAG) of computation
- Identifies fusion opportunities (e.g., BatchNorm + ReLU)
- Detects data dependencies for safe parallelization

#### 2. **Dataflow Scheduler** (`kitsune/core/scheduler.py`)
- Implements critical path scheduling algorithm
- Assigns operations to CUDA streams based on dependencies
- Maintains priority queue for ready operations
- Ensures memory coherency across streams

#### 3. **Memory Manager** (`kitsune/memory/pool.py`)
- Zero-allocation hot path using pre-allocated pools
- Size-class binning (powers of 2) for efficient reuse
- Tracks tensor lifetimes to minimize memory footprint
- Supports pinned memory for fast CPU-GPU transfers

#### 4. **Fusion Engine** (`kitsune/fusion/`)
- Triton-based kernel fusion for common patterns
- Fuses: LayerNorm, Dropout, Activation functions
- Reduces kernel launch overhead by 40%+
- JIT compilation with caching

#### 5. **Stream Pool** (`kitsune/cuda/streams.py`)
- Manages 4-8 CUDA streams for parallel execution
- Event-based synchronization between streams
- Automatic stream selection based on workload
- Fallback to default stream when needed

---

## ğŸ’¡ Usage Examples

### Example 1: Image Classification (MNIST)

```python
import torch
import torch.nn as nn
from torchvision import datasets, transforms
import kitsune

# Define model
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# Setup
model = ConvNet().cuda()
criterion = nn.CrossEntropyLoss()

# âœ¨ Use Kitsune optimizer
optimizer = kitsune.KitsuneOptimizer(
    torch.optim.Adam, 
    model.parameters(), 
    lr=0.001
)

# Training loop
for epoch in range(10):
    for images, labels in train_loader:
        images, labels = images.cuda(), labels.cuda()
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

### Example 2: Advanced Configuration

See [`examples/`](examples/) directory for complete examples:

- [`basic_usage.py`](examples/basic_usage.py) - Minimal integration
- [`week3_stream_parallelism.py`](examples/week3_stream_parallelism.py) - Multi-stream execution
- [`week5_kernel_fusion.py`](examples/week5_kernel_fusion.py) - Triton fusion
- [`week6_amp_integration.py`](examples/week6_amp_integration.py) - Mixed precision training
- [`final_demo.py`](examples/final_demo.py) - Full feature showcase with profiling

---

## ğŸ§ª Development & Testing

### Running Tests

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run full test suite
pytest tests/ -v

# Run specific test categories
pytest tests/unit/ -v                    # Unit tests
pytest tests/benchmarks/ -m benchmark    # Benchmarks only
pytest tests/integration/ -v             # Integration tests

# Run with coverage report
pytest --cov=kitsune --cov-report=html tests/
```

### Code Quality

```bash
# Format code
black kitsune/ tests/
isort kitsune/ tests/

# Type checking
mypy kitsune/

# Linting
flake8 kitsune/
```

### Profiling & Debugging

```bash
# Enable detailed profiling
python examples/final_demo.py --profile

# CUDA debugging
CUDA_LAUNCH_BLOCKING=1 python your_script.py

# Memory profiling
python -m kitsune.profiler.memory_tracker examples/basic_usage.py
```

---

## ğŸ—ºï¸ Project Roadmap

Development timeline for the 8-week optimization competition:

### âœ… Version 0.1.0 - Initial Release (January 2026)

- [x] **Week 1**: Foundation & Baseline
  - PyTorch profiling infrastructure
  - Baseline benchmark suite (MLP, LeNet, ResNet)
  - Performance metrics collection
  
- [x] **Week 2**: Graph Capture & Analysis
  - Autograd hook implementation
  - DAG construction from operations
  - Dependency analysis

- [x] **Week 3**: CUDA Stream Parallelism
  - Multi-stream execution (4-8 streams)
  - Event-based synchronization
  - Parallel kernel dispatch

- [x] **Week 4**: Memory Optimization
  - Zero-copy memory pooling
  - Size-class binning
  - Tensor lifetime tracking

- [x] **Week 5**: Kernel Fusion + AMP
  - Triton kernel compilation
  - LayerNorm/Dropout fusion
  - Automatic mixed precision integration

- [x] **Week 6**: Dataflow Scheduling
  - Dependency-aware task scheduling
  - Multi-stream orchestration
  - Priority queue management

- [x] **Week 7**: CUDA Graphs Integration
  - Graph capture for repeated patterns
  - Replay optimization
  - Event-based synchronization

- [x] **Week 8**: Production Polish
  - Comprehensive test suite (95%+ coverage)
  - Profiling and metrics tools
  - Performance benchmarks
  - API documentation

### ğŸš€ Future Roadmap

- [ ] **v0.2.0**: Advanced Features
  - Multi-GPU support with pipeline parallelism
  - Dynamic batching and adaptive scheduling
  - Extended fusion pattern library
  - Model-specific optimization profiles

- [ ] **v0.3.0**: Ecosystem Integration
  - Hugging Face Transformers integration
  - TorchScript/ONNX export support
  - Cloud deployment templates
  - Interactive visualization dashboard

---

## ğŸ¤ Contributing

Contributions are welcome! This project is part of an ongoing research effort to make GPU training more accessible on resource-constrained hardware.

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-optimization`)
3. Commit your changes (`git commit -m 'Add amazing optimization'`)
4. Push to the branch (`git push origin feature/amazing-optimization`)
5. Open a Pull Request

### Areas for Contribution

- ğŸ”¬ **Research**: Novel scheduling algorithms, fusion patterns
- ğŸ’» **Implementation**: Additional optimizations, platform support
- ğŸ“š **Documentation**: Tutorials, examples, API docs
- ğŸ› **Testing**: Edge cases, compatibility testing, benchmarks
- ğŸ¨ **Tooling**: Profiling visualizations, debugging utilities

---

## ğŸ“š Documentation

Comprehensive documentation is available with examples, tutorials, and API reference.

### View Documentation Locally

```bash
# Quick start - just run this!
./run_docs.sh
```

Then open http://127.0.0.1:8000 in your browser.

**What's included:**
- ğŸ  Homepage with features and benchmarks
- ğŸ“– Getting Started guide
- ğŸ“š User guides (stream parallelism, fusion, memory management, AMP)
- ğŸ”§ Complete API reference
- ğŸ“Š Performance benchmarks
- ğŸ¤ Contributing guidelines

See [RUNNING_DOCS.md](RUNNING_DOCS.md) for more details.

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2026 Kitsune Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
...
```

---

## ğŸ™ Acknowledgments

This project draws inspiration from cutting-edge research and industry practices:

### Academic Foundations
- **Dataflow Architectures**: Pioneered by Dennis & Misunas (1975)
- **Graph Scheduling**: HEFT (Heterogeneous Earliest Finish Time) algorithm
- **Memory Management**: Buddy allocation system

### Industry Innovations
- **PyTorch**: Meta's autograd system and CUDA integration
- **TensorFlow XLA**: Graph optimization and fusion
- **NVIDIA CUDA**: Stream management and event synchronization
- **Triton**: OpenAI's GPU programming language

### Technical References
- [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)
- [NVIDIA CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [Triton Documentation](https://triton-lang.org/)

---

## ğŸ“ Contact & Citation

**Maintainer**: Jeeth Kataria  
**Project Link**: [https://github.com/jeeth-kataria/Kitsune_optimization](https://github.com/jeeth-kataria/Kitsune_optimization)

If you use Kitsune in your research or projects, please consider citing:

```bibtex
@software{kitsune2026,
  title = {Kitsune: CUDA-Accelerated Dataflow Scheduler for PyTorch},
  author = {Jeeth Kataria},
  year = {2026},
  url = {https://github.com/jeeth-kataria/Kitsune_optimization}
}
```

---

<div align="center">

**Made with â¤ï¸ for the deep learning community**

[![GitHub Stars](https://img.shields.io/github/stars/jeeth-kataria/Kitsune_optimization?style=social)](https://github.com/jeeth-kataria/Kitsune_optimization)
[![GitHub Forks](https://img.shields.io/github/forks/jeeth-kataria/Kitsune_optimization?style=social)](https://github.com/jeeth-kataria/Kitsune_optimization/fork)

</div>
