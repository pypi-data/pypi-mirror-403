"""
Base Scanner for RepliMap.

All resource scanners inherit from BaseScanner and implement the scan() method
to extract resources from AWS and add them to the graph.

Key Improvements:
- Uses BOTO_CONFIG to disable boto3 internal retries (prevents retry storm)
- Uses with_retry decorator for coordinated retry logic
- Supports circuit breaker pattern for resilient scanning
- Sanitizes sensitive data before adding to graph
- Automatic sanitization via _add_resource_safe() method
"""

from __future__ import annotations

import logging
import os
from abc import ABC, abstractmethod
from concurrent.futures import as_completed
from typing import TYPE_CHECKING, Any, ClassVar

import boto3
from botocore.exceptions import ClientError

from replimap.core.aws_config import BOTO_CONFIG
from replimap.core.concurrency import create_thread_pool
from replimap.core.pagination import PaginationStream, RobustPaginator
from replimap.core.rate_limiter import AWSRateLimiter
from replimap.core.retry import (
    with_retry,  # noqa: F401 - Re-export for backward compatibility
)
from replimap.core.security.global_sanitizer import GlobalSanitizer
from replimap.core.security.session_manager import SessionManager

if TYPE_CHECKING:
    from collections.abc import Callable

    from replimap.core import GraphEngine
    from replimap.core.models import ResourceNode


logger = logging.getLogger(__name__)

# Configuration for parallel scanning
MAX_SCANNER_WORKERS = int(os.environ.get("REPLIMAP_MAX_WORKERS", "4"))

# NOTE: Retry configuration is now in replimap.core.retry
# The with_retry decorator is re-exported above for backward compatibility


# Intra-scanner parallelization config
INTRA_SCANNER_WORKERS = int(os.environ.get("REPLIMAP_INTRA_SCANNER_WORKERS", "8"))


def parallel_process_items(
    items: list[Any],
    processor: Callable[[Any], Any],
    max_workers: int | None = None,
    description: str = "items",
) -> tuple[list[Any], list[tuple[Any, Exception]]]:
    """
    Process a list of items in parallel.

    Useful for intra-scanner parallelization (e.g., processing S3 buckets).

    Args:
        items: List of items to process
        processor: Function to process each item
        max_workers: Maximum parallel workers (default: REPLIMAP_INTRA_SCANNER_WORKERS)
        description: Description for logging

    Returns:
        Tuple of (successful_results, failed_items_with_errors)
    """
    if not items:
        return [], []

    workers = max_workers or INTRA_SCANNER_WORKERS
    results: list[Any] = []
    failures: list[tuple[Any, Exception]] = []

    # For small batches, process sequentially
    if len(items) <= 2 or workers <= 1:
        for item in items:
            try:
                result = processor(item)
                if result is not None:
                    results.append(result)
            except Exception as e:
                failures.append((item, e))
                logger.warning(
                    f"Failed to process {description} item: {type(e).__name__}: {e}"
                )
        return results, failures

    # Process in parallel using tracked thread pool
    # Global signal handler will shutdown on Ctrl-C
    executor = create_thread_pool(
        max_workers=min(workers, len(items)),
        thread_name_prefix=f"parallel-{description[:10]}-",
    )
    try:
        future_to_item = {executor.submit(processor, item): item for item in items}

        for future in as_completed(future_to_item):
            item = future_to_item[future]
            try:
                result = future.result()
                if result is not None:
                    results.append(result)
            except Exception as e:
                failures.append((item, e))
                logger.warning(
                    f"Failed to process {description} item: {type(e).__name__}: {e}"
                )
    finally:
        executor.shutdown(wait=True)

    if failures:
        logger.warning(
            f"Parallel processing: {len(results)} succeeded, {len(failures)} failed"
        )

    return results, failures


class ScannerError(Exception):
    """Base exception for scanner errors."""

    pass


class AWSPermissionError(ScannerError):
    """Raised when AWS permissions are insufficient."""

    pass


# Backward compatibility alias (avoid shadowing built-in PermissionError)
PermissionDeniedError = AWSPermissionError


class BaseScanner(ABC):
    """
    Abstract base class for AWS resource scanners.

    Each scanner is responsible for:
    1. Calling AWS APIs to retrieve resources
    2. Converting AWS responses to ResourceNodes
    3. Adding nodes and dependencies to the graph
    4. Sanitizing sensitive data before storage

    Subclasses must implement:
    - resource_types: List of Terraform resource types this scanner handles
    - scan(): The main scanning logic

    Optionally set:
    - depends_on_types: Resource types that must be scanned first
    - service_name: AWS service name for sanitization context
    """

    # Terraform resource types this scanner handles
    resource_types: ClassVar[list[str]] = []

    # Resource types this scanner depends on (must be scanned first)
    # Scanners with dependencies run in phase 2, after phase 1 completes
    depends_on_types: ClassVar[list[str]] = []

    # AWS service name - set by subclasses for sanitization context
    service_name: str = ""

    def __init__(
        self,
        session: boto3.Session,
        region: str,
        session_manager: SessionManager | None = None,
        rate_limiter: AWSRateLimiter | None = None,
        sanitizer: GlobalSanitizer | None = None,
        sanitize_enabled: bool = True,
    ) -> None:
        """
        Initialize the scanner with AWS credentials.

        Args:
            session: Configured boto3 session
            region: AWS region to scan
            session_manager: Optional SessionManager for credential refresh.
                           If not provided, will try to get global instance.
            rate_limiter: Optional rate limiter. If not provided, creates new one.
            sanitizer: Optional GlobalSanitizer for data sanitization.
                      If not provided, creates default instance.
            sanitize_enabled: Whether to enable automatic sanitization (default: True).
        """
        self.session = session
        self.region = region
        self._clients: dict[str, object] = {}
        self.rate_limiter = rate_limiter or AWSRateLimiter()
        self.sanitizer = sanitizer or GlobalSanitizer()
        self.sanitize_enabled = sanitize_enabled

        # Sanitization statistics
        self._sanitization_stats: dict[str, Any] = {
            "resources_sanitized": 0,
            "fields_redacted": 0,
            "patterns_found": [],
        }

        # Get session manager - try provided, then global, then None
        if session_manager is not None:
            self.session_manager = session_manager
        elif SessionManager.is_initialized():
            self.session_manager = SessionManager.get_instance()
        else:
            self.session_manager = None

    def get_client(self, service_name: str) -> object:
        """
        Get or create a boto3 client for the specified service.

        Clients are cached for reuse within the scanner.
        All clients are created with BOTO_CONFIG which:
        - Disables boto3 internal retries (we handle retries ourselves)
        - Sets connection and read timeouts
        - Uses signature v4

        Args:
            service_name: AWS service name (e.g., 'ec2', 's3')

        Returns:
            Configured boto3 client
        """
        if service_name not in self._clients:
            self._clients[service_name] = self.session.client(
                service_name,
                region_name=self.region,
                config=BOTO_CONFIG,  # CRITICAL: Prevents retry storm
            )
        return self._clients[service_name]

    def _sanitize_config(self, config: dict[str, Any]) -> dict[str, Any]:
        """
        Sanitize resource configuration.

        Called automatically by _add_resource_safe().
        Can also be called directly for custom handling.

        Args:
            config: Raw resource configuration from AWS API

        Returns:
            Sanitized configuration with sensitive data redacted
        """
        if not self.sanitize_enabled:
            return config

        result = self.sanitizer.sanitize_with_result(config, self.service_name)

        if result.was_modified:
            self._sanitization_stats["resources_sanitized"] += 1
            self._sanitization_stats["fields_redacted"] += result.redacted_count
            self._sanitization_stats["patterns_found"].extend(result.findings)

            logger.debug(
                f"Sanitized resource: {result.redacted_count} fields, "
                f"findings: {result.findings[:3]}..."
            )

        return result.data

    def _add_resource_safe(
        self,
        graph: GraphEngine,
        resource_node: ResourceNode,
    ) -> None:
        """
        Add resource to graph with automatic sanitization.

        This is the preferred method for adding resources.
        Ensures all sensitive data is redacted before storage.

        Usage:
            node = ResourceNode(id=..., config=raw_config, ...)
            self._add_resource_safe(graph, node)

        Args:
            graph: The GraphEngine to add the resource to
            resource_node: The ResourceNode to add
        """
        # Sanitize configuration
        if self.sanitize_enabled and resource_node.config:
            resource_node.config = self._sanitize_config(resource_node.config)

        # Add to graph
        graph.add_resource(resource_node)

    def get_sanitization_stats(self) -> dict[str, Any]:
        """
        Get sanitization statistics for this scanner.

        Returns:
            Dictionary with sanitization metrics:
            - resources_sanitized: Number of resources that had data redacted
            - fields_redacted: Total number of fields redacted
            - patterns_found: List of pattern matches found
        """
        return self._sanitization_stats.copy()

    @abstractmethod
    def scan(self, graph: GraphEngine) -> None:
        """
        Scan AWS resources and add them to the graph.

        This method should:
        1. Call AWS APIs to list resources
        2. Create ResourceNode instances for each resource
        3. Add nodes to the graph
        4. Establish dependency edges

        Args:
            graph: The GraphEngine to populate

        Raises:
            ScannerError: If scanning fails
            AWSPermissionError: If AWS permissions are insufficient
        """
        pass

    def _extract_tags(self, tag_list: list[dict] | None) -> dict[str, str]:
        """
        Convert AWS tag list to dictionary.

        AWS returns tags as [{"Key": "Name", "Value": "my-resource"}, ...].
        This converts to {"Name": "my-resource", ...}.

        Args:
            tag_list: AWS tag list or None

        Returns:
            Dictionary of tag key-value pairs
        """
        if not tag_list:
            return {}
        return {tag["Key"]: tag["Value"] for tag in tag_list}

    def scan_paginated(
        self,
        client: Any,
        method_name: str,
        **kwargs: Any,
    ) -> PaginationStream:
        """
        Robust paginated scanning with automatic retry and credential refresh.

        Replaces boto3's all-or-nothing paginator with a resilient system that:
        - Isolates page-level failures (single page failure doesn't kill scan)
        - Provides automatic retry with exponential backoff
        - Reports partial success statistics
        - Integrates with rate limiter for throttling protection
        - Handles credential expiration via SessionManager refresh

        When credentials expire mid-scan:
        1. Catches ExpiredToken/InvalidClientTokenId errors
        2. Triggers SessionManager.force_refresh() (may prompt for MFA)
        3. Gets fresh client and continues scan
        4. Partial results already collected are preserved

        Usage:
            stream = self.scan_paginated(ec2, 'describe_instances')
            for instance in stream:
                graph.add_resource(instance)

            if not stream.stats.is_complete:
                logger.warning(f"Partial scan: {stream.stats.success_rate:.0%}")
                for error in stream.stats.errors:
                    logger.error(f"  - {error}")

        Args:
            client: Boto3 service client
            method_name: AWS API method name (e.g., 'describe_instances')
            **kwargs: Additional arguments to pass to the API

        Returns:
            PaginationStream that can be iterated and provides stats
        """
        paginator = RobustPaginator(
            client=client,
            method_name=method_name,
            rate_limiter=self.rate_limiter,
            session_manager=self.session_manager,
        )
        return paginator.paginate(**kwargs)

    def _handle_aws_error(self, error: ClientError, operation: str) -> None:
        """
        Handle AWS API errors with appropriate logging and exceptions.

        Provides user-friendly error messages with actionable hints.

        Args:
            error: The boto3 ClientError
            operation: Description of the operation that failed

        Raises:
            AWSPermissionError: If the error is access-related
            ScannerError: For other AWS errors
        """
        error_code = error.response.get("Error", {}).get("Code", "Unknown")
        error_message = error.response.get("Error", {}).get("Message", str(error))

        # User-friendly error messages with hints
        error_hints = {
            # Permission errors
            "AccessDenied": (
                f"Permission denied for {operation}. "
                f"Ensure your IAM role/user has read-only access to this service. "
                f"Required: {self._get_required_permissions(operation)}"
            ),
            "AccessDeniedException": (
                f"Access denied for {operation}. "
                f"Check IAM policies attached to your role/user."
            ),
            "UnauthorizedAccess": (
                f"Unauthorized access for {operation}. "
                f"Verify you're using the correct AWS profile and region."
            ),
            "InvalidClientTokenId": (
                "Invalid AWS credentials. Check that your AWS access key ID is correct "
                "and the credentials haven't been rotated."
            ),
            "ExpiredToken": (
                "AWS security token has expired. "
                "If using MFA, run 'replimap' again to refresh your session. "
                "If using SSO, run 'aws sso login' first."
            ),
            "ExpiredTokenException": (
                "Session credentials expired. Refresh your credentials and retry."
            ),
            "SignatureDoesNotMatch": (
                "AWS signature mismatch. This usually means your secret access key "
                "is incorrect or your system clock is significantly off."
            ),
            # Throttling errors
            "Throttling": (
                f"AWS is rate-limiting requests for {operation}. "
                "Consider reducing parallel workers with REPLIMAP_MAX_WORKERS=2"
            ),
            "ThrottlingException": (
                "Too many API requests. RepliMap will retry automatically. "
                "If this persists, reduce concurrency."
            ),
            "RequestLimitExceeded": (
                "AWS API request limit exceeded. Wait a moment and retry. "
                "Consider running the scan during off-peak hours."
            ),
            # Service errors
            "ServiceUnavailable": (
                f"AWS {operation} service is temporarily unavailable. "
                "Check AWS Service Health Dashboard and retry later."
            ),
            "InternalError": (
                f"AWS internal error during {operation}. "
                "This is an AWS-side issue. Wait a few minutes and retry."
            ),
            # Resource errors
            "ResourceNotFoundException": (
                f"Resource not found during {operation}. "
                "The resource may have been deleted or doesn't exist in this region."
            ),
            "NoSuchEntity": (
                f"Entity not found during {operation}. "
                "Verify the resource exists and you're scanning the correct account."
            ),
            # Validation errors
            "ValidationException": (
                f"Invalid request for {operation}: {error_message}"
            ),
            "InvalidParameterValue": (
                f"Invalid parameter in {operation}: {error_message}"
            ),
        }

        # Get user-friendly message or fall back to AWS message
        friendly_message = error_hints.get(error_code, None)

        # Check for permission-related errors
        permission_errors = {
            "AccessDenied",
            "UnauthorizedAccess",
            "AccessDeniedException",
            "InvalidClientTokenId",
            "ExpiredToken",
            "ExpiredTokenException",
            "SignatureDoesNotMatch",
            "MissingAuthenticationToken",
        }

        if error_code in permission_errors:
            log_msg = (
                friendly_message or f"Permission denied: {operation} - {error_message}"
            )
            logger.error(log_msg)
            raise AWSPermissionError(log_msg)

        # For other errors, log with context
        log_msg = (
            friendly_message
            or f"AWS error during {operation}: {error_code} - {error_message}"
        )
        logger.error(log_msg)
        raise ScannerError(log_msg)

    def _get_required_permissions(self, operation: str) -> str:
        """Get required IAM permissions for common operations."""
        permission_map = {
            "EC2 scanning": "ec2:Describe*",
            "VPC scanning": "ec2:DescribeVpcs, ec2:DescribeSubnets, ec2:DescribeSecurityGroups",
            "RDS scanning": "rds:Describe*",
            "S3 scanning": "s3:ListAllMyBuckets, s3:GetBucketLocation, s3:GetBucketTagging",
            "IAM scanning": "iam:List*, iam:Get*",
            "ElastiCache scanning": "elasticache:Describe*",
            "Lambda scanning": "lambda:List*, lambda:GetFunction",
            "CloudWatch scanning": "logs:DescribeLogGroups, cloudwatch:DescribeAlarms",
            "EBS scanning": "ec2:DescribeVolumes",
            "networking scanning": "ec2:DescribeInternetGateways, ec2:DescribeNatGateways, ec2:DescribeRouteTables",
        }
        for key, perms in permission_map.items():
            if key.lower() in operation.lower():
                return perms
        return "appropriate read-only permissions for this AWS service"


class ScannerRegistry:
    """
    Registry for available scanners.

    Provides a central place to register and retrieve scanners,
    enabling dynamic discovery and execution.
    """

    _scanners: ClassVar[list[type[BaseScanner]]] = []

    @classmethod
    def register(cls, scanner_class: type[BaseScanner]) -> type[BaseScanner]:
        """
        Register a scanner class.

        Can be used as a decorator:
            @ScannerRegistry.register
            class MyScanner(BaseScanner):
                ...

        Args:
            scanner_class: The scanner class to register

        Returns:
            The same scanner class (for decorator use)
        """
        if scanner_class not in cls._scanners:
            cls._scanners.append(scanner_class)
            logger.debug(f"Registered scanner: {scanner_class.__name__}")
        return scanner_class

    @classmethod
    def get_all(cls) -> list[type[BaseScanner]]:
        """Get all registered scanner classes."""
        return cls._scanners.copy()

    @classmethod
    def get_for_type(cls, resource_type: str) -> type[BaseScanner] | None:
        """
        Get the scanner that handles a specific resource type.

        Args:
            resource_type: Terraform resource type (e.g., 'aws_vpc')

        Returns:
            Scanner class if found, None otherwise
        """
        for scanner_class in cls._scanners:
            if resource_type in scanner_class.resource_types:
                return scanner_class
        return None

    @classmethod
    def clear(cls) -> None:
        """Clear all registered scanners (useful for testing)."""
        cls._scanners.clear()


def _compute_scanner_phases(
    scanner_classes: list[type[BaseScanner]],
) -> list[list[type[BaseScanner]]]:
    """
    Compute execution phases for scanners based on dependency graph.

    Uses topological sorting to determine the correct order, grouping
    scanners into phases where all scanners in a phase can run in parallel.

    Algorithm:
    1. Build resource_type -> scanner mapping
    2. Build scanner dependency graph (scanner A depends on scanner B if
       A.depends_on_types includes any type produced by B)
    3. Topologically sort and group into levels

    Args:
        scanner_classes: List of scanner classes to organize

    Returns:
        List of phases, where each phase is a list of scanner classes
        that can run in parallel
    """
    if not scanner_classes:
        return []

    # Build resource_type -> scanner mapping
    type_to_scanner: dict[str, type[BaseScanner]] = {}
    for sc in scanner_classes:
        for resource_type in sc.resource_types:
            type_to_scanner[resource_type] = sc

    # Build scanner dependency graph (adjacency list: scanner -> scanners it depends on)
    scanner_deps: dict[type[BaseScanner], set[type[BaseScanner]]] = {
        sc: set() for sc in scanner_classes
    }

    for sc in scanner_classes:
        for dep_type in sc.depends_on_types:
            if dep_type in type_to_scanner:
                provider = type_to_scanner[dep_type]
                if provider != sc:  # Avoid self-dependency
                    scanner_deps[sc].add(provider)

    # Compute in-degree: for each scanner, count the number of dependencies it has
    in_degree: dict[type[BaseScanner], int] = {
        sc: len(scanner_deps[sc]) for sc in scanner_classes
    }

    # Kahn's algorithm for topological sort with level grouping
    phases: list[list[type[BaseScanner]]] = []
    remaining = set(scanner_classes)

    while remaining:
        # Find all scanners with in-degree 0 (no unprocessed dependencies)
        ready = [sc for sc in remaining if in_degree[sc] == 0]

        if not ready:
            # Cycle detected - fall back to running remaining scanners together
            logger.warning(
                f"Dependency cycle detected among scanners: {[s.__name__ for s in remaining]}"
            )
            phases.append(list(remaining))
            break

        phases.append(ready)

        # Remove processed scanners and update in-degrees
        for sc in ready:
            remaining.remove(sc)
            # Decrease in-degree of scanners that depended on this one
            for other in remaining:
                if sc in scanner_deps[other]:
                    scanner_deps[other].remove(sc)
                    in_degree[other] -= 1

    return phases


def get_total_scanner_count() -> int:
    """Return the total number of registered scanners."""
    return len(ScannerRegistry.get_all())


def run_all_scanners(
    session: boto3.Session,
    region: str,
    graph: GraphEngine,
    parallel: bool = True,
    max_workers: int | None = None,
    on_scanner_complete: Callable[[str, bool], None] | None = None,
) -> dict[str, Exception | None]:
    """
    Run all registered scanners.

    Scanners are executed in phases determined by their dependency graph:
    - Scanners with no dependencies run first
    - Scanners depending on those run next, and so on
    - Scanners within each phase run in parallel

    This ensures scanners that query the graph for resources populated by
    other scanners will find those resources (handles transitive dependencies).

    Args:
        session: Configured boto3 session
        region: AWS region to scan
        graph: The GraphEngine to populate
        parallel: If True, run scanners in parallel (default: True)
        max_workers: Maximum parallel workers (default: REPLIMAP_MAX_WORKERS or 4)
        on_scanner_complete: Optional callback(scanner_name, success) after each scanner

    Returns:
        Dictionary mapping scanner names to exceptions (None if successful)
    """
    results: dict[str, Exception | None] = {}
    scanner_classes = ScannerRegistry.get_all()

    if not scanner_classes:
        return results

    # Compute phases based on dependency graph
    phases = _compute_scanner_phases(scanner_classes)
    workers = max_workers or MAX_SCANNER_WORKERS

    for i, phase_scanners in enumerate(phases, 1):
        if not phase_scanners:
            continue

        scanner_names = [sc.__name__ for sc in phase_scanners]
        logger.debug(
            f"Phase {i}: Running {len(phase_scanners)} scanners: {scanner_names}"
        )

        if parallel and workers > 1 and len(phase_scanners) > 1:
            phase_results = _run_scanners_parallel(
                session, region, graph, phase_scanners, workers, on_scanner_complete
            )
        else:
            phase_results = _run_scanners_sequential(
                session, region, graph, phase_scanners, on_scanner_complete
            )

        results.update(phase_results)

    return results


def _run_scanners_sequential(
    session: boto3.Session,
    region: str,
    graph: GraphEngine,
    scanner_classes: list[type[BaseScanner]],
    on_complete: Callable[[str, bool], None] | None = None,
) -> dict[str, Exception | None]:
    """Run scanners sequentially."""
    results: dict[str, Exception | None] = {}

    for scanner_class in scanner_classes:
        scanner_name = scanner_class.__name__
        logger.info(f"Running {scanner_name}...")

        try:
            scanner = scanner_class(session, region)
            scanner.scan(graph)
            results[scanner_name] = None
            logger.info(f"{scanner_name} completed successfully")
            if on_complete:
                on_complete(scanner_name, True)
        except Exception as e:
            results[scanner_name] = e
            logger.error(f"{scanner_name} failed: {e}")
            if on_complete:
                on_complete(scanner_name, False)

    return results


def _run_scanners_parallel(
    session: boto3.Session,
    region: str,
    graph: GraphEngine,
    scanner_classes: list[type[BaseScanner]],
    max_workers: int,
    on_complete: Callable[[str, bool], None] | None = None,
) -> dict[str, Exception | None]:
    """Run scanners in parallel using ThreadPoolExecutor."""
    import time as time_module

    results: dict[str, Exception | None] = {}
    timings: dict[str, float] = {}

    def run_single_scanner(
        scanner_class: type[BaseScanner],
    ) -> tuple[str, Exception | None, float]:
        scanner_name = scanner_class.__name__
        logger.info(f"Running {scanner_name}...")
        start_time = time_module.time()
        try:
            scanner = scanner_class(session, region)
            scanner.scan(graph)
            elapsed = time_module.time() - start_time
            logger.info(f"{scanner_name} completed in {elapsed:.1f}s")
            return (scanner_name, None, elapsed)
        except Exception as e:
            elapsed = time_module.time() - start_time
            logger.error(f"{scanner_name} failed after {elapsed:.1f}s: {e}")
            return (scanner_name, e, elapsed)

    # Use tracked thread pool - global signal handler will shutdown on Ctrl-C
    executor = create_thread_pool(
        max_workers=max_workers,
        thread_name_prefix="scanner-",
    )
    try:
        futures = {
            executor.submit(run_single_scanner, sc): sc for sc in scanner_classes
        }

        for future in as_completed(futures):
            scanner_name, error, elapsed = future.result()
            results[scanner_name] = error
            timings[scanner_name] = elapsed
            if on_complete:
                on_complete(scanner_name, error is None)
    finally:
        executor.shutdown(wait=True)

    # Log timing summary for performance analysis
    if timings:
        sorted_timings = sorted(timings.items(), key=lambda x: x[1], reverse=True)
        logger.debug("Scanner timing summary (slowest first):")
        for name, elapsed in sorted_timings[:5]:
            logger.debug(f"  {name}: {elapsed:.1f}s")

    return results
