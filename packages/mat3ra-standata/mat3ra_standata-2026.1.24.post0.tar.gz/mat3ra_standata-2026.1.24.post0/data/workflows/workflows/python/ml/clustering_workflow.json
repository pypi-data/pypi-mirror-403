{
    "name": "Python ML Train Clustering",
    "subworkflows": [
        {
            "_id": "03e3f15b-2b22-5bb4-8bfd-6839d28a1ba9",
            "name": "Set Up the Job",
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "properties": [],
            "model": {
                "type": "unknown",
                "subtype": "unknown",
                "method": {
                    "type": "unknown",
                    "subtype": "unknown",
                    "data": {}
                }
            },
            "units": [
                {
                    "name": "Set Workflow Mode",
                    "type": "assignment",
                    "operand": "IS_WORKFLOW_RUNNING_TO_PREDICT",
                    "value": "False",
                    "input": [],
                    "flowchartId": "head-set-predict-status",
                    "tags": [
                        "pyml:workflow-type-setter"
                    ],
                    "status": "idle",
                    "statusTrack": [],
                    "head": true,
                    "next": "head-fetch-training-data",
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    }
                },
                {
                    "name": "Fetch Dataset",
                    "type": "io",
                    "subtype": "input",
                    "enableRender": true,
                    "flowchartId": "head-fetch-training-data",
                    "input": [
                        {
                            "basename": "{{DATASET_BASENAME}}",
                            "objectData": {
                                "CONTAINER": "",
                                "NAME": "{{DATASET_FILEPATH}}",
                                "PROVIDER": "",
                                "REGION": ""
                            }
                        }
                    ],
                    "source": "object_storage",
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "head": false,
                    "next": "head-branch-on-predict-status",
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    }
                },
                {
                    "name": "Train or Predict?",
                    "type": "condition",
                    "input": [
                        {
                            "name": "IS_WORKFLOW_RUNNING_TO_PREDICT",
                            "scope": "global"
                        }
                    ],
                    "results": [],
                    "preProcessors": [],
                    "postProcessors": [],
                    "then": "head-fetch-trained-model",
                    "else": "end-of-ml-train-head",
                    "statement": "IS_WORKFLOW_RUNNING_TO_PREDICT",
                    "maxOccurrences": 100,
                    "flowchartId": "head-branch-on-predict-status",
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "head": false,
                    "next": "head-fetch-trained-model",
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    }
                },
                {
                    "name": "Fetch Trained Model as file",
                    "type": "io",
                    "subtype": "input",
                    "enableRender": true,
                    "flowchartId": "head-fetch-trained-model",
                    "input": [
                        {
                            "basename": "",
                            "objectData": {
                                "CONTAINER": "",
                                "NAME": "",
                                "PROVIDER": "",
                                "REGION": ""
                            }
                        }
                    ],
                    "source": "object_storage",
                    "tags": [
                        "set-io-unit-filenames"
                    ],
                    "status": "idle",
                    "statusTrack": [],
                    "head": false,
                    "next": "end-of-ml-train-head",
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    }
                },
                {
                    "name": "End Setup",
                    "type": "assignment",
                    "operand": "IS_SETUP_COMPLETE",
                    "value": "True",
                    "input": [],
                    "flowchartId": "end-of-ml-train-head",
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "head": false,
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    }
                }
            ]
        },
        {
            "_id": "30acc5cd-54e6-5f05-aafd-413ee8a69aa1",
            "name": "Machine Learning",
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "properties": [
                "workflow:pyml_predict",
                "file_content"
            ],
            "model": {
                "type": "unknown",
                "subtype": "unknown",
                "method": {
                    "type": "unknown",
                    "subtype": "unknown",
                    "data": {}
                }
            },
            "units": [
                {
                    "type": "execution",
                    "name": "Setup Variables and Packages",
                    "head": true,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "c3608488-0259-5ff4-8b90-11c6e60d6c85",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "settings.py",
                                "templateName": "pyml_settings.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:setup_variables_packages",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "enableRender": true,
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport os\nimport pickle\n\n# ==================================================\n# Variables modified in the Important Settings menu\n# ==================================================\n# Variables in this section can (and oftentimes need to) be modified by the user in the \"Important Settings\" tab\n# of a workflow.\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"{{ mlSettings.target_column_name }}\"\n\n# The type of ML problem being performed. Can be either \"regression\", \"classification,\" or \"clustering.\"\nproblem_category = \"{{ mlSettings.problem_category }}\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {% raw %}{{IS_WORKFLOW_RUNNING_TO_PREDICT}}{% endraw %}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\nelse:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{% raw %}{{ CONTEXT_DIR_RELATIVE_PATH }}{% endraw %}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))\n\n# Create a Class for a DummyScaler()\nclass DummyScaler:\n    \"\"\"\n    This class is a 'DummyScaler' which trivially acts on data by returning it unchanged.\n    \"\"\"\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        return X\n\n    def fit_transform(self, X):\n        return X\n\n    def inverse_transform(self, X):\n        return X\n\nif 'target_scaler' not in context:\n    context.save(DummyScaler(), 'target_scaler')\n",
                            "contextProviders": [
                                {
                                    "name": "MLSettingsDataManager"
                                }
                            ],
                            "executableName": "python",
                            "name": "settings.py",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport os\nimport pickle\n\n# ==================================================\n# Variables modified in the Important Settings menu\n# ==================================================\n# Variables in this section can (and oftentimes need to) be modified by the user in the \"Important Settings\" tab\n# of a workflow.\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"target\"\n\n# The type of ML problem being performed. Can be either \"regression\", \"classification,\" or \"clustering.\"\nproblem_category = \"regression\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {{IS_WORKFLOW_RUNNING_TO_PREDICT}}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{{DATASET_BASENAME}}\"\nelse:\n    datafile = \"{{DATASET_BASENAME}}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{{ CONTEXT_DIR_RELATIVE_PATH }}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))\n\n# Create a Class for a DummyScaler()\nclass DummyScaler:\n    \"\"\"\n    This class is a 'DummyScaler' which trivially acts on data by returning it unchanged.\n    \"\"\"\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        return X\n\n    def fit_transform(self, X):\n        return X\n\n    def inverse_transform(self, X):\n        return X\n\nif 'target_scaler' not in context:\n    context.save(DummyScaler(), 'target_scaler')\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ],
                    "next": "cb69ea2a-7efc-56b4-8bbe-0de1e70c49e3"
                },
                {
                    "type": "execution",
                    "name": "Data Input",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "cb69ea2a-7efc-56b4-8bbe-0de1e70c49e3",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "data_input_read_csv_pandas.py",
                                "templateName": "data_input_read_csv_pandas.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:data_input:read_csv:pandas",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying. In this case, we must convert any labels provided to be categorical.\n        # Specifically, labels are encoded with values between 0 and (N_Classes - 1)\n        if settings.is_classification:\n            label_encoder = sklearn.preprocessing.LabelEncoder()\n            target = label_encoder.fit_transform(target)\n            context.save(label_encoder, \"label_encoder\")\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "data_input_read_csv_pandas.py",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying. In this case, we must convert any labels provided to be categorical.\n        # Specifically, labels are encoded with values between 0 and (N_Classes - 1)\n        if settings.is_classification:\n            label_encoder = sklearn.preprocessing.LabelEncoder()\n            target = label_encoder.fit_transform(target)\n            context.save(label_encoder, \"label_encoder\")\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ],
                    "next": "7fff5212-6c6d-586b-9997-4d4485e09383"
                },
                {
                    "type": "execution",
                    "name": "Train Test Split",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "7fff5212-6c6d-586b-9997-4d4485e09383",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "data_input_train_test_split_sklearn.py",
                                "templateName": "data_input_train_test_split_sklearn.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:data_input:train_test_split:sklearn",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to perform a train/test split                     #\n#                                                                   #\n#   Splits the dataset into a training and testing set. The         #\n#   variable `percent_held_as_test` controls how much of the        #\n#   input dataset is removed for use as a testing set. By default,  #\n#   this unit puts 20% of the dataset into the testing set, and     #\n#   places the remaining 80% into the training set.                 #\n#                                                                   #\n#   Does nothing in the case of predictions.                        #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nimport numpy as np\nimport settings\nimport sklearn.model_selection\n\n# `percent_held_as_test` is the amount of the dataset held out as the testing set. If it is set to 0.2,\n# then 20% of the dataset is held out as a testing set. The remaining 80% is the training set.\npercent_held_as_test = {{ mlTrainTestSplit.fraction_held_as_test_set }}\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Load training data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n\n        # Combine datasets to facilitate train/test split\n\n        # Do train/test split\n        train_descriptors, test_descriptors, train_target, test_target = sklearn.model_selection.train_test_split(\n            train_descriptors, train_target, test_size=percent_held_as_test)\n\n        # Set the flag for using a train/test split\n        context.save(True, \"is_using_train_test_split\")\n\n        # Save training data\n        context.save(train_target, \"train_target\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_target, \"test_target\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n    # Predict\n    else:\n        pass\n",
                            "contextProviders": [
                                {
                                    "name": "MLTrainTestSplitDataManager"
                                }
                            ],
                            "executableName": "python",
                            "name": "data_input_train_test_split_sklearn.py",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to perform a train/test split                     #\n#                                                                   #\n#   Splits the dataset into a training and testing set. The         #\n#   variable `percent_held_as_test` controls how much of the        #\n#   input dataset is removed for use as a testing set. By default,  #\n#   this unit puts 20% of the dataset into the testing set, and     #\n#   places the remaining 80% into the training set.                 #\n#                                                                   #\n#   Does nothing in the case of predictions.                        #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nimport numpy as np\nimport settings\nimport sklearn.model_selection\n\n# `percent_held_as_test` is the amount of the dataset held out as the testing set. If it is set to 0.2,\n# then 20% of the dataset is held out as a testing set. The remaining 80% is the training set.\npercent_held_as_test = 0.2\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Load training data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n\n        # Combine datasets to facilitate train/test split\n\n        # Do train/test split\n        train_descriptors, test_descriptors, train_target, test_target = sklearn.model_selection.train_test_split(\n            train_descriptors, train_target, test_size=percent_held_as_test)\n\n        # Set the flag for using a train/test split\n        context.save(True, \"is_using_train_test_split\")\n\n        # Save training data\n        context.save(train_target, \"train_target\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_target, \"test_target\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n    # Predict\n    else:\n        pass\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ],
                    "next": "799de7dc-9394-571b-8e0d-3ff876a3df02"
                },
                {
                    "type": "execution",
                    "name": "Data Standardize",
                    "head": false,
                    "results": [],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "799de7dc-9394-571b-8e0d-3ff876a3df02",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "pre_processing_standardization_sklearn.py",
                                "templateName": "pre_processing_standardization_sklearn.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "pyml:pre_processing:standardization:sklearn",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "status": "idle",
                    "statusTrack": [],
                    "tags": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a standard deviation of 1. It then saves the data for use       #\n#   further down the road in the workflow, for use in               #\n#   un-transforming the data.                                       #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "pre_processing_standardization_sklearn.py",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a standard deviation of 1. It then saves the data for use       #\n#   further down the road in the workflow, for use in               #\n#   un-transforming the data.                                       #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ],
                    "next": "8dfc61c3-067d-5ea8-bd26-7296628d707a"
                },
                {
                    "type": "execution",
                    "name": "Model Train and Predict",
                    "head": false,
                    "results": [
                        {
                            "name": "workflow:pyml_predict"
                        }
                    ],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "8dfc61c3-067d-5ea8-bd26-7296628d707a",
                    "preProcessors": [],
                    "postProcessors": [],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "model_random_forest_classification_sklearn.py",
                                "templateName": "model_random_forest_classification_sklearn.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [
                            "workflow:pyml_predict"
                        ],
                        "name": "pyml:model:random_forest_classification:sklearn",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "tags": [
                        "remove-all-results",
                        "creates-predictions-csv-during-predict-phase"
                    ],
                    "status": "idle",
                    "statusTrack": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ------------------------------------------------------------ #\n# Workflow unit for a random forest classification model with  #\n# Scikit-Learn. Parameters derived from Scikit-Learn's         #\n# defaults.                                                    #\n#                                                              #\n# When then workflow is in Training mode, the model is trained #\n# and then it is saved, along with the confusion matrix. When  #\n# the workflow is run in Predict mode, the model is loaded,    #\n# predictions are made, they are un-transformed using the      #\n# trained scaler from the training run, and they are written   #\n# to a filee named \"predictions.csv\"                           #\n# ------------------------------------------------------------ #\n\n\nimport numpy as np\nimport settings\nimport sklearn.ensemble\nimport sklearn.metrics\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        test_target = context.load(\"test_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the Model\n        model = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=100,\n            criterion=\"gini\",\n            max_depth=None,\n            min_samples_split=2,\n            min_samples_leaf=1,\n            min_weight_fraction_leaf=0.0,\n            max_features=\"auto\",\n            max_leaf_nodes=None,\n            min_impurity_decrease=0.0,\n            bootstrap=True,\n            oob_score=False,\n            verbose=0,\n            class_weight=None,\n            ccp_alpha=0.0,\n            max_samples=None,\n        )\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"random_forest\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Save the probabilities of the model\n        test_probabilities = model.predict_proba(test_descriptors)\n        context.save(test_probabilities, \"test_probabilities\")\n\n        # Print some information to the screen for the regression problem\n        confusion_matrix = sklearn.metrics.confusion_matrix(test_target, test_predictions)\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix)\n        context.save(confusion_matrix, \"confusion_matrix\")\n\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"random_forest\")\n\n        # Make some predictions\n        predictions = model.predict(descriptors)\n\n        # Transform predictions back to their original labels\n        label_encoder: sklearn.preprocessing.LabelEncoder = context.load(\"label_encoder\")\n        predictions = label_encoder.inverse_transform(predictions)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\", fmt=\"%s\")\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "model_random_forest_classification_sklearn.py",
                            "rendered": "# ------------------------------------------------------------ #\n# Workflow unit for a random forest classification model with  #\n# Scikit-Learn. Parameters derived from Scikit-Learn's         #\n# defaults.                                                    #\n#                                                              #\n# When then workflow is in Training mode, the model is trained #\n# and then it is saved, along with the confusion matrix. When  #\n# the workflow is run in Predict mode, the model is loaded,    #\n# predictions are made, they are un-transformed using the      #\n# trained scaler from the training run, and they are written   #\n# to a filee named \"predictions.csv\"                           #\n# ------------------------------------------------------------ #\n\n\nimport numpy as np\nimport settings\nimport sklearn.ensemble\nimport sklearn.metrics\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        test_target = context.load(\"test_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the Model\n        model = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=100,\n            criterion=\"gini\",\n            max_depth=None,\n            min_samples_split=2,\n            min_samples_leaf=1,\n            min_weight_fraction_leaf=0.0,\n            max_features=\"auto\",\n            max_leaf_nodes=None,\n            min_impurity_decrease=0.0,\n            bootstrap=True,\n            oob_score=False,\n            verbose=0,\n            class_weight=None,\n            ccp_alpha=0.0,\n            max_samples=None,\n        )\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"random_forest\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Save the probabilities of the model\n        test_probabilities = model.predict_proba(test_descriptors)\n        context.save(test_probabilities, \"test_probabilities\")\n\n        # Print some information to the screen for the regression problem\n        confusion_matrix = sklearn.metrics.confusion_matrix(test_target, test_predictions)\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix)\n        context.save(confusion_matrix, \"confusion_matrix\")\n\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"random_forest\")\n\n        # Make some predictions\n        predictions = model.predict(descriptors)\n\n        # Transform predictions back to their original labels\n        label_encoder: sklearn.preprocessing.LabelEncoder = context.load(\"label_encoder\")\n        predictions = label_encoder.inverse_transform(predictions)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\", fmt=\"%s\")\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ],
                    "next": "35436b4a-cd9c-5089-ab42-665c4f9ba049"
                },
                {
                    "type": "execution",
                    "name": "ROC Curve Plot",
                    "head": false,
                    "results": [
                        {
                            "basename": "my_roc_plot.png",
                            "filetype": "image",
                            "name": "file_content"
                        }
                    ],
                    "monitors": [
                        {
                            "name": "standard_output"
                        }
                    ],
                    "flowchartId": "35436b4a-cd9c-5089-ab42-665c4f9ba049",
                    "preProcessors": [],
                    "postProcessors": [
                        {
                            "name": "remove_virtual_environment"
                        }
                    ],
                    "application": {
                        "name": "python",
                        "shortName": "py",
                        "summary": "Python Script",
                        "build": "GNU",
                        "isDefault": true,
                        "version": "3.10.13",
                        "schemaVersion": "2022.8.16"
                    },
                    "executable": {
                        "isDefault": true,
                        "monitors": [
                            "standard_output"
                        ],
                        "name": "python",
                        "schemaVersion": "2022.8.16"
                    },
                    "flavor": {
                        "applicationName": "python",
                        "executableName": "python",
                        "input": [
                            {
                                "name": "post_processing_roc_curve_sklearn.py",
                                "templateName": "post_processing_roc_curve_sklearn.py"
                            },
                            {
                                "name": "requirements.txt",
                                "templateName": "pyml_requirements.txt"
                            }
                        ],
                        "monitors": [
                            "standard_output"
                        ],
                        "results": [
                            "file_content"
                        ],
                        "name": "pyml:post_processing:roc_curve:sklearn",
                        "schemaVersion": "2022.8.16",
                        "isDefault": false
                    },
                    "tags": [
                        "remove-all-results"
                    ],
                    "status": "idle",
                    "statusTrack": [],
                    "input": [
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   ROC Curve Generator                                             #\n#                                                                   #\n#   Computes and displays the Receiver Operating Characteristic     #\n#   (ROC) curve. This is restricted to binary classification tasks. #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport settings\nimport sklearn.metrics\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        test_target = context.load(\"test_target\").flatten()\n        # Slice the first column because Sklearn's ROC curve prefers probabilities for the positive class\n        test_probabilities = context.load(\"test_probabilities\")[:, 1]\n\n        # Exit if there's more than one label in the predictions\n        if len(set(test_target)) > 2:\n            exit()\n\n        # ROC curve function in sklearn prefers the positive class\n        false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(test_target, test_probabilities,\n                                                                                        pos_label=1)\n        thresholds[0] -= 1  # Sklearn arbitrarily adds 1 to the first threshold\n        roc_auc = np.round(sklearn.metrics.auc(false_positive_rate, true_positive_rate), 3)\n\n        # Plot the curve\n        fig, ax = plt.subplots()\n        points = np.array([false_positive_rate, true_positive_rate]).T.reshape(-1, 1, 2)\n        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n        norm = plt.Normalize(thresholds.min(), thresholds.max())\n        lc = matplotlib.collections.LineCollection(segments, cmap='jet', norm=norm, linewidths=2)\n        lc.set_array(thresholds)\n        line = ax.add_collection(lc)\n        fig.colorbar(line, ax=ax).set_label('Threshold')\n\n        # Padding to ensure we see the line\n        ax.margins(0.01)\n\n        plt.title(f\"ROC curve, AUC={roc_auc}\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.tight_layout()\n        plt.savefig(\"my_roc_curve.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a plot when predicting...\n        pass\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "post_processing_roc_curve_sklearn.py",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   ROC Curve Generator                                             #\n#                                                                   #\n#   Computes and displays the Receiver Operating Characteristic     #\n#   (ROC) curve. This is restricted to binary classification tasks. #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport settings\nimport sklearn.metrics\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        test_target = context.load(\"test_target\").flatten()\n        # Slice the first column because Sklearn's ROC curve prefers probabilities for the positive class\n        test_probabilities = context.load(\"test_probabilities\")[:, 1]\n\n        # Exit if there's more than one label in the predictions\n        if len(set(test_target)) > 2:\n            exit()\n\n        # ROC curve function in sklearn prefers the positive class\n        false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(test_target, test_probabilities,\n                                                                                        pos_label=1)\n        thresholds[0] -= 1  # Sklearn arbitrarily adds 1 to the first threshold\n        roc_auc = np.round(sklearn.metrics.auc(false_positive_rate, true_positive_rate), 3)\n\n        # Plot the curve\n        fig, ax = plt.subplots()\n        points = np.array([false_positive_rate, true_positive_rate]).T.reshape(-1, 1, 2)\n        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n        norm = plt.Normalize(thresholds.min(), thresholds.max())\n        lc = matplotlib.collections.LineCollection(segments, cmap='jet', norm=norm, linewidths=2)\n        lc.set_array(thresholds)\n        line = ax.add_collection(lc)\n        fig.colorbar(line, ax=ax).set_label('Threshold')\n\n        # Padding to ensure we see the line\n        ax.margins(0.01)\n\n        plt.title(f\"ROC curve, AUC={roc_auc}\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.tight_layout()\n        plt.savefig(\"my_roc_curve.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a plot when predicting...\n        pass\n",
                            "schemaVersion": "2022.8.16"
                        },
                        {
                            "applicationName": "python",
                            "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "contextProviders": [],
                            "executableName": "python",
                            "name": "requirements.txt",
                            "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                            "schemaVersion": "2022.8.16"
                        }
                    ]
                }
            ]
        }
    ],
    "units": [
        {
            "name": "Set Up the Job",
            "type": "subworkflow",
            "_id": "03e3f15b-2b22-5bb4-8bfd-6839d28a1ba9",
            "status": "idle",
            "statusTrack": [],
            "flowchartId": "5b51df93-15dd-5440-90fd-a3ffa264b7d8",
            "tags": [],
            "head": true,
            "next": "90738aae-daac-599f-913f-29fb6acdff00"
        },
        {
            "name": "Machine Learning",
            "type": "subworkflow",
            "_id": "30acc5cd-54e6-5f05-aafd-413ee8a69aa1",
            "status": "idle",
            "statusTrack": [],
            "flowchartId": "90738aae-daac-599f-913f-29fb6acdff00",
            "tags": [],
            "head": false
        }
    ],
    "properties": [],
    "_id": "f447c6df-3b7b-5b8e-a0cc-1a743847ceed",
    "workflows": [],
    "isUsingDataset": true,
    "schemaVersion": "2022.8.16",
    "isDefault": false,
    "application": {
        "name": "python"
    }
}
