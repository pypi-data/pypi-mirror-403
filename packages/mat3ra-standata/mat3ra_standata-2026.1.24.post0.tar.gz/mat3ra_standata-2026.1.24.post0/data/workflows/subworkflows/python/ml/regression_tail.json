{
    "_id": "30acc5cd-54e6-5f05-aafd-413ee8a69aa1",
    "name": "Machine Learning",
    "application": {
        "name": "python"
    },
    "properties": [
        "workflow:pyml_predict",
        "file_content"
    ],
    "model": {
        "type": "unknown",
        "subtype": "unknown",
        "method": {
            "type": "unknown",
            "subtype": "unknown",
            "data": {}
        }
    },
    "units": [
        {
            "type": "execution",
            "name": "Setup Variables and Packages",
            "head": true,
            "results": [],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "c3608488-0259-5ff4-8b90-11c6e60d6c85",
            "preProcessors": [],
            "postProcessors": [],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "settings.py",
                        "templateName": "pyml_settings.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "name": "pyml:setup_variables_packages",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "enableRender": true,
            "status": "idle",
            "statusTrack": [],
            "tags": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport os\nimport pickle\n\n# ==================================================\n# Variables modified in the Important Settings menu\n# ==================================================\n# Variables in this section can (and oftentimes need to) be modified by the user in the \"Important Settings\" tab\n# of a workflow.\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"{{ mlSettings.target_column_name }}\"\n\n# The type of ML problem being performed. Can be either \"regression\", \"classification,\" or \"clustering.\"\nproblem_category = \"{{ mlSettings.problem_category }}\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {% raw %}{{IS_WORKFLOW_RUNNING_TO_PREDICT}}{% endraw %}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\nelse:\n    datafile = \"{% raw %}{{DATASET_BASENAME}}{% endraw %}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{% raw %}{{ CONTEXT_DIR_RELATIVE_PATH }}{% endraw %}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))\n\n# Create a Class for a DummyScaler()\nclass DummyScaler:\n    \"\"\"\n    This class is a 'DummyScaler' which trivially acts on data by returning it unchanged.\n    \"\"\"\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        return X\n\n    def fit_transform(self, X):\n        return X\n\n    def inverse_transform(self, X):\n        return X\n\nif 'target_scaler' not in context:\n    context.save(DummyScaler(), 'target_scaler')\n",
                    "contextProviders": [
                        {
                            "name": "MLSettingsDataManager"
                        }
                    ],
                    "executableName": "python",
                    "name": "settings.py",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   General settings for PythonML jobs on the Exabyte.io Platform   #\n#                                                                   #\n#   This file generally shouldn't be modified directly by users.    #\n#   The \"datafile\" and \"is_workflow_running_to_predict\" variables   #\n#   are defined in the head subworkflow, and are templated into     #\n#   this file. This helps facilitate the workflow's behavior        #\n#   differing whether it is in a \"train\" or \"predict\" mode.         #\n#                                                                   #\n#   Also in this file is the \"Context\" object, which helps maintain #\n#   certain Python objects between workflow units, and between      #\n#   predict runs.                                                   #\n#                                                                   #\n#   Whenever a python object needs to be stored for subsequent runs #\n#   (such as in the case of a trained model), context.save() can be #\n#   called to save it. The object can then be loaded again by using #\n#   context.load().                                                 #\n# ----------------------------------------------------------------- #\n\n\nimport os\nimport pickle\n\n# ==================================================\n# Variables modified in the Important Settings menu\n# ==================================================\n# Variables in this section can (and oftentimes need to) be modified by the user in the \"Important Settings\" tab\n# of a workflow.\n\n# Target_column_name is used during training to identify the variable the model is traing to predict.\n# For example, consider a CSV containing three columns, \"Y\", \"X1\", and \"X2\". If the goal is to train a model\n# that will predict the value of \"Y,\" then target_column_name would be set to \"Y\"\ntarget_column_name = \"target\"\n\n# The type of ML problem being performed. Can be either \"regression\", \"classification,\" or \"clustering.\"\nproblem_category = \"regression\"\n\n# =============================\n# Non user-modifiable variables\n# =============================\n# Variables in this section generally do not need to be modified.\n\n# The problem category, regression or classification or clustering. In regression, the target (predicted) variable\n# is continues. In classification, it is categorical. In clustering, there is no target - a set of labels is\n# automatically generated.\nis_regression = is_classification = is_clustering = False\nif problem_category.lower() == \"regression\":\n    is_regression = True\nelif problem_category.lower() == \"classification\":\n    is_classification = True\nelif problem_category.lower() == \"clustering\":\n    is_clustering = True\nelse:\n    raise ValueError(\n        \"Variable 'problem_category' must be either 'regression', 'classification', or 'clustering'. Check settings.py\")\n\n# The variables \"is_workflow_running_to_predict\" and \"is_workflow_running_to_train\" are used to control whether\n# the workflow is in a \"training\" mode or a \"prediction\" mode. The \"IS_WORKFLOW_RUNNING_TO_PREDICT\" variable is set by\n# an assignment unit in the \"Set Up the Job\" subworkflow that executes at the start of the job. It is automatically\n# changed when the predict workflow is generated, so users should not need to modify this variable.\nis_workflow_running_to_predict = {{IS_WORKFLOW_RUNNING_TO_PREDICT}}\nis_workflow_running_to_train = not is_workflow_running_to_predict\n\n# Sets the datafile variable. The \"datafile\" is the data that will be read in, and will be used by subsequent\n# workflow units for either training or prediction, depending on the workflow mode.\nif is_workflow_running_to_predict:\n    datafile = \"{{DATASET_BASENAME}}\"\nelse:\n    datafile = \"{{DATASET_BASENAME}}\"\n\n# The \"Context\" class allows for data to be saved and loaded between units, and between train and predict runs.\n# Variables which have been saved using the \"Save\" method are written to disk, and the predict workflow is automatically\n# configured to obtain these files when it starts.\n#\n# IMPORTANT NOTE: Do *not* adjust the value of \"context_dir_pathname\" in the Context object. If the value is changed, then\n# files will not be correctly copied into the generated predict workflow. This will cause the predict workflow to be\n# generated in a broken state, and it will not be able to make any predictions.\nclass Context(object):\n    \"\"\"\n    Saves and loads objects from the disk, useful for preserving data between workflow units\n\n    Attributes:\n        context_paths (dict): Dictionary of the format {variable_name: path}, that governs where\n                              pickle saves files.\n\n    Methods:\n        save: Used to save objects to the context directory\n        load: Used to load objects from the context directory\n    \"\"\"\n\n    def __init__(self, context_file_basename=\"workflow_context_file_mapping\"):\n        \"\"\"\n        Constructor for Context objects\n\n        Args:\n            context_file_basename (str): Name of the file to store context paths in\n        \"\"\"\n\n        # Warning: DO NOT modify the context_dir_pathname variable below\n        # vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n        context_dir_pathname = \"{{ CONTEXT_DIR_RELATIVE_PATH }}\"\n        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._context_dir_pathname = context_dir_pathname\n        self._context_file = os.path.join(context_dir_pathname, context_file_basename)\n\n        # Make context dir if it does not exist\n        if not os.path.exists(context_dir_pathname):\n            os.makedirs(context_dir_pathname)\n\n        # Read in the context sources dictionary, if it exists\n        if os.path.exists(self._context_file):\n            with open(self._context_file, \"rb\") as file_handle:\n                self.context_paths: dict = pickle.load(file_handle)\n        else:\n            # Items is a dictionary of {varname: path}\n            self.context_paths = {}\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._update_context()\n\n    def __contains__(self, item):\n        return item in self.context_paths\n\n    def _update_context(self):\n        with open(self._context_file, \"wb\") as file_handle:\n            pickle.dump(self.context_paths, file_handle)\n\n    def load(self, name: str):\n        \"\"\"\n        Returns a contextd object\n\n        Args:\n            name (str): The name in self.context_paths of the object\n        \"\"\"\n        path = self.context_paths[name]\n        with open(path, \"rb\") as file_handle:\n            obj = pickle.load(file_handle)\n        return obj\n\n    def save(self, obj: object, name: str):\n        \"\"\"\n        Saves an object to disk using pickle\n\n        Args:\n            name (str): Friendly name for the object, used for lookup in load() method\n            obj (object): Object to store on disk\n        \"\"\"\n        path = os.path.join(self._context_dir_pathname, f\"{name}.pkl\")\n        self.context_paths[name] = path\n        with open(path, \"wb\") as file_handle:\n            pickle.dump(obj, file_handle)\n        self._update_context()\n\n# Generate a context object, so that the \"with settings.context\" can be used by other units in this workflow.\ncontext = Context()\n\nis_using_train_test_split = \"is_using_train_test_split\" in context and (context.load(\"is_using_train_test_split\"))\n\n# Create a Class for a DummyScaler()\nclass DummyScaler:\n    \"\"\"\n    This class is a 'DummyScaler' which trivially acts on data by returning it unchanged.\n    \"\"\"\n\n    def fit(self, X):\n        return self\n\n    def transform(self, X):\n        return X\n\n    def fit_transform(self, X):\n        return X\n\n    def inverse_transform(self, X):\n        return X\n\nif 'target_scaler' not in context:\n    context.save(DummyScaler(), 'target_scaler')\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ],
            "next": "cb69ea2a-7efc-56b4-8bbe-0de1e70c49e3"
        },
        {
            "type": "execution",
            "name": "Data Input",
            "head": false,
            "results": [],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "cb69ea2a-7efc-56b4-8bbe-0de1e70c49e3",
            "preProcessors": [],
            "postProcessors": [],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "data_input_read_csv_pandas.py",
                        "templateName": "data_input_read_csv_pandas.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "name": "pyml:data_input:read_csv:pandas",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "status": "idle",
            "statusTrack": [],
            "tags": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying. In this case, we must convert any labels provided to be categorical.\n        # Specifically, labels are encoded with values between 0 and (N_Classes - 1)\n        if settings.is_classification:\n            label_encoder = sklearn.preprocessing.LabelEncoder()\n            target = label_encoder.fit_transform(target)\n            context.save(label_encoder, \"label_encoder\")\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "data_input_read_csv_pandas.py",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to read in data for the ML workflow.              #\n#                                                                   #\n#   Also showcased here is the concept of branching based on        #\n#   whether the workflow is in \"train\" or \"predict\" mode.           #\n#                                                                   #\n#   If the workflow is in \"training\" mode, it will read in the data #\n#   before converting it to a Numpy array and save it for use       #\n#   later. During training, we already have values for the output,  #\n#   and this gets saved to \"target.\"                                #\n#                                                                   #\n#   Finally, whether the workflow is in training or predict mode,   #\n#   it will always read in a set of descriptors from a datafile     #\n#   defined in settings.py                                          #\n# ----------------------------------------------------------------- #\n\n\nimport pandas\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    data = pandas.read_csv(settings.datafile)\n\n    # Train\n    # By default, we don't do train/test splitting: the train and test represent the same dataset at first.\n    # Other units (such as a train/test splitter) down the line can adjust this as-needed.\n    if settings.is_workflow_running_to_train:\n\n        # Handle the case where we are clustering\n        if settings.is_clustering:\n            target = data.to_numpy()[:, 0]  # Just get the first column, it's not going to get used anyway\n        else:\n            target = data.pop(settings.target_column_name).to_numpy()\n\n        # Handle the case where we are classifying. In this case, we must convert any labels provided to be categorical.\n        # Specifically, labels are encoded with values between 0 and (N_Classes - 1)\n        if settings.is_classification:\n            label_encoder = sklearn.preprocessing.LabelEncoder()\n            target = label_encoder.fit_transform(target)\n            context.save(label_encoder, \"label_encoder\")\n\n        target = target.reshape(-1, 1)  # Reshape array from a row vector into a column vector\n\n        context.save(target, \"train_target\")\n        context.save(target, \"test_target\")\n\n        descriptors = data.to_numpy()\n\n        context.save(descriptors, \"train_descriptors\")\n        context.save(descriptors, \"test_descriptors\")\n\n    else:\n        descriptors = data.to_numpy()\n        context.save(descriptors, \"descriptors\")\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ],
            "next": "7fff5212-6c6d-586b-9997-4d4485e09383"
        },
        {
            "type": "execution",
            "name": "Train Test Split",
            "head": false,
            "results": [],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "7fff5212-6c6d-586b-9997-4d4485e09383",
            "preProcessors": [],
            "postProcessors": [],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "data_input_train_test_split_sklearn.py",
                        "templateName": "data_input_train_test_split_sklearn.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "name": "pyml:data_input:train_test_split:sklearn",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "status": "idle",
            "statusTrack": [],
            "tags": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to perform a train/test split                     #\n#                                                                   #\n#   Splits the dataset into a training and testing set. The         #\n#   variable `percent_held_as_test` controls how much of the        #\n#   input dataset is removed for use as a testing set. By default,  #\n#   this unit puts 20% of the dataset into the testing set, and     #\n#   places the remaining 80% into the training set.                 #\n#                                                                   #\n#   Does nothing in the case of predictions.                        #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nimport numpy as np\nimport settings\nimport sklearn.model_selection\n\n# `percent_held_as_test` is the amount of the dataset held out as the testing set. If it is set to 0.2,\n# then 20% of the dataset is held out as a testing set. The remaining 80% is the training set.\npercent_held_as_test = {{ mlTrainTestSplit.fraction_held_as_test_set }}\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Load training data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n\n        # Combine datasets to facilitate train/test split\n\n        # Do train/test split\n        train_descriptors, test_descriptors, train_target, test_target = sklearn.model_selection.train_test_split(\n            train_descriptors, train_target, test_size=percent_held_as_test)\n\n        # Set the flag for using a train/test split\n        context.save(True, \"is_using_train_test_split\")\n\n        # Save training data\n        context.save(train_target, \"train_target\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_target, \"test_target\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n    # Predict\n    else:\n        pass\n",
                    "contextProviders": [
                        {
                            "name": "MLTrainTestSplitDataManager"
                        }
                    ],
                    "executableName": "python",
                    "name": "data_input_train_test_split_sklearn.py",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Workflow Unit to perform a train/test split                     #\n#                                                                   #\n#   Splits the dataset into a training and testing set. The         #\n#   variable `percent_held_as_test` controls how much of the        #\n#   input dataset is removed for use as a testing set. By default,  #\n#   this unit puts 20% of the dataset into the testing set, and     #\n#   places the remaining 80% into the training set.                 #\n#                                                                   #\n#   Does nothing in the case of predictions.                        #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nimport numpy as np\nimport settings\nimport sklearn.model_selection\n\n# `percent_held_as_test` is the amount of the dataset held out as the testing set. If it is set to 0.2,\n# then 20% of the dataset is held out as a testing set. The remaining 80% is the training set.\npercent_held_as_test = 0.2\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Load training data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n\n        # Combine datasets to facilitate train/test split\n\n        # Do train/test split\n        train_descriptors, test_descriptors, train_target, test_target = sklearn.model_selection.train_test_split(\n            train_descriptors, train_target, test_size=percent_held_as_test)\n\n        # Set the flag for using a train/test split\n        context.save(True, \"is_using_train_test_split\")\n\n        # Save training data\n        context.save(train_target, \"train_target\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_target, \"test_target\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n    # Predict\n    else:\n        pass\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ],
            "next": "799de7dc-9394-571b-8e0d-3ff876a3df02"
        },
        {
            "type": "execution",
            "name": "Data Standardize",
            "head": false,
            "results": [],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "799de7dc-9394-571b-8e0d-3ff876a3df02",
            "preProcessors": [],
            "postProcessors": [],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "pre_processing_standardization_sklearn.py",
                        "templateName": "pre_processing_standardization_sklearn.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "name": "pyml:pre_processing:standardization:sklearn",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "status": "idle",
            "statusTrack": [],
            "tags": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a standard deviation of 1. It then saves the data for use       #\n#   further down the road in the workflow, for use in               #\n#   un-transforming the data.                                       #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "pre_processing_standardization_sklearn.py",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Sklearn Standard Scaler workflow unit                           #\n#                                                                   #\n#   This workflow unit scales the data such that it a mean of 0 and #\n#   a standard deviation of 1. It then saves the data for use       #\n#   further down the road in the workflow, for use in               #\n#   un-transforming the data.                                       #\n#                                                                   #\n#   It is important that new predictions are made by scaling the    #\n#   new inputs using the mean and variance of the original training #\n#   set. As a result, the scaler gets saved in the Training phase.  #\n#                                                                   #\n#   During a predict workflow, the scaler is loaded, and the        #\n#   new examples are scaled using the stored scaler.                #\n# ----------------------------------------------------------------- #\n\n\nimport settings\nimport sklearn.preprocessing\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_target = context.load(\"test_target\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Descriptor Scaler\n        scaler = sklearn.preprocessing.StandardScaler\n        descriptor_scaler = scaler()\n        train_descriptors = descriptor_scaler.fit_transform(train_descriptors)\n        test_descriptors = descriptor_scaler.transform(test_descriptors)\n        context.save(descriptor_scaler, \"descriptor_scaler\")\n        context.save(train_descriptors, \"train_descriptors\")\n        context.save(test_descriptors, \"test_descriptors\")\n\n        # Our target is only continuous if it's a regression problem\n        if settings.is_regression:\n            target_scaler = scaler()\n            train_target = target_scaler.fit_transform(train_target)\n            test_target = target_scaler.transform(test_target)\n            context.save(target_scaler, \"target_scaler\")\n            context.save(train_target, \"train_target\")\n            context.save(test_target, \"test_target\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Get the scaler\n        descriptor_scaler = context.load(\"descriptor_scaler\")\n\n        # Scale the data\n        descriptors = descriptor_scaler.transform(descriptors)\n\n        # Store the data\n        context.save(descriptors, \"descriptors\")\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ],
            "next": "8dfc61c3-067d-5ea8-bd26-7296628d707a"
        },
        {
            "type": "execution",
            "name": "Model Train and Predict",
            "head": false,
            "results": [
                {
                    "name": "workflow:pyml_predict"
                }
            ],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "8dfc61c3-067d-5ea8-bd26-7296628d707a",
            "preProcessors": [],
            "postProcessors": [],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "model_mlp_sklearn.py",
                        "templateName": "model_mlp_sklearn.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "results": [
                    "workflow:pyml_predict"
                ],
                "name": "pyml:model:multilayer_perceptron:sklearn",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "tags": [
                "remove-all-results",
                "creates-predictions-csv-during-predict-phase"
            ],
            "status": "idle",
            "statusTrack": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ------------------------------------------------------------ #\n# Workflow unit to train a simple feedforward neural network   #\n# model on a regression problem using scikit-learn. In this    #\n# template, we use the default values for hidden_layer_sizes,  #\n# activation, solver, and learning rate. Other parameters are  #\n# available (consult the sklearn docs), but in this case, we   #\n# only include those relevant to the Adam optimizer. Sklearn   #\n# Docs: Sklearn docs:http://scikit-learn.org/stable/modules/ge #\n# nerated/sklearn.neural_network.MLPRegressor.html             #\n#                                                              #\n# When then workflow is in Training mode, the model is trained #\n# and then it is saved, along with the RMSE and some           #\n# predictions made using the training data (e.g. for use in a  #\n# parity plot or calculation of other error metrics). When the #\n# workflow is run in Predict mode, the model is loaded,        #\n# predictions are made, they are un-transformed using the      #\n# trained scaler from the training run, and they are written   #\n# to a file named \"predictions.csv\"                            #\n# ------------------------------------------------------------ #\n\n\nimport numpy as np\nimport settings\nimport sklearn.metrics\nimport sklearn.neural_network\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        test_target = context.load(\"test_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the Model\n        model = sklearn.neural_network.MLPRegressor(\n            hidden_layer_sizes=(100,),\n            activation=\"relu\",\n            solver=\"adam\",\n            max_iter=300,\n            early_stopping=False,\n            validation_fraction=0.1,\n        )\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"multilayer_perceptron\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Scale predictions so they have the same shape as the saved target\n        train_predictions = train_predictions.reshape(-1, 1)\n        test_predictions = test_predictions.reshape(-1, 1)\n\n        # Scale for RMSE calc on the test set\n        target_scaler = context.load(\"target_scaler\")\n\n        # Unflatten the target\n        test_target = test_target.reshape(-1, 1)\n        y_true = target_scaler.inverse_transform(test_target)\n        y_pred = target_scaler.inverse_transform(test_predictions)\n\n        # RMSE\n        mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        print(f\"RMSE = {rmse}\")\n        context.save(rmse, \"RMSE\")\n\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"multilayer_perceptron\")\n\n        # Make some predictions\n        predictions = model.predict(descriptors)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\", fmt=\"%s\")\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "model_mlp_sklearn.py",
                    "rendered": "# ------------------------------------------------------------ #\n# Workflow unit to train a simple feedforward neural network   #\n# model on a regression problem using scikit-learn. In this    #\n# template, we use the default values for hidden_layer_sizes,  #\n# activation, solver, and learning rate. Other parameters are  #\n# available (consult the sklearn docs), but in this case, we   #\n# only include those relevant to the Adam optimizer. Sklearn   #\n# Docs: Sklearn docs:http://scikit-learn.org/stable/modules/ge #\n# nerated/sklearn.neural_network.MLPRegressor.html             #\n#                                                              #\n# When then workflow is in Training mode, the model is trained #\n# and then it is saved, along with the RMSE and some           #\n# predictions made using the training data (e.g. for use in a  #\n# parity plot or calculation of other error metrics). When the #\n# workflow is run in Predict mode, the model is loaded,        #\n# predictions are made, they are un-transformed using the      #\n# trained scaler from the training run, and they are written   #\n# to a file named \"predictions.csv\"                            #\n# ------------------------------------------------------------ #\n\n\nimport numpy as np\nimport settings\nimport sklearn.metrics\nimport sklearn.neural_network\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        test_target = context.load(\"test_target\")\n        train_descriptors = context.load(\"train_descriptors\")\n        test_descriptors = context.load(\"test_descriptors\")\n\n        # Flatten the targets\n        train_target = train_target.flatten()\n        test_target = test_target.flatten()\n\n        # Initialize the Model\n        model = sklearn.neural_network.MLPRegressor(\n            hidden_layer_sizes=(100,),\n            activation=\"relu\",\n            solver=\"adam\",\n            max_iter=300,\n            early_stopping=False,\n            validation_fraction=0.1,\n        )\n\n        # Train the model and save\n        model.fit(train_descriptors, train_target)\n        context.save(model, \"multilayer_perceptron\")\n        train_predictions = model.predict(train_descriptors)\n        test_predictions = model.predict(test_descriptors)\n\n        # Scale predictions so they have the same shape as the saved target\n        train_predictions = train_predictions.reshape(-1, 1)\n        test_predictions = test_predictions.reshape(-1, 1)\n\n        # Scale for RMSE calc on the test set\n        target_scaler = context.load(\"target_scaler\")\n\n        # Unflatten the target\n        test_target = test_target.reshape(-1, 1)\n        y_true = target_scaler.inverse_transform(test_target)\n        y_pred = target_scaler.inverse_transform(test_predictions)\n\n        # RMSE\n        mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        print(f\"RMSE = {rmse}\")\n        context.save(rmse, \"RMSE\")\n\n        context.save(train_predictions, \"train_predictions\")\n        context.save(test_predictions, \"test_predictions\")\n\n    # Predict\n    else:\n        # Restore data\n        descriptors = context.load(\"descriptors\")\n\n        # Restore model\n        model = context.load(\"multilayer_perceptron\")\n\n        # Make some predictions\n        predictions = model.predict(descriptors)\n\n        # Save the predictions to file\n        np.savetxt(\"predictions.csv\", predictions, header=\"prediction\", comments=\"\", fmt=\"%s\")\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ],
            "next": "1ca76a49-a3c7-5fa2-b693-538b599ecd7c"
        },
        {
            "type": "execution",
            "name": "Parity Plot",
            "head": false,
            "results": [
                {
                    "basename": "my_parity_plot.png",
                    "filetype": "image",
                    "name": "file_content"
                }
            ],
            "monitors": [
                {
                    "name": "standard_output"
                }
            ],
            "flowchartId": "1ca76a49-a3c7-5fa2-b693-538b599ecd7c",
            "preProcessors": [],
            "postProcessors": [
                {
                    "name": "remove_virtual_environment"
                }
            ],
            "application": {
                "name": "python",
                "shortName": "py",
                "summary": "Python Script",
                "build": "GNU",
                "isDefault": true,
                "version": "3.10.13",
                "schemaVersion": "2022.8.16"
            },
            "executable": {
                "isDefault": true,
                "monitors": [
                    "standard_output"
                ],
                "name": "python",
                "schemaVersion": "2022.8.16"
            },
            "flavor": {
                "applicationName": "python",
                "executableName": "python",
                "input": [
                    {
                        "name": "post_processing_parity_plot_matplotlib.py",
                        "templateName": "post_processing_parity_plot_matplotlib.py"
                    },
                    {
                        "name": "requirements.txt",
                        "templateName": "pyml_requirements.txt"
                    }
                ],
                "monitors": [
                    "standard_output"
                ],
                "results": [
                    "file_content"
                ],
                "name": "pyml:post_processing:parity_plot:matplotlib",
                "schemaVersion": "2022.8.16",
                "isDefault": false
            },
            "tags": [
                "remove-all-results"
            ],
            "status": "idle",
            "statusTrack": [],
            "input": [
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Parity plot generation unit                                     #\n#                                                                   #\n#   This unit generates a parity plot based on the known values     #\n#   in the training data, and the predicted values generated        #\n#   using the training data.                                        #\n#                                                                   #\n#   Because this metric compares predictions versus a ground truth, #\n#   it doesn't make sense to generate the plot when a predict       #\n#   workflow is being run (because in that case, we generally don't #\n#   know the ground truth for the values being predicted). Hence,   #\n#   this unit does nothing if the workflow is in \"predict\" mode.    #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.pyplot as plt\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_predictions = context.load(\"train_predictions\")\n        test_target = context.load(\"test_target\")\n        test_predictions = context.load(\"test_predictions\")\n\n        # Un-transform the data\n        target_scaler = context.load(\"target_scaler\")\n        train_target = target_scaler.inverse_transform(train_target)\n        train_predictions = target_scaler.inverse_transform(train_predictions)\n        test_target = target_scaler.inverse_transform(test_target)\n        test_predictions = target_scaler.inverse_transform(test_predictions)\n\n        # Plot the data\n        plt.scatter(train_target, train_predictions, c=\"#203d78\", label=\"Training Set\")\n        if settings.is_using_train_test_split:\n            plt.scatter(test_target, test_predictions, c=\"#67ac5b\", label=\"Testing Set\")\n        plt.xlabel(\"Actual Value\")\n        plt.ylabel(\"Predicted Value\")\n\n        # Scale the plot\n        target_range = (min(min(train_target), min(test_target)),\n                        max(max(train_target), max(test_target)))\n        predictions_range = (min(min(train_predictions), min(test_predictions)),\n                             max(max(train_predictions), max(test_predictions)))\n\n        limits = (min(min(target_range), min(target_range)),\n                  max(max(predictions_range), max(predictions_range)))\n        plt.xlim = (limits[0], limits[1])\n        plt.ylim = (limits[0], limits[1])\n\n        # Draw a parity line, as a guide to the eye\n        plt.plot((limits[0], limits[1]), (limits[0], limits[1]), c=\"black\", linestyle=\"dotted\", label=\"Parity\")\n        plt.legend()\n\n        # Save the figure\n        plt.tight_layout()\n        plt.savefig(\"my_parity_plot.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a plot when predicting...\n        pass\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "post_processing_parity_plot_matplotlib.py",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#   Parity plot generation unit                                     #\n#                                                                   #\n#   This unit generates a parity plot based on the known values     #\n#   in the training data, and the predicted values generated        #\n#   using the training data.                                        #\n#                                                                   #\n#   Because this metric compares predictions versus a ground truth, #\n#   it doesn't make sense to generate the plot when a predict       #\n#   workflow is being run (because in that case, we generally don't #\n#   know the ground truth for the values being predicted). Hence,   #\n#   this unit does nothing if the workflow is in \"predict\" mode.    #\n# ----------------------------------------------------------------- #\n\n\nimport matplotlib.pyplot as plt\nimport settings\n\nwith settings.context as context:\n    # Train\n    if settings.is_workflow_running_to_train:\n        # Restore the data\n        train_target = context.load(\"train_target\")\n        train_predictions = context.load(\"train_predictions\")\n        test_target = context.load(\"test_target\")\n        test_predictions = context.load(\"test_predictions\")\n\n        # Un-transform the data\n        target_scaler = context.load(\"target_scaler\")\n        train_target = target_scaler.inverse_transform(train_target)\n        train_predictions = target_scaler.inverse_transform(train_predictions)\n        test_target = target_scaler.inverse_transform(test_target)\n        test_predictions = target_scaler.inverse_transform(test_predictions)\n\n        # Plot the data\n        plt.scatter(train_target, train_predictions, c=\"#203d78\", label=\"Training Set\")\n        if settings.is_using_train_test_split:\n            plt.scatter(test_target, test_predictions, c=\"#67ac5b\", label=\"Testing Set\")\n        plt.xlabel(\"Actual Value\")\n        plt.ylabel(\"Predicted Value\")\n\n        # Scale the plot\n        target_range = (min(min(train_target), min(test_target)),\n                        max(max(train_target), max(test_target)))\n        predictions_range = (min(min(train_predictions), min(test_predictions)),\n                             max(max(train_predictions), max(test_predictions)))\n\n        limits = (min(min(target_range), min(target_range)),\n                  max(max(predictions_range), max(predictions_range)))\n        plt.xlim = (limits[0], limits[1])\n        plt.ylim = (limits[0], limits[1])\n\n        # Draw a parity line, as a guide to the eye\n        plt.plot((limits[0], limits[1]), (limits[0], limits[1]), c=\"black\", linestyle=\"dotted\", label=\"Parity\")\n        plt.legend()\n\n        # Save the figure\n        plt.tight_layout()\n        plt.savefig(\"my_parity_plot.png\", dpi=600)\n\n    # Predict\n    else:\n        # It might not make as much sense to draw a plot when predicting...\n        pass\n",
                    "schemaVersion": "2022.8.16"
                },
                {
                    "applicationName": "python",
                    "content": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "contextProviders": [],
                    "executableName": "python",
                    "name": "requirements.txt",
                    "rendered": "# ----------------------------------------------------------------- #\n#                                                                   #\n#  Example Python package requirements for the Mat3ra platform      #\n#                                                                   #\n#  Will be used as follows:                                         #\n#                                                                   #\n#    1. A runtime directory for this calculation is created         #\n#    2. This list is used to populate a Python virtual environment  #\n#    3. The virtual environment is activated                        #\n#    4. The Python process running the script included within this  #\n#       job is started                                              #\n#                                                                   #\n#  For more information visit:                                      #\n#   - https://pip.pypa.io/en/stable/reference/pip_install           #\n#   - https://virtualenv.pypa.io/en/stable/                         #\n#                                                                   #\n# ----------------------------------------------------------------- #\n\nmatplotlib\nnumpy\nscikit-learn\nxgboost\npandas\n",
                    "schemaVersion": "2022.8.16"
                }
            ]
        }
    ]
}
