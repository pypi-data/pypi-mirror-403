# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import Dict, Union, Iterable, Optional
from typing_extensions import Literal, Required, TypeAlias, TypedDict

from .._types import SequenceNotStr

__all__ = [
    "CompletionCreateParamsBase",
    "Prediction",
    "PredictionPredictedOutput",
    "PredictionPredictedOutputContentUnionMember1",
    "PredictionPredictedOutputContentUnionMember1ImageURL",
    "ResponseFormat",
    "Thinking",
    "ThinkingThinkingConfigEnabled",
    "ThinkingThinkingConfigDisabled",
    "CompletionCreateParamsNonStreaming",
    "CompletionCreateParamsStreaming",
]


class CompletionCreateParamsBase(TypedDict, total=False):
    model: Required[str]
    """The name of the model to use.

    Example: `"accounts/fireworks/models/kimi-k2-instruct-0905"`
    """

    prompt: Required[Union[str, SequenceNotStr[str], Iterable[int], Iterable[Iterable[int]]]]
    """The prompt to generate completions for.

    It can be a single string or an array of strings.

    It can also be an array of integers or an array of integer arrays, which allows
    to pass already tokenized prompt.

    If multiple prompts are specified, several choices with corresponding `index`
    will be returned in the output.
    """

    context_length_exceeded_behavior: Literal["error", "truncate"]
    """
    What to do if the token count of prompt plus `max_tokens` exceeds the model's
    context window.

    Passing `truncate` limits the `max_tokens` to at most
    `context_window_length - prompt_length`. This is the default.

    Passing `error` would trigger a request error.

    The default of `'truncate'` is selected as it allows to ask for high
    `max_tokens` value while respecting the context window length without having to
    do client-side prompt tokenization.

    Note, that it differs from OpenAI's behavior that matches that of `error`.
    """

    echo: Optional[bool]
    """Echo back the prompt in addition to the completion."""

    echo_last: Optional[int]
    """Echo back the last N tokens of the prompt in addition to the completion.

    This is useful for obtaining logprobs of the prompt suffix but without
    transferring too much data. Passing `echo_last=len(prompt)` is the same as
    `echo=True`
    """

    frequency_penalty: Optional[float]
    """Number between -2.0 and 2.0.

    Positive values penalize new tokens based on their existing frequency in the
    text so far, decreasing the model's likelihood to repeat the same line verbatim.

    Reasonable value is around 0.1 to 1 if the aim is to just reduce repetitive
    samples somewhat. If the aim is to strongly suppress repetition, then one can
    increase the coefficients up to 2, but this can noticeably degrade the quality
    of samples. Negative values can be used to increase the likelihood of
    repetition.

    See also `presence_penalty` for penalizing tokens that have at least one
    appearance at a fixed rate.

    OpenAI compatible (follows OpenAI's conventions for handling token frequency and
    repetition penalties).

    Required range: `-2 <= x <= 2`
    """

    ignore_eos: bool
    """
    This setting controls whether the model should ignore the End of Sequence (EOS)
    token. When set to `True`, the model will continue generating tokens even after
    the EOS token is produced. By default, it stops when the EOS token is reached.
    """

    images: Union[SequenceNotStr[str], Iterable[SequenceNotStr[str]], None]
    """The list of base64 encoded images for visual language completition generation.

    They should be formatted as MIME_TYPE,<base64 encoded str>

    eg. data:image/jpeg;base64,<base64 encoded str>

    Additionally, the number of images provided should match the number of '<image>'
    special token in the prompt
    """

    logit_bias: Optional[Dict[str, float]]
    """Modify the likelihood of specified tokens appearing in the completion.

    Accepts a json object that maps tokens (specified by their token ID in the
    tokenizer) to an associated bias value from -100 to 100. Mathematically, the
    bias is added to the logits generated by the model prior to sampling.
    """

    logprobs: Union[int, bool, None]
    """Include log probabilities in the response.

    This accepts either a boolean or an integer:

    If set to `true`, log probabilities are included and the number of alternatives
    can be controlled via `top_logprobs` (OpenAI-compatible behavior).

    If set to an integer N (0-5), include log probabilities for up to N most likely
    tokens per position in the legacy format.

    The API will always return the logprob of the sampled token, so there may be up
    to `logprobs+1` elements in the response when an integer is used. The maximum
    value for the integer form is 5.
    """

    max_completion_tokens: Optional[int]
    """Alias for max_tokens. Cannot be specified together with max_tokens."""

    max_tokens: Optional[int]
    """The maximum number of tokens to generate in the completion.

    If the token count of your prompt plus max_tokens exceeds the model's context
    length, the behavior depends on context_length_exceeded_behavior. By default,
    max_tokens will be lowered to fit in the context window instead of returning an
    error.
    """

    metadata: Optional[Dict[str, str]]
    """Additional metadata to store with the request for tracing/distillation."""

    min_p: Optional[float]
    """Minimum probability threshold for token selection.

    Only tokens with probability >= min_p are considered for selection. This is an
    alternative to `top_p` and `top_k` sampling.

    Required range: `0 <= x <= 1`
    """

    mirostat_lr: Optional[float]
    """
    Specifies the learning rate for the Mirostat sampling algorithm, which controls
    how quickly the model adjusts its token distribution to maintain the target
    perplexity. A smaller value slows down the adjustments, leading to more stable
    but gradual shifts, while higher values speed up corrections at the cost of
    potential instability.
    """

    mirostat_target: Optional[float]
    """Defines the target perplexity for the Mirostat algorithm.

    Perplexity measures the unpredictability of the generated text, with higher
    values encouraging more diverse and creative outputs, while lower values
    prioritize predictability and coherence. The algorithm dynamically adjusts the
    token selection to maintain this target during text generation.

    If not specified, Mirostat sampling is disabled.
    """

    n: int
    """How many completions to generate for each prompt.

    **Note:** Because this parameter generates many completions, it can quickly
    consume your token quota. Use carefully and ensure that you have reasonable
    settings for `max_tokens` and `stop`.

    Required range: `1 <= x <= 128`

    Example: `1`
    """

    perf_metrics_in_response: Optional[bool]
    """Whether to include performance metrics in the response body.

    **Non-streaming requests:** Performance metrics are always included in response
    headers (e.g., `fireworks-prompt-tokens`,
    `fireworks-server-time-to-first-token`). Setting this to `true` additionally
    includes the same metrics in the response body under the `perf_metrics` field.

    **Streaming requests:** Performance metrics are only included in the response
    body under the `perf_metrics` field in the final chunk (when `finish_reason` is
    set). This is because headers may not be accessible during streaming.

    The response body `perf_metrics` field contains the following metrics:

    **Basic Metrics (all deployments):**

    - `prompt-tokens`: Number of tokens in the prompt
    - `cached-prompt-tokens`: Number of cached prompt tokens
    - `server-time-to-first-token`: Time from request start to first token (in
      seconds)
    - `server-processing-time`: Total processing time (in seconds, only for
      completed requests)

    **Predicted Outputs Metrics:**

    - `speculation-prompt-tokens`: Number of speculative prompt tokens
    - `speculation-prompt-matched-tokens`: Number of matched speculative prompt
      tokens (for completed requests)

    **Dedicated Deployment Only Metrics:**

    - `speculation-generated-tokens`: Number of speculative generated tokens (for
      completed requests)
    - `speculation-acceptance`: Speculation acceptance rates by position
    - `backend-host`: Hostname of the backend server
    - `num-concurrent-requests`: Number of concurrent requests
    - `deployment`: Deployment name
    - `tokenizer-queue-duration`: Time spent in tokenizer queue
    - `tokenizer-duration`: Time spent in tokenizer
    - `prefill-queue-duration`: Time spent in prefill queue
    - `prefill-duration`: Time spent in prefill
    - `generation-queue-duration`: Time spent in generation queue
    """

    prediction: Optional[Prediction]
    """OpenAI-compatible predicted output for speculative decoding.

    Can be a PredictedOutput object or a simple string. Automatically transformed to
    speculation.
    """

    presence_penalty: Optional[float]
    """Number between -2.0 and 2.0.

    Positive values penalize new tokens based on whether they appear in the text so
    far, increasing the model's likelihood to talk about new topics.

    Reasonable value is around 0.1 to 1 if the aim is to just reduce repetitive
    samples somewhat. If the aim is to strongly suppress repetition, then one can
    increase the coefficients up to 2, but this can noticeably degrade the quality
    of samples. Negative values can be used to increase the likelihood of
    repetition.

    See also `frequency_penalty` for penalizing tokens at an increasing rate
    depending on how often they appear.

    OpenAI compatible (follows OpenAI's conventions for handling token frequency and
    repetition penalties).

    Required range: `-2 <= x <= 2`
    """

    prompt_cache_isolation_key: Optional[str]
    """Isolation key for prompt caching to separate cache entries."""

    raw_output: Optional[bool]
    """Return raw output from the model."""

    reasoning_effort: Union[Literal["low", "medium", "high", "none"], int, bool, None]
    """Controls reasoning behavior for supported models.

    When enabled, the model's reasoning appears in the `reasoning_content` field of
    the response, separate from the final answer in `content`.

    **Accepted values:**

    - **String** (OpenAI-compatible): `'low'`, `'medium'`, or `'high'` to enable
      reasoning with varying effort levels; `'none'` to disable reasoning.
    - **Boolean** (Fireworks extension): `true` to enable reasoning, `false` to
      disable it.
    - **Integer** (Fireworks extension): A positive integer to set a hard token
      limit on reasoning output (only effective for grammar-based reasoning models).

    **Important:** Boolean values are normalized internally: `true` becomes
    `'medium'`, and `false` becomes `'none'`. This normalization happens before
    model-specific validation, so if a model doesn't support `'none'`, passing
    `false` will produce an error referencing `'none'`.

    **Model-specific behavior:**

    - **Qwen3 (e.g., Qwen3-8B)**: Grammar-based reasoning. Default reasoning on. Use
      `'none'` or `false` to disable. Supports integer token limits to cap reasoning
      output. `'low'` maps to a default token limit (~3000 tokens).
    - **MiniMax M2**: Reasoning is required (always on). Defaults to `'medium'` when
      omitted. Accepts only string `reasoning_effort`: `'low'`, `'medium'`, or
      `'high'`. `'none'` and boolean values are rejected.
    - **DeepSeek V3.1, DeepSeek V3.2**: Binary on/off reasoning. Default reasoning
      on. Use `'none'` or `false` to disable; effort levels and integers have no
      additional effect.
    - **GLM 4.5, GLM 4.5 Air, GLM 4.6, GLM 4.7**: Binary on/off reasoning. Default
      reasoning on. Use `'none'` or `false` to disable; effort levels and integers
      have no additional effect.
    - **Harmony (OpenAI GPT-OSS 120B, GPT-OSS 20B)**: Accepts only `'low'`,
      `'medium'`, or `'high'`. Does not support `'none'`, `false`, or integer values
      â€” using these will return an error (e.g., "Invalid reasoning effort: none").
      When omitted, defaults to `'medium'`. Lower effort produces faster responses
      with shorter reasoning.
    """

    reasoning_history: Optional[Literal["disabled", "interleaved", "preserved"]]
    """
    Controls how historical assistant reasoning content is included in the prompt
    for multi-turn conversations.

    **Accepted values:**

    - `null`: Use model/template default behavior (for **GLM-4.7**, the
      model/template default is `'interleaved'`, i.e. historical reasoning is
      cleared by default)
    - `'disabled'`: Strip `reasoning_content` from all messages before prompt
      construction
    - `'interleaved'`: Strip `reasoning_content` from messages up to (and including)
      the last user message
    - `'preserved'`: Preserve historical `reasoning_content` across the conversation

    **Model support:**

    | Model            | Default         | Supported values                             |
    | ---------------- | --------------- | -------------------------------------------- |
    | Kimi K2 Instruct | `'preserved'`   | `'disabled'`, `'interleaved'`, `'preserved'` |
    | MiniMax M2       | `'interleaved'` | `'disabled'`, `'interleaved'`                |
    | GLM-4.7          | `'interleaved'` | `'disabled'`, `'interleaved'`, `'preserved'` |
    | GLM-4.6          | `'interleaved'` | `'disabled'`, `'interleaved'`                |

    For other models, refer to the model provider's documentation.

    **Note:** This parameter controls prompt formatting only. To disable reasoning
    computation entirely, use `reasoning_effort='none'`.
    """

    repetition_penalty: Optional[float]
    """Applies a penalty to repeated tokens to discourage or encourage repetition.

    A value of `1.0` means no penalty, allowing free repetition. Values above `1.0`
    penalize repetition, reducing the likelihood of repeating tokens. Values between
    `0.0` and `1.0` reward repetition, increasing the chance of repeated tokens. For
    a good balance, a value of `1.2` is often recommended. Note that the penalty is
    applied to both the generated output and the prompt in decoder-only models.

    Required range: `0 <= x <= 2`
    """

    response_format: Optional[ResponseFormat]
    """Allows to force the model to produce specific output format.

    Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the
    message the model generates is valid JSON.

    If `"type"` is `"json_schema"`, a JSON schema must be provided. E.g.,
    `response_format = {"type": "json_schema", "json_schema": <json_schema>}`.

    Important: when using JSON mode, it's crucial to also instruct the model to
    produce JSON via a system or user message. Without this, the model may generate
    an unending stream of whitespace until the generation reaches the token limit,
    resulting in a long-running and seemingly "stuck" request.

    Also note that the message content may be partially cut off if
    `finish_reason="length"`, which indicates the generation exceeded `max_tokens`
    or the conversation exceeded the max context length. In this case the return
    value might not be a valid JSON.
    """

    return_token_ids: Optional[bool]
    """Return token IDs alongside text to avoid retokenization drift."""

    seed: Optional[int]
    """Random seed for deterministic sampling."""

    speculation: Union[str, Iterable[int], None]
    """Speculative decoding prompt or token IDs to speed up generation."""

    stop: Union[str, SequenceNotStr[str], None]
    """Up to 4 sequences where the API will stop generating further tokens.

    The returned text will NOT contain the stop sequence.
    """

    temperature: Optional[float]
    """What sampling temperature to use, between 0 and 2.

    Higher values like 0.8 will make the output more random, while lower values like
    0.2 will make it more focused and deterministic.

    We generally recommend altering this or top_p but not both.

    Required range: `0 <= x <= 2`

    Example: `1`
    """

    thinking: Optional[Thinking]
    """Configuration for enabling extended thinking (Anthropic-compatible format).

    This is an alternative to `reasoning_effort` for controlling reasoning behavior.

    **Format:**

    - `{"type": "enabled"}` - Enable thinking (equivalent to
      `reasoning_effort: true`)
    - `{"type": "enabled", "budget_tokens": <int>}` - Enable thinking with a token
      budget (equivalent to `reasoning_effort: <int>`). Must be >= 1024.
    - `{"type": "disabled"}` - Disable thinking (equivalent to
      `reasoning_effort: "none"`)

    **Note:** Cannot be specified together with `reasoning_effort`. If both are
    provided, a validation error will be raised.
    """

    top_k: Optional[int]
    """
    Top-k sampling is another sampling method where the k most probable next tokens
    are filtered and the probability mass is redistributed among only those k next
    tokens. The value of k controls the number of candidates for the next token at
    each step during text generation. Must be between 0 and 100.

    Required range: `0 <= x <= 100`

    Example: `50`
    """

    top_logprobs: Optional[int]
    """
    An integer between 0 and 5 specifying the number of most likely tokens to return
    at each token position, each with an associated log probability. The minimum
    value is 0 and the maximum value is 5.

    When `logprobs` is set, `top_logprobs` can be used to modify how many top log
    probabilities are returned. If `top_logprobs` is not set, the API will return up
    to `logprobs` tokens per position.

    Required range: `0 <= x <= 5`
    """

    top_p: Optional[float]
    """
    An alternative to sampling with temperature, called nucleus sampling, where the
    model considers the results of the tokens with top_p probability mass. So 0.1
    means only the tokens comprising the top 10% probability mass are considered.

    We generally recommend altering this or temperature but not both.

    Required range: `0 <= x <= 1`

    Example: `1`
    """

    typical_p: Optional[float]
    """Typical-p sampling is an alternative to nucleus sampling.

    It considers the most typical tokens whose cumulative probability is at most
    typical_p.

    Required range: `0 <= x <= 1`
    """

    user: Optional[str]
    """
    A unique identifier representing your end-user, which can help monitor and
    detect abuse.
    """


class PredictionPredictedOutputContentUnionMember1ImageURL(TypedDict, total=False):
    url: Required[str]

    detail: Optional[str]


class PredictionPredictedOutputContentUnionMember1(TypedDict, total=False):
    type: Required[str]

    image_url: Optional[PredictionPredictedOutputContentUnionMember1ImageURL]

    text: Optional[str]


class PredictionPredictedOutput(TypedDict, total=False):
    """OpenAI-compatible struct for the "speculation" field."""

    content: Required[Union[str, Iterable[PredictionPredictedOutputContentUnionMember1]]]

    type: Literal["content"]


Prediction: TypeAlias = Union[PredictionPredictedOutput, str]


class ResponseFormat(TypedDict, total=False):
    """Allows to force the model to produce specific output format.

    Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

    If `"type"` is `"json_schema"`, a JSON schema must be provided. E.g., `response_format = {"type": "json_schema", "json_schema": <json_schema>}`.

    Important: when using JSON mode, it's crucial to also instruct the model to produce JSON via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request.

    Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length. In this case the return value might not be a valid JSON.
    """

    type: Required[Literal["json_object", "json_schema", "grammar", "text"]]

    grammar: Optional[str]

    json_schema: Union[str, Dict[str, object], None]

    schema: Union[str, Dict[str, object], None]


class ThinkingThinkingConfigEnabled(TypedDict, total=False):
    """Configuration for enabling extended thinking (Anthropic-compatible format)."""

    type: Required[Literal["enabled"]]

    budget_tokens: Optional[int]
    """Determines how many tokens the model can use for its internal reasoning process.

    Larger budgets can enable more thorough analysis for complex problems, improving
    response quality. Must be >= 1024 if specified.
    """


class ThinkingThinkingConfigDisabled(TypedDict, total=False):
    """Configuration for disabling extended thinking (Anthropic-compatible format)."""

    type: Required[Literal["disabled"]]


Thinking: TypeAlias = Union[ThinkingThinkingConfigEnabled, ThinkingThinkingConfigDisabled]


class CompletionCreateParamsNonStreaming(CompletionCreateParamsBase, total=False):
    stream: Optional[Literal[False]]
    """Whether to stream back partial progress.

    If set, tokens will be sent as data-only
    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
    as they become available, with the stream terminated by a `data: [DONE]`
    message.
    """


class CompletionCreateParamsStreaming(CompletionCreateParamsBase):
    stream: Required[Literal[True]]
    """Whether to stream back partial progress.

    If set, tokens will be sent as data-only
    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
    as they become available, with the stream terminated by a `data: [DONE]`
    message.
    """


CompletionCreateParams = Union[CompletionCreateParamsNonStreaming, CompletionCreateParamsStreaming]
