# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import builtins
from typing import Dict, List, Union, Optional
from typing_extensions import Literal, TypeAlias

from ..._models import BaseModel
from ..shared.log_probs import LogProbs
from ..shared.raw_output import RawOutput
from ..shared.usage_info import UsageInfo
from ..shared.new_log_probs import NewLogProbs
from ..shared.chat_completion_message_tool_call import ChatCompletionMessageToolCall

__all__ = ["ChatCompletionChunk", "Choice", "ChoiceDelta", "ChoiceLogprobs"]


class ChoiceDelta(BaseModel):
    """The message delta"""

    content: Optional[str] = None
    """The contents of the chunk message"""

    reasoning_content: Optional[str] = None
    """The reasoning or thinking process generated by the model.

    This field is only available for certain reasoning models (GLM 4.5, GLM 4.5 Air,
    GPT OSS 120B, GPT OSS 20B) and contains the model's internal reasoning that
    would otherwise appear in `<think></think>` tags within the content field.
    """

    role: Optional[str] = None
    """The role of the author of this message"""

    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None


ChoiceLogprobs: TypeAlias = Union[LogProbs, NewLogProbs, None]


class Choice(BaseModel):
    """A streamed chat completion choice."""

    delta: ChoiceDelta
    """The message delta"""

    index: int
    """The index of the chat completion choice"""

    finish_reason: Optional[Literal["stop", "length", "function_call", "tool_calls"]] = None
    """The reason the model stopped generating tokens.

    This will be "stop" if the model hit a natural stop point or a provided stop
    sequence, or "length" if the maximum number of tokens specified in the request
    was reached
    """

    logprobs: Optional[ChoiceLogprobs] = None
    """Legacy log probabilities format"""

    prompt_token_ids: Optional[List[int]] = None
    """Token IDs for the prompt (when return_token_ids=true)"""

    raw_output: Optional[RawOutput] = None
    """
    Extension of OpenAI that returns low-level interaction of what the model sees,
    including the formatted prompt and function calls
    """

    token_ids: Optional[List[int]] = None
    """Token IDs for this chunk (when return_token_ids=true)"""


class ChatCompletionChunk(BaseModel):
    """The streamed response message from a /v1/chat/completions call."""

    id: str
    """A unique identifier of the response"""

    choices: List[Choice]
    """The list of streamed chat completion choices"""

    created: int
    """The Unix time in seconds when the response was generated"""

    model: str
    """The model used for the chat completion"""

    object: Optional[str] = None
    """The object type, which is always "chat.completion.chunk" """

    perf_metrics: Optional[Dict[str, builtins.object]] = None
    """See parameter [perf_metrics_in_response](#body-perf-metrics-in-response)"""

    prompt_token_ids: Optional[List[int]] = None
    """Token IDs for the prompt (when return_token_ids=true, sent in first chunk)"""

    usage: Optional[UsageInfo] = None
    """Usage statistics."""
