"""
CI acceptance backtests (ThetaData) â€” runs the *same Strategy Library demo scripts* we use locally.

User requirement (non-negotiable):
- These tests must execute the Strategy Library acceptance demos (copied verbatim into
  `tests/backtest/acceptance_strategies/`) in a subprocess, with the same prod-like env flags.
- They must FAIL if any run tries to use the remote Data Downloader / queue (warm S3 cache invariant).

Implementation notes:
- We run each script in an isolated run directory so that `logs/` is clean and parseable.
- Expected metrics (Total Return / CAGR% / Max Drawdown) come from
  `tests/backtest/acceptance_backtests_baselines.json` (generated from Strategy Library `logs/`).
  We assert these *strictly* (0.01% resolution) to catch even small correctness drift.
"""

from __future__ import annotations

import json
import os
import re
import subprocess
import sys
import time
import uuid
from dataclasses import dataclass
from datetime import date, timedelta
from decimal import Decimal
from pathlib import Path

import pandas as pd
import pytest

# Acceptance backtests are regular backtests that must be queue-free (warm S3 invariant)
# and run in CI.
pytestmark = [pytest.mark.acceptance_backtest]

# Headline metrics are written at 0.01% resolution in `*_tearsheet.csv`.
# We keep CI strict by default and only allow a 0.01% tolerance to avoid rare float->string
# edge cases while still catching any meaningful correctness drift.
_METRIC_TOLERANCE_CENTIPERCENT = 1


def _is_ci() -> bool:
    return (os.environ.get("GITHUB_ACTIONS", "").lower() == "true") or bool(os.environ.get("CI"))


def _require_env(keys: list[str]) -> None:
    missing = [k for k in keys if not os.environ.get(k)]
    if not missing:
        return
    message = f"Missing required env vars for acceptance backtests: {missing}"
    if _is_ci():
        pytest.fail(message)
    pytest.skip(message)


def _centipercent(text: str) -> int:
    """
    Convert percent strings into *centipercent* integers (0.01% units).

    Examples:
    - "48.86%" -> 4886
    - "-17%" -> -1700
    - "8,585%" -> 858500

    Notes:
    - We intentionally assert at 0.01% resolution (the tearsheet CSV is written at this granularity).
    - Anything finer is not representable and should be treated as a serialization bug.
    """
    s = str(text).strip()
    if not s.endswith("%"):
        raise ValueError(f"Expected percent string ending with '%', got {text!r}")
    s = s[:-1].replace(",", "").strip()
    scaled = Decimal(s) * Decimal("100")
    if scaled != scaled.to_integral_value():
        raise ValueError(f"Percent value {text!r} is not representable at 0.01% resolution.")
    return int(scaled)


def _read_tearsheet_metrics_centipercent(tearsheet_csv: Path) -> dict[str, int]:
    df = pd.read_csv(tearsheet_csv)
    if "Metric" not in df.columns or "Strategy" not in df.columns:
        raise AssertionError(f"Unexpected tearsheet CSV columns: {list(df.columns)}")

    def _get(metric_name: str) -> int:
        row = df.loc[df["Metric"] == metric_name]
        if row.empty:
            raise AssertionError(f"Missing metric {metric_name!r} in {tearsheet_csv}")
        return _centipercent(row["Strategy"].iloc[0])

    return {
        "total_return": _get("Total Return"),
        "cagr": _get("CAGR% (Annual Return)"),
        "max_drawdown": _get("Max Drawdown"),
    }


def _find_single(paths: list[Path], description: str) -> Path:
    if len(paths) != 1:
        raise AssertionError(f"Expected exactly 1 {description}, found {len(paths)}: {[p.name for p in paths]}")
    return paths[0]


def _base_env(repo_root: Path) -> dict[str, str]:
    env = dict(os.environ)
    env.update(
        {
            # The acceptance subprocess should behave like GitHub CI (where CI=true is always set),
            # so any CI-only guardrails in the data path are consistently exercised locally too.
            "CI": "true",
            "IS_BACKTESTING": "True",
            # Acceptance backtests are intended to validate ThetaData + downloader + S3 warm-cache
            # behavior. Many Strategy Library demo scripts default to Polygon for minute-level runs,
            # so force ThetaData here regardless of the script's `datasource_class=` argument.
            "BACKTESTING_DATA_SOURCE": "thetadata",
            "SHOW_PLOT": "False",
            "SHOW_INDICATORS": "False",
            # Never open the tearsheet in a browser during tests.
            "SHOW_TEARSHEET": "False",
            "BACKTESTING_QUIET_LOGS": "false",
            "BACKTESTING_SHOW_PROGRESS_BAR": "false",
            "SAVE_LOGFILE": env.get("SAVE_LOGFILE", "true"),
            # Match Strategy Library/Demos/.env (prod-like acceptance flags).
            "LUMIBOT_CACHE_BACKEND": "s3",
            "LUMIBOT_CACHE_MODE": "readwrite",
            # Default to the CI-provided cache namespace when available (keeps CI + local aligned
            # with the current shared warm-cache version). Fall back to the historical ThetaData
            # acceptance namespace for local/dev runs that don't set the secret.
            "LUMIBOT_CACHE_S3_VERSION": env.get("LUMIBOT_CACHE_S3_VERSION", "v44"),
            "THETADATA_USE_QUEUE": "true",
            "DATADOWNLOADER_API_KEY_HEADER": env.get("DATADOWNLOADER_API_KEY_HEADER", "X-Downloader-Key"),
            "DATADOWNLOADER_SKIP_LOCAL_START": env.get("DATADOWNLOADER_SKIP_LOCAL_START", "true"),
        }
    )

    # Ensure we always import the checked-out source tree (even when running in a temp cwd).
    env["PYTHONPATH"] = f"{repo_root}:{env.get('PYTHONPATH', '')}".strip(":")
    env.setdefault("PYTHONHASHSEED", "0")
    return env


@dataclass(frozen=True)
class _BaselineCase:
    slug: str
    strategy_name: str
    script_filename: str
    start_date: str  # BACKTESTING_START (YYYY-MM-DD)
    end_date: str  # BACKTESTING_END (YYYY-MM-DD, exclusive)
    data_source: str
    baseline_run_id: str
    expected_metrics_centipercent: dict[str, int]
    baseline_backtest_time_seconds: float | None
    max_backtest_time_seconds: int


def _repo_root() -> Path:
    return Path(__file__).resolve().parents[2]


def _baselines_path(repo_root: Path) -> Path:
    return repo_root / "tests" / "backtest" / "acceptance_backtests_baselines.json"


def _load_baselines() -> dict[str, _BaselineCase]:
    repo_root = _repo_root()
    path = _baselines_path(repo_root)
    payload = json.loads(path.read_text(encoding="utf-8"))
    cases = payload.get("cases") or []

    out: dict[str, _BaselineCase] = {}
    for raw in cases:
        slug = str(raw["slug"])
        if slug in out:
            raise AssertionError(f"Duplicate baseline slug in {path}: {slug}")
        out[slug] = _BaselineCase(
            slug=slug,
            strategy_name=str(raw["strategy_name"]),
            script_filename=str(raw["script_filename"]),
            start_date=str(raw["start_date"]),
            end_date=str(raw["end_date"]),
            data_source=str(raw["data_source"]),
            baseline_run_id=str(raw["baseline_run_id"]),
            expected_metrics_centipercent=dict(raw["metrics_centipercent"]),
            baseline_backtest_time_seconds=raw.get("backtest_time_seconds"),
            max_backtest_time_seconds=int(raw["max_backtest_time_seconds"]),
        )

    if not out:
        raise AssertionError(f"No baseline cases found in {path}")
    return out


_BASELINES_BY_SLUG = _load_baselines()


def _baseline(slug: str) -> _BaselineCase:
    try:
        return _BASELINES_BY_SLUG[slug]
    except KeyError as exc:
        raise AssertionError(f"Unknown acceptance baseline slug {slug!r}. Update {_baselines_path(_repo_root())}.") from exc


def _runs_root(repo_root: Path) -> Path:
    return repo_root / "tests" / "backtest" / "_acceptance_runs"


def _expected_settings_end_date(end_date_exclusive: str) -> str:
    # LumiBot treats BACKTESTING_END as exclusive and writes backtesting_end as (end-1day) at 23:59.
    end = date.fromisoformat(end_date_exclusive)
    return (end - timedelta(days=1)).isoformat()


def _assert_settings_match_window(case: _BaselineCase, payload: dict[str, object]) -> None:
    start = str(payload.get("backtesting_start") or "")
    end = str(payload.get("backtesting_end") or "")

    if not start.startswith(case.start_date):
        raise AssertionError(f"{case.slug}: settings backtesting_start={start!r} does not start with {case.start_date!r}")

    expected_end_date = _expected_settings_end_date(case.end_date)
    if not re.match(rf"^{re.escape(expected_end_date)}\s+23:59:00", end):
        raise AssertionError(
            f"{case.slug}: settings backtesting_end={end!r} does not match expected date {expected_end_date!r} @ 23:59:00"
        )


def _require_acceptance_env(case: _BaselineCase) -> None:
    required_s3 = [
        "LUMIBOT_CACHE_S3_BUCKET",
        "LUMIBOT_CACHE_S3_PREFIX",
        "LUMIBOT_CACHE_S3_REGION",
        "LUMIBOT_CACHE_S3_ACCESS_KEY_ID",
        "LUMIBOT_CACHE_S3_SECRET_ACCESS_KEY",
    ]
    required_downloader = [
        "DATADOWNLOADER_BASE_URL",
        "DATADOWNLOADER_API_KEY",
    ]
    required_thetadata_creds = [
        "THETADATA_USERNAME",
        "THETADATA_PASSWORD",
    ]

    if case.data_source == "thetadata":
        _require_env(required_thetadata_creds + required_downloader + required_s3)
        return

    if case.data_source == "ibkr":
        # IBKR acceptance runs are still cache-backed (warm S3 invariant). They should be able to
        # run without touching the downloader, but we still require downloader wiring so any
        # accidental network usage fails loudly and is actionable.
        _require_env(required_downloader + required_s3)
        return

    # Other data sources (e.g. yahoo) don't require downloader/cache secrets, but they still require
    # non-empty ThetaData credentials due to Strategy.backtest() validation in shared code paths.
    _require_env(required_thetadata_creds)


def _run_subprocess(
    *,
    cmd: list[str],
    cwd: Path,
    env: dict[str, str],
    stdout_path: Path,
    stderr_path: Path,
    timeout_s: int,
) -> int:
    """Run a subprocess and stream stdout/stderr to files (no log-scraping)."""
    with stdout_path.open("w", encoding="utf-8") as stdout_file, stderr_path.open("w", encoding="utf-8") as stderr_file:
        try:
            proc = subprocess.run(
                cmd,
                cwd=str(cwd),
                env=env,
                stdout=stdout_file,
                stderr=stderr_file,
                timeout=timeout_s,
                check=False,
            )
        except subprocess.TimeoutExpired as exc:
            raise AssertionError(f"Acceptance backtest timed out after {timeout_s}s. run_dir={cwd}") from exc
    return int(proc.returncode)


def _run_script(case: _BaselineCase) -> tuple[Path, dict[str, int]]:
    repo_root = Path(__file__).resolve().parents[2]
    _require_acceptance_env(case)

    script_path = repo_root / "tests" / "backtest" / "acceptance_strategies" / case.script_filename
    assert script_path.exists(), f"Missing strategy script: {script_path}"

    runs_root = _runs_root(repo_root)
    runs_root.mkdir(parents=True, exist_ok=True)

    run_id = f"{case.slug}_{time.strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    run_dir = runs_root / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    (run_dir / "logs").mkdir(parents=True, exist_ok=True)

    env = _base_env(repo_root)
    # CI runners start with an empty disk cache. Local developer machines often have a warm cache
    # under the default appdirs location, which can hide missing S3 objects and cause CI-only
    # downloader usage. Force an isolated cache root per run so local == CI.
    env["LUMIBOT_CACHE_FOLDER"] = str(run_dir / "cache")
    env["BACKTESTING_START"] = case.start_date
    env["BACKTESTING_END"] = case.end_date
    env["BACKTESTING_DATA_SOURCE"] = case.data_source
    if case.data_source == "ibkr":
        # IBKR acceptance is currently staged on the v2 cache namespace (conid registry + warm bars).
        # Keeping ThetaData acceptance on v44 avoids churn for the existing CI baselines.
        env["LUMIBOT_CACHE_S3_VERSION"] = "v2"

    stdout_path = run_dir / "stdout.txt"
    stderr_path = run_dir / "stderr.txt"

    max_inner_s = float(case.max_backtest_time_seconds)
    # Hard kill timeout (outer) to prevent silent hangs. The strict regression gate is asserted
    # on `backtest_time_seconds` from settings.json (inner timer).
    timeout_s = int(max(60.0, max_inner_s + 600.0))

    returncode = _run_subprocess(
        cmd=[sys.executable, str(script_path)],
        cwd=run_dir,
        env=env,
        stdout_path=stdout_path,
        stderr_path=stderr_path,
        timeout_s=timeout_s,
    )

    if returncode != 0:
        tail = ""
        try:
            tail = (stderr_path.read_text(errors="ignore") + "\n" + stdout_path.read_text(errors="ignore"))[-8000:]
        except Exception:
            tail = "(failed to read stdout/stderr tail)"
        raise AssertionError(f"{case.slug} failed (exit={returncode}). run_dir={run_dir}\n--- tail ---\n{tail}")

    logs_dir = run_dir / "logs"
    settings = _find_single(
        sorted(logs_dir.glob(f"{case.strategy_name}_*_settings.json")),
        f"{case.strategy_name} settings.json",
    )
    tearsheet_csv = _find_single(
        sorted(logs_dir.glob(f"{case.strategy_name}_*_tearsheet.csv")),
        f"{case.strategy_name} tearsheet.csv",
    )

    # Artifact sanity
    _find_single(sorted(logs_dir.glob(f"{case.strategy_name}_*_trades.csv")), f"{case.strategy_name} trades.csv")
    _find_single(sorted(logs_dir.glob(f"{case.strategy_name}_*_logs.csv")), f"{case.strategy_name} logs.csv")

    metrics = _read_tearsheet_metrics_centipercent(tearsheet_csv)

    expected = case.expected_metrics_centipercent
    mismatches: list[str] = []
    for key in ("total_return", "cagr", "max_drawdown"):
        actual = int(metrics[key])
        exp = int(expected[key])
        if abs(actual - exp) > _METRIC_TOLERANCE_CENTIPERCENT:
            mismatches.append(f"{key}: actual={actual} expected={exp} (tol={_METRIC_TOLERANCE_CENTIPERCENT})")
    if mismatches:
        raise AssertionError(
            f"{case.slug} metrics mismatch (centipercent) "
            f"(baseline_run_id={case.baseline_run_id})\n"
            + "\n".join(mismatches)
            + "\n"
            + f"actual_metrics_centipercent={metrics}\n"
            + f"expected_metrics_centipercent={expected}\n"
            + f"tearsheet={tearsheet_csv}\nrun_dir={run_dir}"
        )

    payload = json.loads(settings.read_text(encoding="utf-8"))
    _assert_settings_match_window(case, payload)

    # Structural (non-log-based) validation: ThetaData acceptance backtests must not touch the
    # downloader/queue because S3 is expected to already be warm for these canonical windows.
    #
    # NOTE: IBKR acceptance currently allows small conid discovery calls (secdef/search) while
    # the conid registry/backfill is being operationalized.
    if case.data_source in {"thetadata"}:
        queue = payload.get("thetadata_queue_telemetry") or {}
        try:
            submit_requests = int(queue.get("submit_requests") or 0)
        except Exception:
            submit_requests = 0
        if submit_requests:
            first_path = queue.get("first_request_path")
            first_param_keys = queue.get("first_request_param_keys")
            first_params = queue.get("first_request_params")
            raise AssertionError(
                f"{case.slug} attempted {submit_requests} downloader queue submission(s) "
                f"(first_request_path={first_path!r}, "
                f"first_request_param_keys={first_param_keys!r}, "
                f"first_request_params={first_params!r}). Expected fully warm S3 cache.\n"
                f"settings={settings}\nrun_dir={run_dir}"
            )

    inner_s = payload.get("backtest_time_seconds")
    if isinstance(inner_s, (int, float)) and inner_s > max_inner_s:
        raise AssertionError(
            f"{case.slug} backtest_time_seconds regression: actual={inner_s:.1f}s max={max_inner_s:.1f}s "
            f"(baseline={case.baseline_backtest_time_seconds})\nsettings={settings}\nrun_dir={run_dir}"
        )

    return run_dir, metrics


def test_acceptance_aapl_deep_dip_calls() -> None:
    _run_script(_baseline("aapl_deep_dip_calls"))


def test_acceptance_leaps_alpha_picks() -> None:
    # Short window: must trade UBER/CLS/MFC (both legs). Metrics are annualized; we still assert strictly
    # against the baseline tearsheet, because this is deterministic given fixed data and code.
    short = _baseline("leaps_alpha_picks_short")
    run_dir, _ = _run_script(short)

    # Verify required tickers traded (both legs show up in trades.csv).
    trades_csv = _find_single(
        sorted((run_dir / "logs").glob(f"{short.strategy_name}_*_trades.csv")),
        "Leaps trades.csv",
    )
    trades = pd.read_csv(trades_csv)
    symbols = set(str(s).upper() for s in trades.get("symbol", pd.Series(dtype=str)).dropna().tolist())
    for required in ("UBER", "CLS", "MFC"):
        assert required in symbols, f"Expected {required} to be traded in short window; got symbols={sorted(symbols)[:25]}"


def test_acceptance_tqqq_sma200() -> None:
    _run_script(_baseline("tqqq_sma200_thetadata"))


def test_acceptance_backdoor_butterfly() -> None:
    _run_script(_baseline("backdoor_butterfly_full_year"))


def test_acceptance_meli_deep_drawdown() -> None:
    _run_script(_baseline("meli_deep_drawdown"))


def test_acceptance_backdoor_smartlimit() -> None:
    _run_script(_baseline("backdoor_smartlimit"))


def test_acceptance_spx_short_straddle() -> None:
    _run_script(_baseline("spx_short_straddle_repro"))


def test_acceptance_ibkr_crypto_btc_usd() -> None:
    _run_script(_baseline("ibkr_crypto_acceptance_btc_usd"))


def test_acceptance_ibkr_mes_futures_acceptance() -> None:
    _run_script(_baseline("ibkr_mes_futures_acceptance"))
