Metadata-Version: 2.3
Name: jabs-vision
Version: 0.1.0
Summary: 
Author: Glen Beane, Alexander Berger-Liedtka, Keith Sheppard
License: Proprietary
Requires-Dist: jabs-io
Requires-Dist: jabs-core
Requires-Dist: hydra-core>=1.3.2
Requires-Dist: torch>=2.9.1
Requires-Dist: torchvision>=0.24.1
Requires-Dist: mlflow>=3.8.1 ; extra == 'mlflow'
Requires-Dist: matplotlib>=3.10.8 ; extra == 'plot'
Requires-Dist: submitit>=1.5.4 ; extra == 'submitit'
Requires-Dist: timm>=1.0.0 ; extra == 'timm'
Requires-Dist: wandb>=0.24.0 ; extra == 'wandb'
Requires-Python: >=3.10, <3.15
Project-URL: Repository, https://github.com/KumarLabJax/JABS-behavior-classifier
Project-URL: Issues, https://github.com/KumarLabJax/JABS-behavior-classifier/issues
Provides-Extra: mlflow
Provides-Extra: plot
Provides-Extra: submitit
Provides-Extra: timm
Provides-Extra: wandb
Description-Content-Type: text/markdown

# JABS Vision (`jabs-vision`)

This package handles raw video processing and deep learning training and inference.

## Overview

`jabs-vision` is responsible for converting raw video frames into pose estimation data.
It houses the heavy machine learning frameworks and GPU-accelerated code.

## Responsibilities

- **Pose Estimation Inference**: Running deep learning models (PyTorch) on video frames
  to detect keypoints.
- **Static Object Detection**
- **Segmentation Masking**
- **Identity Matching**: Tracking individual animals across frames using vectorized
  features.
- **Video Processing**: Handling frame extraction and normalization for input to vision
  models.

## Key Components

- `jabs.vision.inference`: Wrappers for pose estimation model execution.
- `jabs.vision.tracking`: Identity matching and re-identification logic.

## Dependencies

- `torch` / `torchvision`
- `opencv-python-headless`
- `jabs-io`, `jabs-core`
