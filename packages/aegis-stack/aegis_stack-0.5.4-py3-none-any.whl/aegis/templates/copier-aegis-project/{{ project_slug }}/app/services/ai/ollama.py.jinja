{%- if ollama_mode != "none" -%}
"""
Ollama API client for health checks and model discovery.

Provides a lightweight client for interacting with local Ollama server.
This module is separate from ETL to ensure availability regardless of AI backend.
"""

from datetime import datetime

import httpx
from pydantic import BaseModel, computed_field

from app.core.log import logger

# Default Ollama server URL for local development
OLLAMA_DEFAULT_URL = "http://localhost:11434"


class OllamaModelDetails(BaseModel):
    """Model details from Ollama API."""

    parent_model: str = ""
    format: str  # e.g., "gguf"
    family: str  # e.g., "qwen2", "llama"
    families: list[str] = []  # e.g., ["qwen2"]
    parameter_size: str  # e.g., "7.6B", "14.8B"
    quantization_level: str  # e.g., "Q4_K_M", "Q8_0"


class OllamaModel(BaseModel):
    """Model data from Ollama's /api/tags endpoint."""

    name: str  # Model name (e.g., "qwen2.5:7b")
    model: str  # Same as name
    size: int  # Model size in bytes (on disk)
    digest: str  # Model digest/hash
    modified_at: datetime  # When model was last modified
    details: OllamaModelDetails

    @computed_field
    @property
    def size_gb(self) -> float:
        """Get model size in gigabytes."""
        return round(self.size / (1024**3), 2)

    @computed_field
    @property
    def model_id(self) -> str:
        """Get model ID for catalog - keeps full name including tag."""
        return self.name


class OllamaRunningModel(BaseModel):
    """Model data from Ollama's /api/ps endpoint (running models)."""

    name: str  # Model name (e.g., "qwen2.5:7b")
    model: str  # Same as name
    size: int  # Model size in bytes
    size_vram: int  # VRAM currently used by this model
    digest: str  # Model digest/hash
    details: OllamaModelDetails
    expires_at: datetime  # When model will be unloaded if idle
    context_length: int = 0  # Context window size

    @computed_field
    @property
    def size_vram_gb(self) -> float:
        """Get VRAM usage in gigabytes."""
        return round(self.size_vram / (1024**3), 2)

    @computed_field
    @property
    def is_warm(self) -> bool:
        """Check if model is warm (loaded and ready)."""
        return True  # If it's in /api/ps, it's warm


class OllamaTagsResponse(BaseModel):
    """Response from /api/tags endpoint."""

    models: list[OllamaModel] = []


class OllamaPsResponse(BaseModel):
    """Response from /api/ps endpoint."""

    models: list[OllamaRunningModel] = []


class OllamaVersionResponse(BaseModel):
    """Response from /api/version endpoint."""

    version: str


class OllamaServerStatus(BaseModel):
    """Comprehensive status information from Ollama server."""

    available: bool  # Server is reachable
    version: str | None = None  # Server version
    running_models: list[OllamaRunningModel] = []  # Currently loaded models
    installed_models: list[OllamaModel] = []  # All installed models
    total_vram_gb: float = 0.0  # Total VRAM used by running models

    @computed_field
    @property
    def installed_models_count(self) -> int:
        """Get count of installed models."""
        return len(self.installed_models)

    @computed_field
    @property
    def running_models_count(self) -> int:
        """Get count of running models."""
        return len(self.running_models)


class OllamaClient:
    """Client for fetching model data from local Ollama server."""

    TIMEOUT = 10.0  # Local server should respond quickly

    def __init__(self, base_url: str = OLLAMA_DEFAULT_URL) -> None:
        """Initialize the Ollama client.

        Args:
            base_url: Base URL for Ollama server (default: OLLAMA_DEFAULT_URL)
        """
        self.base_url = base_url.rstrip("/")

    async def fetch_version(self) -> str | None:
        """Fetch Ollama server version.

        Returns:
            Version string or None if unavailable.
        """
        url = f"{self.base_url}/api/version"

        try:
            async with httpx.AsyncClient(timeout=self.TIMEOUT) as client:
                response = await client.get(url)
                response.raise_for_status()
                data = OllamaVersionResponse.model_validate(response.json())
                return data.version
        except Exception as e:
            logger.debug(f"Failed to fetch Ollama version: {e}")
            return None

    async def fetch_models(self) -> list[OllamaModel]:
        """Fetch all installed models from Ollama.

        Returns:
            List of OllamaModel objects for locally installed models.

        Raises:
            httpx.HTTPError: If the request fails.
            httpx.ConnectError: If Ollama server is not running.
        """
        url = f"{self.base_url}/api/tags"

        try:
            async with httpx.AsyncClient(timeout=self.TIMEOUT) as client:
                response = await client.get(url)
                response.raise_for_status()
                data = OllamaTagsResponse.model_validate(response.json())
                logger.info(f"Fetched {len(data.models)} models from Ollama")
                return data.models
        except httpx.ConnectError as e:
            logger.error(f"Cannot connect to Ollama at {self.base_url}: {e}")
            raise

    async def is_available(self) -> bool:
        """Check if Ollama server is running and accessible.

        Returns:
            True if Ollama is available, False otherwise.
        """
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except Exception:
            return False

    async def fetch_running_models(self) -> list[OllamaRunningModel]:
        """Fetch currently running (loaded) models from Ollama.

        Uses the /api/ps endpoint to get models currently in memory.

        Returns:
            List of OllamaRunningModel objects for loaded models.

        Raises:
            httpx.HTTPError: If the request fails.
            httpx.ConnectError: If Ollama server is not running.
        """
        url = f"{self.base_url}/api/ps"

        try:
            async with httpx.AsyncClient(timeout=self.TIMEOUT) as client:
                response = await client.get(url)
                response.raise_for_status()
                data = OllamaPsResponse.model_validate(response.json())
                return data.models
        except httpx.ConnectError as e:
            logger.error(f"Cannot connect to Ollama at {self.base_url}: {e}")
            raise

    async def load_model(self, model_name: str, keep_alive: str = "10m") -> bool:
        """Load a model into VRAM (warm it up).

        Uses the /api/generate endpoint with an empty prompt to load
        the model into memory without generating text.

        Args:
            model_name: Name of the model to load (e.g., 'qwen2.5:7b')
            keep_alive: How long to keep the model in memory (default: '10m')

        Returns:
            True if model was loaded successfully, False otherwise.
        """
        url = f"{self.base_url}/api/generate"

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    url,
                    json={
                        "model": model_name,
                        "prompt": "",
                        "keep_alive": keep_alive,
                    },
                )
                response.raise_for_status()
                logger.info(f"Successfully loaded model: {model_name}")
                return True
        except httpx.ConnectError as e:
            logger.error(f"Cannot connect to Ollama at {self.base_url}: {e}")
            return False
        except httpx.HTTPStatusError as e:
            logger.error(f"Failed to load model {model_name}: {e}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error loading model {model_name}: {e}")
            return False

    async def unload_model(self, model_name: str) -> bool:
        """Unload a model from VRAM (free up memory).

        Uses the /api/generate endpoint with keep_alive=0 to immediately
        unload the model from memory.

        Args:
            model_name: Name of the model to unload (e.g., 'qwen2.5:7b')

        Returns:
            True if model was unloaded successfully, False otherwise.
        """
        url = f"{self.base_url}/api/generate"

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    url,
                    json={
                        "model": model_name,
                        "prompt": "",
                        "keep_alive": 0,
                    },
                )
                response.raise_for_status()
                logger.info(f"Successfully unloaded model: {model_name}")
                return True
        except httpx.ConnectError as e:
            logger.error(f"Cannot connect to Ollama at {self.base_url}: {e}")
            return False
        except httpx.HTTPStatusError as e:
            logger.error(f"Failed to unload model {model_name}: {e}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error unloading model {model_name}: {e}")
            return False

    async def get_server_status(self) -> OllamaServerStatus:
        """Get comprehensive Ollama server status.

        Returns:
            OllamaServerStatus with availability, running models, and metrics.
        """
        try:
            # Check availability first
            available = await self.is_available()
            if not available:
                return OllamaServerStatus(available=False)

            # Fetch all data in parallel-ish (could use asyncio.gather)
            version = await self.fetch_version()
            running_models = await self.fetch_running_models()
            installed_models = await self.fetch_models()

            # Calculate total VRAM usage
            total_vram_gb = sum(m.size_vram_gb for m in running_models)

            return OllamaServerStatus(
                available=True,
                version=version,
                running_models=running_models,
                installed_models=installed_models,
                total_vram_gb=round(total_vram_gb, 2),
            )

        except Exception as e:
            logger.error(f"Failed to get Ollama server status: {e}")
            return OllamaServerStatus(available=False)
{%- else -%}
# Ollama client not included - ollama_mode is "none"
{%- endif %}
