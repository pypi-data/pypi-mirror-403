"""
AI provider factory for creating AI model instances.

{% if ai_framework == "pydantic-ai" %}
Uses PydanticAI agents with support for multiple providers.
{% else %}
Uses LangChain chat models with support for multiple providers.
{% endif %}
"""
{% if ai_framework == "pydantic-ai" %}

import json
import os
from typing import Any

import httpx
from pydantic_ai import Agent
from pydantic_ai.settings import ModelSettings

from app.core.log import logger

from .config import AIServiceConfig
from .models import AIProvider


# Lazy loading of provider model classes to avoid import errors
def _get_model_class(provider: AIProvider):
    """Get model class for provider with lazy import to avoid dependency issues."""
    if provider == AIProvider.OPENAI:
        from pydantic_ai.models.openai import OpenAIChatModel

        return OpenAIChatModel
    elif provider == AIProvider.ANTHROPIC:
        from pydantic_ai.models.anthropic import AnthropicModel

        return AnthropicModel
    elif provider == AIProvider.GOOGLE:
        from pydantic_ai.models.google import GoogleModel

        return GoogleModel
    elif provider == AIProvider.GROQ:
        from pydantic_ai.models.groq import GroqModel

        return GroqModel
    elif provider == AIProvider.MISTRAL:
        from pydantic_ai.models.openai import OpenAIChatModel

        return OpenAIChatModel  # Mistral uses OpenAI-compatible API
    elif provider == AIProvider.COHERE:
        from pydantic_ai.models.openai import OpenAIChatModel

        return OpenAIChatModel  # Cohere can use OpenAI-compatible interface
    elif provider == AIProvider.OLLAMA:
        from pydantic_ai.models.openai import OpenAIChatModel

        return OpenAIChatModel  # Ollama uses OpenAI-compatible API
    elif provider == AIProvider.PUBLIC:
        from pydantic_ai.models.openai import OpenAIChatModel

        return OpenAIChatModel  # Public endpoints use OpenAI-compatible API
    else:
        raise ProviderError(f"Unsupported provider: {provider}")


class ProviderError(Exception):
    """Exception raised when provider setup fails."""

    pass


class ProviderNotInstalledError(ProviderError):
    """Raised when a provider's dependencies are not installed."""

    def __init__(self, provider: str, cli_command: str):
        self.provider = provider
        self.cli_command = cli_command
        super().__init__(f"{provider} provider is not installed")


def get_agent(
    config: AIServiceConfig,
    settings: Any,
    system_prompt_override: str | None = None,
) -> Agent:
    """
    Create a PydanticAI Agent for the configured provider.

    Falls back to demo mode if no API keys are configured for immediate testing.

    Args:
        config: AI service configuration
        settings: Application settings for API keys
        system_prompt_override: Optional custom system prompt (for RAG mode)

    Returns:
        Agent: Configured PydanticAI Agent

    Raises:
        ProviderError: If agent creation fails
    """
    try:
        # Special handling for PUBLIC provider (no API key required)
        if config.provider == AIProvider.PUBLIC:
            return _create_public_agent(config, system_prompt_override)

        # Special handling for OLLAMA provider (local, no API key required)
        if config.provider == AIProvider.OLLAMA:
            return _create_ollama_agent(config, settings, system_prompt_override)

        # Check if we have API key for this provider
        provider_config = config.get_provider_config(settings)

        if not provider_config.api_key:
            raise ProviderError(
                f"No API key configured for {config.provider}. "
                f"Set {_get_env_var_name(config.provider)} environment variable. "
                f"For free usage without API keys, try 'public' provider or Groq: https://console.groq.com/keys"
            )

        # Set environment variable for PydanticAI 1.0+ (they expect env vars)
        _set_provider_env_var(config.provider, provider_config.api_key)

        # Get model class for provider (with lazy loading)
        model_class = _get_model_class(config.provider)

        # For PydanticAI 1.0+, NO api_key parameter - just model_name
        model_kwargs = {"model_name": config.model}

        # Add provider-specific configuration
        if config.provider == AIProvider.MISTRAL:
            model_kwargs["base_url"] = "https://api.mistral.ai/v1"
        elif config.provider == AIProvider.COHERE:
            model_kwargs["base_url"] = "https://api.cohere.ai/v1"
        elif config.provider == AIProvider.OLLAMA:
            # Ollama uses OpenAI-compatible API at /v1 endpoint
            base_url = settings.ollama_base_url_effective
            model_kwargs["base_url"] = f"{base_url}/v1"

        # Create model instance (PydanticAI 1.0+ style)
        model = model_class(**model_kwargs)

        # Determine system prompt
        system_prompt = system_prompt_override or (
            "You are a helpful AI assistant. Provide clear, accurate, "
            "and concise responses. Be friendly and professional."
        )

        # Create agent with system prompt and model settings
        agent = Agent(
            model=model,
            model_settings=ModelSettings(
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                timeout=config.timeout_seconds,
            ),
            system_prompt=system_prompt,
        )

        return agent

    except Exception as e:
        error_str = str(e)
        # Check for missing dependency error (pydantic-ai pattern)
        if "Please install" in error_str and "to use the" in error_str:
            # Raise specific exception - CLI will handle display (no logging here)
            provider_name = config.provider.value.lower()
            raise ProviderNotInstalledError(
                provider=config.provider.value,
                cli_command=f"{{ project_slug }} ai add-provider {provider_name}",
            ) from e

        # Other errors still log and raise normally
        error_msg = f"Failed to create agent for {config.provider}: {e}"
        logger.error(error_msg)
        raise ProviderError(error_msg) from e


def _create_public_agent(
    config: AIServiceConfig,
    system_prompt_override: str | None = None,
) -> Agent:
    """
    Create agent for PUBLIC provider using free public endpoints.

    Uses LLM7.io service which provides free access through an OpenAI-compatible API.

    Args:
        config: AI service configuration
        system_prompt_override: Optional custom system prompt (for RAG mode)
    """
    # Lazy imports - only needed for PUBLIC provider
    from openai import AsyncOpenAI

    from pydantic_ai.models.openai import OpenAIChatModel
    from pydantic_ai.providers.openai import OpenAIProvider

    try:
        # Create a custom HTTP client that fixes LLM7.io response format
        class FixedLLM7Client(httpx.AsyncClient):
            """Custom HTTP client that adds missing index field to LLM7.io responses."""

            async def send(self, request, **kwargs):
                response = await super().send(request, **kwargs)

                # If this is a chat completions request, fix the response
                if (
                    "/chat/completions" in str(request.url)
                    and request.method.upper() == "POST"
                ):
                    try:
                        # Check if this is a streaming response first
                        content_type = response.headers.get("content-type", "")
                        if (
                            "text/plain" in content_type
                            or "text/event-stream" in content_type
                            or "application/x-ndjson" in content_type
                        ):
                            return response

                        # Only process non-streaming JSON responses
                        if not response.headers.get("content-type", "").startswith(
                            "application/json"
                        ):
                            return response

                        # Get response text
                        response_text = response.text
                        data = json.loads(response_text)

                        # Add missing index field to choices
                        if "choices" in data and isinstance(data["choices"], list):
                            for i, choice in enumerate(data["choices"]):
                                if "index" not in choice or choice["index"] is None:
                                    choice["index"] = i

                        # Monkey patch the response to return fixed content
                        fixed_content = json.dumps(data)
                        response._content = fixed_content.encode()
                        response._text = fixed_content

                    except Exception as e:
                        logger.warning(f"Failed to fix LLM7.io response: {e}")

                return response

        # Create custom HTTP client
        custom_http_client = FixedLLM7Client()

        # Create the AsyncOpenAI client directly
        openai_client = AsyncOpenAI(
            api_key="unused",
            base_url="https://api.llm7.io/v1",
            http_client=custom_http_client,
        )

        # Create provider using the custom openai_client
        provider = OpenAIProvider(openai_client=openai_client)

        # Create OpenAI model using the provider
        model_name = (
            config.model if config.model and config.model != "auto" else "gpt-4o-mini"
        )
        model = OpenAIChatModel(model_name=model_name, provider=provider)

        # Determine system prompt
        system_prompt = system_prompt_override or (
            "You are a helpful AI assistant powered by free public endpoints. "
            "Provide clear, accurate, and concise responses. "
            "Be friendly and professional."
        )

        # Create agent with system prompt and model settings
        agent = Agent(
            model=model,
            model_settings=ModelSettings(
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                timeout=config.timeout_seconds,
            ),
            system_prompt=system_prompt,
        )

        return agent

    except Exception as e:
        error_msg = (
            f"Public endpoint failed ({e}). Here are reliable free alternatives:\n\n"
            "RECOMMENDED - Groq (Fastest, Most Generous Free Tier):\n"
            "   1. Visit: https://console.groq.com/keys\n"
            "   2. Get API key: export GROQ_API_KEY=your_key_here\n"
            "   3. Switch: {{ project_slug }} ai config set-provider groq\n\n"
            "Google AI Studio (Also Free):\n"
            "   1. Visit: https://aistudio.google.com/app/apikey\n"
            "   2. Get API key: export GOOGLE_API_KEY=your_key_here\n"
            "   3. Switch: {{ project_slug }} ai config set-provider google"
        )
        logger.error(f"Failed to create PUBLIC agent: {e}")
        raise ProviderError(error_msg) from e


def _create_ollama_agent(
    config: AIServiceConfig,
    settings: Any,
    system_prompt_override: str | None = None,
) -> Agent:
    """
    Create agent for OLLAMA provider using local Ollama server.

    Uses Ollama's OpenAI-compatible API endpoint.

    Args:
        config: AI service configuration
        settings: Application settings for base URL
        system_prompt_override: Optional custom system prompt (for RAG mode)
    """
    # Lazy imports - only needed for OLLAMA provider
    from openai import AsyncOpenAI

    from pydantic_ai.models.openai import OpenAIChatModel
    from pydantic_ai.providers.openai import OpenAIProvider

    try:
        # Get Ollama base URL from settings (uses effective URL for Docker/local auto-detection)
        base_url = settings.ollama_base_url_effective

        # Create OpenAI client pointing to Ollama's OpenAI-compatible endpoint
        openai_client = AsyncOpenAI(
            api_key="ollama",  # Ollama doesn't need auth but client requires something
            base_url=f"{base_url}/v1",
        )

        # Create provider and model
        provider = OpenAIProvider(openai_client=openai_client)
        model = OpenAIChatModel(model_name=config.model, provider=provider)

        # Determine system prompt
        system_prompt = system_prompt_override or (
            "You are a helpful AI assistant running locally via Ollama. "
            "Provide clear, accurate, and concise responses. "
            "Be friendly and professional."
        )

        # Create agent with system prompt and model settings
        agent = Agent(
            model=model,
            model_settings=ModelSettings(
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                timeout=config.timeout_seconds,
            ),
            system_prompt=system_prompt,
        )

        return agent

    except Exception as e:
        error_msg = (
            f"Ollama connection failed ({e}).\n\n"
            "Make sure Ollama is running:\n"
            "   1. Install Ollama: https://ollama.com/download\n"
            "   2. Start Ollama: ollama serve\n"
            "   3. Pull a model: ollama pull llama3.2\n"
            "   4. Sync models: {{ project_slug }} llm sync --source=ollama\n\n"
            "If Ollama is running on a different host/port, set OLLAMA_BASE_URL in .env"
        )
        logger.error(f"Failed to create OLLAMA agent: {e}")
        raise ProviderError(error_msg) from e


def _get_env_var_name(provider: AIProvider) -> str:
    """Get the environment variable name for a provider."""
    env_var_map = {
        AIProvider.OPENAI: "OPENAI_API_KEY",
        AIProvider.ANTHROPIC: "ANTHROPIC_API_KEY",
        AIProvider.GOOGLE: "GOOGLE_API_KEY",
        AIProvider.GROQ: "GROQ_API_KEY",
        AIProvider.MISTRAL: "MISTRAL_API_KEY",
        AIProvider.COHERE: "COHERE_API_KEY",
        AIProvider.OLLAMA: "OLLAMA_API_KEY",
        AIProvider.PUBLIC: "PUBLIC_API_KEY",
    }

    result = env_var_map.get(provider)
    if result:
        return result
    else:
        return f"{str(provider).upper()}_API_KEY"


def _set_provider_env_var(provider: AIProvider, api_key: str) -> None:
    """Set environment variable for provider API key."""
    env_var_map = {
        AIProvider.OPENAI: "OPENAI_API_KEY",
        AIProvider.ANTHROPIC: "ANTHROPIC_API_KEY",
        AIProvider.GOOGLE: "GOOGLE_API_KEY",
        AIProvider.GROQ: "GROQ_API_KEY",
        AIProvider.MISTRAL: "MISTRAL_API_KEY",
        AIProvider.COHERE: "COHERE_API_KEY",
        AIProvider.OLLAMA: "OLLAMA_API_KEY",
        AIProvider.PUBLIC: "PUBLIC_API_KEY",
    }

    env_var = env_var_map.get(provider)
    if env_var and api_key:
        os.environ[env_var] = api_key


def validate_provider_support(provider: AIProvider) -> bool:
    """Check if a provider is supported."""
    try:
        _get_model_class(provider)
        return True
    except ProviderError:
        return False


def get_supported_providers() -> list[AIProvider]:
    """Get list of supported providers."""
    return [
        AIProvider.OPENAI,
        AIProvider.ANTHROPIC,
        AIProvider.GOOGLE,
        AIProvider.GROQ,
        AIProvider.MISTRAL,
        AIProvider.COHERE,
        AIProvider.OLLAMA,
        AIProvider.PUBLIC,
    ]


def get_provider_model_class(provider: AIProvider):
    """Get the PydanticAI model class for a provider."""
    return _get_model_class(provider)

{% else %}
# LangChain implementation

from typing import Any

from app.core.log import logger

from .config import AIServiceConfig
from .models import AIProvider


# System prompt used for all conversations
SYSTEM_PROMPT = (
    "You are a helpful AI assistant. Provide clear, accurate, "
    "and concise responses. Be friendly and professional."
)


class ProviderError(Exception):
    """Exception raised when provider setup fails."""

    pass


class ProviderNotInstalledError(ProviderError):
    """Raised when a provider's dependencies are not installed."""

    def __init__(self, provider: str, cli_command: str):
        self.provider = provider
        self.cli_command = cli_command
        super().__init__(f"{provider} provider is not installed")


def get_llm(config: AIServiceConfig, settings: Any):
    """
    Create a LangChain chat model for the configured provider.

    Args:
        config: AI service configuration
        settings: Application settings for API keys

    Returns:
        BaseChatModel: Configured LangChain chat model

    Raises:
        ProviderError: If model creation fails
    """
    try:
        provider = config.provider

        if provider == AIProvider.OPENAI:
            return _create_openai_llm(config, settings)
        elif provider == AIProvider.ANTHROPIC:
            return _create_anthropic_llm(config, settings)
        elif provider == AIProvider.GOOGLE:
            return _create_google_llm(config, settings)
        elif provider == AIProvider.GROQ:
            return _create_groq_llm(config, settings)
        elif provider == AIProvider.MISTRAL:
            return _create_mistral_llm(config, settings)
        elif provider == AIProvider.COHERE:
            return _create_cohere_llm(config, settings)
        elif provider == AIProvider.OLLAMA:
            return _create_ollama_llm(config, settings)
        elif provider == AIProvider.PUBLIC:
            return _create_public_llm(config)
        else:
            raise ProviderError(f"Unsupported provider: {provider}")

    except ImportError as e:
        error_msg = f"Missing LangChain package for {config.provider}: {e}"
        logger.error(error_msg)
        raise ProviderError(error_msg) from e
    except Exception as e:
        error_msg = f"Failed to create LLM for {config.provider}: {e}"
        logger.error(error_msg)
        raise ProviderError(error_msg) from e


def _create_openai_llm(config: AIServiceConfig, settings: Any):
    """Create OpenAI chat model."""
    from langchain_openai import ChatOpenAI

    api_key = getattr(settings, "OPENAI_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for OpenAI. "
            "Set OPENAI_API_KEY environment variable."
        )

    return ChatOpenAI(
        model=config.model or "gpt-4o-mini",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_anthropic_llm(config: AIServiceConfig, settings: Any):
    """Create Anthropic chat model."""
    from langchain_anthropic import ChatAnthropic

    api_key = getattr(settings, "ANTHROPIC_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for Anthropic. "
            "Set ANTHROPIC_API_KEY environment variable."
        )

    return ChatAnthropic(
        model=config.model or "claude-3-5-sonnet-20241022",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_google_llm(config: AIServiceConfig, settings: Any):
    """Create Google Generative AI chat model."""
    from langchain_google_genai import ChatGoogleGenerativeAI

    api_key = getattr(settings, "GOOGLE_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for Google. "
            "Set GOOGLE_API_KEY environment variable. "
            "Get a free key at: https://aistudio.google.com/app/apikey"
        )

    return ChatGoogleGenerativeAI(
        model=config.model or "gemini-1.5-flash",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        google_api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_groq_llm(config: AIServiceConfig, settings: Any):
    """Create Groq chat model."""
    from langchain_groq import ChatGroq

    api_key = getattr(settings, "GROQ_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for Groq. "
            "Set GROQ_API_KEY environment variable. "
            "Get a free key at: https://console.groq.com/keys"
        )

    return ChatGroq(
        model=config.model or "llama-3.1-70b-versatile",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_mistral_llm(config: AIServiceConfig, settings: Any):
    """Create Mistral chat model."""
    from langchain_mistralai import ChatMistralAI

    api_key = getattr(settings, "MISTRAL_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for Mistral. "
            "Set MISTRAL_API_KEY environment variable."
        )

    return ChatMistralAI(
        model=config.model or "mistral-large-latest",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_cohere_llm(config: AIServiceConfig, settings: Any):
    """Create Cohere chat model."""
    from langchain_cohere import ChatCohere

    api_key = getattr(settings, "COHERE_API_KEY", None)
    if not api_key:
        raise ProviderError(
            "No API key configured for Cohere. "
            "Set COHERE_API_KEY environment variable."
        )

    return ChatCohere(
        model=config.model or "command-r-plus",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        cohere_api_key=api_key,
        timeout=config.timeout_seconds,
    )


def _create_ollama_llm(config: AIServiceConfig, settings: Any):
    """Create Ollama chat model using OpenAI-compatible endpoint."""
    from langchain_openai import ChatOpenAI

    # Get Ollama base URL from settings (uses effective URL for Docker/local auto-detection)
    base_url = settings.ollama_base_url_effective

    return ChatOpenAI(
        model=config.model or "llama3.2",
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key="ollama",  # Ollama doesn't require auth but client needs something
        base_url=f"{base_url}/v1",
        timeout=config.timeout_seconds,
    )


def _create_public_llm(config: AIServiceConfig):
    """Create LLM using free public endpoint (LLM7.io)."""
    from langchain_openai import ChatOpenAI

    model_name = (
        config.model if config.model and config.model != "auto" else "gpt-4o-mini"
    )

    return ChatOpenAI(
        model=model_name,
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        api_key="unused",
        base_url="https://api.llm7.io/v1",
        timeout=config.timeout_seconds,
    )


def validate_provider_support(provider: AIProvider) -> bool:
    """Check if a provider is supported."""
    supported = [
        AIProvider.OPENAI,
        AIProvider.ANTHROPIC,
        AIProvider.GOOGLE,
        AIProvider.GROQ,
        AIProvider.MISTRAL,
        AIProvider.COHERE,
        AIProvider.OLLAMA,
        AIProvider.PUBLIC,
    ]
    return provider in supported


def get_supported_providers() -> list[AIProvider]:
    """Get list of supported providers."""
    return [
        AIProvider.OPENAI,
        AIProvider.ANTHROPIC,
        AIProvider.GOOGLE,
        AIProvider.GROQ,
        AIProvider.MISTRAL,
        AIProvider.COHERE,
        AIProvider.OLLAMA,
        AIProvider.PUBLIC,
    ]
{% endif %}
