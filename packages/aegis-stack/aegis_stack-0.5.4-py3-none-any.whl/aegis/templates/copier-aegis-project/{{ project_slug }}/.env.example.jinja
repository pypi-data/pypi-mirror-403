# Aegis Stack Environment Variables Example
# Copy this file to .env and customize for your environment

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================

# Environment: dev, qa, stg, prod
APP_ENV=dev

# Port to run the webserver on
PORT=8000

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Auto-reload server on code changes (development only)
AUTO_RELOAD=true

# =============================================================================
# DOCKER SETTINGS
# =============================================================================

# Docker image tag (used by docker-compose)
AEGIS_STACK_TAG=aegis-stack:latest

# Version tag for builds
AEGIS_STACK_VERSION=dev

# Environment file to use (defaults to .env)
# AEGIS_STACK_ENV_FILE=.env

# =============================================================================
# COMPONENT SETTINGS
# =============================================================================

{%- if include_database %}
# Database settings
{% if database_engine == "postgres" %}
# PostgreSQL database
# For Docker: postgresql://postgres:postgres@postgres:5432/{{ project_slug }}
# For local: postgresql://postgres:postgres@localhost:5432/{{ project_slug }}
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/{{ project_slug }}

# Override for local CLI commands (when not running in Docker)
# Uncomment for local development:
DATABASE_URL_LOCAL=postgresql://postgres:postgres@localhost:5432/{{ project_slug }}
{% else %}
# SQLite database
DATABASE_URL=sqlite:///./data/app.db
{% endif %}
# DATABASE_ENGINE_ECHO=false  # Set to true to log all SQL queries (debugging)
{% endif %}

{% if include_redis or include_cache %}
# Redis settings
# For Docker: redis://redis:6379 (default)
# For local development: redis://localhost:6379
REDIS_URL=redis://redis:6379

# Override for local CLI commands (when not running in Docker)
# Uncomment for local development:
REDIS_URL_LOCAL=redis://localhost:6379
# REDIS_HOST=redis
# REDIS_PORT=6379
# REDIS_PASSWORD=

# Redis connection settings (for high-load scenarios)
# REDIS_CONN_TIMEOUT=5  # Connection timeout in seconds
# REDIS_CONN_RETRIES=5  # Number of connection retry attempts
# REDIS_CONN_RETRY_DELAY=1  # Delay between retries in seconds
{% endif %}

{%- if include_scheduler %}
# Scheduler settings
# SCHEDULER_FORCE_UPDATE=false  # Set to true to override existing jobs from code on restart
{% endif %}

# API Keys and Secrets (add as needed)
# SECRET_KEY=your-super-secret-key-here
# API_KEY=your-api-key-here

{%- if include_ai %}
# =============================================================================
# AI SERVICE CONFIGURATION
# =============================================================================

# AI Service Settings
AI_ENABLED=true
AI_PROVIDER=public  # Primary provider: openai, anthropic, google, groq, mistral, cohere, public
AI_MODEL=auto  # Model to use (PUBLIC provider uses available free models)
AI_TEMPERATURE=0.7  # Response creativity (0.0-2.0)
AI_MAX_TOKENS=4906  # Maximum response length
AI_TIMEOUT_SECONDS=120.0  # Request timeout

# Conversation Management
AI_MAX_CONVERSATION_LENGTH=50  # Max messages per conversation
AI_CONVERSATION_TIMEOUT_HOURS=24  # Auto-cleanup old conversations

# Provider API Keys (optional - many offer free tiers)
# PUBLIC provider works immediately without API keys (currently set as default)
# Get your API keys from:
# - OpenAI: https://platform.openai.com/api-keys
# - Anthropic: https://console.anthropic.com/
# - Google: https://aistudio.google.com/app/apikey
# - Groq: https://console.groq.com/keys (FREE - Recommended for getting started)
# - Mistral: https://console.mistral.ai/api-keys/
# - Cohere: https://dashboard.cohere.com/api-keys

# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# GOOGLE_API_KEY=AI...
# GROQ_API_KEY=gsk_...  # FREE tier available!
# MISTRAL_API_KEY=...
# COHERE_API_KEY=...

{% if ollama_mode != "none" %}
# Ollama Configuration (Local LLM Inference)
{% if ollama_mode == "host" %}
# Connecting to Ollama on host machine (Mac/Windows Docker Desktop)
OLLAMA_BASE_URL=http://host.docker.internal:11434
# For local CLI commands when not running in Docker:
OLLAMA_BASE_URL_LOCAL=http://localhost:11434
{% elif ollama_mode == "docker" %}
# Connecting to Ollama Docker container
OLLAMA_BASE_URL=http://ollama:11434
# For local CLI commands when not running in Docker:
OLLAMA_BASE_URL_LOCAL=http://localhost:11434
{% endif %}
{% endif %}

# Provider-Specific Notes:
# - Public: Works immediately without API keys (uses free endpoints, actual model varies)
# - Groq: Excellent free tier with fast inference (recommended for testing)
# - Google: Generous free tier with Gemini Flash model
# - OpenAI: Requires paid account but excellent quality
# - Anthropic: Requires paid account, very good for reasoning
# - Mistral: Good open source models, some free options
# - Cohere: Enterprise focused, limited free tier
# - Ollama: Free local inference, requires Ollama installed (https://ollama.com)
{% endif %}

{% if ai_rag %}
# =============================================================================
# RAG SERVICE CONFIGURATION
# =============================================================================

# RAG Service Settings
RAG_ENABLED=true
RAG_PERSIST_DIRECTORY=./data/chromadb
RAG_EMBEDDING_PROVIDER=sentence-transformers  # or "openai"
RAG_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5  # Free, local embedding model

# Model cache directory for local development
# Uncomment to use a local cache (Docker uses system default)
# RAG_MODEL_CACHE_DIR=./data/models

# Chunking settings
RAG_CHUNK_SIZE=2000
RAG_CHUNK_OVERLAP=400
RAG_DEFAULT_TOP_K=15
RAG_CHAT_TOP_K=15
{% endif %}

{% if include_comms %}
# =============================================================================
# COMMUNICATIONS SERVICE CONFIGURATION
# =============================================================================

# Email Service (Resend)
# Sign up at https://resend.com (free tier: 100 emails/day)
# RESEND_API_KEY=re_xxxxxxxxxxxx
# RESEND_FROM_EMAIL=noreply@yourdomain.com  # Must be a verified domain in Resend

# SMS/Voice Service (Twilio)
# Sign up at https://twilio.com/try-twilio ($15 free trial credit)
# Buy a toll-free number to stay on trial (avoids A2P 10DLC registration)
# TWILIO_ACCOUNT_SID=ACxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# TWILIO_AUTH_TOKEN=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# TWILIO_PHONE_NUMBER=+18001234567  # Your toll-free Twilio number (E.164 format)
# TWILIO_MESSAGING_SERVICE_SID=MGxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # Required for SMS
{% endif %}

{%- if include_worker %}
# =============================================================================
# WORKER SETTINGS
# =============================================================================
{%- if worker_backend == "taskiq" %}

# Worker processes - controls parallelism for CPU-bound tasks (TaskIQ only)
# Each process has its own Python GIL, enabling true parallelism
# LOAD_TEST_WORKER_PROCESSES=4  # Load test worker (default: 4)
# SYSTEM_WORKER_PROCESSES=2     # System worker (default: 2)
{%- endif %}

# Worker timeouts and retries
# LOAD_TEST_WORKER_TIMEOUT=60   # Load test timeout in seconds
# SYSTEM_WORKER_TIMEOUT=1800    # System worker timeout (30 min for long tasks)
# LOAD_TEST_WORKER_MAX_TRIES=1  # Load test retries (1 = no retry)
# SYSTEM_WORKER_MAX_TRIES=5     # System worker retries

# Worker health check settings
# WORKER_HEALTH_CHECK_INTERVAL=15  # Health check interval in seconds (default: 15)
{%- endif %}

# =============================================================================
# DEVELOPMENT OVERRIDES
# =============================================================================

# Worker auto-reload settings
# WORKER_WATCH=false  # Disable worker auto-reload (useful for debugging)

# Override for local development
# Use this to point to local services instead of Docker services
# DATABASE_HOST=host.docker.internal  # For connecting to host DB from container