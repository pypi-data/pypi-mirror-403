"""
AI service core implementation.

This module provides the main AIService class that handles AI chat functionality,
conversation management, and provider integration.
"""

import uuid
from collections.abc import AsyncIterator
from datetime import UTC, datetime
from typing import Any

{% if ai_framework == "pydantic-ai" %}
from pydantic_ai.exceptions import ModelRetry, UnexpectedModelBehavior
{% endif %}
{% if ai_backend != "memory" %}
from sqlalchemy import Integer, func
from sqlmodel import Session, select

from app.core.db import db_session, engine
from .llm_catalog_context import get_llm_catalog_context
from .models.llm import LargeLanguageModel, LLMPrice, LLMUsage, LLMVendor
{% endif %}

from app.core.log import logger

from .config import get_ai_config
from .conversation import ConversationManager
from .models import (
    Conversation,
    ConversationMessage,
    MessageRole,
    StreamingConversation,
    StreamingMessage,
)
{% if ai_framework == "pydantic-ai" %}
from .providers import ProviderNotInstalledError, get_agent
{% else %}
from .providers import ProviderNotInstalledError, get_llm
{% endif %}
from .health_context import HealthContext
from .usage_context import UsageContext
from .prompts import build_system_prompt
{% if ai_rag %}
from app.services.rag.config import get_rag_config
from app.services.rag.service import RAGService
from .rag_context import RAGContext
from .rag_stats_context import RAGStatsContext
{% endif %}


{% if ai_rag %}
def _is_test_or_fixture_file(source_path: str) -> bool:
    """
    Check if a file path indicates a test file or fixture.

    Used in RAG pipeline to deprioritize test files in search results.
    Source files are returned before test files to improve context quality.

    Args:
        source_path: File path from RAG search result metadata

    Returns:
        True if the path matches test/fixture patterns, False otherwise

    Pipeline position:
        RAG Search → Score Filter → Dedupe → **This Filter** → Top-K Selection

    Examples:
        >>> _is_test_or_fixture_file("app/services/ai/llm_usage.py")
        False
        >>> _is_test_or_fixture_file("tests/test_llm_usage.py")
        True
        >>> _is_test_or_fixture_file("tests/fixtures/llm_fixtures.py")
        True
        >>> _is_test_or_fixture_file("app/components/conftest.py")
        True
    """
    if not source_path:
        return False

    path_lower = source_path.lower()

    # Test file patterns - ordered by specificity
    test_patterns = [
        "/tests/",          # tests directory
        "/test_",           # test_something.py
        "_test.py",         # something_test.py
        "/fixtures/",       # fixtures directory
        "_fixtures.py",     # fixture files
        "/conftest.py",     # pytest conftest
    ]

    return any(pattern in path_lower for pattern in test_patterns)


{% endif %}
class AIServiceError(Exception):
    """Base exception for AI service errors."""

    pass


class ProviderError(AIServiceError):
    """Exception raised when AI provider fails."""

    pass


class ConversationError(AIServiceError):
    """Exception raised when conversation management fails."""

    pass


class AIService:
    """
    Core AI service for chat functionality.

    Handles chat completions, conversation management, and provider abstraction.
    """

    def __init__(self, settings: Any):
        """Initialize AI service with configuration."""
        self.settings = settings
        self.config = get_ai_config(settings)
        self.conversation_manager = ConversationManager()
{% if ai_rag %}
        self._rag_service: RAGService | None = None
{% endif %}

    def refresh_config(self) -> None:
        """Refresh config after .env changes (e.g., after /model command).

        Call this after updating .env to ensure self.config reflects current values.
        This is called by slash commands when provider or model changes.
        """
        from app.core.config import Settings
        self.config = get_ai_config(Settings())

{% if ai_rag %}
    @property
    def rag_service(self) -> RAGService:
        """Lazy initialization of RAG service."""
        if self._rag_service is None:
            config = get_rag_config(self.settings)
            self._rag_service = RAGService(config)
        return self._rag_service
{% endif %}

    async def chat(
        self,
        message: str,
        conversation_id: str | None = None,
        user_id: str = "default",
{% if ai_rag %}
        use_rag: bool = False,
        rag_collection: str | None = None,
        rag_top_k: int | None = None,
{% endif %}
    ) -> ConversationMessage:
        """
        Send a chat message and get AI response.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership
{% if ai_rag %}
            use_rag: Enable RAG context injection from indexed codebase
            rag_collection: Collection to search (uses config default if None)
            rag_top_k: Number of results to retrieve (uses config default if None)
{% endif %}

        Returns:
            ConversationMessage: The AI's response message

        Raises:
            AIServiceError: If service is disabled or not configured
            ProviderError: If AI provider fails
            ConversationError: If conversation management fails
        """
        if not self.config.enabled:
            raise AIServiceError("AI service is disabled")

        try:
            # Get fresh config for metadata (reflects current .env after use-provider)
            current = self.config

            # Setup conversation and add user message
            conversation = self._setup_conversation(
                message, conversation_id, user_id, config=current
            )

{% if ai_rag %}
            # Build RAG context if enabled
            rag_context: RAGContext | None = None
            if use_rag:
                rag_context = await self._build_rag_context(
                    query=message,
                    collection=rag_collection,
                    top_k=rag_top_k,
                )

{% endif %}
            # Build health context (always included, refreshed per message)
            health_context, health_warning = await self._build_health_context()

            # Build usage context for self-awareness
            usage_context = self._build_usage_context()

            # Build LLM catalog context
            catalog_context = self._build_catalog_context()

{% if ai_rag %}
            # Build RAG stats context (always included when RAG enabled)
            rag_stats_context = await self._build_rag_stats_context()

{% endif %}
{% if ai_framework == "pydantic-ai" %}
            # Prepare agent and conversation context
            agent, conversation_context = self._prepare_agent_and_context(
                conversation,
{% if ai_rag %}
                rag_context=rag_context,
                rag_stats_context=rag_stats_context,
{% endif %}
                health_context=health_context,
                health_warning=health_warning,
                usage_context=usage_context,
                catalog_context=catalog_context,
            )

            # Get AI response
            start_time = datetime.now(UTC)
            result = await agent.run(conversation_context)
            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add AI response to conversation
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, result.output, message_id=str(uuid.uuid4())
            )
{% else %}
            # Prepare LLM and messages with full context
            llm, messages = self._prepare_llm_and_messages(
{% if ai_rag %}
                conversation, rag_context, rag_stats_context, health_context, health_warning, usage_context, catalog_context
{% else %}
                conversation, None, None, health_context, health_warning, usage_context, catalog_context
{% endif %}
            )

            # Get AI response
            start_time = datetime.now(UTC)
            result = await llm.ainvoke(messages)
            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add AI response to conversation
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, result.content, message_id=str(uuid.uuid4())
            )
{% endif %}

            # Store conversation ID in message metadata for easy lookup
            ai_message.metadata["conversation_id"] = conversation.id
            ai_message.metadata["response_time_ms"] = response_time_ms

{% if ai_rag %}
            # Add RAG sources to response metadata if RAG was used
            if rag_context and rag_context.results:
                ai_message.metadata["rag_sources"] = rag_context.to_metadata()
                ai_message.metadata["rag_collection"] = rag_context.collection

{% endif %}
{% if ai_backend != "memory" %}
            # Record usage tracking
            usage = self._extract_usage(result)
            self._record_usage("chat", usage, user_id)

            # Add TPS (tokens per second) to metadata for performance monitoring
            output_tokens = usage.get("output_tokens", 0)
            if output_tokens > 0 and response_time_ms > 0:
                ai_message.metadata["gen_tps"] = round(
                    output_tokens * 1000 / response_time_ms, 1
                )
            ai_message.metadata["input_tokens"] = usage.get("input_tokens", 0)
            ai_message.metadata["output_tokens"] = output_tokens
{% endif %}

            # Finalize conversation (update metadata and save)
            self._finalize_conversation(conversation, response_time_ms)

            return ai_message

{% if ai_framework == "pydantic-ai" %}
        except (ModelRetry, UnexpectedModelBehavior) as e:
            error_msg = f"AI provider error: {e}"
            logger.error(error_msg)
            raise ProviderError(error_msg) from e
{% endif %}
        except Exception as e:
            error_msg = f"Chat processing failed: {e}"
            logger.error(error_msg)
            raise AIServiceError(error_msg) from e

    async def stream_chat(
        self,
        message: str,
        conversation_id: str | None = None,
        user_id: str = "default",
        stream_delta: bool = False,
{% if ai_rag %}
        use_rag: bool = False,
        rag_collection: str | None = None,
        rag_top_k: int | None = None,
{% endif %}
    ) -> AsyncIterator[StreamingMessage]:
        """
        Stream a chat message with real-time response generation.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership
            stream_delta: Whether to stream delta changes or full content
{% if ai_rag %}
            use_rag: Enable RAG context injection from indexed codebase
            rag_collection: Collection to search (uses config default if None)
            rag_top_k: Number of results to retrieve (uses config default if None)
{% endif %}

        Yields:
            StreamingMessage: Real-time message chunks

        Raises:
            AIServiceError: If service is disabled or not configured
            ProviderError: If AI provider fails
            ConversationError: If conversation management fails
        """
        if not self.config.enabled:
            raise AIServiceError("AI service is disabled")

        try:
            # Get fresh config for metadata (reflects current .env after use-provider)
            current = self.config

            # Setup conversation and add user message
            conversation = self._setup_conversation(
                message, conversation_id, user_id, config=current
            )

            # Create streaming conversation wrapper
            streaming_conv = StreamingConversation(conversation=conversation)
            streaming_conv.reset_stream()

{% if ai_rag %}
            # Build RAG context if enabled
            rag_context: RAGContext | None = None
            if use_rag:
                rag_context = await self._build_rag_context(
                    query=message,
                    collection=rag_collection,
                    top_k=rag_top_k,
                )

{% endif %}
            # Build health context (always included, refreshed per message)
            health_context, health_warning = await self._build_health_context()

            # Build usage context for self-awareness
            usage_context = self._build_usage_context()

            # Build LLM catalog context
            catalog_context = self._build_catalog_context()

{% if ai_rag %}
            # Build RAG stats context (always included when RAG enabled)
            rag_stats_context = await self._build_rag_stats_context()

{% endif %}
{% if ai_framework == "pydantic-ai" %}
            # Prepare agent and conversation context
            agent, conversation_context = self._prepare_agent_and_context(
                conversation,
{% if ai_rag %}
                rag_context=rag_context,
                rag_stats_context=rag_stats_context,
{% endif %}
                health_context=health_context,
                health_warning=health_warning,
                usage_context=usage_context,
                catalog_context=catalog_context,
            )
{% else %}
            # Prepare LLM and messages with full context
            llm, messages = self._prepare_llm_and_messages(
{% if ai_rag %}
                conversation, rag_context, rag_stats_context, health_context, health_warning, usage_context, catalog_context
{% else %}
                conversation, None, None, health_context, health_warning, usage_context, catalog_context
{% endif %}
            )
{% endif %}

            # Start streaming
            start_time = datetime.now(UTC)

            # Generate a message ID for the streaming response
            message_id = str(uuid.uuid4())

{% if ai_framework == "pydantic-ai" %}
            # Use PydanticAI's run_stream method for streaming
{% if ai_backend != "memory" %}
            stream_usage: dict[str, int] = {"input_tokens": 0, "output_tokens": 0}
{% endif %}
            async with agent.run_stream(conversation_context) as result:
                # Stream text chunks
                async for text_chunk in result.stream_text(delta=stream_delta):
                    # Accumulate content
                    total_content = streaming_conv.accumulate_content(
                        text_chunk, is_delta=stream_delta
                    )

                    # Yield streaming message chunk
                    yield StreamingMessage(
                        content=text_chunk if stream_delta else total_content,
                        is_final=False,
                        is_delta=stream_delta,
                        message_id=message_id,
                        conversation_id=conversation.id,
                        metadata={
                            "provider": current.provider,
                            "model": current.model,
                            "stream_delta": stream_delta,
                        },
                    )

{% if ai_backend != "memory" %}
                # Capture usage after streaming completes (still inside async with)
                if hasattr(result, "usage") and callable(result.usage):
                    usage_obj = result.usage()
                    if usage_obj:
                        stream_usage = {
                            "input_tokens": usage_obj.request_tokens or 0,
                            "output_tokens": usage_obj.response_tokens or 0,
                        }
{% endif %}
{% else %}
            # Use LangChain's astream method for streaming
            async for chunk in llm.astream(messages):
                # Get content from chunk
                text_chunk = chunk.content if hasattr(chunk, 'content') else str(chunk)
                if not text_chunk:
                    continue

                # Accumulate content (LangChain always streams deltas)
                total_content = streaming_conv.accumulate_content(
                    text_chunk, is_delta=True
                )

                # Yield streaming message chunk
                yield StreamingMessage(
                    content=text_chunk if stream_delta else total_content,
                    is_final=False,
                    is_delta=stream_delta,
                    message_id=message_id,
                    conversation_id=conversation.id,
                    metadata={
                        "provider": current.provider,
                        "model": current.model,
                        "stream_delta": stream_delta,
                    },
                )
{% endif %}

            end_time = datetime.now(UTC)
            response_time_ms = (end_time - start_time).total_seconds() * 1000

            # Add final message to conversation using accumulated streaming content
            final_content = streaming_conv.accumulated_content or "No content received"
            ai_message = conversation.add_message(
                MessageRole.ASSISTANT, final_content, message_id=message_id
            )

            # Store conversation metadata
            ai_message.metadata["conversation_id"] = conversation.id

{% if ai_rag %}
            # Add RAG sources to response metadata if RAG was used
            rag_metadata: dict[str, Any] = {}
            if rag_context and rag_context.results:
                ai_message.metadata["rag_sources"] = rag_context.to_metadata()
                ai_message.metadata["rag_collection"] = rag_context.collection
                rag_metadata = {
                    "rag_sources": rag_context.to_metadata(),
                    "rag_collection": rag_context.collection,
                }

{% endif %}
{% if ai_backend != "memory" %}
{% if ai_framework == "pydantic-ai" %}
            # Record usage tracking (PydanticAI provides streaming usage)
            self._record_usage("stream_chat", stream_usage, user_id)
{% else %}
            # LangChain streaming doesn't provide token counts
            # Record with zero tokens - actual usage tracking requires non-streaming
            self._record_usage("stream_chat", {"input_tokens": 0, "output_tokens": 0}, user_id)
{% endif %}
{% endif %}

            # Finalize conversation (update metadata and save)
            self._finalize_conversation(
                conversation, response_time_ms, is_streaming=True
            )

            # Yield final streaming message
{% if ai_backend != "memory" %}
            # Calculate cost for status line
            input_tokens = stream_usage.get("input_tokens", 0)
            output_tokens = stream_usage.get("output_tokens", 0)
            cost = self.calculate_cost(input_tokens, output_tokens)

            # Calculate TPS (tokens per second) for performance metrics
            # This is especially useful for Ollama but works for all providers
            gen_tps: float | None = None
            if output_tokens > 0 and response_time_ms > 0:
                gen_tps = round(output_tokens * 1000 / response_time_ms, 1)
{% endif %}

            final_metadata = {
                "provider": current.provider,
                "model": current.model,
                "response_time_ms": response_time_ms,
                "stream_complete": True,
{% if ai_backend != "memory" %}
                # Token usage and cost for CLI status line
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "cost": cost,
                # TPS for performance monitoring (especially useful for Ollama)
                "gen_tps": gen_tps,
{% endif %}
            }
{% if ai_rag %}
            final_metadata.update(rag_metadata)
{% endif %}
            yield StreamingMessage(
                content=final_content,
                is_final=True,
                is_delta=False,
                message_id=message_id,
                conversation_id=conversation.id,
                metadata=final_metadata,
            )

{% if ai_framework == "pydantic-ai" %}
        except (ModelRetry, UnexpectedModelBehavior) as e:
            error_msg = f"AI provider streaming error: {e}"
            logger.error(error_msg)
            raise ProviderError(error_msg) from e
{% endif %}
        except ProviderNotInstalledError:
            # Re-raise without wrapping - CLI will handle display
            raise
        except Exception as e:
            error_msg = f"Streaming failed: {e}"
            logger.error(error_msg)
            raise AIServiceError(error_msg) from e

    def _setup_conversation(
        self,
        message: str,
        conversation_id: str | None,
        user_id: str,
        config: "AIConfig | None" = None,
    ) -> Conversation:
        """
        Get or create conversation and add user message.

        Args:
            message: The user's message
            conversation_id: Optional conversation ID (creates new if None)
            user_id: User identifier for conversation ownership
            config: Optional fresh config (uses self.config if not provided)

        Returns:
            Conversation: The conversation with user message added

        Raises:
            ConversationError: If conversation_id provided but not found
        """
        # Use provided config or fall back to cached
        cfg = config or self.config

        # Get or create conversation
        if conversation_id:
            conversation = self.conversation_manager.get_conversation(conversation_id)
            if not conversation:
                raise ConversationError(f"Conversation {conversation_id} not found")
        else:
            conversation = self.conversation_manager.create_conversation(
                provider=cfg.provider,
                model=cfg.model,
                user_id=user_id,
            )

        # Add user message to conversation
        conversation.add_message(MessageRole.USER, message)

        return conversation

{% if ai_framework == "pydantic-ai" %}
    def _prepare_agent_and_context(
        self,
        conversation: Conversation,
{% if ai_rag %}
        rag_context: RAGContext | None = None,
        rag_stats_context: RAGStatsContext | None = None,
{% endif %}
        health_context: HealthContext | None = None,
        health_warning: str | None = None,
        usage_context: UsageContext | None = None,
        catalog_context: str | None = None,
    ) -> tuple[Any, str]:
        """
        Create agent for request and build conversation context.

        Args:
            conversation: The conversation to prepare context from
{% if ai_rag %}
            rag_context: Optional RAG context to inject into system prompt
            rag_stats_context: Optional RAG stats context for collection awareness
{% endif %}
            health_context: Optional health context to inject into system prompt
            health_warning: Optional warning if server is down
            usage_context: Optional usage context for self-awareness
            catalog_context: Optional LLM catalog context for model awareness

        Returns:
            tuple[Any, str]: (agent instance, conversation context string)
        """
        # Build system prompt with project context and optional contexts
        # Get fresh config for current model/provider
        config = self.config

        # Use compact mode for Ollama (smaller context for better instruction following)
        is_compact = config.provider.value == "ollama"
        logger.debug(f"Compact mode: {is_compact} (provider={config.provider.value})")

        formatted_rag = None
{% if ai_rag %}
        if rag_context and rag_context.results:
            formatted_rag = rag_context.format_for_prompt(include_metadata=True)

        formatted_rag_stats = None
        if rag_stats_context:
            formatted_rag_stats = rag_stats_context.format_for_prompt(compact=is_compact)
{% endif %}

        formatted_health = None
        if health_context:
            formatted_health = health_context.format_for_prompt(compact=is_compact)
            logger.debug(f"Health context for prompt:\n{formatted_health}")

        formatted_usage = None
        if usage_context:
            formatted_usage = usage_context.format_for_prompt(compact=is_compact)

        system_prompt_override = build_system_prompt(
            self.settings,
            rag_context=formatted_rag,
{% if ai_rag %}
            rag_stats_context=formatted_rag_stats,
{% endif %}
            health_context=formatted_health,
            usage_context=formatted_usage,
            catalog_context=catalog_context,
            use_rag=formatted_rag is not None,
            current_model=config.model,
            current_provider=config.provider.value,
        )

        # Add server-down warning if applicable
        if health_warning:
            system_prompt_override += f"\n\n**NOTICE**: {health_warning}"

        # Create agent for this request
        agent = get_agent(config, self.settings, system_prompt_override)

        # Build conversation context for AI
        conversation_context = self._build_conversation_context(conversation)

        return agent, conversation_context
{% else %}
    def _prepare_llm_and_messages(
        self,
        conversation: Conversation,
{% if ai_rag %}
        rag_context: RAGContext | None = None,
        rag_stats_context: RAGStatsContext | None = None,
{% else %}
        rag_context: Any | None = None,
        rag_stats_context: Any | None = None,
{% endif %}
        health_context: HealthContext | None = None,
        health_warning: str | None = None,
        usage_context: UsageContext | None = None,
        catalog_context: str | None = None,
    ) -> tuple[Any, list]:
        """
        Create LLM instance and build message list for LangChain.

        Args:
            conversation: The conversation to prepare messages from
            rag_context: Optional RAG context with code search results
            rag_stats_context: Optional RAG stats context for collection awareness
            health_context: Optional health context with system status
            health_warning: Optional warning if server is down
            usage_context: Optional usage context with AI stats
            catalog_context: Optional LLM catalog context for model awareness

        Returns:
            tuple[Any, list]: (LLM instance, list of message tuples)
        """
        # Create LLM for this request
        llm = get_llm(self.config, self.settings)

        # Build messages list for LangChain
        messages = self._build_langchain_messages(
            conversation, rag_context, rag_stats_context, health_context, health_warning, usage_context, catalog_context
        )

        return llm, messages

    def _build_langchain_messages(
        self,
        conversation: Conversation,
{% if ai_rag %}
        rag_context: RAGContext | None = None,
        rag_stats_context: RAGStatsContext | None = None,
{% else %}
        rag_context: Any | None = None,
        rag_stats_context: Any | None = None,
{% endif %}
        health_context: HealthContext | None = None,
        health_warning: str | None = None,
        usage_context: UsageContext | None = None,
        catalog_context: str | None = None,
    ) -> list:
        """
        Build LangChain message list from conversation history.

        Args:
            conversation: The conversation with message history
            rag_context: Optional RAG context with code search results
            rag_stats_context: Optional RAG stats context for collection awareness
            health_context: Optional health context with system status
            health_warning: Optional warning if server is down
            usage_context: Optional usage context with AI stats
            catalog_context: Optional LLM catalog context for model awareness

        Returns:
            list: List of message tuples for LangChain
        """
        # Use compact mode for Ollama (smaller context for better instruction following)
        is_compact = self.config.provider.value == "ollama"

        # Build dynamic system prompt with Illiana's persona
        rag_str = rag_context.format_for_prompt() if rag_context else None
        rag_stats_str = rag_stats_context.format_for_prompt(compact=is_compact) if rag_stats_context else None
        health_str = health_context.format_for_prompt(compact=is_compact) if health_context else None
        usage_str = usage_context.format_for_prompt(compact=is_compact) if usage_context else None

        system_prompt = build_system_prompt(
            settings=self.settings,
            rag_context=rag_str,
            rag_stats_context=rag_stats_str,
            health_context=health_str,
            usage_context=usage_str,
            catalog_context=catalog_context,
            use_rag=rag_context is not None,
        )

        # Add server-down warning if applicable
        if health_warning:
            system_prompt += f"\n\n**NOTICE**: {health_warning}"

        messages = [("system", system_prompt)]

        if not conversation.messages:
            return messages

        # For continuous conversation, include recent message history
        # Limit to last 10 messages to manage context window
        recent_messages = conversation.messages[-10:]

        # Format messages for LangChain
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                messages.append(("human", msg.content))
            elif msg.role == MessageRole.ASSISTANT:
                messages.append(("assistant", msg.content))

        return messages
{% endif %}

    def _finalize_conversation(
        self,
        conversation: Conversation,
        response_time_ms: float,
        is_streaming: bool = False,
    ) -> None:
        """
        Update conversation metadata and save.

        Args:
            conversation: The conversation to finalize
            response_time_ms: Response time in milliseconds
            is_streaming: Whether this was a streaming response
        """
        # Update conversation metadata
        metadata_update = {
            "last_response_time_ms": response_time_ms,
            "total_messages": conversation.get_message_count(),
            "last_activity": datetime.now(UTC).isoformat(),
        }

        if is_streaming:
            metadata_update["streaming"] = True

        conversation.metadata.update(metadata_update)

        # Save conversation
        self.conversation_manager.save_conversation(conversation)

{% if ai_framework == "pydantic-ai" %}
    def _build_conversation_context(self, conversation: Conversation) -> str:
        """
        Build conversation context for AI from message history.

        Args:
            conversation: The conversation with message history

        Returns:
            str: Formatted conversation context for AI
        """
        if not conversation.messages:
            return ""

        # For continuous conversation, include recent message history
        # Limit to last 10 messages to manage context window
        recent_messages = conversation.messages[-10:]

        # Format messages for context
        context_parts = []
        for msg in recent_messages[:-1]:  # Exclude the latest message (just added)
            if msg.role == MessageRole.USER:
                context_parts.append(f"User: {msg.content}")
            elif msg.role == MessageRole.ASSISTANT:
                context_parts.append(f"Assistant: {msg.content}")

        # Add the current user message
        latest_message = conversation.get_last_message()
        if latest_message and latest_message.role == MessageRole.USER:
            if context_parts:
                # Include conversation history + current message
                return "\n".join(context_parts) + f"\n\nUser: {latest_message.content}"
            else:
                # First message in conversation
                return latest_message.content

        return ""
{% endif %}

{% if ai_rag %}
    async def _build_rag_context(
        self,
        query: str,
        collection: str | None = None,
        top_k: int | None = None,
    ) -> RAGContext:
        """
        Build RAG context by searching the vector store.

        Args:
            query: Search query (usually the user's message)
            collection: Collection to search (uses config default if None)
            top_k: Number of results (uses config default if None)

        Returns:
            RAGContext with search results

        Raises:
            AIServiceError: If no collection is specified
        """
        # Determine collection to use
        target_collection = collection or self.config.rag_default_collection
        if not target_collection:
            raise AIServiceError(
                "No RAG collection specified. Either pass rag_collection parameter "
                "or set RAG_CHAT_DEFAULT_COLLECTION in config."
            )

        # Determine top_k
        k = top_k or self.config.rag_top_k

        # Search for relevant content
        results = await self.rag_service.search(
            query=query,
            collection_name=target_collection,
            top_k=k,
        )

        # Filter by minimum score
        filtered_results = [
            r for r in results if r.score >= self.config.rag_min_score
        ]

        # Deduplicate by source + start_line (same chunk can appear multiple times)
        seen: set[str] = set()
        deduped_results = []
        for r in filtered_results:
            key = f"{r.metadata.get('source', '')}:{r.metadata.get('start_line', 0)}"
            if key not in seen:
                seen.add(key)
                deduped_results.append(r)

        # Prioritize source files over test/fixture files
        # Pipeline: Score Filter → Dedupe → Separate → Prioritize → Top-K
        source_results = []
        test_results = []
        for r in deduped_results:
            if _is_test_or_fixture_file(r.metadata.get("source", "")):
                test_results.append(r)
            else:
                source_results.append(r)

        # Return source files first, fill remaining slots with test files
        results_to_use = source_results[:k]
        if len(results_to_use) < k:
            results_to_use.extend(test_results[: k - len(results_to_use)])

        return RAGContext(
            query=query,
            collection=target_collection,
            results=results_to_use,
        )

    async def _build_rag_stats_context(self) -> RAGStatsContext | None:
        """
        Build RAG stats context from RAG service status.

        Gives Illiana awareness about indexed collections, document counts,
        and RAG configuration.

        Returns:
            RAGStatsContext with current stats, or None if unavailable
        """
        try:
            status = self.rag_service.get_service_status()
            collections = await self.rag_service.list_collections()

            collection_info = []
            for name in collections:
                stats = await self.rag_service.get_collection_stats(name)
                if stats:
                    collection_info.append({
                        "name": name,
                        "count": stats.get("count", 0),
                    })

            return RAGStatsContext(
                enabled=status.get("enabled", False),
                collection_count=len(collections),
                collections=collection_info,
                embedding_model=status.get("embedding_model"),
                chunk_size=status.get("chunk_size"),
                chunk_overlap=status.get("chunk_overlap"),
                default_top_k=status.get("default_top_k"),
                last_activity=status.get("last_activity"),
            )
        except Exception as e:
            logger.debug(f"Failed to build RAG stats context: {e}")
            return None

{% endif %}
    async def _build_health_context(
        self,
    ) -> tuple[HealthContext | None, str | None]:
        """
        Build health context by fetching current system status.

        Tries HTTP endpoint first (has all registered components from FastAPI),
        falls back to local get_system_status() if server not running.

        Returns:
            Tuple of (HealthContext, warning_message):
            - HealthContext with system status, or None if complete failure
            - Warning message if server unreachable, otherwise None
        """
        # Try HTTP endpoint first - has all registered components
        try:
            import httpx

            base_url = getattr(self.settings, "API_BASE_URL", "http://localhost:8000")
            async with httpx.AsyncClient(timeout=5.0) as client:
                resp = await client.get(f"{base_url}/health/detailed")
                if resp.status_code == 200:
                    data = resp.json()
                    status = self._parse_http_health(data)
                    return HealthContext(status=status), None  # Success, no warning
        except Exception as e:
            # Server not running or other error, falling back to local
            logger.debug(
                "ai_service.http_health_check_failed",
                exc_type=type(e).__name__,
                exc_message=str(e),
            )

        # HTTP failed - note this for the prompt
        warning = (
            "Backend webserver is not running. I can't check full system health. "
            "Start it with the run command to get complete status."
        )

        # Fallback to local (limited components without FastAPI startup)
        try:
            from app.services.system.health import get_system_status

            status = await get_system_status()
            return HealthContext(status=status), warning
        except Exception as e:
            logger.warning(f"Failed to build health context: {e}")
            return None, warning

    def _parse_http_health(self, data: dict[str, Any]) -> "SystemStatus":
        """Parse HTTP health response into SystemStatus model."""
        from app.services.system.models import ComponentStatus, SystemStatus

        def parse_component(comp: dict[str, Any]) -> ComponentStatus:
            return ComponentStatus(
                name=comp["name"],
                status=comp["status"],
                message=comp["message"],
                response_time_ms=comp.get("response_time_ms"),
                metadata=comp.get("metadata", {}),
                sub_components={
                    k: parse_component(v)
                    for k, v in comp.get("sub_components", {}).items()
                },
                healthy=comp.get("healthy", True),
            )

        return SystemStatus(
            overall_healthy=data["healthy"],
            components={
                k: parse_component(v) for k, v in data["components"].items()
            },
            timestamp=datetime.now(),
        )

    def _build_usage_context(self) -> UsageContext | None:
        """
        Build usage context from AI service statistics.

        Gives Illiana self-awareness about her own usage patterns,
        token consumption, costs, and success rates.

        Returns:
            UsageContext with current stats, or None if unavailable
        """
{% if ai_backend != "memory" %}
        try:
            stats = self.get_usage_stats(recent_limit=5)

            # Find top model by request count
            top_model = None
            top_pct = 0.0
            models = stats.get("models", [])
            if models:
                top = max(models, key=lambda m: m.get("requests", 0))
                top_model = top.get("model_title")
                top_pct = top.get("percentage", 0.0)

            return UsageContext(
                total_tokens=stats.get("total_tokens", 0),
                total_requests=stats.get("total_requests", 0),
                total_cost=stats.get("total_cost", 0.0),
                success_rate=stats.get("success_rate", 100.0),
                top_model=top_model,
                top_model_percentage=top_pct,
                recent_requests=len(stats.get("recent_activity", [])),
            )
        except Exception as e:
            logger.debug(f"Failed to build usage context: {e}")
            return None
{% else %}
        # Usage tracking requires database backend
        return None
{% endif %}

    def _build_catalog_context(self) -> str | None:
        """
        Build LLM catalog context from database.

        Provides Illiana with awareness of available LLM models,
        their pricing, and capabilities.

        Returns:
            Formatted catalog context string, or None if unavailable
        """
{% if ai_backend != "memory" %}
        try:
            with Session(engine) as session:
                return get_llm_catalog_context(session)
        except Exception as e:
            logger.debug(f"Failed to build catalog context: {e}")
            return None
{% else %}
        # Catalog context requires database backend
        return None
{% endif %}

    def get_conversation(self, conversation_id: str) -> Conversation | None:
        """Get a conversation by ID."""
        return self.conversation_manager.get_conversation(conversation_id)

    def list_conversations(self, user_id: str = "default") -> list[Conversation]:
        """List all conversations for a user."""
        return self.conversation_manager.list_conversations(user_id)

    def get_service_status(self) -> dict[str, Any]:
        """Get current service status and metrics."""
        # Use get_stats() which works for both memory and SQLite backends
        stats = self.conversation_manager.get_stats()
        total_conversations = stats["total_conversations"]

        return {
            "enabled": self.config.enabled,
            "provider": self.config.provider,
            "model": self.config.model,
            "agent_initialized": True,  # Agents created per request, always available
            "total_conversations": total_conversations,
            "configuration_valid": len(
                self.config.validate_configuration(self.settings)
            )
            == 0,
        }

    def validate_service(self) -> list[str]:
        """Validate service configuration and return any issues."""
        errors = []

        # Check provider dependencies first
        dep_errors = self._validate_provider_dependencies()
        errors.extend(dep_errors)

        # Check configuration
        config_errors = self.config.validate_configuration(self.settings)
        errors.extend(config_errors)

        return errors

    def _validate_provider_dependencies(self) -> list[str]:
        """
        Validate that required dependencies are installed for configured provider.

        Returns:
            list[str]: List of error messages if dependencies are missing
        """
        from .provider_management import check_provider_dependency_installed

        errors = []
        provider_value = self.config.provider.value

        if not check_provider_dependency_installed(provider_value):
            errors.append(
                f"Missing dependency for {provider_value} provider. "
                f"Run: {{ project_slug }} ai add-provider {provider_value}"
            )

        return errors

{% if ai_backend != "memory" %}
    def _extract_usage(self, result: Any) -> dict[str, int]:
        """
        Extract token usage from AI response.

        Args:
            result: The response from the AI provider

        Returns:
            dict: Token usage with input_tokens and output_tokens
        """
{% if ai_framework == "pydantic-ai" %}
        # PydanticAI stores usage in result.usage (attribute, not method)
        if hasattr(result, "usage") and result.usage and not callable(result.usage):
            return {
                "input_tokens": result.usage.request_tokens or 0,
                "output_tokens": result.usage.response_tokens or 0,
            }
{% else %}
        # LangChain stores usage in response_metadata
        if hasattr(result, "response_metadata") and result.response_metadata is not None:
            token_usage = result.response_metadata.get("token_usage") or {}
            return {
                "input_tokens": token_usage.get("prompt_tokens", 0),
                "output_tokens": token_usage.get("completion_tokens", 0),
            }
{% endif %}
        return {"input_tokens": 0, "output_tokens": 0}

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calculate cost for given token usage.

        Looks up the current model's pricing and calculates the total cost.
        Returns 0.0 if model or pricing not found.

        Args:
            input_tokens: Number of input/prompt tokens
            output_tokens: Number of output/completion tokens

        Returns:
            Total cost in USD
        """
        model_name = self.config.model

        # Strip vendor prefix if present (e.g., "openai/gpt-4o" -> "gpt-4o")
        if "/" in model_name:
            model_name = model_name.split("/", 1)[1]

        try:
            with db_session() as session:
                # Look up the LLM by model_id
                stmt = select(LargeLanguageModel).where(
                    LargeLanguageModel.model_id == model_name
                )
                llm = session.exec(stmt).first()

                if not llm:
                    return 0.0

                # Get latest price for this LLM
                price_stmt = (
                    select(LLMPrice)
                    .where(LLMPrice.llm_id == llm.id)
                    .order_by(LLMPrice.effective_date.desc())
                )
                price = session.exec(price_stmt).first()

                if not price:
                    return 0.0

                input_cost = input_tokens * price.input_cost_per_token
                output_cost = output_tokens * price.output_cost_per_token
                return input_cost + output_cost

        except Exception as e:
            logger.warning("Failed to calculate cost", error=str(e), model=model_name)
            return 0.0

    def _record_usage(
        self,
        action: str,
        usage: dict[str, int],
        user_id: str,
        success: bool = True,
        error_message: str | None = None,
    ) -> None:
        """
        Record LLM usage with cost calculation.

        Args:
            action: The action type (e.g., "chat", "stream_chat")
            usage: Token usage dict with input_tokens and output_tokens
            user_id: User identifier
            success: Whether the request succeeded
            error_message: Error message if request failed
        """
        model_name = self.config.model

        # Strip vendor prefix if present (e.g., "openai/gpt-4o" -> "gpt-4o")
        if "/" in model_name:
            model_name = model_name.split("/", 1)[1]

        try:
            with db_session() as session:
                # Look up the LLM by model_id
                stmt = select(LargeLanguageModel).where(
                    LargeLanguageModel.model_id == model_name
                )
                llm = session.exec(stmt).first()

                if not llm:
                    # Record usage anyway with 0 cost for unknown models
                    logger.warning(
                        "LLM not found in catalog - usage recorded without cost",
                        model_id=model_name,
                    )
                    llm_usage = LLMUsage(
                        action=action,
                        model_id=model_name,
                        user_id=user_id,
                        timestamp=datetime.now(UTC),
                        input_tokens=usage.get("input_tokens", 0),
                        output_tokens=usage.get("output_tokens", 0),
                        total_cost=0.0,
                        success=success,
                        error_message=error_message,
                    )
                    session.add(llm_usage)
                    logger.info(
                        "Usage committed (unknown model)",
                        model=model_name,
                        tokens=usage,
                        cost=0.0,
                    )
                    return

                # Get latest price for this LLM
                price_stmt = (
                    select(LLMPrice)
                    .where(LLMPrice.llm_id == llm.id)
                    .order_by(LLMPrice.effective_date.desc())
                )
                price = session.exec(price_stmt).first()

                # Calculate cost
                if price:
                    input_cost = usage.get("input_tokens", 0) * price.input_cost_per_token
                    output_cost = usage.get("output_tokens", 0) * price.output_cost_per_token
                    total_cost = input_cost + output_cost
                else:
                    logger.warning("No price found for LLM", llm_id=llm.id)
                    total_cost = 0.0

                # Create usage record
                llm_usage = LLMUsage(
                    action=action,
                    model_id=llm.model_id,
                    user_id=user_id,
                    timestamp=datetime.now(UTC),
                    input_tokens=usage.get("input_tokens", 0),
                    output_tokens=usage.get("output_tokens", 0),
                    total_cost=total_cost,
                    success=success,
                    error_message=error_message,
                )
                session.add(llm_usage)

            # Log after commit (context manager auto-commits on exit)
            logger.info(
                "Usage committed to database",
                model=model_name,
                tokens=usage,
                cost=total_cost,
            )

        except Exception as e:
            # Don't fail the request if usage tracking fails
            logger.error("Failed to record LLM usage", error=str(e))

    def get_usage_stats(
        self,
        user_id: str | None = None,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
        recent_limit: int = 10,
    ) -> dict[str, Any]:
        """
        Get aggregated LLM usage statistics.

        All aggregations are performed at the SQL level for scalability.

        Args:
            user_id: Optional filter by user
            start_time: Optional start of time range
            end_time: Optional end of time range
            recent_limit: Number of recent activities to return

        Returns:
            dict containing totals, model breakdown, and recent activity
        """
        try:
            with db_session() as session:
                totals = self._get_usage_totals(session, user_id, start_time, end_time)
                models = self._get_model_breakdown(session, user_id, start_time, end_time)
                recent = self._get_recent_activity(
                    session, user_id, start_time, end_time, recent_limit
                )

                return {
                    **totals,
                    "models": models,
                    "recent_activity": recent,
                }
        except Exception as e:
            logger.error("Failed to get usage stats", error=str(e))
            return {
                "total_tokens": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "total_cost": 0.0,
                "total_requests": 0,
                "success_rate": 100.0,
                "models": [],
                "recent_activity": [],
            }

    def _get_usage_totals(
        self,
        session: Any,
        user_id: str | None,
        start_time: datetime | None,
        end_time: datetime | None,
    ) -> dict[str, Any]:
        """Get aggregated usage totals using SQL-level aggregations."""
        stmt = select(
            func.coalesce(func.sum(LLMUsage.input_tokens), 0).label("input_tokens"),
            func.coalesce(func.sum(LLMUsage.output_tokens), 0).label("output_tokens"),
            func.coalesce(func.sum(LLMUsage.total_cost), 0.0).label("total_cost"),
            func.count(LLMUsage.id).label("total_requests"),
            func.sum(func.cast(LLMUsage.success, Integer)).label("success_count"),
        )

        stmt = self._apply_usage_filters(stmt, user_id, start_time, end_time)
        result = session.exec(stmt).first()

        input_tokens = result.input_tokens if result else 0
        output_tokens = result.output_tokens if result else 0
        total_requests = result.total_requests if result else 0
        success_count = result.success_count if result else 0

        success_rate = (
            (success_count / total_requests * 100) if total_requests > 0 else 100.0
        )

        return {
            "total_tokens": input_tokens + output_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_cost": float(result.total_cost) if result else 0.0,
            "total_requests": total_requests,
            "success_rate": round(success_rate, 1),
        }

    def _get_model_breakdown(
        self,
        session: Any,
        user_id: str | None,
        start_time: datetime | None,
        end_time: datetime | None,
    ) -> list[dict[str, Any]]:
        """Get usage breakdown by model using SQL GROUP BY.

        Uses LEFT JOIN to include orphan usage records (models not in catalog).
        """
        stmt = (
            select(
                LLMUsage.model_id,
                func.coalesce(
                    LargeLanguageModel.title, LLMUsage.model_id
                ).label("title"),
                func.coalesce(LLMVendor.name, "unknown").label("vendor"),
                func.coalesce(LLMVendor.color, "#808080").label("vendor_color"),
                func.count(LLMUsage.id).label("requests"),
                func.coalesce(
                    func.sum(LLMUsage.input_tokens + LLMUsage.output_tokens), 0
                ).label("tokens"),
                func.coalesce(func.sum(LLMUsage.total_cost), 0.0).label("cost"),
            )
            .outerjoin(
                LargeLanguageModel, LLMUsage.model_id == LargeLanguageModel.model_id
            )
            .outerjoin(LLMVendor, LargeLanguageModel.llm_vendor_id == LLMVendor.id)
            .group_by(
                LLMUsage.model_id,
                LargeLanguageModel.title,
                LLMVendor.name,
                LLMVendor.color,
            )
            .order_by(func.count(LLMUsage.id).desc())
        )

        stmt = self._apply_usage_filters(stmt, user_id, start_time, end_time)
        results = session.exec(stmt).all()

        # Calculate total requests for percentage
        total_requests = sum(r.requests for r in results) if results else 0

        models = []
        for r in results:
            pct = (r.requests / total_requests * 100) if total_requests > 0 else 0
            models.append(
                {
                    "model_id": r.model_id,
                    "model_title": r.title,
                    "vendor": r.vendor,
                    "vendor_color": r.vendor_color,
                    "requests": r.requests,
                    "tokens": r.tokens,
                    "cost": float(r.cost),
                    "percentage": round(pct, 1),
                }
            )

        return models

    def _get_recent_activity(
        self,
        session: Any,
        user_id: str | None,
        start_time: datetime | None,
        end_time: datetime | None,
        limit: int,
    ) -> list[dict[str, Any]]:
        """Get recent usage activity with model info.

        Uses LEFT JOIN to include orphan usage records (models not in catalog).
        """
        stmt = (
            select(
                LLMUsage.timestamp,
                LLMUsage.model_id,
                LLMUsage.input_tokens,
                LLMUsage.output_tokens,
                LLMUsage.total_cost,
                LLMUsage.success,
                LLMUsage.action,
            )
            .order_by(LLMUsage.timestamp.desc())
            .limit(limit)
        )

        stmt = self._apply_usage_filters(stmt, user_id, start_time, end_time)
        results = session.exec(stmt).all()

        return [
            {
                "timestamp": r.timestamp.isoformat(),
                "model": r.model_id,
                "input_tokens": r.input_tokens,
                "output_tokens": r.output_tokens,
                "cost": float(r.total_cost),
                "success": r.success,
                "action": r.action,
            }
            for r in results
        ]

    def _apply_usage_filters(
        self,
        stmt: Any,
        user_id: str | None,
        start_time: datetime | None,
        end_time: datetime | None,
    ) -> Any:
        """Apply common filters to usage queries."""
        if user_id:
            stmt = stmt.where(LLMUsage.user_id == user_id)
        if start_time:
            stmt = stmt.where(LLMUsage.timestamp >= start_time)
        if end_time:
            stmt = stmt.where(LLMUsage.timestamp <= end_time)
        return stmt
{% endif %}
