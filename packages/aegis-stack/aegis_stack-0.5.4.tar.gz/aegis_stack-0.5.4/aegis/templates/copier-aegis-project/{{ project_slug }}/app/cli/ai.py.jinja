"""
AI service CLI commands.

Command-line interface for AI service management and chat functionality.
"""

import os
import shutil

import typer
from prompt_toolkit import PromptSession
from prompt_toolkit.formatted_text import HTML
from prompt_toolkit.key_binding import KeyBindings
from prompt_toolkit.shortcuts import CompleteStyle
from prompt_toolkit.styles import Style
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table

from app.cli.chat_completer import ChatCompleter
from app.cli.slash_commands import SlashCommandHandler
from app.cli.status_line import ChatSessionState, create_toolbar_callback
from ..core.config import settings
from ..core.log import setup_logging, suppress_logs
from ..services.ai.config import get_ai_config
from ..services.ai.models import (
    AIProvider,
    MessageRole,
    get_free_providers,
    get_provider_capabilities,
)
from ..services.ai.providers import ProviderNotInstalledError

# Initialize logging at module load
setup_logging()

# Provider display name aliases
PROVIDER_DISPLAY_NAMES: dict[str, str] = {
    "public": "LLM7.io",
    "unknown": "LLM7.io",
}


def get_provider_display_name(provider: AIProvider | str) -> str:
    """Get display name for a provider, with aliases for branding."""
    value = provider.value if isinstance(provider, AIProvider) else provider
    return PROVIDER_DISPLAY_NAMES.get(value, value)


app = typer.Typer(help="AI service management and chat commands")
console = Console()


@app.command()
def status() -> None:
    """Show AI service status, configuration, and validation."""
    ai_config = get_ai_config(settings)

    typer.secho("AI Service Status", fg=typer.colors.BLUE, bold=True)
    typer.secho("=" * 40, dim=True)

    # Basic info
    typer.echo(
        typer.style("Engine: ", fg=typer.colors.CYAN) + "{{ ai_framework }}"
    )
    status_color = typer.colors.GREEN if ai_config.enabled else typer.colors.RED
    status_text = "Enabled" if ai_config.enabled else "Disabled"
    typer.echo(
        typer.style("Status: ", fg=typer.colors.CYAN)
        + typer.style(status_text, fg=status_color)
    )
    typer.echo(
        typer.style("Provider: ", fg=typer.colors.CYAN) + get_provider_display_name(ai_config.provider)
    )
    typer.echo(typer.style("Model: ", fg=typer.colors.CYAN) + str(ai_config.model))
    typer.echo(
        typer.style("Temperature: ", fg=typer.colors.CYAN)
        + str(ai_config.temperature)
    )
    typer.echo(
        typer.style("Max Tokens: ", fg=typer.colors.CYAN) + str(ai_config.max_tokens)
    )

    # API Key status
    provider_config = ai_config.get_provider_config(settings)
    api_key_color = typer.colors.GREEN if provider_config.api_key else typer.colors.RED
    api_key_text = "Set" if provider_config.api_key else "Not set"
    typer.echo(
        typer.style("API Key: ", fg=typer.colors.CYAN)
        + typer.style(api_key_text, fg=api_key_color)
    )

    # Validation
    typer.echo("")
    errors = ai_config.validate_configuration(settings)
    if not errors:
        typer.secho("✓ Configuration valid", fg=typer.colors.GREEN)
        capabilities = get_provider_capabilities(ai_config.provider)
        if capabilities.free_tier_available:
            typer.echo("  " + typer.style("Free tier", fg=typer.colors.CYAN))
        if capabilities.supports_streaming:
            typer.echo("  " + typer.style("Streaming supported", fg=typer.colors.CYAN))
    else:
        typer.secho("✗ Configuration issues:", fg=typer.colors.RED)
        for error in errors:
            typer.echo("  " + typer.style("•", fg=typer.colors.RED) + f" {error}")

        # Suggest free providers if API key issues
        if any("API key" in error for error in errors):
            free_providers = get_free_providers()
            if free_providers:
                providers_list = ", ".join(p.value for p in free_providers)
                typer.echo("")
                typer.echo(
                    typer.style("Tip: ", fg=typer.colors.YELLOW)
                    + f"Try free providers: {providers_list}"
                )

    # Available providers count
    available = ai_config.get_available_providers(settings)
    typer.echo("")
    typer.echo(
        typer.style("Available providers: ", fg=typer.colors.CYAN)
        + f"{len(available)} (run 'ai providers' to list)"
    )


@app.command()
def providers() -> None:
    """List all available AI providers with installation status."""
    from ..services.ai.provider_management import check_provider_dependency_installed

    ai_config = get_ai_config(settings)
    available = ai_config.get_available_providers(settings)
    free_providers = get_free_providers()

    table = Table(title="AI Providers", width=115)
    table.add_column("Provider", style="cyan", width=10)
    table.add_column("Installed", width=9)
    table.add_column("API Key", width=8)
    table.add_column("Status", style="green", width=26, no_wrap=True)
    table.add_column("Free", style="yellow", width=4)
    table.add_column("Stream", width=6, justify="center")
    table.add_column("Functions", width=9, justify="center")
    table.add_column("Vision", width=6, justify="center")

    for provider in AIProvider:
        capabilities = get_provider_capabilities(provider)
        is_installed = check_provider_dependency_installed(provider.value)
        is_available = provider in available
        is_current = provider == ai_config.provider

        # Determine API key status
        # LOCAL providers (PUBLIC, OLLAMA) don't require API keys
        local_providers = {AIProvider.PUBLIC, AIProvider.OLLAMA}
        if provider in local_providers:
            has_api_key = True  # Local providers don't need API keys
            api_key_display = "[dim]N/A[/dim]"
        else:
            env_var = f"{provider.value.upper()}_API_KEY"
            has_api_key = bool(getattr(settings, env_var, None))
            api_key_display = "[green]Yes[/green]" if has_api_key else "[red]No[/red]"

        # Determine status
        if is_current:
            if not is_installed:
                status = "[bold red]Current (Not installed)[/bold red]"
            elif not has_api_key and provider not in local_providers:
                status = "[bold yellow]Current (Need API key)[/bold yellow]"
            elif provider == AIProvider.OLLAMA:
                status = "[bold green]Current (Local)[/bold green]"
            else:
                status = "[bold green]Current[/bold green]"
        elif is_available:
            status = "Ready" if provider not in local_providers else "Local"
        elif is_installed and not has_api_key:
            status = "[yellow]Need API key[/yellow]"
        elif is_installed and provider == AIProvider.OLLAMA:
            status = "[cyan]Local[/cyan]"
        elif not is_installed:
            status = "[red]Not installed[/red]"
        else:
            status = "[red]Error[/red]"

        installed_display = "[green]Yes[/green]" if is_installed else "[red]No[/red]"
        free_tier = "Yes" if provider in free_providers else "No"

        table.add_row(
            get_provider_display_name(provider),
            installed_display,
            api_key_display,
            status,
            free_tier,
            "[green]Yes[/green]" if capabilities.supports_streaming else "[dim]No[/dim]",
            "[green]Yes[/green]" if capabilities.supports_function_calling else "[dim]No[/dim]",
            "[green]Yes[/green]" if capabilities.supports_vision else "[dim]No[/dim]",
        )

    console.print(table)
    console.print()
    console.print(
        "[dim]Tip: Run '{{ project_slug }} ai add-provider <name>' "
        "to install missing providers.[/dim]"
    )


@app.command("add-provider")
def add_provider(
    provider: str = typer.Argument(
        ...,
        help="Provider name (openai, anthropic, google, groq, mistral, cohere)",
    ),
    set_default: bool = typer.Option(
        True,
        "--set-default/--no-set-default",
        help="Set as default provider after adding",
    ),
    skip_api_key: bool = typer.Option(
        False,
        "--skip-api-key",
        help="Skip API key configuration",
    ),
    yes: bool = typer.Option(
        False,
        "--yes",
        "-y",
        help="Skip confirmation prompts",
    ),
) -> None:
    """
    Add a new AI provider to your project.

    Installs required dependencies and optionally configures the API key.

    Examples:
        ai add-provider google           # Add Google with API key prompt
        ai add-provider groq --skip-api-key  # Just install dependencies
        ai add-provider anthropic -y     # Install without prompts
    """
    from rich.progress import Progress, SpinnerColumn, TextColumn

    from ..services.ai.provider_management import (
        PROVIDER_API_KEY_URLS,
        get_env_var_name,
        get_existing_api_key,
        get_missing_dependency,
        get_valid_provider_names,
        install_provider_dependency,
        update_env_file,
        validate_provider_name,
    )

    # Validate provider name
    provider_enum = validate_provider_name(provider)
    if not provider_enum:
        valid_names = ", ".join(get_valid_provider_names())
        console.print(f"[red]Invalid provider: {provider}[/red]")
        console.print(f"[dim]Valid providers: {valid_names}[/dim]")
        raise typer.Exit(1)

    console.print()
    console.print(f"[bold cyan]Adding {provider} provider...[/bold cyan]")
    console.print()

    # Check if dependency is installed
    missing_dep = get_missing_dependency(provider)
    if missing_dep:
        console.print(f"  [dim]>[/dim] Checking dependencies...")
        # Escape brackets for Rich (otherwise [google] is interpreted as style)
        display_dep = missing_dep.replace("[", r"\[")
        console.print(f"    [yellow]{display_dep} not installed[/yellow]")
        console.print()

        # Prompt for installation unless --yes
        if not yes:
            confirm = typer.confirm(f"Install {missing_dep}?", default=True)
            if not confirm:
                console.print("[yellow]Installation cancelled[/yellow]")
                raise typer.Exit(0)

        # Install with progress spinner
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
            transient=True,
        ) as progress:
            progress.add_task(description="Installing...", total=None)
            success, message = install_provider_dependency(provider)

        if success:
            console.print(f"  [green]✓[/green] {message}")
        else:
            console.print(f"  [red]✗[/red] {message}")
            console.print()
            console.print("[yellow]Try installing manually:[/yellow]")
            console.print(f"  [cyan]uv add {missing_dep}[/cyan]")
            console.print("  [dim]or[/dim]")
            console.print(f"  [cyan]pip install {missing_dep}[/cyan]")
            raise typer.Exit(1)
    else:
        console.print(f"  [green]✓[/green] Dependencies already installed")

    # Handle API key configuration
    if provider_enum != AIProvider.PUBLIC and not skip_api_key:
        env_var_name = get_env_var_name(provider)
        existing_key = get_existing_api_key(provider)

        console.print()
        if existing_key:
            console.print(f"  [green]✓[/green] {env_var_name} already set")
        else:
            api_key_url = PROVIDER_API_KEY_URLS.get(provider.lower())
            console.print(f"  [dim]>[/dim] Checking API key...")
            console.print(f"    [yellow]No {env_var_name} found[/yellow]")
            console.print()

            if api_key_url:
                console.print(f"  Get your API key at: [cyan]{api_key_url}[/cyan]")
                console.print()

            # Prompt for API key
            try:
                api_key = Prompt.ask(
                    f"  Enter your {provider.upper()} API key (or press Enter to skip)",
                    console=console,
                    password=True,
                    default="",
                )
            except (KeyboardInterrupt, EOFError):
                console.print("\n[yellow]Skipped API key configuration[/yellow]")
                api_key = ""

            if api_key:
                update_env_file({env_var_name: api_key})
                console.print(f"  [green]✓[/green] {env_var_name} saved to .env")
            else:
                console.print(
                    f"  [yellow]![/yellow] Remember to set {env_var_name} in .env"
                )

    # Set as default provider
    if set_default:
        update_env_file({"AI_PROVIDER": provider.lower()})
        console.print()
        console.print(f"  [green]✓[/green] AI_PROVIDER set to [cyan]{provider}[/cyan]")

    # Success message
    console.print()
    console.print("[bold green]Provider added successfully![/bold green]")
    console.print()
    console.print(f"[dim]Test with: {{ project_slug }} ai chat \"Hello\"[/dim]")


@app.command("use-provider")
def use_provider(
    provider: str = typer.Argument(
        ...,
        help="Provider name to switch to",
    ),
) -> None:
    """
    Switch to a different AI provider.

    Validates that dependencies are installed and API key is configured.

    Examples:
        ai use-provider google    # Switch to Google
        ai use-provider openai    # Switch to OpenAI
        ai use-provider public    # Switch to free PUBLIC provider
    """
    from ..services.ai.provider_management import (
        check_provider_dependency_installed,
        get_env_var_name,
        get_existing_api_key,
        get_valid_provider_names,
        update_env_file,
        validate_provider_name,
    )

    # Validate provider name
    provider_enum = validate_provider_name(provider)
    if not provider_enum:
        valid_names = ", ".join(get_valid_provider_names())
        console.print(f"[red]Invalid provider: {provider}[/red]")
        console.print(f"[dim]Valid providers: {valid_names}[/dim]")
        raise typer.Exit(1)

    console.print()
    console.print(f"[bold cyan]Switching to {provider} provider...[/bold cyan]")
    console.print()

    # Check if dependency is installed
    if not check_provider_dependency_installed(provider):
        console.print(f"  [red]✗[/red] Dependencies not installed for {provider}")
        console.print()
        console.print("[yellow]Run this command first:[/yellow]")
        console.print(
            f"  [cyan]{{ project_slug }} ai add-provider {provider}[/cyan]"
        )
        raise typer.Exit(1)

    console.print(f"  [green]✓[/green] Dependencies installed")

    # Check API key (except for PUBLIC)
    if provider_enum != AIProvider.PUBLIC:
        env_var_name = get_env_var_name(provider)
        existing_key = get_existing_api_key(provider)

        if not existing_key:
            console.print(f"  [yellow]![/yellow] No {env_var_name} configured")
            console.print()
            console.print("[yellow]Consider running:[/yellow]")
            console.print(
                f"  [cyan]{{ project_slug }} ai add-provider {provider}[/cyan]"
            )
            console.print("[dim]to configure your API key[/dim]")
            console.print()

            # Still allow switching, just warn
            confirm = typer.confirm("Switch anyway?", default=False)
            if not confirm:
                raise typer.Exit(0)
        else:
            console.print(f"  [green]✓[/green] {env_var_name} configured")

    # Update AI_PROVIDER in .env
    update_env_file({"AI_PROVIDER": provider.lower()})

    console.print()
    console.print(f"[bold green]Switched to {provider}![/bold green]")
    console.print()
    console.print(f"[dim]Current provider: {provider}[/dim]")


@app.command()
def chat(
    message: str | None = typer.Argument(None, help="Message to send to AI"),
    stream: bool = typer.Option(
        True, "--stream/--no-stream", help="Enable streaming output"
    ),
    conversation_id: str | None = typer.Option(
        None, "--conversation-id", "-c", help="Continue existing conversation"
    ),
    new: bool = typer.Option(
        False, "--new", "-n", help="Start a fresh conversation (don't resume)"
    ),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show conversation metadata"
    ),
{% if ai_rag %}
    use_rag: bool | None = typer.Option(
        None, "--rag/--no-rag", help="Enable RAG (auto-enables if collections exist)"
    ),
    collection: str | None = typer.Option(
        None, "--collection", help="RAG collection to search"
    ),
    top_k: int | None = typer.Option(
        None, "--top-k", "-k", help="Number of RAG results to use"
    ),
    show_sources: bool = typer.Option(
        False, "--sources/--no-sources", help="Show source references in output"
    ),
{% endif %}
) -> None:
    """Send a chat message or start interactive session.

    By default, resumes the most recent conversation. Use --new for a fresh start.

    Examples:
        ai chat "What is Python?"     - Send message (resumes last conversation)
        ai chat                       - Interactive session (resumes last conversation)
        ai chat --new                 - Start fresh conversation
        ai chat -c abc123 "Continue"  - Continue a specific conversation
{% if ai_rag %}
        ai chat --rag "How does auth work?" - Query with RAG context
        ai chat --rag -k 10 --collection my-codebase "Explain the API"
{% endif %}
    """
    import asyncio

    from app.services.ai.service import AIService

    async def run_chat() -> None:
        nonlocal conversation_id
        try:
            with suppress_logs():
                ai_service = AIService(settings)

                # Resume most recent conversation by default
                if not new and not conversation_id:
                    convos = ai_service.list_conversations(user_id)
                    if convos:
                        conversation_id = convos[0].id
                        typer.echo(
                            f"Resuming conversation {conversation_id[:8]}... "
                            "(use --new for fresh start)",
                            err=True,
                        )

{% if ai_rag %}
                # Auto-detect RAG if not explicitly set
                rag_enabled = use_rag
                rag_collection = collection
                if use_rag is None:
                    from app.services.rag.config import get_rag_config
                    from app.services.rag.service import RAGService

                    try:
                        rag_config = get_rag_config(settings)
                        rag_service = RAGService(rag_config)
                        collections = await rag_service.list_collections()
                        if collections:
                            rag_enabled = True
                            if not rag_collection:
                                rag_collection = collections[0]
                            typer.echo(
                                f"RAG auto-enabled with collection '{rag_collection}' "
                                "(use --no-rag to disable)",
                                err=True,
                            )
                        else:
                            rag_enabled = False
                    except Exception:
                        rag_enabled = False

{% endif %}
                if message:
                    # Single message mode
                    await _send_message(
                        ai_service,
                        message,
                        conversation_id,
                        user_id,
                        stream,
                        verbose,
{% if ai_rag %}
                        use_rag=rag_enabled,
                        rag_collection=rag_collection,
                        rag_top_k=top_k,
                        show_sources=show_sources,
{% endif %}
                    )
                else:
                    # Interactive session mode
                    await _interactive_chat_session(
                        ai_service,
                        conversation_id,
{% if ai_rag %}
                        use_rag=rag_enabled,
                        rag_collection=rag_collection,
                        rag_top_k=top_k,
                        show_sources=show_sources,
{% endif %}
                    )
        except KeyboardInterrupt:
            typer.echo("\nChat interrupted", err=True)
            raise typer.Exit(1)
        except Exception as e:
            error_str = str(e)
{% if ai_rag %}
            # Provide helpful guidance for RAG collection errors
            if "No RAG collection specified" in error_str:
                console.print()
                console.print("[yellow]RAG requires indexed content.[/yellow]")
                console.print()
                console.print("To get started:")
                console.print()
                console.print("  1. Index your codebase:")
                console.print("     [cyan]{{ project_slug }} rag index . --collection my-code[/cyan]")
                console.print()
                console.print("  2. Query with the collection:")
                console.print('     [cyan]{{ project_slug }} ai chat --rag --collection my-code "Your question"[/cyan]')
                console.print()
                console.print("  [dim]Tip: Run '{{ project_slug }} rag list' to see existing collections.[/dim]")
                raise typer.Exit(1)
{% endif %}
            typer.echo(f"Error: {e}", err=True)
            raise typer.Exit(1)

    asyncio.run(run_chat())


@app.command()
def conversations(
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
    limit: int = typer.Option(
        10, "--limit", "-l", help="Number of conversations to show"
    ),
) -> None:
    """List conversations for a user."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    convos = ai_service.list_conversations(user_id)[:limit]

    if not convos:
        typer.echo(f"No conversations found for user: {user_id}")
        return

    typer.echo(f"Conversations for {user_id}:")
    typer.echo("")

    for conv in convos:
        title = conv.title or "Untitled"
        messages = conv.get_message_count()
        updated = conv.updated_at.strftime("%Y-%m-%d %H:%M")

        typer.echo(f"• {conv.id[:8]}... - {title}")
        typer.echo(f"  {messages} messages | {updated}")
        typer.echo("")


@app.command()
def history(
    conversation_id: str = typer.Argument(..., help="Conversation ID"),
    user_id: str = typer.Option("cli-user", "--user-id", "-u", help="User identifier"),
) -> None:
    """View conversation history."""
    from app.services.ai.service import AIService

    with suppress_logs():
        ai_service = AIService(settings)
    conversation = ai_service.get_conversation(conversation_id)

    if not conversation:
        typer.echo(f"Error: Conversation not found: {conversation_id}")
        raise typer.Exit(1)

    # Check if user owns conversation
    if conversation.metadata.get("user_id") != user_id:
        typer.echo("Error: Access denied: You don't own this conversation")
        raise typer.Exit(1)

    typer.echo(f"Conversation: {conversation_id}")
    if conversation.title:
        typer.echo(f"Title: {conversation.title}")
    typer.echo(f"Provider: {get_provider_display_name(conversation.provider)}")
    typer.echo(f"Messages: {conversation.get_message_count()}")
    typer.echo("")

    for i, msg in enumerate(conversation.messages):
        timestamp = msg.timestamp.strftime("%H:%M:%S")
        role_icon = "" if msg.role == MessageRole.USER else ""

        typer.echo(f"{role_icon} [{timestamp}] {msg.content}")
        if i < len(conversation.messages) - 1:
            typer.echo("")


{% if ai_backend != "memory" %}
# ============================================================================
# Usage Statistics Command (requires database backend)
# ============================================================================


@app.command()
def usage(
    url: str = typer.Option(
        None,
        "--url",
        "-u",
        help="API base URL (default: localhost:8000)",
    ),
    user_id: str | None = typer.Option(
        None,
        "--user-id",
        help="Filter by user ID",
    ),
    recent: int = typer.Option(
        10,
        "--recent",
        "-r",
        help="Number of recent activities to show",
        min=1,
        max=50,
    ),
    json_output: bool = typer.Option(
        False,
        "--json",
        "-j",
        help="Output raw JSON",
    ),
) -> None:
    """
    Show AI usage statistics.

    Displays comprehensive LLM usage analytics including token counts,
    costs, model breakdown, and recent activity.

    Examples:
        ai usage              - Show full usage report
        ai usage --json       - Output raw JSON data
        ai usage -r 20        - Show 20 recent activities
    """
    import asyncio
    import json
    import sys
    from datetime import datetime

    import httpx
    from pydantic import BaseModel

    from app.core.constants import APIEndpoints, Defaults
    from app.core.formatting import format_cost, format_number, format_percentage

    # Response models matching API schema
    class ModelUsageStats(BaseModel):
        model_id: str
        model_title: str
        vendor: str
        vendor_color: str
        requests: int
        tokens: int
        cost: float
        percentage: float

    class RecentActivity(BaseModel):
        timestamp: str
        model: str
        input_tokens: int
        output_tokens: int
        cost: float
        success: bool
        action: str

    class UsageStatsResponse(BaseModel):
        total_tokens: int
        input_tokens: int
        output_tokens: int
        total_cost: float
        total_requests: int
        success_rate: float
        models: list[ModelUsageStats]
        recent_activity: list[RecentActivity]

    # CLI-specific color utilities
    def get_success_color(rate: float) -> str:
        if rate >= 95:
            return "green"
        elif rate >= 66.7:  # At least 2/3 success rate
            return "yellow"
        return "red"

    def get_vendor_color(vendor: str, fallback: str = "white") -> str:
        colors = {
            "openai": "green",
            "anthropic": "bright_magenta",
            "google": "blue",
            "groq": "yellow",
            "mistral": "cyan",
            "cohere": "bright_red",
            "public": "bright_cyan",
            "llm7.io": "bright_cyan",
        }
        return colors.get(vendor.lower(), fallback)

    def get_vendor_display_name(vendor: str) -> str:
        """Get display name for a vendor, with aliases for branding."""
        return PROVIDER_DISPLAY_NAMES.get(vendor.lower(), vendor)

    # Display functions
    def display_summary_panel(stats: UsageStatsResponse) -> None:
        success_color = get_success_color(stats.success_rate)
        summary_lines = [
            f"[bold cyan]Total Tokens:[/bold cyan]   {format_number(stats.total_tokens)}",
            f"[bold cyan]Total Cost:[/bold cyan]     {format_cost(stats.total_cost)}",
            f"[bold cyan]Total Requests:[/bold cyan] {format_number(stats.total_requests)}",
            f"[bold cyan]Success Rate:[/bold cyan]   "
            f"[{success_color}]{format_percentage(stats.success_rate)}[/{success_color}]",
        ]
        console.print(
            Panel(
                "\n".join(summary_lines),
                title="[bold magenta]AI Usage Summary[/bold magenta]",
                border_style="magenta",
                padding=(1, 2),
            )
        )

    def display_token_breakdown(stats: UsageStatsResponse) -> None:
        total = stats.total_tokens
        if total == 0:
            console.print("\n[dim]No token usage recorded.[/dim]")
            return
        input_pct = (stats.input_tokens / total) * 100
        output_pct = (stats.output_tokens / total) * 100
        console.print("\n[bold blue]Token Breakdown[/bold blue]")
        input_str = format_number(stats.input_tokens)
        output_str = format_number(stats.output_tokens)
        console.print(f"  [cyan]Input:[/cyan]  {input_str:>12} ({input_pct:.0f}%)")
        console.print(f"  [cyan]Output:[/cyan] {output_str:>12} ({output_pct:.0f}%)")
        bar_width = 40
        input_bars = int((input_pct / 100) * bar_width)
        output_bars = bar_width - input_bars
        bar = (
            f"[purple]{'█' * input_bars}[/purple]"
            f"[bright_magenta]{'█' * output_bars}[/bright_magenta]"
        )
        console.print(f"\n  {bar}")
        legend = "[purple]█ Input[/purple]  [bright_magenta]█ Output[/bright_magenta]"
        console.print(f"  {legend}")

    def display_model_usage(stats: UsageStatsResponse) -> None:
        if not stats.models:
            console.print("\n[dim]No model usage data available.[/dim]")
            return
        console.print("\n[bold blue]Model Usage[/bold blue]")
        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("Model", style="cyan", no_wrap=True)
        table.add_column("Vendor", style="dim")
        table.add_column("Requests", justify="right")
        table.add_column("Tokens", justify="right")
        table.add_column("Cost", justify="right", style="green")
        table.add_column("Share", justify="right")
        for model in stats.models:
            vendor_color = get_vendor_color(model.vendor, model.vendor_color)
            vendor_display = get_vendor_display_name(model.vendor)
            table.add_row(
                model.model_title,
                f"[{vendor_color}]{vendor_display}[/{vendor_color}]",
                format_number(model.requests),
                format_number(model.tokens),
                format_cost(model.cost),
                format_percentage(model.percentage),
            )
        console.print(table)

    def display_recent_activity(stats: UsageStatsResponse) -> None:
        if not stats.recent_activity:
            console.print("\n[dim]No recent activity.[/dim]")
            return
        console.print("\n[bold blue]Recent Activity[/bold blue]")
        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("Time", style="dim")
        table.add_column("Model", style="cyan")
        table.add_column("Action")
        table.add_column("Tokens", justify="right")
        table.add_column("Cost", justify="right", style="green")
        table.add_column("Status", justify="center")
        for activity in stats.recent_activity:
            try:
                dt = datetime.fromisoformat(activity.timestamp.replace("Z", "+00:00"))
                time_str = dt.strftime("%H:%M:%S")
            except ValueError:
                time_str = activity.timestamp[:8]
            status = "[green]OK[/green]" if activity.success else "[red]FAIL[/red]"
            total_tokens = activity.input_tokens + activity.output_tokens
            table.add_row(
                time_str,
                activity.model,
                activity.action,
                format_number(total_tokens),
                format_cost(activity.cost),
                status,
            )
        console.print(table)

    # Async fetch function
    async def get_usage_stats(
        base_url: str,
        filter_user_id: str | None = None,
        recent_limit: int = 10,
    ) -> UsageStatsResponse:
        api_url = f"{base_url}{APIEndpoints.AI_USAGE_STATS}"
        params: dict[str, str | int] = {"recent_limit": recent_limit}
        if filter_user_id:
            params["user_id"] = filter_user_id
        timeout = httpx.Timeout(Defaults.API_TIMEOUT)
        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                response = await client.get(api_url, params=params)
                response.raise_for_status()
                return UsageStatsResponse.model_validate(response.json())
            except httpx.ConnectError:
                raise ConnectionError(
                    f"Cannot connect to API server at {base_url}. "
                    "Make sure the application is running."
                ) from None
            except httpx.TimeoutException:
                raise TimeoutError(
                    f"API request timed out after {Defaults.API_TIMEOUT} seconds."
                ) from None
            except httpx.HTTPStatusError as e:
                raise RuntimeError(
                    f"API error {e.response.status_code}: {e.response.text}"
                ) from None

    # Main execution
    base_url = url or getattr(settings, "API_BASE_URL", "http://localhost:8000")
    try:
        stats = asyncio.run(get_usage_stats(base_url, user_id, recent))
        if json_output:
            print(json.dumps(stats.model_dump(), indent=2))
            return
        display_summary_panel(stats)
        display_token_breakdown(stats)
        display_model_usage(stats)
        display_recent_activity(stats)
    except ConnectionError as e:
        console.print(f"[red]Connection Error:[/red] {e}")
        sys.exit(1)
    except TimeoutError as e:
        console.print(f"[red]Timeout Error:[/red] {e}")
        sys.exit(1)
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        sys.exit(1)
{% endif %}


# ============================================================================
# Internal helper functions
# ============================================================================


async def _send_message(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    stream: bool,
    verbose: bool,
{% if ai_rag %}
    use_rag: bool = False,
    rag_collection: str | None = None,
    rag_top_k: int | None = None,
    show_sources: bool = True,
{% endif %}
) -> None:
    """Send a single message and display the response."""
    # Disable streaming for PUBLIC provider (fake streaming causes duplicates)
    use_streaming = stream
    if ai_service.config.provider == AIProvider.PUBLIC:
        use_streaming = False

    if use_streaming:
        await _stream_chat_response(
            ai_service,
            message,
            conversation_id,
            user_id,
            verbose=verbose,
{% if ai_rag %}
            use_rag=use_rag,
            rag_collection=rag_collection,
            rag_top_k=rag_top_k,
            show_sources=show_sources,
{% endif %}
        )
    else:
        # Show thinking spinner for non-streaming responses
        from rich.live import Live
        from rich.spinner import Spinner

        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            response = await ai_service.chat(
                message=message,
                conversation_id=conversation_id,
                user_id=user_id,
{% if ai_rag %}
                use_rag=use_rag,
                rag_collection=rag_collection,
                rag_top_k=rag_top_k,
{% endif %}
            )
        finally:
            spinner_live.stop()

        # Use shared rendering functions
        from app.cli.ai_rendering import (
            render_ai_header,
            render_conversation_metadata,
            render_markdown_response,
        )

        # Show conversation info (only in verbose mode)
        conv_id = response.metadata.get("conversation_id", "unknown")
        conversation = ai_service.get_conversation(conv_id)
        if verbose and conversation:
            typer.echo(f"Conversation: {conversation.id}")
            if conversation.title:
                typer.echo(f"Title: {conversation.title}")
            console.print()

        # Render response
        console.print()  # Blank line between You: and Illiana:
        render_ai_header(console, inline=True)
        render_markdown_response(console, response.content)

{% if ai_rag %}
        # Display source references if RAG was used
        if show_sources and response.metadata.get("rag_sources"):
            console.print()
            console.print("---", style="dim")
            console.print("[bold]Sources:[/bold]", style="cyan")
            for i, source in enumerate(response.metadata["rag_sources"], 1):
                file_name = source.get("file", "Unknown")
                start_line = source.get("start_line")
                end_line = source.get("end_line")
                score = source.get("score", 0.0)
                # Prefer line numbers, fall back to chunk index
                if start_line and end_line:
                    location = f"lines {start_line}-{end_line}"
                else:
                    location = f"chunk {source.get('chunk_index', 0)}"
                console.print(
                    f"  [{i}] {file_name} ({location}) - score: {score:.2f}",
                    style="dim",
                )

{% endif %}
        # Show response metadata (only in verbose mode)
        if verbose and conversation:
            response_time = conversation.metadata.get("last_response_time_ms")
            render_conversation_metadata(
                console,
                conversation.id,
                message_count=conversation.get_message_count(),
                response_time=response_time,
            )


async def _interactive_chat_session(
    ai_service,
    conversation_id: str | None = None,
{% if ai_rag %}
    use_rag: bool = False,
    rag_collection: str | None = None,
    rag_top_k: int | None = None,
    show_sources: bool = False,
{% endif %}
) -> None:
    """Start an interactive chat session with continuous conversation."""
    import asyncio

    from app import __aegis_version__

    # Illiana boot sequence
    ai_config = get_ai_config(settings)

    console.print()
    console.print(f"[bold bright_magenta]Illiana[/bold bright_magenta] [dim]v{__aegis_version__}[/dim]")
    console.print()

    # Boot steps with brief delays for effect
    console.print("  [dim]>[/dim] Initializing...", end="")
    await asyncio.sleep(0.15)
    console.print(" [green]OK[/green]")

    console.print(f"  [dim]>[/dim] Connecting to [cyan]{get_provider_display_name(ai_config.provider)}[/cyan]...", end="")
    # Warm up the agent (lazy imports, model initialization)
    from app.services.ai.providers import get_agent
    _ = get_agent(ai_config, settings)
    console.print(" [green]OK[/green]")

    console.print(f"  [dim]>[/dim] Model: [cyan]{ai_config.model}[/cyan]")

{% if ai_rag %}
    # Check RAG status - verify collections exist if RAG is enabled
    if use_rag:
        try:
            from app.services.rag.config import get_rag_config
            from app.services.rag.service import RAGService

            rag_config = get_rag_config(settings)
            rag_service = RAGService(rag_config)
            collections = await rag_service.list_collections()
            if collections:
                rag_status = "[green]ON[/green]"
            else:
                rag_status = "[yellow]OFF[/yellow] [dim](no collections)[/dim]"
        except Exception:
            rag_status = "[yellow]OFF[/yellow] [dim](unavailable)[/dim]"
    else:
        rag_status = "[dim]OFF[/dim]"
    console.print(f"  [dim]>[/dim] RAG: {rag_status}")
{% endif %}

    # Fetch and display health status
    console.print("  [dim]>[/dim] Health: ", end="")
    try:
        from app.services.system.health import get_system_status

        status = await get_system_status()
        health_pct = status.health_percentage
        if status.overall_healthy:
            console.print(f"[green]OK[/green] [dim]({health_pct:.0f}%)[/dim]")
        else:
            unhealthy_count = len(status.unhealthy_components)
            console.print(f"[yellow]DEGRADED[/yellow] [dim]({health_pct:.0f}%, {unhealthy_count} issues)[/dim]")
    except Exception:
        console.print("[dim]N/A[/dim]")

    await asyncio.sleep(0.1)
    console.print()
    console.print("  [bold green]Online[/bold green]")
    console.print()
    console.print("[dim]Type /help for commands • Esc then Enter to insert newline • 'exit' to quit[/dim]")
    console.print()

    # Track conversation for context
    current_conversation_id = conversation_id

    # Load cumulative tokens/cost from resumed conversation
    initial_tokens = 0
    initial_cost = 0.0
    if current_conversation_id:
        resumed_conversation = ai_service.get_conversation(current_conversation_id)
        if resumed_conversation:
            initial_tokens = resumed_conversation.metadata.get("cumulative_tokens", 0)
            initial_cost = resumed_conversation.metadata.get("cumulative_cost", 0.0)

    # Initialize status line state for prompt_toolkit toolbar
    session_state = ChatSessionState(
        provider=get_provider_display_name(ai_config.provider),
        model=ai_config.model,
{% if ai_rag %}
        rag_enabled=use_rag,
        rag_collection=rag_collection,
        show_sources=show_sources,
{% else %}
        rag_enabled=False,
        rag_collection=None,
        show_sources=False,
{% endif %}
        cumulative_tokens=initial_tokens,
        cumulative_cost=initial_cost,
        version=__aegis_version__,
    )
    toolbar_callback = create_toolbar_callback(session_state)

    # Initialize slash command handler for in-session commands
    command_handler = SlashCommandHandler(
        ai_service=ai_service,
        session_state=session_state,
        console=console,
        current_conversation_id=current_conversation_id,
{% if ai_rag %}
        rag_enabled=use_rag,
        rag_collection=rag_collection,
        show_sources=show_sources,
{% endif %}
    )

    # Pre-load model cache for tab completion (includes Ollama models)
    await command_handler.load_model_cache()
{% if ai_rag %}
    # Pre-load collection cache for /rag tab completion
    await command_handler.load_collection_cache()
{% endif %}
    # Create completer for slash command autocomplete
    chat_completer = ChatCompleter(command_handler)

    # Create key bindings for common operations
    key_bindings = KeyBindings()

    @key_bindings.add("c-l")
    def _clear_screen(event: object) -> None:
        """Clear screen on Ctrl+L."""
        os.system("cls" if os.name == "nt" else "clear")

    @key_bindings.add("enter")
    def _submit(event: object) -> None:
        """Submit on Enter."""
        event.current_buffer.validate_and_handle()

    @key_bindings.add("escape", "enter")
    def _newline(event: object) -> None:
        """Insert newline on Escape+Enter (or Alt+Enter)."""
        event.current_buffer.insert_text("\n")

    # Create prompt session for async input with status line
    # Style: toolbar blends with terminal, completion menu has dark theme
    prompt_style = Style.from_dict({
        "bottom-toolbar": "noreverse",
        # Dark completion menu styling
        "completion-menu": "bg:#1a1a1a",
        "completion-menu.completion": "fg:#cccccc bg:#1a1a1a",
        "completion-menu.completion.current": "fg:#ffffff bg:#0066cc bold",
        "completion-menu.meta": "fg:#666666 bg:#1a1a1a",
        "completion-menu.meta.completion.current": "fg:#ffffff bg:#0066cc",
    })
    prompt_session: PromptSession[str] = PromptSession(
        message=HTML("<b><ansigreen>You: </ansigreen></b>"),
        bottom_toolbar=toolbar_callback,
        style=prompt_style,
        completer=chat_completer,
        key_bindings=key_bindings,
        complete_style=CompleteStyle.COLUMN,  # Show single-column popup menu
        multiline=True,  # Enable multi-line input (Esc+Enter or Alt+Enter for newline)
    )

    # Divider before chat starts
    terminal_width = shutil.get_terminal_size().columns
    console.print("─" * terminal_width, style="dim")

    while True:
        try:
            # Get user input with prompt_toolkit async (supports status line)
            try:
                user_message = await prompt_session.prompt_async()
            except (KeyboardInterrupt, EOFError):
                console.print("\n[yellow]Chat session ended[/yellow]")
                break

            # Handle slash commands first
            if command_handler.is_slash_command(user_message):
                result = await command_handler.execute(user_message)
                if result:
                    if result.message:
                        style = "red" if not result.success else "dim"
                        console.print(Panel(result.message, border_style=style))
                    if result.should_exit:
                        break
                    if result.new_conversation_id == "new":
                        current_conversation_id = None
                        command_handler.current_conversation_id = None
                    # Update session state if command changed settings
                    if result.update_provider or result.update_model:
                        session_state.update_provider(
                            result.update_provider or session_state.provider,
                            result.update_model or session_state.model,
                        )
{% if ai_rag %}
                    if result.update_rag is not None:
                        use_rag = result.update_rag
                        session_state.toggle_rag(use_rag, result.update_rag_collection)
                        command_handler.rag_enabled = use_rag
                        if result.update_rag_collection:
                            rag_collection = result.update_rag_collection
                            command_handler.rag_collection = rag_collection
                    if result.update_show_sources is not None:
                        show_sources = result.update_show_sources
                        session_state.toggle_sources(show_sources)
                        command_handler.show_sources = show_sources
{% endif %}
                continue

            # Check for exit commands (kept for backwards compatibility)
            if user_message.lower().strip() in ["exit", "quit", "bye", "q"]:
                console.print("[yellow]Goodbye![/yellow]")
                break

            if not user_message.strip():
                console.print("[dim]Please enter a message or type /help.[/dim]")
                continue

            console.print()  # Blank line between You: and Illiana:

            # Disable streaming for PUBLIC provider
            use_streaming = True
            if ai_service.config.provider == AIProvider.PUBLIC:
                use_streaming = False

            try:
                if use_streaming:
                    returned_conversation_id = await _stream_chat_response(
                        ai_service,
                        user_message,
                        current_conversation_id,
                        "cli-user",
{% if ai_rag %}
                        use_rag=use_rag,
                        rag_collection=rag_collection,
                        rag_top_k=rag_top_k,
                        show_sources=show_sources,
{% endif %}
                        session_state=session_state,
                    )
                    if returned_conversation_id:
                        current_conversation_id = returned_conversation_id
                        command_handler.current_conversation_id = (
                            returned_conversation_id
                        )
                else:
                    # Show thinking spinner for non-streaming responses
                    from rich.live import Live
                    from rich.spinner import Spinner

                    spinner = Spinner(
                        "dots", text="Thinking...", style="bright_blue"
                    )
                    spinner_live = Live(
                        spinner,
                        console=console,
                        refresh_per_second=20,
                        transient=True,
                    )
                    spinner_live.start()

                    try:
                        response = await ai_service.chat(
                            message=user_message,
                            conversation_id=current_conversation_id,
                            user_id="cli-user",
{% if ai_rag %}
                            use_rag=use_rag,
                            rag_collection=rag_collection,
                            rag_top_k=rag_top_k,
{% endif %}
                        )
                    finally:
                        spinner_live.stop()

                    # Use shared rendering functions
                    from app.cli.ai_rendering import (
                        render_ai_header,
                        render_markdown_response,
                    )

                    render_ai_header(console, inline=True)
                    render_markdown_response(console, response.content)

{% if ai_rag %}
                    # Display source references if RAG was used
                    if show_sources and response.metadata.get("rag_sources"):
                        console.print()
                        console.print("---", style="dim")
                        console.print("[bold]Sources:[/bold]", style="cyan")
                        for i, source in enumerate(response.metadata["rag_sources"], 1):
                            file_name = source.get("file", "Unknown")
                            start_line = source.get("start_line")
                            end_line = source.get("end_line")
                            score = source.get("score", 0.0)
                            # Prefer line numbers, fall back to chunk index
                            if start_line and end_line:
                                location = f"lines {start_line}-{end_line}"
                            else:
                                location = f"chunk {source.get('chunk_index', 0)}"
                            console.print(
                                f"  [{i}] {file_name} ({location}) - score: {score:.2f}",
                                style="dim",
                            )

{% endif %}
                    # Update conversation reference
                    current_conversation_id = response.metadata.get(
                        "conversation_id", current_conversation_id
                    )
                    command_handler.current_conversation_id = current_conversation_id
            except ProviderNotInstalledError as e:
                # Clean display for missing provider
                console.print()
                console.print(f"[yellow]Provider not installed:[/yellow] {e.provider}")
                console.print()
                console.print(f"Run [green]'{e.cli_command}'[/green] to install it.")
                console.print()
            except Exception as stream_error:
                console.print(f"[red]Error: {stream_error}[/red]")
                console.print(
                    "[dim]Try a different provider or check your connection.[/dim]"
                )

            console.print()  # Add space after response

        except Exception as e:
            console.print(f"[red]Error: {e}[/red]")
            console.print(
                "[dim]You can continue chatting or type 'exit' to quit.[/dim]"
            )


async def _stream_chat_response(
    ai_service,
    message: str,
    conversation_id: str | None,
    user_id: str,
    verbose: bool = False,
{% if ai_rag %}
    use_rag: bool = False,
    rag_collection: str | None = None,
    rag_top_k: int | None = None,
    show_sources: bool = True,
{% endif %}
    session_state: ChatSessionState | None = None,
) -> str | None:
    """
    Stream chat response with real-time markdown rendering.

    Returns:
        The conversation ID for continuing the conversation, or None if interrupted.
    """
    import signal

    from rich.live import Live
    from rich.spinner import Spinner

    from app.cli.ai_rendering import StreamingMarkdownRenderer

    renderer = StreamingMarkdownRenderer(console)
    conversation_info = None
    response_time = None
{% if ai_rag %}
    rag_sources = None
{% endif %}

    # Set up signal handler for graceful interruption
    interrupted = False

    def signal_handler(signum, frame):
        nonlocal interrupted
        interrupted = True

    old_handler = signal.signal(signal.SIGINT, signal_handler)

    try:
        header_shown = False
        import asyncio

        # Show thinking spinner initially
        spinner = Spinner("dots", text="Thinking...", style="bright_blue")
        spinner_live = Live(
            spinner, console=console, refresh_per_second=20, transient=True
        )
        spinner_live.start()

        try:
            processed_content = set()

            async with asyncio.timeout(settings.AI_TIMEOUT_SECONDS):
                async for chunk in ai_service.stream_chat(
                    message=message,
                    conversation_id=conversation_id,
                    user_id=user_id,
                    stream_delta=True,
{% if ai_rag %}
                    use_rag=use_rag,
                    rag_collection=rag_collection,
                    rag_top_k=rag_top_k,
{% endif %}
                ):
                    if interrupted:
                        spinner_live.stop()
                        console.print(
                            "\nStreaming interrupted", style="yellow"
                        )
                        break

                    # Skip duplicate content only for non-delta mode (fake streaming)
                    # Delta mode sends unique incremental content that should never
                    # be skipped - LangChain sends character-level tokens where
                    # common chars like "a", "i", "e" would be incorrectly filtered
                    if not chunk.is_delta:
                        if chunk.content in processed_content:
                            if chunk.is_final:
                                conversation_info = chunk.conversation_id
                                response_time = chunk.metadata.get("response_time_ms")
                            continue
                        processed_content.add(chunk.content)

                    if chunk.is_delta and chunk.content:
                        if not header_shown:
                            spinner_live.stop()
                            console.print("Illiana: ", style="bright_magenta", end="")
                            header_shown = True
                        renderer.add_delta(chunk.content)

                    if chunk.is_final:
                        conversation_info = chunk.conversation_id
                        response_time = chunk.metadata.get("response_time_ms")
{% if ai_rag %}
                        rag_sources = chunk.metadata.get("rag_sources")
{% endif %}

                        # Update status line token count and cost
                        if session_state is not None:
                            input_tokens = chunk.metadata.get("input_tokens", 0)
                            output_tokens = chunk.metadata.get("output_tokens", 0)
                            cost = chunk.metadata.get("cost", 0.0)
                            session_state.add_tokens(input_tokens, output_tokens)
                            session_state.add_cost(cost)

                            # Persist cumulative totals to conversation metadata
                            if conversation_info:
                                conversation = ai_service.get_conversation(
                                    conversation_info
                                )
                                if conversation:
                                    conversation.metadata["cumulative_tokens"] = (
                                        session_state.cumulative_tokens
                                    )
                                    conversation.metadata["cumulative_cost"] = (
                                        session_state.cumulative_cost
                                    )
                                    ai_service.conversation_manager.save_conversation(
                                        conversation
                                    )

                        break
        except TimeoutError:
            spinner_live.stop()
            console.print(
                "\nError: Request timed out. Try a faster model or increase AI_TIMEOUT_SECONDS.",
                style="red",
            )
            return None
        except RuntimeError as e:
            # WORKAROUND: anyio/prompt_toolkit event loop incompatibility
            # When PydanticAI's anyio-based streaming completes inside prompt_toolkit's
            # asyncio loop, the cancel scope cleanup can raise RuntimeError. The
            # response is already fully streamed at this point, so we ignore it.
            # String matching is intentional - no specific exception type exists.
            if "cancel scope" in str(e).lower():
                pass  # Response streamed successfully, ignore cleanup error
            else:
                raise
        finally:
            if spinner_live.is_started:
                spinner_live.stop()

        if not interrupted:
            renderer.finalize()
            console.print()

{% if ai_rag %}
            # Display source references if RAG was used
            if show_sources and rag_sources:
                console.print("---", style="dim")
                console.print("[bold]Sources:[/bold]", style="cyan")
                for i, source in enumerate(rag_sources, 1):
                    file_name = source.get("file", "Unknown")
                    start_line = source.get("start_line")
                    end_line = source.get("end_line")
                    score = source.get("score", 0.0)
                    # Prefer line numbers, fall back to chunk index
                    if start_line and end_line:
                        location = f"lines {start_line}-{end_line}"
                    else:
                        location = f"chunk {source.get('chunk_index', 0)}"
                    console.print(
                        f"  [{i}] {file_name} ({location}) - score: {score:.2f}",
                        style="dim",
                    )
                console.print()

{% endif %}
            if verbose and conversation_info:
                conversation = ai_service.get_conversation(conversation_info)
                if conversation:
                    console.print(f"Conversation: {conversation.id}", style="dim")
                    console.print(
                        f"Messages: {conversation.get_message_count()}", style="dim"
                    )
                    if response_time:
                        console.print(
                            f"Response time: {response_time:.1f}ms", style="dim"
                        )

    except ProviderNotInstalledError as e:
        # Clean display for missing provider - no need to re-raise
        console.print()
        console.print(f"[yellow]Provider not installed:[/yellow] {e.provider}")
        console.print()
        console.print(f"Run [green]'{e.cli_command}'[/green] to install it.")
        console.print()
        return None

    except Exception as e:
        # WORKAROUND: Same anyio/prompt_toolkit issue can bubble up here
        # See inner handler comment for full explanation
        if "cancel scope" in str(e).lower():
            pass  # Response streamed successfully, ignore cleanup error
        elif not interrupted:
            console.print(f"Streaming error: {e}", style="red")
            raise

    finally:
        signal.signal(signal.SIGINT, old_handler)

    return conversation_info if not interrupted else None


if __name__ == "__main__":
    app()
