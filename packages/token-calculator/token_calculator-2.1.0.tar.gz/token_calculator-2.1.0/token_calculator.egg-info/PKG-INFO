Metadata-Version: 2.4
Name: token-calculator
Version: 2.1.0
Summary: LLM Token Optimization and Cost Management for AI Product Managers and Developers
Author: TokenCost Contributors
License: MIT
Keywords: llm,tokens,optimization,cost,ai,gpt,claude
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tiktoken>=0.5.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Dynamic: license-file

# ðŸŽ¯ Token Calculator

[![PyPI version](https://badge.fury.io/py/token-calculator.svg)](https://badge.fury.io/py/token-calculator)
[![Downloads](https://pepy.tech/badge/token-calculator)](https://pepy.tech/project/token-calculator)

**Production-Ready LLM Cost Management and Observability for AI Product Managers**

Token Calculator is the comprehensive toolkit for building, monitoring, and optimizing production AI agents. Track costs across multi-agent workflows, detect context rot before it causes hallucinations, and make data-driven decisions about model selectionâ€”all with enterprise-grade observability.

## ðŸŽ¯ Built for AI Product Managers

If you're building AI agents in production, you know the challenges:

- ðŸ’¸ **Cost Blindness**: You don't see costs until the monthly bill arrives
- ðŸ¤– **Multi-Agent Complexity**: Hard to track which agent in your workflow costs what
- ðŸ”¥ **Context Rot**: Conversations degrade over time, causing hallucinations
- ðŸ“Š **No Visibility**: Can't debug token usage through complex agent workflows
- ðŸŽ² **Model Selection**: Guessing which model offers the best cost/quality trade-off
- âš ï¸ **Production Incidents**: Context overflows break your app at 2 AM

**Token Calculator solves all of these problems.**

## âœ¨ Key Features for Production AI

### ðŸ“Š **Cost Tracking with Multi-Dimensional Analysis**

Track every LLM call with custom labels, query costs by any dimension, and identify cost anomalies before they become incidents.

```python
from token_calculator import CostTracker, create_storage

# Track with custom dimensions
tracker = CostTracker(
    storage=create_storage("sqlite", db_path="costs.db"),
    default_labels={"environment": "production", "team": "ai"}
)

tracker.track_call(
    model="gpt-4",
    input_tokens=1000,
    output_tokens=500,
    agent_id="customer-support",
    user_id="user-123",
    session_id="session-456"
)

# Query costs by any dimension
report = tracker.get_costs(
    start_date="this-month",
    group_by=["agent_id", "model"],
    filters={"environment": "production"}
)
print(report)
# Output:
# Cost Report (1,234 calls)
#   Total Cost: $456.78
#   Breakdown:
#     customer-support | gpt-4: $234.56
#     rag-agent | gpt-4o: $123.45
```

### ðŸ¤– **Multi-Agent Workflow Tracking**

Track token usage across complex agent orchestrations, identify bottlenecks, and optimize inter-agent communication.

```python
from token_calculator import WorkflowTracker

tracker = WorkflowTracker(workflow_id="customer-support-v2")

# Track each agent in your workflow
with tracker.track_agent("router", model="gpt-4o-mini") as ctx:
    result = router.run(query)
    ctx.track_call(input_tokens=150, output_tokens=20)

with tracker.track_agent("executor", model="gpt-4") as ctx:
    final = executor.run(result)
    ctx.track_call(input_tokens=800, output_tokens=300)

# Analyze workflow
analysis = tracker.analyze()
print(analysis)
# Output:
# Workflow Analysis: customer-support-v2
#   Total Cost: $0.0520
#   Bottleneck: executor ($0.0450)
#   Efficiency: 75/100
#   Recommendations:
#     â€¢ executor accounts for >50% of cost
```

### ðŸ¥ **Context Health Monitoring**

Detect context rot, prevent hallucinations, and intelligently compress conversations before quality degrades.

```python
from token_calculator import ConversationMonitor

monitor = ConversationMonitor(model="gpt-4", agent_id="support-agent")

for user_msg, assistant_msg in conversation:
    monitor.add_turn(user_msg, assistant_msg)

    health = monitor.check_health()

    if health.status == "context_rot":
        # Compress before quality degrades
        compressed = monitor.compress_context(
            strategy="semantic",
            target_tokens=4000,
            keep_recent=3
        )
        # Reset conversation with compressed context

print(health)
# Output:
# âš ï¸ Context Health: CONTEXT_ROT
#   Quality Score: 65/100
#   Context Usage: 78.5%
#   Rot: 45.0%
#   Warnings:
#     âš ï¸  45% of context appears irrelevant
#   Recommendations:
#     ðŸ’¡ Use compress_context() to remove irrelevant context
```

### ðŸ“ˆ **Cost Forecasting & Budgeting**

Forecast future costs, set budgets, and get alerted before you overspend.

```python
from token_calculator import CostForecaster, BudgetTracker

forecaster = CostForecaster(storage=tracker.storage)

# Forecast next month
forecast = forecaster.forecast_monthly(agent_id="rag-agent")
print(forecast)
# Output:
# ðŸ“ˆ Monthly Forecast:
#   Predicted: $1,234.56
#   Range: $987.65 - $1,481.47
#   Trend: increasing

# Set budget and track
budget = BudgetTracker(storage=tracker.storage)
budget.set_budget(amount=10000, period="monthly")

status = budget.get_status()
if not status.on_track:
    print(f"âš ï¸ Projected overage: ${status.projected_overage:.2f}")
```

### ðŸš¨ **Real-Time Alerting**

Get notified immediately when costs spike, contexts overflow, or budgets are exceeded.

```python
from token_calculator import AlertManager, AlertRule

alerts = AlertManager(webhook_url="https://hooks.slack.com/...")

# Cost spike alert
alerts.add_rule(AlertRule(
    name="cost-spike",
    condition=lambda e: e.cost > 1.0,
    severity="warning",
    message_template="High cost call: ${cost:.2f} for {agent_id}",
    channels=["console", "webhook"]
))

# Budget alert
alerts.add_budget_alert(
    budget_amount=10000,
    threshold_pct=0.8,  # Alert at 80%
    severity="warning"
)

# Alerts trigger automatically
triggered = alerts.check_event(event)
```

### ðŸŽ¯ **Model Recommendation Engine**

Stop guessing which model to use. Get data-driven recommendations based on your usage patterns.

```python
from token_calculator import ModelSelector

selector = ModelSelector(storage=tracker.storage)

# Get recommendation
rec = selector.recommend(
    current_model="gpt-4",
    requirements={"max_cost_per_1k": 0.01},
    usage_context="simple_qa"
)

print(rec)
# Output:
# ðŸ’¡ Model Recommendation: gpt-4o-mini
#    Current: gpt-4
#    Monthly Savings: $450.00
#    Quality Impact: -10%
#    Confidence: 85%
#    Reasoning: gpt-4o-mini costs <50% of gpt-4. Fast, cost-effective for simple Q&A

# A/B test the recommendation
test = selector.create_ab_test(
    name="gpt4-vs-gpt4o",
    model_a="gpt-4",
    model_b="gpt-4o",
    traffic_split=0.1,
    duration_days=7
)

# After 7 days...
results = selector.get_test_results(test)
print(results.recommendation)
```

### ðŸ”Œ **One-Line LangChain Integration**

Already using LangChain? Add tracking with one line of code.

```python
from langchain_openai import ChatOpenAI
from token_calculator import CostTracker, create_storage
from token_calculator.integrations.langchain import TokenCalculatorCallback

tracker = CostTracker(storage=create_storage("sqlite", db_path="costs.db"))

callback = TokenCalculatorCallback(
    tracker=tracker,
    agent_id="my-agent",
    environment="production"
)

# Just add callbacks parameter!
llm = ChatOpenAI(callbacks=[callback])

# All LLM calls are now tracked automatically
result = llm.invoke("Hello!")

# Check costs
report = tracker.get_costs(start_date="today")
```

## ðŸ“¦ Installation

```bash
pip install token-calculator
```

Optional dependencies:

```bash
# For LangChain integration
pip install token-calculator[langchain]

# For PostgreSQL storage
pip install token-calculator[postgres]

# All optional dependencies
pip install token-calculator[all]
```

## ðŸš€ Quick Start

### 1. Basic Cost Tracking

```python
from token_calculator import CostTracker, create_storage

tracker = CostTracker(
    storage=create_storage("sqlite", db_path="costs.db")
)

# Track LLM calls
tracker.track_call(
    model="gpt-4",
    input_tokens=1000,
    output_tokens=500,
    agent_id="my-agent"
)

# Get costs
report = tracker.get_costs(start_date="this-month")
print(f"Total cost: ${report.total_cost:.2f}")
```

### 2. Multi-Agent Workflow

```python
from token_calculator import WorkflowTracker

tracker = WorkflowTracker(workflow_id="my-workflow")

with tracker.track_agent("planner", model="gpt-4o") as ctx:
    # Your agent code
    ctx.track_call(input_tokens=500, output_tokens=100)

with tracker.track_agent("executor", model="gpt-4") as ctx:
    # Your agent code
    ctx.track_call(input_tokens=1000, output_tokens=300)

analysis = tracker.analyze()
print(f"Total cost: ${analysis.total_cost:.4f}")
```

### 3. Context Health Monitoring

```python
from token_calculator import ConversationMonitor

monitor = ConversationMonitor(model="gpt-4")

monitor.add_turn(
    user_message="What's the weather?",
    assistant_message="I don't have real-time weather data."
)

health = monitor.check_health()
if health.status != "healthy":
    print(health.recommendations)
```

## ðŸ“š Complete Examples

### AI Product Manager Daily Workflow

See [`examples/ai_pm_daily_workflow.py`](examples/ai_pm_daily_workflow.py) for a complete example showing:

- âœ… Morning cost review and anomaly detection
- âœ… Budget tracking and forecasting
- âœ… Multi-agent workflow tracking
- âœ… Context health monitoring
- âœ… Setting up alerts
- âœ… Model selection and A/B testing
- âœ… Incident investigation
- âœ… Weekly executive reporting

### LangChain Integration

See [`examples/langchain_integration.py`](examples/langchain_integration.py) for:

- âœ… Basic LangChain integration
- âœ… Chain tracking
- âœ… Multi-agent RAG systems
- âœ… Production monitoring
- âœ… Model optimization

## ðŸ—ï¸ Architecture

Token Calculator uses a modular architecture:

```
Application Layer (Your Code)
    â†“
Tracking Layer (CostTracker, WorkflowTracker, ConversationMonitor)
    â†“
Intelligence Layer (Forecaster, ModelSelector, HealthCheck)
    â†“
Alert Layer (AlertManager, BudgetTracker)
    â†“
Storage Layer (SQLite, PostgreSQL, In-Memory)
```

### Storage Backends

- **In-Memory**: Fast, for testing/development
- **SQLite**: Production-ready for single-machine deployments
- **PostgreSQL**: Multi-instance production deployments

```python
# SQLite
storage = create_storage("sqlite", db_path="costs.db")

# PostgreSQL
storage = create_storage(
    "postgresql",
    host="localhost",
    database="token_calculator",
    user="user",
    password="pass"
)

# In-Memory
storage = create_storage("memory")
```

## ðŸ“Š Supported Models

**40+ models** across 6 providers:

- âœ… **OpenAI**: GPT-4, GPT-4 Turbo, GPT-4o, GPT-4o-mini, GPT-3.5 Turbo
- âœ… **Anthropic**: Claude 4.5 Opus, Claude 3.5 Sonnet, Claude 3.5 Haiku
- âœ… **Google**: Gemini Pro, Gemini 1.5 Pro, Gemini 1.5 Flash
- âœ… **Meta**: Llama 2, Llama 3, Llama 3.1 (all sizes)
- âœ… **Mistral**: Mistral 7B, 8x7B, Small, Medium, Large
- âœ… **Cohere**: Command, Command R, Command R+

## ðŸŽ¯ Use Cases

### For AI Product Managers

- ðŸ“Š Track costs across all agents and workflows
- ðŸŽ¯ Identify which agents/users drive costs
- ðŸ“ˆ Forecast costs and plan budgets
- ðŸš¨ Get alerted before incidents
- ðŸ’¡ Optimize model selection for cost/quality
- ðŸ“‹ Generate executive reports

### For AI Engineers

- ðŸ” Debug token usage in complex workflows
- ðŸ¥ Monitor context health and prevent degradation
- âš¡ Optimize prompts systematically
- ðŸ§ª A/B test different models
- ðŸ”Œ Integrate with existing LangChain apps

### For AI Teams

- ðŸ’° Shared budget tracking
- ðŸ“Š Cross-team cost visibility
- ðŸŽ¯ Standardized monitoring
- ðŸš¨ Centralized alerting
- ðŸ“ˆ Trend analysis

## ðŸ”§ Configuration

### Environment Variables

```bash
# Storage
export TOKEN_CALC_STORAGE=sqlite
export TOKEN_CALC_STORAGE_PATH=/path/to/costs.db

# Alerts
export TOKEN_CALC_WEBHOOK_URL=https://hooks.slack.com/...

# Default labels
export TOKEN_CALC_DEFAULT_LABELS=environment:production,team:ai
```

### Configuration File

```yaml
# token_calculator.yaml
storage:
  backend: sqlite
  path: ./costs.db

tracking:
  default_labels:
    environment: production
    team: ai-platform

alerts:
  rules:
    - name: budget-exceeded
      type: budget
      threshold: 1.0
      severity: critical

budgets:
  - name: monthly-prod
    amount: 10000
    period: monthly
```

## ðŸ“– Documentation

- [Product Requirements Document](PRD.md) - Vision and requirements
- [Architecture Design](ARCHITECTURE.md) - Technical architecture
- [Gap Analysis](GAP_ANALYSIS.md) - Feature roadmap

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ðŸ™ Acknowledgments

Built for AI Product Managers building the future of AI agents.

## ðŸ“ž Support

- ðŸ› **Issues**: [GitHub Issues](https://github.com/arunaryamdn/token-calculator/issues)
- ðŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/arunaryamdn/token-calculator/discussions)
- ðŸ“§ **Email**: [Contact](mailto:support@tokencalculator.com)

---

**Built with â¤ï¸ for AI Product Managers**

Stop guessing. Start measuring. Build better AI agents.
