# Prometheus Alerting Rules for obskit
# =====================================
#
# This file contains pre-built alerting rules for services using obskit.
# Import these rules into your Prometheus configuration.
#
# Usage:
#   In prometheus.yml:
#     rule_files:
#       - "/path/to/prometheus_rules.yml"
#
# Alert Severity Levels:
#   - critical: Immediate action required
#   - warning: Attention needed, may escalate
#   - info: Informational, monitor closely

groups:
  # ========================================================================
  # RED Method Alerts
  # ========================================================================
  - name: red_method_alerts
    interval: 30s
    rules:
      # High Error Rate Alert
      - alert: HighErrorRate
        expr: |
          sum(rate({__name__=~".*_errors_total"}[5m])) by (service, operation, error_type)
          /
          sum(rate({__name__=~".*_requests_total"}[5m])) by (service, operation)
          > 0.01
        for: 5m
        labels:
          severity: critical
          alert_type: error_rate
        annotations:
          summary: "High error rate detected: {{ $labels.operation }}"
          description: |
            Error rate is {{ $value | humanizePercentage }} for operation {{ $labels.operation }}.
            Error type: {{ $labels.error_type }}
            Service: {{ $labels.service }}

      # Critical Error Rate (Very High)
      - alert: CriticalErrorRate
        expr: |
          sum(rate({__name__=~".*_errors_total"}[5m])) by (service, operation)
          /
          sum(rate({__name__=~".*_requests_total"}[5m])) by (service, operation)
          > 0.10
        for: 2m
        labels:
          severity: critical
          alert_type: error_rate
        annotations:
          summary: "CRITICAL: Error rate > 10% for {{ $labels.operation }}"
          description: |
            Error rate is {{ $value | humanizePercentage }} for {{ $labels.operation }}.
            Service: {{ $labels.service }}

      # High Latency Alert (P95)
      - alert: HighLatencyP95
        expr: |
          histogram_quantile(0.95,
            sum(rate({__name__=~".*_request_duration_seconds_bucket"}[5m])) by (le, operation, service)
          ) > 0.5
        for: 10m
        labels:
          severity: warning
          alert_type: latency
        annotations:
          summary: "P95 latency > 500ms for {{ $labels.operation }}"
          description: |
            P95 latency is {{ $value | humanizeDuration }} for operation {{ $labels.operation }}.
            Service: {{ $labels.service }}

      # Critical Latency Alert (P99)
      - alert: CriticalLatencyP99
        expr: |
          histogram_quantile(0.99,
            sum(rate({__name__=~".*_request_duration_seconds_bucket"}[5m])) by (le, operation, service)
          ) > 1.0
        for: 5m
        labels:
          severity: critical
          alert_type: latency
        annotations:
          summary: "P99 latency > 1s for {{ $labels.operation }}"
          description: |
            P99 latency is {{ $value | humanizeDuration }} for operation {{ $labels.operation }}.
            Service: {{ $labels.service }}

      # Low Request Rate (Service May Be Down)
      - alert: LowRequestRate
        expr: |
          sum(rate({__name__=~".*_requests_total"}[5m])) by (service, operation) < 0.1
        for: 15m
        labels:
          severity: warning
          alert_type: availability
        annotations:
          summary: "Low request rate for {{ $labels.operation }}"
          description: |
            Request rate is {{ $value | humanize }} req/s for {{ $labels.operation }}.
            Service may be experiencing issues.
            Service: {{ $labels.service }}

  # ========================================================================
  # Golden Signals Alerts
  # ========================================================================
  - name: golden_signals_alerts
    interval: 30s
    rules:
      # High Saturation Alert
      - alert: HighSaturation
        expr: |
          {__name__=~".*_saturation"} > 0.90
        for: 5m
        labels:
          severity: warning
          alert_type: saturation
        annotations:
          summary: "High saturation detected: {{ $labels.resource }}"
          description: |
            Saturation is {{ $value | humanizePercentage }} for resource {{ $labels.resource }}.
            Service: {{ $labels.service }}

      # Critical Saturation Alert
      - alert: CriticalSaturation
        expr: |
          {__name__=~".*_saturation"} > 0.95
        for: 2m
        labels:
          severity: critical
          alert_type: saturation
        annotations:
          summary: "CRITICAL: Saturation > 95% for {{ $labels.resource }}"
          description: |
            Saturation is {{ $value | humanizePercentage }} for resource {{ $labels.resource }}.
            Service: {{ $labels.service }}

      # High Queue Depth
      - alert: HighQueueDepth
        expr: |
          {__name__=~".*_queue_depth"} > 1000
        for: 5m
        labels:
          severity: warning
          alert_type: queue
        annotations:
          summary: "High queue depth: {{ $labels.queue }}"
          description: |
            Queue depth is {{ $value | humanize }} for queue {{ $labels.queue }}.
            Service: {{ $labels.service }}

  # ========================================================================
  # USE Method Alerts (Infrastructure)
  # ========================================================================
  - name: use_method_alerts
    interval: 30s
    rules:
      # High CPU Utilization
      - alert: HighCPUUtilization
        expr: |
          {__name__=~".*_utilization", resource="cpu"} > 0.90
        for: 10m
        labels:
          severity: warning
          alert_type: infrastructure
        annotations:
          summary: "High CPU utilization: {{ $value | humanizePercentage }}"
          description: |
            CPU utilization is {{ $value | humanizePercentage }}.
            Resource: {{ $labels.resource }}

      # High Memory Utilization
      - alert: HighMemoryUtilization
        expr: |
          {__name__=~".*_utilization", resource="memory"} > 0.90
        for: 5m
        labels:
          severity: warning
          alert_type: infrastructure
        annotations:
          summary: "High memory utilization: {{ $value | humanizePercentage }}"
          description: |
            Memory utilization is {{ $value | humanizePercentage }}.
            Resource: {{ $labels.resource }}

      # CPU Saturation (Processes Waiting)
      - alert: CPUSaturation
        expr: |
          {__name__=~".*_saturation", resource="cpu"} > 10
        for: 5m
        labels:
          severity: warning
          alert_type: infrastructure
        annotations:
          summary: "CPU saturation: {{ $value }} processes waiting"
          description: |
            {{ $value }} processes are waiting for CPU.
            Resource: {{ $labels.resource }}

      # Infrastructure Errors
      - alert: InfrastructureErrors
        expr: |
          rate({__name__=~".*_errors_total"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
          alert_type: infrastructure
        annotations:
          summary: "Infrastructure errors detected: {{ $labels.resource }}"
          description: |
            Error rate is {{ $value | humanize }} errors/s for resource {{ $labels.resource }}.
            Error type: {{ $labels.error_type }}

  # ========================================================================
  # Service Health Alerts
  # ========================================================================
  - name: service_health_alerts
    interval: 30s
    rules:
      # Service Down (No Metrics)
      - alert: ServiceDown
        expr: |
          absent({__name__=~".*_requests_total"}) or
          sum(rate({__name__=~".*_requests_total"}[15m])) == 0
        for: 15m
        labels:
          severity: critical
          alert_type: availability
        annotations:
          summary: "Service appears to be down: No metrics received"
          description: |
            No metrics have been received from the service in the last 15 minutes.
            The service may be down or metrics collection is broken.

      # Service Degraded (High Error Rate + High Latency)
      - alert: ServiceDegraded
        expr: |
          (
            sum(rate({__name__=~".*_errors_total"}[5m])) by (service)
            /
            sum(rate({__name__=~".*_requests_total"}[5m])) by (service)
            > 0.05
          ) and (
            histogram_quantile(0.95,
              sum(rate({__name__=~".*_request_duration_seconds_bucket"}[5m])) by (le, service)
            ) > 1.0
          )
        for: 5m
        labels:
          severity: critical
          alert_type: degradation
        annotations:
          summary: "Service degraded: High errors and latency"
          description: |
            Service {{ $labels.service }} is experiencing both high error rates and high latency.
            Error rate: > 5%
            P95 latency: > 1s

  # ========================================================================
  # SLO-Based Alerts
  # ========================================================================
  - name: slo_alerts
    interval: 30s
    rules:
      # Error Budget Burn Rate
      - alert: HighErrorBudgetBurnRate
        expr: |
          # Example: 99.9% availability SLO
          (
            sum(rate({__name__=~".*_errors_total"}[1h])) by (service)
            /
            sum(rate({__name__=~".*_requests_total"}[1h])) by (service)
          ) > 0.001
        for: 1h
        labels:
          severity: warning
          alert_type: slo
        annotations:
          summary: "Error budget burn rate high for {{ $labels.service }}"
          description: |
            Service {{ $labels.service }} is consuming error budget faster than expected.
            Current error rate: {{ $value | humanizePercentage }}

      # Latency SLO Violation
      - alert: LatencySLOViolation
        expr: |
          # Example: P95 latency < 200ms SLO
          histogram_quantile(0.95,
            sum(rate({__name__=~".*_request_duration_seconds_bucket"}[5m])) by (le, service, operation)
          ) > 0.2
        for: 10m
        labels:
          severity: warning
          alert_type: slo
        annotations:
          summary: "Latency SLO violation: {{ $labels.operation }}"
          description: |
            Operation {{ $labels.operation }} is violating latency SLO.
            P95 latency: {{ $value | humanizeDuration }}
            SLO: < 200ms

