{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ef162d4b412362",
   "metadata": {},
   "source": [
    "# Performance Analysis: Earthkit-Climate Indicators\n",
    "\n",
    "This notebook provides a comparative performance analysis of the `earthkit-climate` indicators on the SSP5-8.5 dataset. We compare two execution modes:\n",
    "1. **Lazy Execution**: The baseline approach where dask graphs are built and executed without specific optimizations.\n",
    "2. **Optimized Execution**: An enhanced approach using pre-computation of heavy statistics (like percentiles) and strategic re-chunking.\n",
    "\n",
    "We profile 5 key indicators:\n",
    "- **WSDI**: Warm Spell Duration Index\n",
    "- **CWD**: Maximum Consecutive Wet Days\n",
    "- **DTR**: Daily Temperature Range\n",
    "- **HDD**: Heating Degree Days\n",
    "- **SDII**: Simple Daily Precipitation Intensity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cache_config",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:05.411512492Z",
     "start_time": "2025-12-11T17:35:05.334785047Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from earthkit.data import config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure robust caching to avoid re-downloading\n",
    "cache_dir = os.path.expanduser(\"~/.cache/earthkit/data\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "settings_earthkit = {\n",
    "    \"cache-policy\": \"user\",\n",
    "    \"temporary-directory-root\": cache_dir,\n",
    "}\n",
    "config.set(settings_earthkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dynamic_resources",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:05.451533026Z",
     "start_time": "2025-12-11T17:35:05.415349573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "\n## Resources Used\n\n### Hardware Configuration\nThe performance analysis was conducted on the following hardware (dynamically detected):\n- **CPU**: AMD Ryzen 5 5500H with Radeon Graphics\n- **RAM**: 15.0 GB\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def get_cpu_info():\n",
    "    try:\n",
    "        with open(\"/proc/cpuinfo\", \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"model name\" in line:\n",
    "                    return line.split(\":\")[1].strip()\n",
    "    except Exception:\n",
    "        return \"Unknown CPU\"\n",
    "    return \"Unknown CPU\"\n",
    "\n",
    "\n",
    "def get_ram_info():\n",
    "    try:\n",
    "        with open(\"/proc/meminfo\", \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"MemTotal\" in line:\n",
    "                    total_kb = int(line.split()[1])\n",
    "                    return f\"{total_kb / 1024 / 1024:.1f} GB\"\n",
    "    except Exception:\n",
    "        return \"Unknown RAM\"\n",
    "    return \"Unknown RAM\"\n",
    "\n",
    "\n",
    "cpu_model = get_cpu_info()\n",
    "ram_size = get_ram_info()\n",
    "\n",
    "display(\n",
    "    Markdown(f\"\"\"\n",
    "## Resources Used\n",
    "\n",
    "### Hardware Configuration\n",
    "The performance analysis was conducted on the following hardware (dynamically detected):\n",
    "- **CPU**: {cpu_model}\n",
    "- **RAM**: {ram_size}\n",
    "\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dynamic_dataset_info",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:08.886634702Z",
     "start_time": "2025-12-11T17:35:05.458533533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "\n### Dataset Information\n\nThe analysis uses the following climate datasets derived from CMIP6 projections\n(ACCESS-CM2 model, DeepESD downscaling). These datasets are hosted in the ECMWF\nrepository and are automatically downloaded or cached by **earthkit-data**.\n\n| Variable | Scenario | Dimensions | Size | Status | URL |\n|----------|----------|------------|------|--------|-----|\n| `tasmax` | historical | (time: 7305, lat: 48, lon: 84) | 67.3 MB | Cached | [Download](https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_historical.nc) |\n| `tasmax` | ssp585 | (time: 14610, lat: 48, lon: 84) | 126.9 MB | Cached | [Download](https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc) |\n| `tasmin` | ssp585 | (time: 14610, lat: 48, lon: 84) | 132.1 MB | Cached | [Download](https://sites.ecmwf.int/repository/earthkit-climate/tasmin_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc) |\n| `pr` | ssp585 | (time: 14610, lat: 48, lon: 84) | 111.5 MB | Cached | [Download](https://sites.ecmwf.int/repository/earthkit-climate/pr_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc) |\n\n> **Note**: Dimensions and sizes are extracted dynamically. The first run may\ndownload the files.\n",
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import xarray as xr\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import earthkit.data\n",
    "\n",
    "# Dataset URLs\n",
    "DATASET_URLS = [\n",
    "    \"https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_historical.nc\",\n",
    "    \"https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "    \"https://sites.ecmwf.int/repository/earthkit-climate/tasmin_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "    \"https://sites.ecmwf.int/repository/earthkit-climate/pr_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "]\n",
    "\n",
    "\n",
    "def format_size(size_bytes: float) -> str:\n",
    "    \"\"\"\n",
    "    Convert a byte size into a human-readable string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    size_bytes : float\n",
    "        File size in bytes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Size formatted as B, KB, MB, or GB.\n",
    "    \"\"\"\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.1f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.1f} TB\"\n",
    "\n",
    "\n",
    "def extract_dataset_info(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract key metadata information from a NetCDF dataset available via URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to the NetCDF dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing:\n",
    "        - Variable\n",
    "        - Scenario\n",
    "        - Description\n",
    "        - Dimensions\n",
    "        - Size\n",
    "        - Status (\"Cached\" or \"Remote\")\n",
    "        - URL\n",
    "    \"\"\"\n",
    "    ds = earthkit.data.from_source(\"url\", url)\n",
    "\n",
    "    # Detect file size & cache status\n",
    "    if getattr(ds, \"path\", None) and os.path.exists(ds.path):\n",
    "        size = format_size(os.path.getsize(ds.path))\n",
    "        status = \"Cached\"\n",
    "    else:\n",
    "        size = \"Unknown\"\n",
    "        status = \"Remote\"\n",
    "\n",
    "    # Convert to xarray\n",
    "    xr_ds = ds.to_xarray()\n",
    "\n",
    "    # Extract primary variable\n",
    "    variables = list(xr_ds.data_vars)\n",
    "    variable = f\"`{variables[0]}`\" if variables else \"Unknown\"\n",
    "\n",
    "    # Scenario metadata\n",
    "    scenario = xr_ds.attrs.get(\"scenario\", \"Unknown\")\n",
    "\n",
    "    # Dimension string, e.g. \"(time: 365, lat: 180, lon: 360)\"\n",
    "    dims_str = \"(\" + \", \".join(f\"{k}: {v}\" for k, v in xr_ds.dims.items()) + \")\"\n",
    "\n",
    "    return {\n",
    "        \"Variable\": variable,\n",
    "        \"Scenario\": scenario,\n",
    "        \"Dimensions\": dims_str,\n",
    "        \"Size\": size,\n",
    "        \"Status\": status,\n",
    "        \"URL\": url,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dataset_table(urls: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Create a Markdown table summarizing metadata for a list of dataset URLs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    urls : list of str\n",
    "        List of dataset URLs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A Markdown-formatted table of dataset metadata.\n",
    "    \"\"\"\n",
    "    header = (\n",
    "        \"| Variable | Scenario | Dimensions | Size | Status | URL |\\n\"\n",
    "        \"|----------|----------|------------|------|--------|-----|\\n\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for url in urls:\n",
    "        info = extract_dataset_info(url)\n",
    "        rows.append(\n",
    "            f\"| {info['Variable']} | {info['Scenario']} | \"\n",
    "            f\"{info['Dimensions']} | {info['Size']} | {info['Status']} | \"\n",
    "            f\"[Download]({info['URL']}) |\"\n",
    "        )\n",
    "\n",
    "    return header + \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "# Display Markdown report\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### Dataset Information\n",
    "\n",
    "The analysis uses the following climate datasets derived from CMIP6 projections\n",
    "(ACCESS-CM2 model, DeepESD downscaling). These datasets are hosted in the ECMWF\n",
    "repository and are automatically downloaded or cached by **earthkit-data**.\n",
    "\n",
    "{generate_dataset_table(DATASET_URLS)}\n",
    "\n",
    "> **Note**: Dimensions and sizes are extracted dynamically. The first run may\n",
    "download the files.\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d74c7c204e37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:12.058087337Z",
     "start_time": "2025-12-11T17:35:08.995389510Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from earthkit.climate.indicators.precipitation import (\n",
    "    daily_precipitation_intensity,\n",
    "    maximum_consecutive_wet_days,\n",
    ")\n",
    "from earthkit.climate.indicators.temperature import (\n",
    "    daily_temperature_range,\n",
    "    heating_degree_days,\n",
    "    warm_spell_duration_index,\n",
    ")\n",
    "from earthkit.climate.utils.percentile import percentile_doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c298d73e3b8c1b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:12.095922736Z",
     "start_time": "2025-12-11T17:35:12.075465775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data URLs (Access-CM2)\n",
    "URLS = {\n",
    "    \"tasmax_hist\": \"https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_historical.nc\",\n",
    "    \"pr_ssp\": \"https://sites.ecmwf.int/repository/earthkit-climate/pr_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "    \"tasmin_ssp\": \"https://sites.ecmwf.int/repository/earthkit-climate/tasmin_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "    \"tasmax_ssp\": \"https://sites.ecmwf.int/repository/earthkit-climate/tasmax_gridded_day_CMIP6_ACCESS-CM2_r1i1p1f1_deepESD_day_ssp585.nc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15b477e2834eeb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:13.776963639Z",
     "start_time": "2025-12-11T17:35:12.100018881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    print(\"Loading datasets...\")\n",
    "    tasmax_hist = earthkit.data.from_source(\"url\", URLS[\"tasmax_hist\"]).to_xarray()\n",
    "    tasmax_ssp = earthkit.data.from_source(\"url\", URLS[\"tasmax_ssp\"]).to_xarray()\n",
    "    tasmin_ssp = earthkit.data.from_source(\"url\", URLS[\"tasmin_ssp\"]).to_xarray()\n",
    "    pr_ssp = earthkit.data.from_source(\"url\", URLS[\"pr_ssp\"]).to_xarray()\n",
    "    return tasmax_hist, tasmax_ssp, tasmin_ssp, pr_ssp\n",
    "\n",
    "\n",
    "tasmax_hist, tasmax_ssp, tasmin_ssp, pr_ssp = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad3c950a1553b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:13.872576032Z",
     "start_time": "2025-12-11T17:35:13.810554635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing proxy 'tas' for HDD...\n"
     ]
    }
   ],
   "source": [
    "# Create 'tas' for Heating Degree Days (Mean of Max and Min)\n",
    "print(\"Computing proxy 'tas' for HDD...\")\n",
    "tas_ssp = (tasmax_ssp[\"tasmax\"] + tasmin_ssp[\"tasmin\"]) / 2\n",
    "tas_ssp.name = \"tas\"\n",
    "tas_ssp_ds = tas_ssp.to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39501b21479a8265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:13.893829550Z",
     "start_time": "2025-12-11T17:35:13.875932760Z"
    }
   },
   "outputs": [],
   "source": [
    "def profile_run(name, func, kwargs):\n",
    "    print(f\"  Running {name}...\")\n",
    "    start = time.perf_counter()\n",
    "    res = func(**kwargs)\n",
    "    out = res.to_xarray()\n",
    "    if hasattr(out, \"compute\"):\n",
    "        out.compute()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"  > Done in {elapsed:.4f}s\")\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe90a484de3306",
   "metadata": {},
   "source": [
    "## 1. Lazy Execution (Baseline)\n",
    "In this mode, we simply merge datasets and pass them to the indicators without any specific handling of chunks or pre-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e5144d71920175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:14.273558905Z",
     "start_time": "2025-12-11T17:35:13.897784414Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# WSDI (Lazy)\n",
    "tasmax_per_lazy = percentile_doy(tasmax_hist[\"tasmax\"], per=90)\n",
    "tasmax_per_lazy.name = \"tasmax_per\"\n",
    "wsdi_ds_lazy = xr.merge([tasmax_ssp, tasmax_per_lazy])\n",
    "\n",
    "# CWD (Lazy)\n",
    "cwd_ds_lazy = pr_ssp\n",
    "\n",
    "# DTR (Lazy)\n",
    "dtr_ds_lazy = xr.merge([tasmax_ssp, tasmin_ssp])\n",
    "\n",
    "# HDD (Lazy)\n",
    "hdd_ds_lazy = tas_ssp_ds\n",
    "\n",
    "# SDII (Lazy)\n",
    "sdii_ds_lazy = pr_ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aca6c8dc0f0975d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:57.182908226Z",
     "start_time": "2025-12-11T17:35:14.276631488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running WSDI (Lazy)...\n",
      "  > Done in 29.2237s\n",
      "  Running CWD (Lazy)...\n",
      "  > Done in 6.6821s\n",
      "  Running DTR (Lazy)...\n",
      "  > Done in 2.8461s\n",
      "  Running HDD (Lazy)...\n",
      "  > Done in 2.2843s\n",
      "  Running SDII (Lazy)...\n",
      "  > Done in 1.8180s\n"
     ]
    }
   ],
   "source": [
    "t_wsdi_lazy = profile_run(\"WSDI (Lazy)\", warm_spell_duration_index, {\"ds\": wsdi_ds_lazy})\n",
    "t_cwd_lazy = profile_run(\"CWD (Lazy)\", maximum_consecutive_wet_days, {\"ds\": cwd_ds_lazy})\n",
    "t_dtr_lazy = profile_run(\"DTR (Lazy)\", daily_temperature_range, {\"ds\": dtr_ds_lazy})\n",
    "t_hdd_lazy = profile_run(\"HDD (Lazy)\", heating_degree_days, {\"ds\": hdd_ds_lazy})\n",
    "t_sdii_lazy = profile_run(\"SDII (Lazy)\", daily_precipitation_intensity, {\"ds\": sdii_ds_lazy})\n",
    "\n",
    "results.append({\"Indicator\": \"WSDI\", \"Mode\": \"Lazy\", \"Time\": t_wsdi_lazy})\n",
    "results.append({\"Indicator\": \"CWD\", \"Mode\": \"Lazy\", \"Time\": t_cwd_lazy})\n",
    "results.append({\"Indicator\": \"DTR\", \"Mode\": \"Lazy\", \"Time\": t_dtr_lazy})\n",
    "results.append({\"Indicator\": \"HDD\", \"Mode\": \"Lazy\", \"Time\": t_hdd_lazy})\n",
    "results.append({\"Indicator\": \"SDII\", \"Mode\": \"Lazy\", \"Time\": t_sdii_lazy})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8c972a9f62426",
   "metadata": {},
   "source": [
    "## 2. Optimized Execution\n",
    "In this mode, we apply two key optimizations:\n",
    "1. **Pre-computing Percentiles**: We force the computation of the percentile threshold before passing it to the indicator. This simplifies the dask graph significantly.\n",
    "2. **Re-chunking**: We re-chunk the data along the time dimension (`time=-1`) to ensure optimal processing for time-series based indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f515f3d9cd6338f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:35:59.666352087Z",
     "start_time": "2025-12-11T17:35:57.208333405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pre-computing percentile for WSDI...\n",
      "  > Percentile computed in 2.4377s\n"
     ]
    }
   ],
   "source": [
    "# WSDI (Optimized)\n",
    "print(\"  Pre-computing percentile for WSDI...\")\n",
    "start_per = time.perf_counter()\n",
    "tasmax_per_opt = percentile_doy(tasmax_hist[\"tasmax\"], per=90)\n",
    "tasmax_per_opt.name = \"tasmax_per\"\n",
    "tasmax_per_opt = tasmax_per_opt.compute()\n",
    "print(f\"  > Percentile computed in {time.perf_counter() - start_per:.4f}s\")\n",
    "\n",
    "tasmax_ssp_opt = tasmax_ssp.chunk({\"time\": -1})\n",
    "wsdi_ds_opt = xr.merge([tasmax_ssp_opt, tasmax_per_opt])\n",
    "\n",
    "# CWD (Optimized)\n",
    "cwd_ds_opt = pr_ssp.chunk({\"time\": -1})\n",
    "\n",
    "# DTR (Optimized)\n",
    "tasmin_ssp_opt = tasmin_ssp.chunk({\"time\": -1})\n",
    "dtr_ds_opt = xr.merge([tasmax_ssp_opt, tasmin_ssp_opt])\n",
    "\n",
    "# HDD (Optimized)\n",
    "hdd_ds_opt = tas_ssp_ds.chunk({\"time\": -1})\n",
    "\n",
    "# SDII (Optimized)\n",
    "sdii_ds_opt = pr_ssp.chunk({\"time\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94d03495270e0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:36:16.253595242Z",
     "start_time": "2025-12-11T17:35:59.704488516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running WSDI (Optimized)...\n",
      "  > Done in 5.7216s\n",
      "  Running CWD (Optimized)...\n",
      "  > Done in 4.0565s\n",
      "  Running DTR (Optimized)...\n",
      "  > Done in 2.2120s\n",
      "  Running HDD (Optimized)...\n",
      "  > Done in 2.5596s\n",
      "  Running SDII (Optimized)...\n",
      "  > Done in 1.9346s\n"
     ]
    }
   ],
   "source": [
    "t_wsdi_opt = profile_run(\"WSDI (Optimized)\", warm_spell_duration_index, {\"ds\": wsdi_ds_opt})\n",
    "t_cwd_opt = profile_run(\"CWD (Optimized)\", maximum_consecutive_wet_days, {\"ds\": cwd_ds_opt})\n",
    "t_dtr_opt = profile_run(\"DTR (Optimized)\", daily_temperature_range, {\"ds\": dtr_ds_opt})\n",
    "t_hdd_opt = profile_run(\"HDD (Optimized)\", heating_degree_days, {\"ds\": hdd_ds_opt})\n",
    "t_sdii_opt = profile_run(\"SDII (Optimized)\", daily_precipitation_intensity, {\"ds\": sdii_ds_opt})\n",
    "\n",
    "results.append({\"Indicator\": \"WSDI\", \"Mode\": \"Optimized\", \"Time\": t_wsdi_opt})\n",
    "results.append({\"Indicator\": \"CWD\", \"Mode\": \"Optimized\", \"Time\": t_cwd_opt})\n",
    "results.append({\"Indicator\": \"DTR\", \"Mode\": \"Optimized\", \"Time\": t_dtr_opt})\n",
    "results.append({\"Indicator\": \"HDD\", \"Mode\": \"Optimized\", \"Time\": t_hdd_opt})\n",
    "results.append({\"Indicator\": \"SDII\", \"Mode\": \"Optimized\", \"Time\": t_sdii_opt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563555fb0100d6d",
   "metadata": {},
   "source": [
    "## 3. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28459f4a38c7ce02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T17:36:16.349760661Z",
     "start_time": "2025-12-11T17:36:16.292768260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mode</th>\n",
       "      <th>Lazy</th>\n",
       "      <th>Optimized</th>\n",
       "      <th>Speedup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CWD</th>\n",
       "      <td>6.682069</td>\n",
       "      <td>4.056488</td>\n",
       "      <td>1.647255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTR</th>\n",
       "      <td>2.846073</td>\n",
       "      <td>2.211951</td>\n",
       "      <td>1.286680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HDD</th>\n",
       "      <td>2.284253</td>\n",
       "      <td>2.559555</td>\n",
       "      <td>0.892442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDII</th>\n",
       "      <td>1.817978</td>\n",
       "      <td>1.934581</td>\n",
       "      <td>0.939727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSDI</th>\n",
       "      <td>29.223651</td>\n",
       "      <td>5.721642</td>\n",
       "      <td>5.107563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mode            Lazy  Optimized   Speedup\n",
       "Indicator                                \n",
       "CWD         6.682069   4.056488  1.647255\n",
       "DTR         2.846073   2.211951  1.286680\n",
       "HDD         2.284253   2.559555  0.892442\n",
       "SDII        1.817978   1.934581  0.939727\n",
       "WSDI       29.223651   5.721642  5.107563"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "pivot = df.pivot(index=\"Indicator\", columns=\"Mode\", values=\"Time\")\n",
    "pivot[\"Speedup\"] = pivot[\"Lazy\"] / pivot[\"Optimized\"]\n",
    "pivot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
