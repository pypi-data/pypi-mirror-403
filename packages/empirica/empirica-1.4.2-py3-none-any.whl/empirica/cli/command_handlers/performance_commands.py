"""
Performance Commands - Benchmarking, performance analysis, and optimization
"""

import time
import json
from ..cli_utils import print_component_status, handle_cli_error, format_execution_time, parse_json_safely


def _get_profile_performance_thresholds():
    """Get performance thresholds from investigation profiles"""
    try:
        from empirica.config.profile_loader import ProfileLoader
        
        loader = ProfileLoader()
        universal = loader.universal_constraints
        
        try:
            profile = loader.get_profile('balanced')
            constraints = profile.constraints
            
            return {
                'performance_low': getattr(constraints, 'performance_low_threshold', 0.6),
                'performance_high': getattr(constraints, 'performance_high_threshold', 0.8),
                'engagement_gate': universal.engagement_gate,
            }
        except:
            return {
                'performance_low': 0.6, 
                'performance_high': 0.8,
                'engagement_gate': universal.engagement_gate,
            }
    except Exception:
        return {
            'performance_low': 0.6, 
            'performance_high': 0.8,
            'engagement_gate': 0.6,
        }


def handle_benchmark_command(args):
    """Handle benchmark command for performance testing"""
    try:
        from empirica.components.empirical_performance_analyzer import EmpiricalPerformanceAnalyzer
        
        print("ğŸ“Š Running comprehensive benchmark suite...")
        
        analyzer = EmpiricalPerformanceAnalyzer()
        start_time = time.time()
        
        # Configure benchmark parameters
        benchmark_type = getattr(args, 'type', 'comprehensive')
        iterations = getattr(args, 'iterations', 10)
        include_memory = getattr(args, 'memory', True)
        
        # Run benchmark
        result = analyzer.run_benchmark(
            benchmark_type=benchmark_type,
            iterations=iterations,
            include_memory=include_memory,
            verbose=getattr(args, 'verbose', False)
        )
        
        end_time = time.time()
        
        print(f"âœ… Benchmark complete")
        print(f"   ğŸ Type: {benchmark_type}")
        print(f"   ğŸ”„ Iterations: {iterations}")
        print(f"   â±ï¸ Total time: {format_execution_time(start_time, end_time)}")
        print(f"   ğŸ“Š Overall score: {result.get('overall_score', 0):.2f}")
        
        # Show performance metrics
        if result.get('metrics'):
            print("ğŸ“ˆ Performance metrics:")
            for metric, value in result['metrics'].items():
                if isinstance(value, float):
                    print(f"   â€¢ {metric}: {value:.3f}")
                else:
                    print(f"   â€¢ {metric}: {value}")
        
        # Show component performance
        if result.get('component_performance'):
            thresholds = _get_profile_performance_thresholds()
            print("ğŸ§© Component performance:")
            for component, perf in result['component_performance'].items():
                status = "âœ…" if perf > thresholds['performance_high'] else "âš ï¸" if perf > thresholds['performance_low'] else "âŒ"
                print(f"   {status} {component}: {perf:.2f}")
        
        # Show memory usage if included
        if include_memory and result.get('memory_usage'):
            memory = result['memory_usage']
            print("ğŸ’¾ Memory usage:")
            print(f"   â€¢ Peak: {memory.get('peak_mb', 0):.1f} MB")
            print(f"   â€¢ Average: {memory.get('average_mb', 0):.1f} MB")
            print(f"   â€¢ Current: {memory.get('current_mb', 0):.1f} MB")
        
        # Show recommendations
        if result.get('recommendations'):
            print("ğŸ’¡ Performance recommendations:")
            for rec in result['recommendations']:
                print(f"   â€¢ {rec}")
        
        # Show detailed breakdown if verbose
        if getattr(args, 'verbose', False) and result.get('detailed_breakdown'):
            print("ğŸ” Detailed performance breakdown:")
            for category, details in result['detailed_breakdown'].items():
                print(f"   ğŸ“‚ {category}:")
                for key, value in details.items():
                    print(f"     â€¢ {key}: {value}")
        
    except Exception as e:
        handle_cli_error(e, "Benchmark", getattr(args, 'verbose', False))


def handle_performance_command(args):
    """Handle performance command (consolidates performance + benchmark)"""
    try:
        # Check if --benchmark flag is set (replaces old 'benchmark' command)
        if getattr(args, 'benchmark', False):
            # Redirect to benchmark
            return handle_benchmark_command(args)

        from empirica.components.empirical_performance_analyzer import EmpiricalPerformanceAnalyzer

        print("âš¡ Running performance analysis...")

        analyzer = EmpiricalPerformanceAnalyzer()
        
        # Configure analysis
        target = getattr(args, 'target', 'system')
        context = parse_json_safely(getattr(args, 'context', None))
        detailed = getattr(args, 'detailed', False)
        
        # Run performance analysis
        result = analyzer.analyze_performance(
            target=target,
            context=context,
            detailed=detailed
        )
        
        print(f"âœ… Performance analysis complete")
        print(f"   ğŸ¯ Target: {target}")
        print(f"   ğŸ“Š Performance score: {result.get('performance_score', 0):.2f}")
        print(f"   ğŸ† Grade: {result.get('performance_grade', 'unknown')}")
        
        # Show performance dimensions
        if result.get('dimensions'):
            thresholds = _get_profile_performance_thresholds()
            print("ğŸ“ Performance dimensions:")
            for dimension, score in result['dimensions'].items():
                status = "ğŸŸ¢" if score > thresholds['performance_high'] else "ğŸŸ¡" if score > thresholds['performance_low'] else "ğŸ”´"
                print(f"   {status} {dimension}: {score:.2f}")
        
        # Show bottlenecks
        if result.get('bottlenecks'):
            print("ğŸš§ Identified bottlenecks:")
            for bottleneck in result['bottlenecks']:
                severity = bottleneck.get('severity', 'medium')
                emoji = "ğŸ”´" if severity == 'high' else "ğŸŸ¡" if severity == 'medium' else "ğŸŸ¢"
                print(f"   {emoji} {bottleneck.get('description', 'Unknown bottleneck')}")
        
        # Show optimization suggestions
        if result.get('optimizations'):
            print("ğŸš€ Optimization suggestions:")
            for opt in result['optimizations']:
                impact = opt.get('impact', 'medium')
                emoji = "âš¡" if impact == 'high' else "ğŸ“ˆ" if impact == 'medium' else "ğŸ“Š"
                print(f"   {emoji} {opt.get('suggestion', 'Unknown optimization')}")
                if opt.get('effort'):
                    print(f"     Effort: {opt['effort']}")
        
        # Show detailed metrics if requested
        if detailed and result.get('detailed_metrics'):
            print("ğŸ” Detailed performance metrics:")
            for category, metrics in result['detailed_metrics'].items():
                print(f"   ğŸ“‚ {category}:")
                for metric, value in metrics.items():
                    print(f"     â€¢ {metric}: {value}")
        
        # Show historical comparison if available
        if result.get('historical_comparison'):
            hist = result['historical_comparison']
            trend = "ğŸ“ˆ" if hist.get('trend') == 'improving' else "ğŸ“‰" if hist.get('trend') == 'declining' else "â¡ï¸"
            print(f"ğŸ“Š Historical trend: {trend} {hist.get('trend', 'stable')}")
            if hist.get('change_percentage'):
                print(f"   Change: {hist['change_percentage']:+.1f}%")
        
    except Exception as e:
        handle_cli_error(e, "Performance analysis", getattr(args, 'verbose', False))