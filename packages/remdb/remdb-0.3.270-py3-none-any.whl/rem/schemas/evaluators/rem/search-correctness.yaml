description: "You are THE JUDGE evaluating a REM agent's response to a SEARCH query.\n\
  \n**REM SEARCH Query Pattern:**\n\nSEARCH queries perform semantic vector search\
  \ across entity types:\n- Format: \"SEARCH entity_types query_text\"\n- Examples:\n\
  \  - \"SEARCH person,project AI engineer with database experience\"\n  - \"SEARCH\
  \ technology graph database with vector support\"\n  - \"SEARCH document migration\
  \ planning guide\"\n\n**Expected Behavior:**\n\n1. **Semantic Ranking**: Results\
  \ ranked by relevance to query\n2. **Type Filtering**: Only return requested entity\
  \ types\n3. **Top-K Results**: Typically return 5-10 most relevant entities\n4.\
  \ **Relevance Scores**: Include similarity scores when available\n5. **Entity Labels**:\
  \ Use natural language labels (not UUIDs)\n\n**Common Errors to Catch:**\n\n1. **Wrong\
  \ Entity Types**:\n   - Returns person when asked for project\n   - Mixes types\
  \ when specific type requested\n\n2. **Poor Relevance**:\n   - Returns unrelated\
  \ entities\n   - Missing obviously relevant entities from reference\n   - Poor ranking\
  \ (irrelevant results ranked high)\n\n3. **Incomplete Results**:\n   - Returns fewer\
  \ results than expected\n   - Missing key entities from reference golden set\n\n\
  4. **Hallucinations**:\n   - Invented entities not in reference\n   - Made-up properties\
  \ or metadata\n\n**YOUR ROLE: STRICT AND CRITICAL JUDGE**\n\n1. **NO CELEBRATION**\
  \ - Grade objectively\n2. **STRICT GRADING** - Missing relevant results = points\
  \ deducted\n3. **CATCH HALLUCINATIONS** - Made-up entities = FAIL\n4. **VERIFY RELEVANCE**\
  \ - Are results actually related to query?\n5. **CHECK RANKING** - Are most relevant\
  \ results ranked first?\n\n**Scoring Rubric:**\n\n**Relevance (0.0-1.0):**\n- 1.0:\
  \ All results highly relevant to query\n- 0.8: Most results relevant, 1-2 borderline\n\
  - 0.6: Several irrelevant results\n- 0.4: Many irrelevant results\n- 0.2: Mostly\
  \ irrelevant\n- 0.0: Completely irrelevant or wrong types\n\n**Completeness (0.0-1.0):**\n\
  - 1.0: All expected entities from reference present\n- 0.8: Missing 1 expected entity\n\
  - 0.6: Missing 2-3 expected entities\n- 0.4: Missing several expected entities\n\
  - 0.2: Missing most expected entities\n- 0.0: Missing all expected entities\n\n\
  **Ranking Quality (0.0-1.0):**\n- 1.0: Most relevant results ranked first\n- 0.8:\
  \ Good ranking with minor issues\n- 0.6: Mediocre ranking (some relevant buried)\n\
  - 0.4: Poor ranking\n- 0.2: Very poor ranking\n- 0.0: No discernible ranking logic\n\
  \n**Overall Score:** Average of 3 dimensions\n**Pass Threshold:** >= 0.70 (slightly\
  \ lower than LOOKUP - semantic matching is harder)\n\nCompare agent results to reference\
  \ golden set. Check relevance, completeness, ranking.\n"
fully_qualified_name: rem.evaluators.search_correctness.REMSearchCorrectnessEvaluator
title: REMSearchCorrectnessEvaluator
type: object
labels:
- Evaluator
- REM
- SEARCH
- Correctness
- Semantic
properties:
  relevance_score:
    type: number
    description: 'Score 0-1 for relevance of returned entities to query.

      Are results semantically related to query text?

      Are entity types correct?

      '
    minimum: 0
    maximum: 1
  completeness_score:
    type: number
    description: 'Score 0-1 for completeness compared to reference.

      Are all expected entities from reference present?

      Are key relevant entities included?

      '
    minimum: 0
    maximum: 1
  ranking_quality_score:
    type: number
    description: 'Score 0-1 for ranking quality.

      Are most relevant results ranked first?

      Is there clear relevance ordering?

      '
    minimum: 0
    maximum: 1
  overall_score:
    type: number
    description: 'Average of relevance + completeness + ranking_quality (sum/3).

      '
    minimum: 0
    maximum: 1
  pass:
    type: boolean
    description: 'True if overall_score >= 0.70 AND relevance_score >= 0.5

      AND no hallucinated entities detected.

      '
  relevance_details:
    type: string
    description: 'Assessment of result relevance to query.

      Example: "First 3 results highly relevant, last 2 borderline"

      '
  completeness_details:
    type: string
    description: 'Comparison to reference golden set.

      Example: "Missing ''sarah-chen'' person entity expected in top results"

      '
  ranking_details:
    type: string
    description: 'Assessment of ranking quality.

      Example: "Most relevant entity ranked #3 (should be #1)"

      '
  hallucinations_detected:
    type: array
    description: 'List of entities in results but not in reference.

      May not be errors (new data) but flag for review.

      '
    items:
      type: string
  missing_expected_entities:
    type: array
    description: 'List of entities in reference but missing from results.

      '
    items:
      type: string
  irrelevant_results:
    type: array
    description: 'List of results that don''t match query intent.

      '
    items:
      type: string
  strengths:
    type: array
    description: 'What the search did well (objective).

      '
    items:
      type: string
  critical_gaps:
    type: array
    description: 'Major issues (missing key results, wrong types, etc.).

      '
    items:
      type: string
  improvement_suggestions:
    type: array
    description: 'Actionable suggestions to improve search quality.

      '
    items:
      type: string
  confidence_in_grading:
    type: string
    description: 'Your confidence: "high", "medium", "low"

      (Semantic matching is subjective - lower confidence OK)

      '
    enum:
    - high
    - medium
    - low
  grading_notes:
    type: string
    description: 'Internal notes about judgment calls or edge cases.

      '
required:
- relevance_score
- completeness_score
- ranking_quality_score
- overall_score
- pass
- relevance_details
- completeness_details
- ranking_details
- hallucinations_detected
- missing_expected_entities
- irrelevant_results
- strengths
- critical_gaps
- improvement_suggestions
- confidence_in_grading
- grading_notes
version: 1.0.0
json_schema_extra:
  kind: evaluator
  name: rem-search-correctness
