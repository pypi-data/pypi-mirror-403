# Getting Started with JarvisCore

Build your first AI agent in 5 minutes!

---

## Choose Your Path

### Profiles (How agents execute)

| Profile | Best For | LLM Required |
|---------|----------|--------------|
| **AutoAgent** | Rapid prototyping, LLM generates code from prompts | Yes |
| **CustomAgent** | Existing code, full control (LangChain, CrewAI, etc.) | Optional |
| **ListenerAgent** | API-first (FastAPI), just implement handlers | Optional |

### Execution Modes (How agents are orchestrated)

| Mode | Use Case | Start Here |
|------|----------|------------|
| **Autonomous** | Single machine, simple pipelines | ‚úÖ This guide |
| **P2P** | Direct agent communication, swarms | [CustomAgent Guide](CUSTOMAGENT_GUIDE.md) |
| **Distributed** | Multi-node production systems | [CustomAgent Guide](CUSTOMAGENT_GUIDE.md) |

**Recommendation:**
- **New to agents?** Start with **AutoAgent + Autonomous mode** below
- **Have existing code?** Jump to **CustomAgent** or **ListenerAgent** sections
- **Building APIs?** See **ListenerAgent + FastAPI** below

---

## What You'll Build

An **AutoAgent** that takes natural language prompts and automatically:
1. Generates Python code using an LLM
2. Executes the code securely in a sandbox
3. Returns the result

**No manual coding required** - just describe what you want!

---

## Prerequisites

- ‚úÖ Python 3.10 or higher
- ‚úÖ An API key from one of these LLM providers:
  - [Claude (Anthropic)](https://console.anthropic.com/) - Recommended
  - [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
  - [Google Gemini](https://ai.google.dev/)
  - Local vLLM server (free, self-hosted)

---

## Step 1: Install JarvisCore (30 seconds)

```bash
pip install jarviscore-framework
```

---

## Step 2: Configure Your LLM (2 minutes)

Initialize your project and create configuration files:

```bash
# Initialize project (creates .env.example and optionally examples)
python -m jarviscore.cli.scaffold --examples

# Copy and configure your environment
cp .env.example .env
```

Edit `.env` and add **ONE** of these API keys:

### Option 1: Claude (Recommended)
```bash
CLAUDE_API_KEY=sk-ant-your-key-here
```

### Option 2: Azure OpenAI
```bash
AZURE_API_KEY=your-key-here
AZURE_ENDPOINT=https://your-resource.openai.azure.com
AZURE_DEPLOYMENT=gpt-4o
```

### Option 3: Google Gemini
```bash
GEMINI_API_KEY=your-key-here
```

### Option 4: Local vLLM (Free, Self-Hosted)
```bash
LLM_ENDPOINT=http://localhost:8000
LLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
```

**Tip:** JarvisCore automatically tries providers in this order:
Claude ‚Üí Azure ‚Üí Gemini ‚Üí vLLM

---

## Step 3: Validate Your Setup (30 seconds)

Run the health check to ensure everything works:

```bash
# Basic check
python -m jarviscore.cli.check

# Test LLM connectivity
python -m jarviscore.cli.check --validate-llm
```

You should see:
```
‚úì Python Version: OK
‚úì JarvisCore Package: OK
‚úì Dependencies: OK
‚úì .env File: OK
‚úì Claude/Azure/Gemini: OK
```

Run the smoke test for end-to-end validation:

```bash
python -m jarviscore.cli.smoketest
```

‚úÖ **If all tests pass**, you're ready to build agents!

---

## Step 4: Build Your First Agent (3 minutes)

Create a file called `my_first_agent.py`:

```python
import asyncio
from jarviscore import Mesh
from jarviscore.profiles import AutoAgent


# 1. Define your agent
class CalculatorAgent(AutoAgent):
    role = "calculator"
    capabilities = ["math", "calculations"]
    system_prompt = """
    You are a math expert. Generate Python code to solve problems.
    Always store the result in a variable named 'result'.
    """


# 2. Create and run
async def main():
    # Initialize the mesh
    mesh = Mesh(mode="autonomous")

    # Add your agent
    mesh.add(CalculatorAgent)

    # Start the mesh
    await mesh.start()

    # Execute a task with a simple prompt
    results = await mesh.workflow("calc-workflow", [
        {
            "agent": "calculator",
            "task": "Calculate the factorial of 10"
        }
    ])

    # Get the result
    result = results[0]
    print(f"Status: {result['status']}")
    print(f"Output: {result['output']}")
    print(f"Execution time: {result['execution_time']:.2f}s")

    # Stop the mesh
    await mesh.stop()


if __name__ == "__main__":
    asyncio.run(main())
```

### Run it:

```bash
python my_first_agent.py
```

### Expected Output:

```
Status: success
Output: 3628800
Execution time: 4.23s
```

**üéâ Congratulations!** You just built an AI agent with zero manual coding!

---

## Step 5: Try CustomAgent (Alternative Path)

If you have existing agents or don't need LLM code generation, use **CustomAgent**:

```python
import asyncio
from jarviscore import Mesh
from jarviscore.profiles import CustomAgent


class MyAgent(CustomAgent):
    role = "processor"
    capabilities = ["data_processing"]

    async def execute_task(self, task):
        """Your existing logic goes here."""
        data = task.get("params", {}).get("data", [])
        result = [x * 2 for x in data]
        return {"status": "success", "output": result}


async def main():
    # CustomAgent uses "distributed" (workflow + P2P) or "p2p" (P2P only)
    mesh = Mesh(mode="distributed", config={
        'bind_port': 7950,
        'node_name': 'custom-node',
    })
    mesh.add(MyAgent)
    await mesh.start()

    results = await mesh.workflow("custom-demo", [
        {"agent": "processor", "task": "Process data", "params": {"data": [1, 2, 3]}}
    ])

    print(results[0]["output"])  # [2, 4, 6]
    await mesh.stop()


asyncio.run(main())
```

**Key Benefits:**
- No LLM API required (no costs!)
- Keep your existing logic
- Works with any framework (LangChain, CrewAI, etc.)

**For more:** See [CustomAgent Guide](CUSTOMAGENT_GUIDE.md) for P2P mode and multi-node examples.

---

## Step 6: ListenerAgent + FastAPI (API-First Path)

Building an API where agents run in the background? **ListenerAgent** eliminates the boilerplate.

### The Problem

With CustomAgent, you write a `run()` loop to handle peer messages:

```python
# CustomAgent - you write the loop
class MyAgent(CustomAgent):
    async def run(self):
        while not self.shutdown_requested:
            msg = await self.peers.receive(timeout=1.0)
            if msg is None:
                continue
            if msg.type == MessageType.REQUEST:
                result = await self.process(msg.data)
                await self.peers.respond(msg, result)
            # ... error handling, logging, etc.
```

### The Solution

With ListenerAgent, just implement handlers:

```python
from jarviscore.profiles import ListenerAgent

class MyAgent(ListenerAgent):
    role = "processor"
    capabilities = ["processing"]

    async def on_peer_request(self, msg):
        """Handle requests - return value is sent as response."""
        return {"result": await self.process(msg.data)}

    async def on_peer_notify(self, msg):
        """Handle fire-and-forget notifications."""
        await self.log_event(msg.data)
```

### FastAPI Integration (3 Lines)

```python
from fastapi import FastAPI, Request
from jarviscore.profiles import ListenerAgent
from jarviscore.integrations.fastapi import JarvisLifespan

class ProcessorAgent(ListenerAgent):
    role = "processor"
    capabilities = ["data_processing"]

    async def on_peer_request(self, msg):
        return {"processed": msg.data.get("task", "").upper()}

# Create agent and integrate with FastAPI
agent = ProcessorAgent()
app = FastAPI(lifespan=JarvisLifespan(agent, mode="p2p", bind_port=7950))

@app.post("/process")
async def process(data: dict, request: Request):
    # Access your agent from the request
    agent = request.app.state.jarvis_agents["processor"]
    return await agent.process(data)

@app.get("/peers")
async def list_peers(request: Request):
    agent = request.app.state.jarvis_agents["processor"]
    return {"peers": agent.peers.list_peers()}
```

Run with: `uvicorn myapp:app --host 0.0.0.0 --port 8000`

**What you get:**
- HTTP endpoints (FastAPI routes) as primary interface
- P2P mesh participation in background
- Auto message dispatch to handlers
- Graceful startup/shutdown handled by JarvisLifespan

**For more:** See [CustomAgent Guide](CUSTOMAGENT_GUIDE.md) for ListenerAgent details.

---

## What Just Happened?

Behind the scenes, JarvisCore:

1. **Received your prompt**: "Calculate the factorial of 10"
2. **Generated Python code** using Claude/Azure/Gemini:
   ```python
   def factorial(n):
       if n == 0 or n == 1:
           return 1
       return n * factorial(n - 1)

   result = factorial(10)
   ```
3. **Executed the code** safely in a sandbox
4. **Returned the result**: 3628800

All from a single natural language prompt!

---

## Step 5: Try More Complex AutoAgent Profile Examples

### Example 1: Data Processing

```python
class DataAgent(AutoAgent):
    role = "data_analyst"
    capabilities = ["data_processing", "statistics"]
    system_prompt = "You are a data analyst. Generate Python code for data tasks."


async def analyze_data():
    mesh = Mesh(mode="autonomous")
    mesh.add(DataAgent)
    await mesh.start()

    results = await mesh.workflow("data-workflow", [
        {
            "agent": "data_analyst",
            "task": """
            Given this list: [10, 20, 30, 40, 50]
            Calculate: mean, median, min, max, sum
            Return as a dict
            """
        }
    ])

    print(results[0]['output'])
    # Output: {'mean': 30.0, 'median': 30, 'min': 10, 'max': 50, 'sum': 150}

    await mesh.stop()
```

### Example 2: Text Processing

```python
class TextAgent(AutoAgent):
    role = "text_processor"
    capabilities = ["text", "nlp"]
    system_prompt = "You are a text processing expert."


async def process_text():
    mesh = Mesh(mode="autonomous")
    mesh.add(TextAgent)
    await mesh.start()

    results = await mesh.workflow("text-workflow", [
        {
            "agent": "text_processor",
            "task": """
            Count the words in this sentence:
            "JarvisCore makes building AI agents incredibly easy"
            """
        }
    ])

    print(results[0]['output'])  # Output: 7

    await mesh.stop()
```

### Example 3: Multi-Step Workflow

```python
async def multi_step_workflow():
    mesh = Mesh(mode="autonomous")
    mesh.add(CalculatorAgent)
    mesh.add(DataAgent)
    await mesh.start()

    results = await mesh.workflow("multi-step", [
        {
            "id": "step1",
            "agent": "calculator",
            "task": "Calculate 5 factorial"
        },
        {
            "id": "step2",
            "agent": "data_analyst",
            "task": "Take the result from step1 and calculate its square root",
            "dependencies": ["step1"]  # Waits for step1 to complete
        }
    ])

    print(f"Factorial(5): {results[0]['output']}")      # 120
    print(f"Square root: {results[1]['output']:.2f}")   # 10.95

    await mesh.stop()
```

---

## Key Concepts

### 1. AutoAgent Profile

The `AutoAgent` profile handles the "prompt ‚Üí code ‚Üí result" workflow automatically:

```python
class MyAgent(AutoAgent):
    role = "unique_name"              # Unique identifier
    capabilities = ["skill1", "skill2"]  # What it can do
    system_prompt = "Instructions for the LLM"  # How to generate code
```

### 2. CustomAgent Profile

The `CustomAgent` profile lets you bring your own execution logic:

```python
class MyAgent(CustomAgent):
    role = "unique_name"
    capabilities = ["skill1", "skill2"]

    async def execute_task(self, task):      # For workflow steps (distributed)
        return {"status": "success", "output": ...}

    async def run(self):                      # For continuous loop (p2p)
        while not self.shutdown_requested:
            msg = await self.peers.receive(timeout=0.5)
            ...
```

### 3. ListenerAgent Profile

The `ListenerAgent` profile is for API-first agents - just implement handlers:

```python
class MyAgent(ListenerAgent):
    role = "unique_name"
    capabilities = ["skill1", "skill2"]

    async def on_peer_request(self, msg):    # Handle requests (return = response)
        return {"result": ...}

    async def on_peer_notify(self, msg):     # Handle notifications (fire-and-forget)
        await self.log(msg.data)
```

### 4. Mesh

The `Mesh` is the orchestrator that manages agents and workflows:

```python
mesh = Mesh(mode="autonomous")  # Or "p2p", "distributed"
mesh.add(MyAgent)               # Register your agent
await mesh.start()              # Initialize
results = await mesh.workflow(...)  # Execute tasks
await mesh.stop()               # Cleanup
```

**Modes:**
- `autonomous`: Workflow engine only (AutoAgent)
- `p2p`: P2P coordinator for agent-to-agent communication (CustomAgent, ListenerAgent)
- `distributed`: Both workflow engine AND P2P (CustomAgent, ListenerAgent)

### 5. Workflow

A workflow is a list of tasks to execute:

```python
results = await mesh.workflow("workflow-id", [
    {
        "agent": "agent_role",     # Which agent to use
        "task": "What to do",      # Natural language prompt
        "dependencies": []         # Optional: wait for other steps
    }
])
```

### 6. Results

Each task returns a result dict:

```python
{
    "status": "success",           # success or failure
    "output": 42,                  # The actual result
    "execution_time": 3.14,        # Seconds
    "repairs": 0,                  # Auto-fix attempts
    "code": "result = 6 * 7",      # Generated code
    "agent_id": "calculator-abc123"
}
```

---

## Configuration Options

### Sandbox Mode

Choose between local or remote code execution:

```bash
# Local (default) - runs on your machine
SANDBOX_MODE=local

# Remote - runs on Azure Container Apps (more secure)
SANDBOX_MODE=remote
SANDBOX_SERVICE_URL=https://your-sandbox-service.com
```

### LLM Settings

Fine-tune LLM behavior:

```bash
# Model selection (provider-specific)
CLAUDE_MODEL=claude-sonnet-4        # Claude
AZURE_DEPLOYMENT=gpt-4o             # Azure
GEMINI_MODEL=gemini-2.0-flash       # Gemini
LLM_MODEL=Qwen/Qwen2.5-Coder-32B   # vLLM

# Generation parameters
LLM_TEMPERATURE=0.0   # 0.0 = deterministic, 1.0 = creative
LLM_MAX_TOKENS=2000   # Max response length
LLM_TIMEOUT=120       # Request timeout (seconds)
```

### Logging

Control log verbosity:

```bash
LOG_LEVEL=INFO   # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

---

## Common Patterns

### Pattern 1: Error Handling

```python
try:
    results = await mesh.workflow("workflow-1", [
        {"agent": "calculator", "task": "Calculate 1/0"}
    ])

    if results[0]['status'] == 'failure':
        print(f"Error: {results[0]['error']}")
        # The agent automatically attempted repairs
        print(f"Repair attempts: {results[0]['repairs']}")

except Exception as e:
    print(f"Workflow failed: {e}")
```

### Pattern 2: Dynamic Tasks

```python
user_input = "Calculate the area of a circle with radius 5"

results = await mesh.workflow("dynamic", [
    {"agent": "calculator", "task": user_input}
])

print(results[0]['output'])  # 78.54
```

### Pattern 3: Context Passing

Pass data between steps:

```python
results = await mesh.workflow("context-workflow", [
    {
        "id": "generate",
        "agent": "data_analyst",
        "task": "Generate a list of 10 random numbers between 1 and 100"
    },
    {
        "id": "analyze",
        "agent": "data_analyst",
        "task": "Calculate statistics on the numbers from step 'generate'",
        "dependencies": ["generate"]
    }
])

numbers = results[0]['output']
stats = results[1]['output']
print(f"Numbers: {numbers}")
print(f"Statistics: {stats}")
```

---

## Troubleshooting

### Issue: "No LLM providers configured"

**Solution:** Check your `.env` file has a valid API key:
```bash
python -m jarviscore.cli.check --validate-llm
```

### Issue: "Task failed: Unknown error"

**Solution:** Check logs for details:
```bash
ls -la logs/
cat logs/<agent>/<latest>.json
```

Run smoke test with verbose output:
```bash
python -m jarviscore.cli.smoketest --verbose
```

### Issue: Slow execution

**Causes:**
1. LLM latency (2-5s per request)
2. Complex prompts
3. Network issues

**Solutions:**
- Use faster models (Claude Haiku, Gemini Flash)
- Simplify prompts
- Use local vLLM for zero-latency

### Issue: Generated code has errors

**Good news:** AutoAgent automatically attempts to fix errors!

It will:
1. Detect the error
2. Ask the LLM to fix the code
3. Retry execution (up to 3 times)

Check `repairs` in the result to see how many fixes were needed.

---

## Next Steps

1. **AutoAgent Guide**: Multi-node distributed mode ‚Üí [AUTOAGENT_GUIDE.md](AUTOAGENT_GUIDE.md)
2. **CustomAgent Guide**: P2P and distributed with your code ‚Üí [CUSTOMAGENT_GUIDE.md](CUSTOMAGENT_GUIDE.md)
3. **User Guide**: Complete documentation ‚Üí [USER_GUIDE.md](USER_GUIDE.md)
4. **API Reference**: [API_REFERENCE.md](API_REFERENCE.md)
5. **Configuration**: [CONFIGURATION.md](CONFIGURATION.md)
6. **Examples**: Check out `examples/` directory

---

## Best Practices

### ‚úÖ DO

- **Be specific in prompts**: "Calculate factorial of 10" > "Do math"
- **Test with simple tasks first**: Validate your setup works
- **Use appropriate models**: Haiku/Flash for simple tasks, Opus/GPT-4 for complex
- **Monitor costs**: Check LLM usage if using paid APIs
- **Read error messages**: They contain helpful hints

### ‚ùå DON'T

- **Use vague prompts**: "Do something" won't work well
- **Expect instant results**: LLM generation takes 2-5 seconds
- **Skip validation**: Always run health check after setup
- **Commit API keys**: Keep `.env` out of version control
- **Ignore logs**: They help debug issues

---

## FAQ

### Q: How much does it cost?

**A:** Depends on your LLM provider:
- **Claude**: ~$3-15 per million tokens (most expensive but best quality)
- **Azure**: ~$3-15 per million tokens (enterprise-grade)
- **Gemini**: $0.10-5 per million tokens (cheapest cloud option)
- **vLLM**: FREE (self-hosted, no API costs)

A typical simple task uses ~500 tokens = $0.0015 with Claude.

### Q: Is the code execution safe?

**A:** Yes! Code runs in an isolated sandbox:
- **Local mode**: Restricted Python environment (no file/network access)
- **Remote mode**: Azure Container Apps (fully isolated containers)

### Q: Can I use my own LLM?

**A:** Yes! Point `LLM_ENDPOINT` to any OpenAI-compatible API:
```bash
LLM_ENDPOINT=http://localhost:8000  # Local vLLM
LLM_ENDPOINT=https://your-api.com   # Custom endpoint
```

### Q: What if the LLM generates bad code?

**A:** AutoAgent automatically detects and fixes errors:
1. Catches syntax/runtime errors
2. Sends error to LLM with fix instructions
3. Retries with corrected code (up to 3 attempts)

Check `repairs` in the result to see how many fixes were needed.

### Q: Can I see the generated code?

**A:** Yes! It's in the result:
```python
result = results[0]
print(result['code'])  # Shows the generated Python code
```

Or check logs:
```bash
cat logs/<agent-role>/<result-id>.json
```

### Q: How do I deploy this in production?

**A:** See the User Guide for:
- Remote sandbox configuration (Azure Container Apps)
- High-availability setup
- Monitoring and logging
- Cost optimization

---

## Support

Need help?

1. **Check docs**: [USER_GUIDE.md](USER_GUIDE.md) | [TROUBLESHOOTING.md](TROUBLESHOOTING.md)
2. **Run diagnostics**:
   ```bash
   python -m jarviscore.cli.check --verbose
   python -m jarviscore.cli.smoketest --verbose
   ```
3. **Check logs**: `cat logs/<agent>/<latest>.json`
4. **Report issues**: [GitHub Issues](https://github.com/Prescott-Data/jarviscore-framework/issues)

---

**üöÄ Happy building with JarvisCore!**
