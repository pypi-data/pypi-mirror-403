# Polars Benchmark Design

## Overview
This design adds a single benchmark script to quantify the LazyFrame build cost, collection cost, and peak memory footprint for the Polars-based factor pipeline. The script is intentionally standalone and uses the public `Factor` API to mirror real usage. It generates a synthetic long-form dataset with `start_time`, `end_time`, `symbol`, and `factor` columns, then builds a fixed operator chain (`ts_mean -> cs_rank -> ts_zscore -> ts_rank`) to represent typical factor workflows. Timing is split into two phases: the lazy-build phase (no `.data` access) and the collection phase (first `.data` call). Memory usage is measured with `tracemalloc` to report peak bytes during collection. The script defaults to a “medium” dataset size (`n_symbols=500`, `n_periods=2000`), with CLI options for other sizes. The output is a concise summary line that includes build time, collect time, and peak memory, plus the resulting row count to ensure correctness. The benchmark does not modify the core engine or any production code paths and uses only existing dependencies (pandas, numpy, polars). Success is defined as producing measurable timing and memory stats without errors, and as a regression guardrail for large-scale workloads. This provides a reproducible baseline for Phase 4.2 and supports future comparisons across changes.
