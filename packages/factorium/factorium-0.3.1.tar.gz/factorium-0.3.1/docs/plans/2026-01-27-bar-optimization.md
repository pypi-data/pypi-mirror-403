# Bar Optimization Implementation Plan (Revised v2)

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** æ“´å…… `load_aggbar` æ”¯æ´ TickBar/VolumeBar/DollarBarï¼Œçµ±ä¸€ APIï¼Œä¸¦ç§»é™¤ `bar.py`ã€‚

**Architecture:** 
- ä½¿ç”¨ DuckDB SQL èšåˆæ‰€æœ‰ bar é¡å‹
- TimeBar æ”¯æ´å¤šæ¨™çš„ï¼Œå…¶ä»– bar é¡å‹åƒ…æ”¯æ´å–®æ¨™çš„
- åŸ `bar.py` ç§»è‡³ `tests/_legacy_bar/` ä½œç‚ºé©—è­‰åŸºæº–

**Tech Stack:** Python, DuckDB, Polars, pytest

**é ä¼°æ™‚é–“:** 6-8 å°æ™‚ï¼ˆåŒ…å«å®Œæ•´æ¸¬è©¦é©—è­‰ï¼Œè€ƒæ…® legacy å°é½Šèˆ‡é‚Šç•Œæ¡ˆä¾‹ï¼‰

---

## ğŸ“ Bar å®šç¾©è¦æ ¼ï¼ˆè©³ç´°ç‰ˆï¼‰

### é€šç”¨è¦å‰‡

| é …ç›® | è¦æ ¼ |
|------|------|
| **å–®ç­†äº¤æ˜“æ‹†åˆ†** | **ä¸æ‹†åˆ†**ã€‚å–®ç­†äº¤æ˜“å®Œæ•´æ­¸å±¬æ–¼ä¸€å€‹ barï¼ˆå³ä½¿è·¨è¶Šå¤šå€‹é–€æª»ï¼‰ |
| **é‚Šç•Œæ­¸å±¬** | ç´¯ç©åˆ°é–€æª»é‚£ç­†**æ­¸å±¬ç•¶å‰ bar**ï¼Œä¸‹ä¸€ç­†é–‹å§‹æ–° bar |
| **æ’åº Tie-breaker** | åŒ timestamp æ™‚ä½¿ç”¨ `(ts, price, volume, is_buyer_maker)` ä¿è­‰é †åºç©©å®š |
| **VWAP é™¤ä»¥é›¶** | ç•¶ `SUM(volume) <= 1e-10` æ™‚å›å‚³ `NULL` |
| **interval å‹åˆ¥** | `float`ï¼ˆçµ±ä¸€ï¼Œå›  volume/dollar å¯èƒ½æ˜¯å°æ•¸ï¼‰ |
| **ç©º bar è™•ç†** | ä¸ç”¢ç”Ÿç©º barï¼ˆåªæœ‰å¯¦éš›æœ‰äº¤æ˜“çš„ bar æ‰è¼¸å‡ºï¼‰ |

### å„ Bar é¡å‹å®šç¾©

| Bar Type | åˆ‡åˆ†æ¢ä»¶ | bar_id è¨ˆç®— | å‚™è¨» |
|----------|----------|-------------|------|
| **TimeBar** | å›ºå®šæ™‚é–“é–“éš” | `(ts - start_time) // interval_ms` | å¯èƒ½ç”¢ç”Ÿé–“éš™ |
| **TickBar** | å›ºå®š tick æ•¸ | `(row_num - 1) // interval_ticks` | é€£çºŒç„¡é–“éš™ |
| **VolumeBar** | ç´¯ç©æˆäº¤é‡ | è¦‹ä¸‹æ–¹æ•¸å­¸æ¨å° | é€£çºŒç„¡é–“éš™ |
| **DollarBar** | ç´¯ç©ç¾å…ƒé‡ | è¦‹ä¸‹æ–¹æ•¸å­¸æ¨å° | é€£çºŒç„¡é–“éš™ |

### ğŸ”¬ Legacy vs DuckDB SQL æ•¸å­¸å°æ‡‰è­‰æ˜

**Legacy VolumeBar é‚è¼¯ï¼ˆNumbaï¼‰ï¼š**

```python
# ç‹€æ…‹æ©Ÿé‚è¼¯
current_volume = 0
bar_id = 0
for i in range(n):
    current_volume += volume[i]    # 1. ç´¯åŠ 
    bar_ids[i] = bar_id            # 2. æŒ‡å®šï¼ˆæ­¸å±¬ç•¶å‰ barï¼‰
    if current_volume >= threshold:
        current_volume = 0         # 3. æ¸…é›¶
        bar_id += 1                # 4. æ› bar
```

**DuckDB SQL ç­‰åƒ¹å…¬å¼ï¼š**

```sql
-- cum_volume = ç´¯ç©åˆ°é€™ç­†ï¼ˆå«ï¼‰çš„ volume
bar_id = FLOOR((cum_volume - volume) / threshold)
```

**æ•¸å­¸è­‰æ˜ï¼š**

è¨­ `v[i]` ç‚ºç¬¬ i ç­†äº¤æ˜“çš„ volumeï¼Œ`C[i] = Î£v[0..i]` ç‚ºç´¯ç© volumeã€‚

Legacy çš„ bar_id å¯¦éš›ä¸Šæ˜¯ã€Œåœ¨è™•ç†ç¬¬ i ç­†**ä¹‹å‰**å·²ç¶“å®Œæˆäº†å¹¾è¼ªç´¯ç©ã€ï¼š
- ç¬¬ i ç­†æ­¸å±¬çš„ bar_id = åœ¨ `v[i]` åŠ å…¥å‰ï¼Œå·²ç¶“æœ‰å¹¾æ¬¡ `cum >= threshold`
- é€™ç­‰åƒ¹æ–¼ `FLOOR((C[i-1]) / threshold)` = `FLOOR((C[i] - v[i]) / threshold)`

**é‚Šç•Œæ¡ˆä¾‹é©—è­‰ï¼š**

| æ¡ˆä¾‹ | volume[] | threshold | Legacy bar_ids | SQL bar_ids | ä¸€è‡´? |
|------|----------|-----------|----------------|-------------|-------|
| åŸºæœ¬ | [10,10,10,10] | 25 | [0,0,0,1] | [0,0,0,1] | âœ… |
| å‰›å¥½ | [10,10,10] | 30 | [0,0,0] | [0,0,0] | âœ… |
| è·¨è¶Š | [5,35,5] | 10 | [0,0,1] | [0,0,1] | âœ… |
| å¤§å–® | [5,100,5] | 30 | [0,0,1] | [0,0,1] | âœ… |

> âš ï¸ **é—œéµæ´å¯Ÿ**ï¼šå…¬å¼ `FLOOR((cum - volume) / threshold)` ç­‰åƒ¹æ–¼ legacy çš„ç‹€æ…‹æ©Ÿé‚è¼¯ã€‚
> é€™æ˜¯å› ç‚º legacy çš„ã€Œæ¸…é›¶ã€è¡Œç‚ºæœ¬è³ªä¸Šæ˜¯è¨ˆç®—ã€Œåˆ°ç›®å‰ç‚ºæ­¢è·¨éäº†å¹¾å€‹é–€æª»ã€ã€‚

---

## ğŸ”§ DuckDB SQL è¨­è¨ˆåŸå‰‡

### 1. é †åºä¿è­‰ç­–ç•¥ï¼ˆé—œéµä¿®æ­£ï¼‰

**å•é¡Œï¼š** `FIRST()/LAST()` åœ¨ `GROUP BY` å¾Œ**ä¸ä¿è­‰é †åº**ã€‚DuckDB çš„è¨ˆç®—è¨ˆç•«å¯èƒ½å› æª”æ¡ˆåˆ†æ®µã€ä¸¦è¡Œè™•ç†ç­‰å› ç´ å°è‡´çµæœä¸ç©©å®šã€‚

**è§£æ±ºæ–¹æ¡ˆï¼š** ä½¿ç”¨ `ARG_MIN/ARG_MAX` æ­é…åºåˆ—è™Ÿï¼ˆseqï¼‰ï¼š

```sql
-- Step 1: åœ¨ numbered CTE ä¸­ç”¢ç”Ÿå”¯ä¸€åºåˆ—è™Ÿï¼ˆä½¿ç”¨å®Œæ•´ tie-breakerï¼‰
ROW_NUMBER() OVER (ORDER BY ts, price, volume, is_buyer_maker) AS seq

-- Step 2: èšåˆæ™‚ä½¿ç”¨ ARG_MIN/ARG_MAX
ARG_MIN(ts, seq) AS start_time,
ARG_MAX(ts, seq) AS end_time,
ARG_MIN(price, seq) AS open,
ARG_MAX(price, seq) AS close,
```

### 2. Tie-breaker ç­–ç•¥ï¼ˆåŒ timestamp è™•ç†ï¼‰

**å•é¡Œï¼š** åŒä¸€å€‹ timestamp å¯èƒ½æœ‰å¤šç­†äº¤æ˜“ï¼Œéœ€è¦ç¢ºä¿æ’åºç©©å®šã€‚

**è§£æ±ºæ–¹æ¡ˆï¼š** ä½¿ç”¨è¤‡åˆæ’åºï¼ˆèˆ‡ Legacy ä¸€è‡´ï¼‰ï¼š

```sql
-- å„ªå…ˆé †åºï¼šts â†’ price â†’ volume â†’ is_buyer_maker
ORDER BY ts, price, volume, is_buyer_maker
```

> âš ï¸ è‹¥åŸå§‹è³‡æ–™æœ‰ `trade_id` æˆ– `agg_trade_id`ï¼Œæ‡‰å„ªå…ˆä½¿ç”¨è©²æ¬„ä½ä½œç‚º tie-breakerã€‚

### 3. VWAP å®‰å…¨é™¤æ³•

```sql
CASE 
    WHEN SUM(volume) <= 1e-10 THEN NULL 
    ELSE SUM(price * volume) / SUM(volume) 
END AS vwap
```

> ä½¿ç”¨ `1e-10` ä½œç‚º EPSILONï¼Œèˆ‡å°ˆæ¡ˆ `constants.py` ä¸€è‡´ã€‚

### 4. SQL åƒæ•¸åŒ–ï¼ˆé¿å…æ³¨å…¥ï¼‰

ä½¿ç”¨ escape å‡½æ•¸é¿å… SQL æ³¨å…¥ï¼š

```python
def escape_sql_string(s: str) -> str:
    """Escape single quotes in SQL string to prevent injection."""
    return s.replace("'", "''")

# ä½¿ç”¨æ–¹å¼
escaped_symbol = escape_sql_string(symbol)
query = f"... WHERE symbol = '{escaped_symbol}' ..."
```

> æœªä¾†å¯è€ƒæ…®æ”¹ç”¨ DuckDB åƒæ•¸åŒ–æŸ¥è©¢ï¼Œä½†ç›®å‰ escape è¶³å¤ æ‡‰ä»˜å…§éƒ¨å·¥å…·éœ€æ±‚ã€‚

---

## Phase 0: ä¿®å¾©ç¾æœ‰ aggregate_time_bars çš„é †åºå•é¡Œ

### Task 0: ä¿®æ­£ aggregate_time_bars ä½¿ç”¨ ARG_MIN/ARG_MAX

**Files:**
- Modify: `src/factorium/data/aggregator.py`

**Changes:**
å°‡ç¾æœ‰çš„ `FIRST(price) AS open, LAST(price) AS close` æ”¹ç‚ºï¼š

```sql
-- åŠ å…¥åºåˆ—è™Ÿ
ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY {ts_col}) AS seq

-- èšåˆä½¿ç”¨ ARG_MIN/ARG_MAX
ARG_MIN(price, seq) AS open,
ARG_MAX(price, seq) AS close,
ARG_MIN(ts, seq) AS first_ts,
ARG_MAX(ts, seq) AS last_ts,
```

**Step: Commit**

```bash
git add src/factorium/data/aggregator.py
git commit -m "fix(aggregator): use ARG_MIN/ARG_MAX to guarantee OHLC order"
```

---

## Phase 1: ä¿ç•™ Legacy Bar ä½œç‚ºæ¸¬è©¦åŸºæº–

### Task 1: å»ºç«‹ tests/_legacy_bar ç›®éŒ„

**Files:**
- Create: `tests/_legacy_bar/__init__.py`
- Create: `tests/_legacy_bar/bar.py` (è¤‡è£½è‡ª `src/factorium/bar.py`)

**Step 1: å»ºç«‹ç›®éŒ„çµæ§‹**

```bash
mkdir -p tests/_legacy_bar
```

**Step 2: è¤‡è£½ bar.py åˆ° _legacy_bar**

```bash
cp src/factorium/bar.py tests/_legacy_bar/bar.py
```

**Step 3: å»ºç«‹ __init__.py**

```python
"""Legacy bar implementations for testing DuckDB aggregation correctness."""

from .bar import BaseBar, TimeBar, TickBar, VolumeBar, DollarBar

__all__ = ["BaseBar", "TimeBar", "TickBar", "VolumeBar", "DollarBar"]
```

**Step 4: Commit**

```bash
git add tests/_legacy_bar/
git commit -m "test: move bar.py to _legacy_bar for aggregation verification"
```

---

## Phase 2: æ“´å…… BarAggregator

### Task 2: å¯¦ä½œ aggregate_tick_bars

**Files:**
- Modify: `src/factorium/data/aggregator.py`

**DuckDB SQL é‚è¼¯ï¼š**
1. ä½¿ç”¨ `ROW_NUMBER() OVER (ORDER BY ts)` ç‚ºæ¯ç­† tick ç·¨è™Ÿï¼ˆä½œç‚º seqï¼‰
2. `FLOOR((seq - 1) / interval_ticks)` è¨ˆç®— bar_id
3. `GROUP BY bar_id` èšåˆï¼Œä½¿ç”¨ `ARG_MIN/ARG_MAX` ä¿è­‰é †åº

```python
def aggregate_tick_bars(
    self,
    parquet_pattern: str,
    symbol: str,
    interval_ticks: int,
    column_mapping: ColumnMapping,
    include_buyer_seller: bool = True,
) -> pd.DataFrame:
    """Aggregate tick data into tick-based OHLCV bars.

    Args:
        parquet_pattern: Glob pattern for Parquet files
        symbol: Single symbol (tick bars don't align across symbols)
        interval_ticks: Number of ticks per bar
        column_mapping: Column name mapping for the data source
        include_buyer_seller: Include buyer/seller statistics

    Returns:
        DataFrame with OHLCV columns
    """
    ts_col = column_mapping.timestamp
    price_col = column_mapping.price
    volume_col = column_mapping.volume
    ibm_col = column_mapping.is_buyer_maker
    
    # Escape symbol to prevent SQL injection
    escaped_symbol = symbol.replace("'", "''")

    buyer_seller_sql = ""
    buyer_seller_cols = ""
    if include_buyer_seller and ibm_col:
        buyer_seller_sql = """
            , SUM(CASE WHEN NOT is_buyer_maker THEN 1 ELSE 0 END) AS num_buyer
            , SUM(CASE WHEN is_buyer_maker THEN 1 ELSE 0 END) AS num_seller
            , SUM(CASE WHEN NOT is_buyer_maker THEN volume ELSE 0 END) AS num_buyer_volume
            , SUM(CASE WHEN is_buyer_maker THEN volume ELSE 0 END) AS num_seller_volume
        """
        buyer_seller_cols = ", num_buyer, num_seller, num_buyer_volume, num_seller_volume"

    query = f"""
        WITH raw_data AS (
            SELECT
                symbol,
                {ts_col} AS ts,
                {price_col} AS price,
                {volume_col} AS volume
                {f", {ibm_col} AS is_buyer_maker" if include_buyer_seller and ibm_col else ""}
            FROM read_parquet('{parquet_pattern}', hive_partitioning=true)
            WHERE symbol = '{escaped_symbol}'
        ),
        numbered AS (
            SELECT
                *,
                ROW_NUMBER() OVER (ORDER BY ts) AS seq,
                (ROW_NUMBER() OVER (ORDER BY ts) - 1) // {interval_ticks} AS bar_id
            FROM raw_data
        ),
        aggregated AS (
            SELECT
                symbol,
                bar_id,
                ARG_MIN(ts, seq) AS start_time,
                ARG_MAX(ts, seq) AS end_time,
                ARG_MIN(price, seq) AS open,
                MAX(price) AS high,
                MIN(price) AS low,
                ARG_MAX(price, seq) AS close,
                SUM(volume) AS volume,
                CASE 
                    WHEN SUM(volume) <= 1e-10 THEN NULL 
                    ELSE SUM(price * volume) / SUM(volume) 
                END AS vwap
                {buyer_seller_sql}
            FROM numbered
            GROUP BY symbol, bar_id
        )
        SELECT
            symbol, start_time, end_time,
            open, high, low, close, volume, vwap
            {buyer_seller_cols}
        FROM aggregated
        ORDER BY bar_id
    """

    try:
        return duckdb.query(query).df()
    except duckdb.IOException as e:
        logger.warning(f"DuckDB tick bar aggregation failed: {e}")
        return pd.DataFrame()
```

---

### Task 3: å¯¦ä½œ aggregate_volume_bars

**Files:**
- Modify: `src/factorium/data/aggregator.py`

**DuckDB SQL é‚è¼¯ï¼š**

Volume bar çš„é—œéµæ˜¯è¦åŒ¹é… legacy çš„é‚è¼¯ï¼š
- ç´¯ç©æˆäº¤é‡
- ç•¶ç´¯ç© >= é–€æª»æ™‚ï¼Œ**ç•¶å‰é€™ç­†ä»å±¬æ–¼èˆŠ bar**ï¼Œä¸‹ä¸€ç­†é–‹å§‹æ–° bar

```python
def aggregate_volume_bars(
    self,
    parquet_pattern: str,
    symbol: str,
    interval_volume: float,
    column_mapping: ColumnMapping,
    include_buyer_seller: bool = True,
) -> pd.DataFrame:
    """Aggregate tick data into volume-based OHLCV bars.
    
    Bar boundary rule: When cumulative volume >= threshold, the current trade
    belongs to the current bar, and the next trade starts a new bar.
    """
    ts_col = column_mapping.timestamp
    price_col = column_mapping.price
    volume_col = column_mapping.volume
    ibm_col = column_mapping.is_buyer_maker
    
    escaped_symbol = symbol.replace("'", "''")

    buyer_seller_sql = ""
    buyer_seller_cols = ""
    if include_buyer_seller and ibm_col:
        buyer_seller_sql = """
            , SUM(CASE WHEN NOT is_buyer_maker THEN 1 ELSE 0 END) AS num_buyer
            , SUM(CASE WHEN is_buyer_maker THEN 1 ELSE 0 END) AS num_seller
            , SUM(CASE WHEN NOT is_buyer_maker THEN volume ELSE 0 END) AS num_buyer_volume
            , SUM(CASE WHEN is_buyer_maker THEN volume ELSE 0 END) AS num_seller_volume
        """
        buyer_seller_cols = ", num_buyer, num_seller, num_buyer_volume, num_seller_volume"

    # Note: This SQL replicates the legacy behavior:
    # - current_volume += volume; bar_ids[i] = bar_id; if current_volume >= threshold: bar_id += 1
    # The key insight is that we need to count how many times the threshold was crossed
    # BEFORE this trade (not including this trade's volume contribution to crossing).
    #
    # We use a cumulative sum approach where bar_id is determined by the cumulative
    # volume at the START of each trade (cum_volume - volume).
    
    query = f"""
        WITH raw_data AS (
            SELECT
                symbol,
                {ts_col} AS ts,
                {price_col} AS price,
                {volume_col} AS volume
                {f", {ibm_col} AS is_buyer_maker" if include_buyer_seller and ibm_col else ""}
            FROM read_parquet('{parquet_pattern}', hive_partitioning=true)
            WHERE symbol = '{escaped_symbol}'
        ),
        numbered AS (
            SELECT
                *,
                ROW_NUMBER() OVER (ORDER BY ts) AS seq
            FROM raw_data
        ),
        cumulative AS (
            SELECT
                *,
                SUM(volume) OVER (ORDER BY seq) AS cum_volume
            FROM numbered
        ),
        with_bar_id AS (
            SELECT
                *,
                -- bar_id based on cumulative volume BEFORE adding this trade
                -- This matches legacy: assign bar_id first, then check threshold
                FLOOR((cum_volume - volume) / {interval_volume})::BIGINT AS bar_id
            FROM cumulative
        ),
        aggregated AS (
            SELECT
                symbol,
                bar_id,
                ARG_MIN(ts, seq) AS start_time,
                ARG_MAX(ts, seq) AS end_time,
                ARG_MIN(price, seq) AS open,
                MAX(price) AS high,
                MIN(price) AS low,
                ARG_MAX(price, seq) AS close,
                SUM(volume) AS volume,
                CASE 
                    WHEN SUM(volume) <= 1e-10 THEN NULL 
                    ELSE SUM(price * volume) / SUM(volume) 
                END AS vwap
                {buyer_seller_sql}
            FROM with_bar_id
            GROUP BY symbol, bar_id
        )
        SELECT
            symbol, start_time, end_time,
            open, high, low, close, volume, vwap
            {buyer_seller_cols}
        FROM aggregated
        ORDER BY bar_id
    """

    try:
        return duckdb.query(query).df()
    except duckdb.IOException as e:
        logger.warning(f"DuckDB volume bar aggregation failed: {e}")
        return pd.DataFrame()
```

---

### Task 4: å¯¦ä½œ aggregate_dollar_bars

**Files:**
- Modify: `src/factorium/data/aggregator.py`

**DuckDB SQL é‚è¼¯ï¼š**
é¡ä¼¼ Volume Barï¼Œä½†ç´¯ç© `price * volume`ï¼ˆdollar volumeï¼‰

```python
def aggregate_dollar_bars(
    self,
    parquet_pattern: str,
    symbol: str,
    interval_dollar: float,
    column_mapping: ColumnMapping,
    include_buyer_seller: bool = True,
) -> pd.DataFrame:
    """Aggregate tick data into dollar-volume based OHLCV bars.
    
    Bar boundary rule: Same as volume bars, but threshold is dollar volume.
    """
    ts_col = column_mapping.timestamp
    price_col = column_mapping.price
    volume_col = column_mapping.volume
    ibm_col = column_mapping.is_buyer_maker
    
    escaped_symbol = symbol.replace("'", "''")

    buyer_seller_sql = ""
    buyer_seller_cols = ""
    if include_buyer_seller and ibm_col:
        buyer_seller_sql = """
            , SUM(CASE WHEN NOT is_buyer_maker THEN 1 ELSE 0 END) AS num_buyer
            , SUM(CASE WHEN is_buyer_maker THEN 1 ELSE 0 END) AS num_seller
            , SUM(CASE WHEN NOT is_buyer_maker THEN volume ELSE 0 END) AS num_buyer_volume
            , SUM(CASE WHEN is_buyer_maker THEN volume ELSE 0 END) AS num_seller_volume
        """
        buyer_seller_cols = ", num_buyer, num_seller, num_buyer_volume, num_seller_volume"

    query = f"""
        WITH raw_data AS (
            SELECT
                symbol,
                {ts_col} AS ts,
                {price_col} AS price,
                {volume_col} AS volume,
                {price_col} * {volume_col} AS dollar_volume
                {f", {ibm_col} AS is_buyer_maker" if include_buyer_seller and ibm_col else ""}
            FROM read_parquet('{parquet_pattern}', hive_partitioning=true)
            WHERE symbol = '{escaped_symbol}'
        ),
        numbered AS (
            SELECT
                *,
                ROW_NUMBER() OVER (ORDER BY ts) AS seq
            FROM raw_data
        ),
        cumulative AS (
            SELECT
                *,
                SUM(dollar_volume) OVER (ORDER BY seq) AS cum_dollar
            FROM numbered
        ),
        with_bar_id AS (
            SELECT
                *,
                FLOOR((cum_dollar - dollar_volume) / {interval_dollar})::BIGINT AS bar_id
            FROM cumulative
        ),
        aggregated AS (
            SELECT
                symbol,
                bar_id,
                ARG_MIN(ts, seq) AS start_time,
                ARG_MAX(ts, seq) AS end_time,
                ARG_MIN(price, seq) AS open,
                MAX(price) AS high,
                MIN(price) AS low,
                ARG_MAX(price, seq) AS close,
                SUM(volume) AS volume,
                CASE 
                    WHEN SUM(volume) <= 1e-10 THEN NULL 
                    ELSE SUM(price * volume) / SUM(volume) 
                END AS vwap
                {buyer_seller_sql}
            FROM with_bar_id
            GROUP BY symbol, bar_id
        )
        SELECT
            symbol, start_time, end_time,
            open, high, low, close, volume, vwap
            {buyer_seller_cols}
        FROM aggregated
        ORDER BY bar_id
    """

    try:
        return duckdb.query(query).df()
    except duckdb.IOException as e:
        logger.warning(f"DuckDB dollar bar aggregation failed: {e}")
        return pd.DataFrame()
```

**Step: Commit all aggregator changes**

```bash
git add src/factorium/data/aggregator.py
git commit -m "feat(aggregator): add tick/volume/dollar bar aggregation with correct ordering"
```

---

## Phase 3: çµ±ä¸€ load_aggbar API

### Task 5: ä¿®æ”¹ BinanceDataLoader.load_aggbar

**Files:**
- Modify: `src/factorium/data/loader.py`

**Changes:**

1. **é‡å‘½å** `load_aggbar_fast` â†’ `load_aggbar`ï¼ˆç§»é™¤èˆŠçš„ `load_aggbar`ï¼‰
2. **æ–°å¢** `bar_type` åƒæ•¸ï¼š`"time"` | `"tick"` | `"volume"` | `"dollar"`
3. **interval å‹åˆ¥æ”¹ç‚º float**ï¼ˆçµ±ä¸€è™•ç† volume/dollar çš„å°æ•¸æƒ…æ³ï¼‰
4. **åš´æ ¼æª¢æŸ¥**ï¼šé TimeBar æ™‚ï¼Œè‹¥ symbols å¤šæ–¼ä¸€å€‹å‰‡æ‹‹å‡º ValueError
5. **è·¯ç”±åˆ°å°æ‡‰çš„ aggregator æ–¹æ³•**

```python
def load_aggbar(
    self,
    symbols: Union[str, List[str]],
    data_type: str,
    market_type: str,
    futures_type: str = "um",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    days: Optional[int] = None,
    interval: float = 60_000.0,
    bar_type: str = "time",
    force_download: bool = False,
    use_cache: bool = True,
) -> "AggBar":
    """Load bar data and return as AggBar.

    Args:
        symbols: Symbol(s) to load. For non-time bars, must be a single symbol.
        data_type: Type of data (e.g., "aggTrades", "trades")
        market_type: "spot" or "futures"
        futures_type: "um" or "cm" (for futures only)
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        days: Number of days from today (alternative to start/end dates)
        interval: Bar interval (meaning depends on bar_type)
            - time: milliseconds (default 60_000 = 1 minute)
            - tick: number of ticks
            - volume: volume threshold (float)
            - dollar: dollar volume threshold (float)
        bar_type: "time", "tick", "volume", or "dollar"
        force_download: Force re-download even if files exist
        use_cache: Use cached aggregated bars if available (time bars only)

    Returns:
        AggBar with aggregated bar data

    Raises:
        ValueError: If bar_type is not "time" and multiple symbols provided
        ValueError: If bar_type is invalid
    """
    # Normalize symbols to list
    if isinstance(symbols, str):
        symbols = [symbols]

    # Validate bar_type
    valid_bar_types = {"time", "tick", "volume", "dollar"}
    if bar_type not in valid_bar_types:
        raise ValueError(f"bar_type must be one of {valid_bar_types}, got '{bar_type}'")
    
    # Validate: non-time bars only support single symbol
    if bar_type != "time" and len(symbols) > 1:
        raise ValueError(
            f"bar_type='{bar_type}' only supports single symbol, "
            f"got {len(symbols)} symbols: {symbols}"
        )

    # ... existing download logic ...

    # Route to appropriate aggregator method
    if bar_type == "time":
        df = self._aggregator.aggregate_time_bars(
            parquet_pattern=parquet_pattern,
            symbols=symbols,
            interval_ms=int(interval),
            start_ts=start_ts,
            end_ts=end_ts,
            column_mapping=column_mapping,
            include_buyer_seller=True,
        )
    elif bar_type == "tick":
        df = self._aggregator.aggregate_tick_bars(
            parquet_pattern=parquet_pattern,
            symbol=symbols[0],
            interval_ticks=int(interval),
            column_mapping=column_mapping,
            include_buyer_seller=True,
        )
    elif bar_type == "volume":
        df = self._aggregator.aggregate_volume_bars(
            parquet_pattern=parquet_pattern,
            symbol=symbols[0],
            interval_volume=float(interval),
            column_mapping=column_mapping,
            include_buyer_seller=True,
        )
    elif bar_type == "dollar":
        df = self._aggregator.aggregate_dollar_bars(
            parquet_pattern=parquet_pattern,
            symbol=symbols[0],
            interval_dollar=float(interval),
            column_mapping=column_mapping,
            include_buyer_seller=True,
        )

    return AggBar.from_df(df)
```

**Step: Commit**

```bash
git add src/factorium/data/loader.py
git commit -m "feat(loader): unify load_aggbar API with bar_type support"
```

---

## Phase 4: ç§»é™¤èˆŠçš„ bar.pyï¼ˆBreaking Changeï¼‰

> âš ï¸ **é€™æ˜¯ç ´å£æ€§è®Šæ›´**ï¼šç§»é™¤ `bar.py` æœƒå½±éŸ¿ä»»ä½• `from factorium import TimeBar` çš„ä½¿ç”¨è€…ã€‚

### Task 6: å¾ src/factorium ç§»é™¤ bar.py

**Files:**
- Delete: `src/factorium/bar.py`
- Modify: `src/factorium/__init__.py` (ç§»é™¤ bar ç›¸é—œ export)
- Modify: `src/factorium/aggbar.py` (ç§»é™¤ BaseBar ç›¸é—œ importï¼Œå¦‚æœæœ‰çš„è©±)
- Move: `tests/test_bar.py` â†’ `tests/_legacy_bar/test_bar.py`

**ç›¸å®¹æ€§ç­–ç•¥ï¼š**

ç”±æ–¼é€™æ˜¯å…§éƒ¨å·¥å…·ï¼Œæ¡ç”¨**ç›´æ¥ç§»é™¤**ç­–ç•¥ï¼ˆä¸åš deprecationï¼‰ï¼š

1. **å…¨ repo æœå°‹**ç¢ºèªç„¡å…¶ä»–å¼•ç”¨ï¼š
   ```bash
   rg "from factorium.bar import|from factorium import.*Bar" --type py
   rg "factorium\.bar\." --type py
   ```

2. **æ›´æ–° CHANGELOG**ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰è¨˜éŒ„ breaking change

3. **æä¾›é·ç§»æŒ‡å—**ï¼š
   ```python
   # èˆŠç”¨æ³•ï¼ˆå°‡è¢«ç§»é™¤ï¼‰
   from factorium import TimeBar, VolumeBar
   bar = VolumeBar(df, interval_volume=1000)
   
   # æ–°ç”¨æ³•
   from factorium.data import load_aggbar
   aggbar = load_aggbar(symbols="BTCUSDT", bar_type="volume", interval=1000, ...)
   ```

**Step 1: å…¨ repo æœå°‹å¼•ç”¨**

```bash
rg "from factorium.bar import|from factorium import.*Bar|factorium\.bar\." --type py
```

è™•ç†æ‰€æœ‰æ‰¾åˆ°çš„å¼•ç”¨ã€‚

**Step 2: æ›´æ–° src/factorium/__init__.py**

ç§»é™¤ä»¥ä¸‹ exportï¼š
```python
# ç§»é™¤é€™äº›è¡Œ
from .bar import BaseBar, TimeBar, TickBar, VolumeBar, DollarBar
```

æ›´æ–° `__all__` ç§»é™¤ Bar é¡åˆ¥ã€‚

**Step 3: æª¢æŸ¥ src/factorium/aggbar.py**

ç¢ºèªæ²’æœ‰ `from .bar import` ç›¸é—œç¨‹å¼ç¢¼ã€‚

**Step 4: ç§»å‹• tests/test_bar.py**

```bash
mv tests/test_bar.py tests/_legacy_bar/test_bar.py
```

æ›´æ–°æ¸¬è©¦ä¸­çš„ importï¼š
```python
# èˆŠ
from factorium import TimeBar, VolumeBar
# æ–°
from tests._legacy_bar import TimeBar, VolumeBar
```

**Step 5: åˆªé™¤ bar.py**

```bash
rm src/factorium/bar.py
```

**Step 6: Commit**

```bash
git add -A
git commit -m "refactor!: remove bar.py from factorium package (BREAKING CHANGE)

Migration: Use load_aggbar(bar_type='volume'|'tick'|'dollar') instead of
VolumeBar/TickBar/DollarBar classes.

Legacy bar classes are preserved in tests/_legacy_bar/ for verification."
```

---

## Phase 5: æ¸¬è©¦èˆ‡é©—è­‰

### Task 7: å»ºç«‹ Aggregator æ¸¬è©¦

**Files:**
- Create: `tests/data/test_aggregator_bars.py`

**æ¸¬è©¦ç­–ç•¥ï¼ˆè©³ç´°ç‰ˆï¼‰ï¼š**

1. **å®Œæ•´æ¬„ä½æ¯”å°**ï¼š
   - OHLCV: open, high, low, close, volume
   - Timing: start_time, end_time
   - Derived: vwap
   - Buyer/Seller: num_buyer, num_seller, num_buyer_volume, num_seller_volume

2. **æµ®é»ç²¾åº¦ç­–ç•¥ï¼ˆæ¬„ä½åˆ†ç´šï¼‰**ï¼š
   | æ¬„ä½é¡å‹ | æ¯”å°æ–¹å¼ | åƒæ•¸ |
   |----------|----------|------|
   | Timestamp | exact | `assert_array_equal` |
   | Price (open/high/low/close) | relative | `rtol=1e-9` |
   | Volume | relative | `rtol=1e-9` |
   | VWAP | relative + NaN | `rtol=1e-9, equal_nan=True` |
   | Count (num_buyer/seller) | exact | `assert_array_equal` |

3. **é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦çŸ©é™£**ï¼š
   | æ¡ˆä¾‹ | æè¿° | é©—è­‰é‡é» |
   |------|------|----------|
   | `test_exact_threshold` | ç´¯ç©å‰›å¥½ç­‰æ–¼é–€æª» | bar_id æ­£ç¢ºæ›æª” |
   | `test_same_timestamp` | åŒ ts å¤šç­† | æ’åºç©©å®šã€OHLC æ­£ç¢º |
   | `test_large_single_trade` | å–®ç­†è·¨å¤šé–€æª» | ä¸æ‹†åˆ†ã€æ­¸å±¬æ­£ç¢º |
   | `test_zero_volume` | volume=0 çš„ bar | VWAP ç‚º NULL |
   | `test_single_trade` | åƒ… 1 ç­†äº¤æ˜“ | edge case |
   | `test_many_small_trades` | å¤§é‡å°å–® | æ•ˆèƒ½ + æ­£ç¢ºæ€§ |

4. **å¤šæª”æ¡ˆä¸€è‡´æ€§æ¸¬è©¦**ï¼š
   - åŒè³‡æ–™åˆ†æˆ 2/3/5 å€‹ parquetï¼Œçµæœå¿…é ˆç›¸åŒ
   - é©—è­‰ DuckDB ä¸¦è¡Œè®€å–ä¸å½±éŸ¿é †åº

5. **äººé€ å¯æ¨å°æ¡ˆä¾‹**ï¼ˆééš¨æ©Ÿï¼‰ï¼š
   ```python
   # å¯æ‰‹ç®—é©—è­‰çš„å›ºå®šæ¡ˆä¾‹
   def test_deterministic_volume_bars():
       trades = pd.DataFrame({
           "ts_init": [1, 2, 3, 4, 5],
           "price": [100, 101, 102, 103, 104],
           "size": [10, 10, 10, 10, 10],  # cum: 10,20,30,40,50
       })
       # interval=25: bar0=[0,1], bar1=[2,3], bar2=[4]
       # æ‰‹ç®—ï¼š
       # bar0: open=100, close=101, high=101, low=100, volume=20
       # bar1: open=102, close=103, high=103, low=102, volume=20
       # bar2: open=104, close=104, volume=10
   ```

```python
"""Tests for BarAggregator comparing with legacy bar implementations."""

import pandas as pd
import numpy as np
import pytest
from pathlib import Path

from factorium.data.aggregator import BarAggregator
from factorium.data.adapters.base import ColumnMapping
from tests._legacy_bar import TickBar, VolumeBar, DollarBar


# === Fixtures ===

@pytest.fixture
def column_mapping() -> ColumnMapping:
    return ColumnMapping(
        timestamp="ts_init",
        price="price",
        volume="size",
        is_buyer_maker="is_buyer_maker",
    )


@pytest.fixture
def sample_trades() -> pd.DataFrame:
    """Create sample trade data with deterministic values."""
    np.random.seed(42)
    n = 1000
    return pd.DataFrame({
        "symbol": ["BTCUSDT"] * n,
        "ts_init": np.arange(n) * 100 + 1700000000000,
        "price": 50000 + np.random.randn(n).cumsum() * 10,
        "size": np.abs(np.random.randn(n)) * 0.1 + 0.01,
        "is_buyer_maker": np.random.choice([True, False], n),
    })


@pytest.fixture
def edge_case_trades() -> pd.DataFrame:
    """Trades with edge cases: same timestamp, exact threshold crossing."""
    return pd.DataFrame({
        "symbol": ["BTCUSDT"] * 10,
        "ts_init": [1000, 1000, 1000, 2000, 2000, 3000, 4000, 5000, 5000, 6000],
        "price": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0],
        "size": [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0],
        "is_buyer_maker": [True, False, True, False, True, False, True, False, True, False],
    })


@pytest.fixture
def deterministic_trades() -> pd.DataFrame:
    """Trades with hand-calculable expected results."""
    return pd.DataFrame({
        "symbol": ["BTCUSDT"] * 5,
        "ts_init": [1000, 2000, 3000, 4000, 5000],
        "price": [100.0, 101.0, 102.0, 103.0, 104.0],
        "size": [10.0, 10.0, 10.0, 10.0, 10.0],  # cum: 10, 20, 30, 40, 50
        "is_buyer_maker": [True, False, True, False, True],
    })


# === Helper Functions ===

def assert_bars_equal(
    duckdb_df: pd.DataFrame, 
    legacy_df: pd.DataFrame, 
    check_cols: list = None,
    rtol: float = 1e-9,
):
    """Assert two bar DataFrames are equal within tolerance.
    
    Uses field-specific comparison strategies:
    - Timestamps: exact match
    - Counts: exact match  
    - Prices/Volumes: relative tolerance
    - VWAP: relative tolerance with NaN handling
    """
    if check_cols is None:
        check_cols = ["open", "high", "low", "close", "volume", "vwap", "start_time", "end_time"]
    
    assert len(duckdb_df) == len(legacy_df), f"Length mismatch: {len(duckdb_df)} vs {len(legacy_df)}"
    
    for col in check_cols:
        if col not in duckdb_df.columns or col not in legacy_df.columns:
            continue
        
        duckdb_vals = duckdb_df[col].values
        legacy_vals = legacy_df[col].values
        
        if col in ["start_time", "end_time", "num_buyer", "num_seller"]:
            # Exact match for timestamps and counts
            np.testing.assert_array_equal(duckdb_vals, legacy_vals, err_msg=f"{col} mismatch")
        elif col == "vwap":
            # Handle NaN/None in vwap
            np.testing.assert_allclose(
                duckdb_vals, legacy_vals, rtol=rtol, equal_nan=True, err_msg=f"{col} mismatch"
            )
        else:
            # Float comparison with tolerance
            np.testing.assert_allclose(
                duckdb_vals, legacy_vals, rtol=rtol, err_msg=f"{col} mismatch"
            )


# === Tick Bar Tests ===

class TestTickBarAggregation:
    def test_tick_bar_matches_legacy(self, sample_trades, column_mapping, tmp_path):
        """DuckDB tick bar aggregation should match legacy implementation."""
        parquet_path = tmp_path / "trades.parquet"
        sample_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_tick_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_ticks=100,
            column_mapping=column_mapping,
        )

        legacy_bar = TickBar(
            sample_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_ticks=100,
        )
        legacy_result = legacy_bar.bars

        assert_bars_equal(duckdb_result, legacy_result)

    def test_tick_bar_same_timestamp(self, edge_case_trades, column_mapping, tmp_path):
        """Tick bars should handle same-timestamp trades correctly."""
        parquet_path = tmp_path / "trades.parquet"
        edge_case_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_tick_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_ticks=5,
            column_mapping=column_mapping,
        )

        legacy_bar = TickBar(
            edge_case_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_ticks=5,
        )
        legacy_result = legacy_bar.bars

        assert_bars_equal(duckdb_result, legacy_result)


# === Volume Bar Tests ===

class TestVolumeBarAggregation:
    def test_volume_bar_matches_legacy(self, sample_trades, column_mapping, tmp_path):
        """DuckDB volume bar aggregation should match legacy implementation."""
        parquet_path = tmp_path / "trades.parquet"
        sample_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_volume_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_volume=5.0,
            column_mapping=column_mapping,
        )

        legacy_bar = VolumeBar(
            sample_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_volume=5.0,
        )
        legacy_result = legacy_bar.bars

        assert_bars_equal(duckdb_result, legacy_result)

    def test_volume_bar_exact_threshold(self, edge_case_trades, column_mapping, tmp_path):
        """Volume bar should correctly handle exact threshold crossing."""
        parquet_path = tmp_path / "trades.parquet"
        edge_case_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_volume_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_volume=30.0,
            column_mapping=column_mapping,
        )

        legacy_bar = VolumeBar(
            edge_case_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_volume=30.0,
        )
        legacy_result = legacy_bar.bars

        assert_bars_equal(duckdb_result, legacy_result)

    def test_volume_bar_deterministic(self, deterministic_trades, column_mapping, tmp_path):
        """Volume bar with hand-calculable expected results."""
        parquet_path = tmp_path / "trades.parquet"
        deterministic_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        result = aggregator.aggregate_volume_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_volume=25.0,  # bar0: [0,1] (20), bar1: [2,3] (20), bar2: [4] (10)
            column_mapping=column_mapping,
        )
        
        # bar_id = FLOOR((cum - vol) / 25)
        # trade 0: (10-10)/25 = 0 â†’ bar 0
        # trade 1: (20-10)/25 = 0 â†’ bar 0
        # trade 2: (30-10)/25 = 0 â†’ bar 0 (cum_before=20)
        # trade 3: (40-10)/25 = 1 â†’ bar 1 (cum_before=30)
        # trade 4: (50-10)/25 = 1 â†’ bar 1 (cum_before=40)
        
        assert len(result) == 2, f"Expected 2 bars, got {len(result)}"
        # Bar 0: trades 0,1,2 â†’ open=100, close=102, high=102, low=100
        # Bar 1: trades 3,4 â†’ open=103, close=104, high=104, low=103


# === Dollar Bar Tests ===

class TestDollarBarAggregation:
    def test_dollar_bar_matches_legacy(self, sample_trades, column_mapping, tmp_path):
        """DuckDB dollar bar aggregation should match legacy implementation."""
        parquet_path = tmp_path / "trades.parquet"
        sample_trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_dollar_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_dollar=50000.0,
            column_mapping=column_mapping,
        )

        legacy_bar = DollarBar(
            sample_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_dollar=50000.0,
        )
        legacy_result = legacy_bar.bars

        assert_bars_equal(duckdb_result, legacy_result)


# === Multi-file Consistency Tests ===

class TestMultiFileConsistency:
    def test_split_parquet_same_result(self, sample_trades, column_mapping, tmp_path):
        """Aggregation should be consistent whether data is in one or multiple files."""
        # Single file
        single_path = tmp_path / "single" / "trades.parquet"
        single_path.parent.mkdir()
        sample_trades.to_parquet(single_path)

        # Split into multiple files
        split_dir = tmp_path / "split"
        split_dir.mkdir()
        n = len(sample_trades)
        sample_trades.iloc[:n//2].to_parquet(split_dir / "part1.parquet")
        sample_trades.iloc[n//2:].to_parquet(split_dir / "part2.parquet")

        aggregator = BarAggregator()
        
        single_result = aggregator.aggregate_tick_bars(
            parquet_pattern=str(single_path),
            symbol="BTCUSDT",
            interval_ticks=100,
            column_mapping=column_mapping,
        )
        
        split_result = aggregator.aggregate_tick_bars(
            parquet_pattern=str(split_dir / "*.parquet"),
            symbol="BTCUSDT",
            interval_ticks=100,
            column_mapping=column_mapping,
        )

        assert_bars_equal(single_result, split_result)

    def test_three_way_split_consistency(self, sample_trades, column_mapping, tmp_path):
        """Verify consistency with 3-way file split."""
        split_dir = tmp_path / "split3"
        split_dir.mkdir()
        n = len(sample_trades)
        sample_trades.iloc[:n//3].to_parquet(split_dir / "part1.parquet")
        sample_trades.iloc[n//3:2*n//3].to_parquet(split_dir / "part2.parquet")
        sample_trades.iloc[2*n//3:].to_parquet(split_dir / "part3.parquet")

        # Compare against legacy (single DataFrame)
        legacy_bar = VolumeBar(
            sample_trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_volume=5.0,
        )
        
        aggregator = BarAggregator()
        split_result = aggregator.aggregate_volume_bars(
            parquet_pattern=str(split_dir / "*.parquet"),
            symbol="BTCUSDT",
            interval_volume=5.0,
            column_mapping=column_mapping,
        )

        assert_bars_equal(split_result, legacy_bar.bars)


# === Edge Case Tests ===

class TestEdgeCases:
    def test_single_trade(self, column_mapping, tmp_path):
        """Handle single trade edge case."""
        single_trade = pd.DataFrame({
            "symbol": ["BTCUSDT"],
            "ts_init": [1000],
            "price": [100.0],
            "size": [1.0],
            "is_buyer_maker": [True],
        })
        parquet_path = tmp_path / "single.parquet"
        single_trade.to_parquet(parquet_path)

        aggregator = BarAggregator()
        result = aggregator.aggregate_tick_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_ticks=10,
            column_mapping=column_mapping,
        )

        assert len(result) == 1
        assert result.iloc[0]["open"] == 100.0
        assert result.iloc[0]["close"] == 100.0
        assert result.iloc[0]["volume"] == 1.0

    def test_large_single_trade_crosses_multiple_thresholds(self, column_mapping, tmp_path):
        """Large single trade should belong to one bar (no splitting)."""
        trades = pd.DataFrame({
            "symbol": ["BTCUSDT", "BTCUSDT", "BTCUSDT"],
            "ts_init": [1000, 2000, 3000],
            "price": [100.0, 101.0, 102.0],
            "size": [5.0, 100.0, 5.0],  # Middle trade crosses multiple thresholds
            "is_buyer_maker": [True, False, True],
        })
        parquet_path = tmp_path / "large_trade.parquet"
        trades.to_parquet(parquet_path)

        aggregator = BarAggregator()
        duckdb_result = aggregator.aggregate_volume_bars(
            parquet_pattern=str(parquet_path),
            symbol="BTCUSDT",
            interval_volume=10.0,
            column_mapping=column_mapping,
        )

        legacy_bar = VolumeBar(
            trades,
            timestamp_col="ts_init",
            price_col="price",
            volume_col="size",
            interval_volume=10.0,
        )

        assert_bars_equal(duckdb_result, legacy_bar.bars)
```

**Step: Commit**

```bash
git add tests/data/test_aggregator_bars.py
git commit -m "test: add comprehensive bar aggregator tests with legacy comparison"
```

---

### Task 8: åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶

**Step 1: åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦**

```bash
pytest -v --tb=short
```

Expected: All tests PASS

**Step 2: æª¢æŸ¥æ˜¯å¦æœ‰éºæ¼çš„ bar.py å¼•ç”¨**

```bash
rg "from factorium.bar import|from factorium import.*Bar|factorium\.bar\." --type py
```

Expected: ç„¡è¼¸å‡ºï¼ˆé™¤äº† tests/_legacy_barï¼‰

**Step 3: æª¢æŸ¥ AggBar å–®æ¨™çš„ç›¸å®¹æ€§**

```python
# é©—è­‰å–®æ¨™çš„ AggBar å¯ä»¥æ­£å¸¸ä½¿ç”¨ Factor API
from factorium import AggBar, Factor
import pandas as pd

df = pd.DataFrame({
    "symbol": ["BTCUSDT"] * 10,
    "start_time": range(10),
    "end_time": range(1, 11),
    "close": [100 + i for i in range(10)],
})
aggbar = AggBar.from_df(df)
factor = aggbar["close"]
print(factor.ts_mean(3).to_pandas())
```

---

## Summary

| Phase | Tasks | ç›®æ¨™ |
|-------|-------|------|
| 0 | Task 0 | ä¿®å¾©ç¾æœ‰ aggregate_time_bars çš„ FIRST/LAST é †åºå•é¡Œ |
| 1 | Task 1 | ä¿ç•™ legacy bar ä½œç‚ºæ¸¬è©¦åŸºæº– |
| 2 | Task 2-4 | æ“´å…… BarAggregator æ”¯æ´ tick/volume/dollar bars |
| 3 | Task 5 | çµ±ä¸€ load_aggbar API |
| 4 | Task 6 | ç§»é™¤èˆŠçš„ bar.pyï¼ˆ**Breaking Change**ï¼‰ |
| 5 | Task 7-8 | æ¸¬è©¦èˆ‡é©—è­‰ |

**é ä¼°æ™‚é–“:** 6-8 å°æ™‚ï¼ˆåŒ…å«å®Œæ•´æ¸¬è©¦èˆ‡ legacy å°é½Šï¼‰

**é¢¨éšªèˆ‡ç·©è§£ï¼š**

| é¢¨éšª | ç·©è§£æªæ–½ |
|------|----------|
| DuckDB `ARG_MIN/ARG_MAX` ç‰ˆæœ¬å·®ç•° | è¦æ±‚ DuckDB >= 0.9 |
| Volume/Dollar bar é‚Šç•Œä¸ä¸€è‡´ | å®Œæ•´çš„é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦ + æ•¸å­¸è­‰æ˜ |
| åŒ timestamp æ’åºä¸ç©©å®š | ä½¿ç”¨å®Œæ•´ tie-breaker `(ts, price, volume, is_buyer_maker)` |
| bar.py ç§»é™¤æ˜¯ breaking change | å…¨ repo æœå°‹ + commit message æ¨™æ³¨ |

**å›æ»¾ç­–ç•¥ï¼š**
- æ¯å€‹ Task æœ‰ç¨ç«‹ commitï¼Œå¯ä»¥å€‹åˆ¥ revert
- Task 6 ä½¿ç”¨ `refactor!:` commit message æ¨™æ³¨ breaking change
- Legacy bar ä¿ç•™åœ¨ `tests/_legacy_bar/`ï¼Œå¯éš¨æ™‚æ¢å¾©

---

## Appendix A: æ¸¬è©¦çŸ©é™£

| æ¸¬è©¦é¡åˆ¥ | è¦†è“‹é …ç›® | æ¸¬è©¦æ•¸é‡ |
|----------|----------|----------|
| **åŸºæœ¬åŠŸèƒ½** | tick/volume/dollar bar èˆ‡ legacy çµæœä¸€è‡´ | 4 |
| **æ¬„ä½å®Œæ•´æ€§** | open, high, low, close, volume, vwap, start_time, end_time | - |
| **é‚Šç•Œæ¡ˆä¾‹** | å‰›å¥½åˆ°é–€æª»ã€åŒ timestamp å¤šç­†ã€è·¨é–€æª»ã€å–®ç­†äº¤æ˜“ | 4 |
| **VWAP å®‰å…¨** | volume=0 æ™‚ vwap ç‚º NULL | 1 |
| **å¤šæª”æ¡ˆä¸€è‡´æ€§** | å–®æª” vs 2-way/3-way åˆ†å‰² parquet | 2 |
| **é †åºç©©å®šæ€§** | åŒ timestamp ä¸åŒåŸ·è¡Œçµæœä¸€è‡´ | 1 |
| **äººé€ æ¡ˆä¾‹** | å¯æ‰‹ç®—é©—è­‰çš„å›ºå®šå€¼ | 1 |

ç¸½è¨ˆ: ~13+ å€‹æ¸¬è©¦æ¡ˆä¾‹

---

## Appendix B: AggBar å‹åˆ¥å¥‘ç´„

### è¼¸å…¥å¥‘ç´„ï¼ˆfrom_df æœŸå¾…çš„ DataFrameï¼‰

| æ¬„ä½ | å‹åˆ¥ | å¿…é ˆ | èªªæ˜ |
|------|------|------|------|
| `symbol` | str | âœ… | æ¨™çš„ä»£ç¢¼ |
| `start_time` | int | âœ… | Bar èµ·å§‹æ™‚é–“æˆ³ï¼ˆmsï¼‰ |
| `end_time` | int | âœ… | Bar çµæŸæ™‚é–“æˆ³ï¼ˆmsï¼‰ |
| `open` | float | âœ… | é–‹ç›¤åƒ¹ |
| `high` | float | âœ… | æœ€é«˜åƒ¹ |
| `low` | float | âœ… | æœ€ä½åƒ¹ |
| `close` | float | âœ… | æ”¶ç›¤åƒ¹ |
| `volume` | float | âœ… | æˆäº¤é‡ |
| `vwap` | float/None | âŒ | æˆäº¤é‡åŠ æ¬Šå¹³å‡åƒ¹ |
| `num_buyer` | int | âŒ | è²·æ–¹äº¤æ˜“ç­†æ•¸ |
| `num_seller` | int | âŒ | è³£æ–¹äº¤æ˜“ç­†æ•¸ |
| `num_buyer_volume` | float | âŒ | è²·æ–¹æˆäº¤é‡ |
| `num_seller_volume` | float | âŒ | è³£æ–¹æˆäº¤é‡ |

### è¼¸å‡ºä¿è­‰

- å–®æ¨™çš„ AggBar å¯æ­£å¸¸ä½¿ç”¨ Factor APIï¼ˆå¦‚ `ts_mean`, `cs_rank` ç­‰ï¼‰
- `symbol` æ¬„ä½å§‹çµ‚å­˜åœ¨ï¼ˆå³ä½¿åªæœ‰ä¸€å€‹æ¨™çš„ï¼‰
- æ™‚é–“æ¬„ä½ç‚ºæ•´æ•¸å‹åˆ¥ï¼ˆæ¯«ç§’æ™‚é–“æˆ³ï¼‰
