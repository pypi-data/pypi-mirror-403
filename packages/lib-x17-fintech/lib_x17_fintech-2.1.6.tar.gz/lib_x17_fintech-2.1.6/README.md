# lib-x17-fintech

ä¸“ä¸šçš„é‡‘èæ•°æ®å¤„ç†åº“ - æä¾›ç»Ÿä¸€çš„å­˜å‚¨æŠ½è±¡ã€æ•°æ®è·å–ã€æ¨¡å¼ç®¡ç†å’Œåºåˆ—åŒ–èƒ½åŠ›ã€‚

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-2467%20passed-brightgreen.svg)](pytest.ini)
[![Coverage](https://img.shields.io/badge/coverage-98%25-brightgreen.svg)](.coverage)

## æ¦‚è¿°

`lib-x17-fintech` æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„é‡‘èæ•°æ®å¤„ç†åº“ï¼Œä¸º X17 ç”Ÿæ€ç³»ç»Ÿæä¾›æ ¸å¿ƒæ•°æ®åŸºç¡€è®¾æ–½ã€‚é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œæä¾›ä»æ•°æ®è·å–åˆ°å­˜å‚¨çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”Œ **å­˜å‚¨æŠ½è±¡** - ç»Ÿä¸€çš„æ¥å£æ”¯æŒæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå’Œ AWS S3
- ğŸ“Š **æ•°æ®è·å–** - å®Œæ•´çš„ Job ç³»ç»Ÿï¼Œæ”¯æŒ Tushareã€Baostock ç­‰æ•°æ®æº
- ğŸ“ **æ¨¡å¼ç®¡ç†** - çµæ´»çš„è¡¨ç»“æ„å®šä¹‰å’ŒéªŒè¯
- ğŸ’¾ **åºåˆ—åŒ–** - å¤šæ ¼å¼æ”¯æŒï¼ˆParquetã€CSVã€JSONã€Pickleï¼‰
- âš¡ **é«˜æ€§èƒ½** - æ™ºèƒ½ç¼“å­˜ã€é‡è¯•å’Œåˆ†é¡µæœºåˆ¶
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ç±»å‹æç¤ºå’Œåè®®æ¥å£

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install lib-x17-fintech

# å¼€å‘ç¯å¢ƒï¼ˆåŒ…å«æµ‹è¯•å·¥å…·ï¼‰
pip install lib-x17-fintech[dev]

# ä»æºç å®‰è£…
git clone <repository-url>
cd lib-x17-fintech
pip install -e .
```

### åŸºæœ¬ç”¨æ³•

#### 1. è·å–é‡‘èæ•°æ®

```python
from xfintech.data.source.tushare import Session, Stock

# åˆ›å»ºä¼šè¯
session = Session(credential="your_tushare_token")

# è·å–è‚¡ç¥¨åˆ—è¡¨
job = Stock(session=session)
result = job.run()

print(result.data)  # pandas DataFrame
```

#### 2. ä¿å­˜æ•°æ®åˆ°å­˜å‚¨

```python
from xfintech.connect import MacOSConnect, Artifact, ConnectRef
from xfintech.serde import PandasSerialiser, DataFormat

# åˆ›å»ºå­˜å‚¨è¿æ¥
connect = MacOSConnect()

# ä¿å­˜æ•°æ®
artifact = Artifact(
    ref=ConnectRef("~/data/stocks.parquet"),
    data=result.data
)
artifact.write(connect, PandasSerialiser, DataFormat.PARQUET)
```

#### 3. å®šä¹‰æ•°æ®æ¨¡å¼

```python
from xfintech.fabric import TableInfo, ColumnInfo, ColumnKind

# å®šä¹‰è¡¨ç»“æ„
schema = TableInfo(
    name="stock_daily",
    columns=[
        ColumnInfo(name="ts_code", kind=ColumnKind.STRING),
        ColumnInfo(name="trade_date", kind=ColumnKind.DATE),
        ColumnInfo(name="open", kind=ColumnKind.FLOAT),
        ColumnInfo(name="close", kind=ColumnKind.FLOAT),
    ]
)

# éªŒè¯æ•°æ®
schema.validate(dataframe)
```

#### 4. å®Œæ•´æ•°æ®ç®¡é“

```python
from xfintech.data.source.tushare import Session, Dayline
from xfintech.data.common import Cache, Retry
from xfintech.connect import S3Connect, Artifact, ConnectRef
from xfintech.serde import PandasSerialiser, DataFormat

# 1. è·å–æ•°æ®ï¼ˆå¸¦ç¼“å­˜å’Œé‡è¯•ï¼‰
session = Session(credential="token")
job = Dayline(
    session=session,
    params={"ts_code": "000001.SZ", "start_date": "20260101"},
    cache=Cache(),
    retry=Retry(retry=3, wait=1.0, rate=2.0)
)
result = job.run()

# 2. ä¿å­˜åˆ° S3
connect = S3Connect(bucket="my-bucket")
artifact = Artifact(
    ref=ConnectRef("data/dayline/000001.parquet"),
    data=result.data,
    meta={"job": job.name, "duration": job.metric.duration}
)
artifact.write(connect, PandasSerialiser, DataFormat.PARQUET)

print(f"âœ… ä¿å­˜ {len(result.data)} è¡Œæ•°æ®ï¼Œè€—æ—¶ {job.metric.duration:.2f} ç§’")
```

## æ¨¡å—æ¶æ„

```
xfintech/
â”œâ”€â”€ connect/        # å­˜å‚¨è¿æ¥æŠ½è±¡
â”‚   â”œâ”€â”€ MacOSConnect    - æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
â”‚   â”œâ”€â”€ S3Connect       - AWS S3
â”‚   â””â”€â”€ Artifact        - æ•°æ®å·¥ä»¶
â”‚
â”œâ”€â”€ data/           # æ•°æ®è·å–åŸºç¡€è®¾æ–½
â”‚   â”œâ”€â”€ common/         - é€šç”¨å·¥å…·ï¼ˆCache, Retry, Metricï¼‰
â”‚   â”œâ”€â”€ job/            - Job ç³»ç»Ÿå’Œæ³¨å†Œä¸­å¿ƒ
â”‚   â”œâ”€â”€ relay/          - ä¸­ç»§å®¢æˆ·ç«¯
â”‚   â””â”€â”€ source/         - æ•°æ®æºï¼ˆTushare, Baostockï¼‰
â”‚
â”œâ”€â”€ fabric/         # æ•°æ®æ¨¡å¼ç®¡ç†
â”‚   â”œâ”€â”€ TableInfo       - è¡¨ç»“æ„å®šä¹‰
â”‚   â”œâ”€â”€ ColumnInfo      - åˆ—ä¿¡æ¯
â”‚   â””â”€â”€ ColumnKind      - æ•°æ®ç±»å‹æšä¸¾
â”‚
â””â”€â”€ serde/          # åºåˆ—åŒ–/ååºåˆ—åŒ–
    â”œâ”€â”€ PythonSerialiser    - Python å¯¹è±¡
    â”œâ”€â”€ PandasSerialiser    - DataFrame
    â””â”€â”€ DataFormat          - æ ¼å¼æšä¸¾
```

## æ¨¡å—è¯¦è§£

### [xfintech.connect](xfintech/connect/README.md) - å­˜å‚¨è¿æ¥

ç»Ÿä¸€çš„å­˜å‚¨æŠ½è±¡å±‚ï¼Œæ”¯æŒå¤šç§åç«¯ã€‚

**ä¸»è¦åŠŸèƒ½**:
- `MacOSConnect` - æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
- `S3Connect` - AWS S3 å¯¹è±¡å­˜å‚¨
- `Artifact` - æ•°æ®å·¥ä»¶ç®¡ç†
- `ConnectRef` - è·¯å¾„å¼•ç”¨

**å¿«é€Ÿç¤ºä¾‹**:
```python
from xfintech.connect import MacOSConnect, Artifact, ConnectRef

connect = MacOSConnect()
artifact = Artifact(ref=ConnectRef("~/data/file.parquet"), data=df)
artifact.write(connect, serialiser, format)
```

[ğŸ“– å®Œæ•´æ–‡æ¡£](xfintech/connect/README.md)

---

### [xfintech.data](xfintech/data/README.md) - æ•°æ®è·å–

å®Œæ•´çš„æ•°æ®è·å–åŸºç¡€è®¾æ–½ï¼ŒJob ç³»ç»Ÿç®¡ç†æ•°æ®è¯·æ±‚ç”Ÿå‘½å‘¨æœŸã€‚

**ä¸»è¦åŠŸèƒ½**:
- `Job` - ä½œä¸šåŸºç±»
- `TushareJob` - Tushare æ•°æ®æº
- `Cache` - æ™ºèƒ½ç¼“å­˜
- `Retry` - æŒ‡æ•°é€€é¿é‡è¯•
- `Metric` - æ€§èƒ½ç›‘æ§

**å¿«é€Ÿç¤ºä¾‹**:
```python
from xfintech.data.source.tushare import Session, Stock
from xfintech.data.common import Cache, Retry

session = Session(credential="token")
job = Stock(
    session=session,
    cache=Cache(),
    retry=Retry(retry=3, wait=1.0)
)
result = job.run()
```

**æ”¯æŒçš„æ•°æ®æº**:
- âœ… Tushare (28+ è‚¡ç¥¨æ•°æ®ç±»)
- âœ… Baostock
- ğŸ”„ æ›´å¤šæ•°æ®æºå¼€å‘ä¸­...

[ğŸ“– å®Œæ•´æ–‡æ¡£](xfintech/data/README.md)

---

### [xfintech.fabric](xfintech/fabric/README.md) - æ•°æ®æ¨¡å¼

çµæ´»çš„è¡¨ç»“æ„å®šä¹‰å’ŒéªŒè¯ç³»ç»Ÿã€‚

**ä¸»è¦åŠŸèƒ½**:
- `TableInfo` - è¡¨ç»“æ„å®šä¹‰
- `ColumnInfo` - åˆ—ä¿¡æ¯ç®¡ç†
- `ColumnKind` - 7 ç§æ•°æ®ç±»å‹
- æ•°æ®éªŒè¯å’Œæ–‡æ¡£ç”Ÿæˆ

**å¿«é€Ÿç¤ºä¾‹**:
```python
from xfintech.fabric import TableInfo, ColumnInfo, ColumnKind

schema = TableInfo(
    name="stock_daily",
    columns=[
        ColumnInfo(name="ts_code", kind=ColumnKind.STRING),
        ColumnInfo(name="close", kind=ColumnKind.FLOAT),
    ]
)

# éªŒè¯æ•°æ®
schema.validate(df)
```

[ğŸ“– å®Œæ•´æ–‡æ¡£](xfintech/fabric/README.md)

---

### [xfintech.serde](xfintech/serde/README.md) - åºåˆ—åŒ–

å¤šæ ¼å¼åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ”¯æŒã€‚

**ä¸»è¦åŠŸèƒ½**:
- `PythonSerialiser` - Python å¯¹è±¡ï¼ˆPickleï¼‰
- `PandasSerialiser` - DataFrameï¼ˆParquet/CSV/JSONï¼‰
- `DataFormat` - æ ¼å¼æšä¸¾
- è‡ªåŠ¨ç±»å‹æ£€æµ‹

**å¿«é€Ÿç¤ºä¾‹**:
```python
from xfintech.serde import PandasSerialiser, DataFormat

# åºåˆ—åŒ–
serialised = PandasSerialiser.serialise(df, DataFormat.PARQUET)

# ååºåˆ—åŒ–
df = PandasSerialiser.deserialise(serialised, DataFormat.PARQUET)
```

**æ”¯æŒæ ¼å¼**:
| æ ¼å¼ | æ‰©å±•å | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|
| Parquet | .parquet | å¤§æ•°æ®é›†ï¼ˆæ¨èï¼‰|
| CSV | .csv | æ–‡æœ¬å¯¼å…¥/å¯¼å‡º |
| JSON | .json | API äº¤äº’ |
| Pickle | .pkl | Python å¯¹è±¡ |

[ğŸ“– å®Œæ•´æ–‡æ¡£](xfintech/serde/README.md)

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ„å»ºæ•°æ®ä»“åº“

```python
from xfintech.data.source.tushare import Session, Dayline
from xfintech.connect import S3Connect, Artifact, ConnectRef
from xfintech.serde import PandasSerialiser, DataFormat
from datetime import datetime, timedelta

def build_data_warehouse():
    """æ„å»ºè‚¡ç¥¨æ—¥çº¿æ•°æ®ä»“åº“"""
    session = Session(credential="token")
    connect = S3Connect(bucket="datawarehouse")
    
    # è·å–è¿‡å» 30 å¤©çš„æ•°æ®
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    symbols = ["000001.SZ", "600000.SH", "000002.SZ"]
    
    for symbol in symbols:
        job = Dayline(
            session=session,
            params={
                "ts_code": symbol,
                "start_date": start_date.strftime("%Y%m%d"),
                "end_date": end_date.strftime("%Y%m%d")
            }
        )
        result = job.run()
        
        # ä¿å­˜åˆ° S3
        path = f"stocks/daily/{symbol}/{end_date.strftime('%Y%m%d')}.parquet"
        artifact = Artifact(ref=ConnectRef(path), data=result.data)
        artifact.write(connect, PandasSerialiser, DataFormat.PARQUET)
        
        print(f"âœ… {symbol}: {len(result.data)} è¡Œ")
```

### åœºæ™¯ 2: æ•°æ®è´¨é‡ç›‘æ§

```python
from xfintech.data.source.tushare import Session, Stock
from xfintech.fabric import TableInfo, ColumnInfo, ColumnKind

def monitor_data_quality():
    """ç›‘æ§æ•°æ®è´¨é‡"""
    # 1. å®šä¹‰æœŸæœ›çš„æ¨¡å¼
    expected_schema = TableInfo(
        name="stock_basic",
        columns=[
            ColumnInfo(name="ts_code", kind=ColumnKind.STRING),
            ColumnInfo(name="name", kind=ColumnKind.STRING),
            ColumnInfo(name="list_date", kind=ColumnKind.DATE),
        ]
    )
    
    # 2. è·å–æ•°æ®
    session = Session(credential="token")
    job = Stock(session=session)
    result = job.run()
    
    # 3. éªŒè¯
    try:
        expected_schema.validate(result.data)
        print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
    except Exception as e:
        print(f"âŒ æ•°æ®è´¨é‡é—®é¢˜: {e}")
```

### åœºæ™¯ 3: å¢é‡æ•°æ®æ›´æ–°

```python
from xfintech.data.source.tushare import Session, TradeDate, Dayline
from xfintech.connect import MacOSConnect, Artifact, ConnectRef
from xfintech.serde import PandasSerialiser, DataFormat
import pandas as pd

def incremental_update():
    """å¢é‡æ›´æ–°æ¯æ—¥æ•°æ®"""
    session = Session(credential="token")
    connect = MacOSConnect()
    
    # 1. è·å–æœ€æ–°äº¤æ˜“æ—¥
    trade_date_job = TradeDate(
        session=session,
        params={"exchange": "SSE"}
    )
    trade_dates = trade_date_job.run().data
    latest_date = trade_dates['cal_date'].max()
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    ref = ConnectRef(f"~/data/daily/{latest_date}.parquet")
    if connect.exists(ref):
        print(f"ğŸ“Œ {latest_date} æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return
    
    # 3. è·å–å¹¶ä¿å­˜æ–°æ•°æ®
    job = Dayline(
        session=session,
        params={"trade_date": latest_date}
    )
    result = job.run()
    
    artifact = Artifact(ref=ref, data=result.data)
    artifact.write(connect, PandasSerialiser, DataFormat.PARQUET)
    
    print(f"âœ… æ›´æ–° {latest_date}: {len(result.data)} è¡Œ")
```

## æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# å¸¦è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=xfintech --cov-report=html

# è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
pytest xfintech/connect/
pytest xfintech/data/
pytest xfintech/fabric/
pytest xfintech/serde/

# æ€§èƒ½åŸºå‡†æµ‹è¯•
pytest --benchmark-only
```

**æµ‹è¯•ç»Ÿè®¡**:
- âœ… 2,467 æµ‹è¯•å…¨éƒ¨é€šè¿‡
- ğŸ“Š 98% ä»£ç è¦†ç›–ç‡
- âš¡ æ‰€æœ‰æµ‹è¯• < 5 ç§’

## å¼€å‘

### é¡¹ç›®ç»“æ„

```bash
lib-x17-fintech/
â”œâ”€â”€ xfintech/              # æºä»£ç 
â”‚   â”œâ”€â”€ connect/           # å­˜å‚¨æ¨¡å—
â”‚   â”œâ”€â”€ data/              # æ•°æ®æ¨¡å—
â”‚   â”œâ”€â”€ fabric/            # æ¨¡å¼æ¨¡å—
â”‚   â””â”€â”€ serde/             # åºåˆ—åŒ–æ¨¡å—
â”œâ”€â”€ tests/                 # æµ‹è¯•ï¼ˆé•œåƒæºç ç»“æ„ï¼‰
â”œâ”€â”€ example/               # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ pyproject.toml         # é¡¹ç›®é…ç½®
â”œâ”€â”€ pytest.ini             # æµ‹è¯•é…ç½®
â”œâ”€â”€ environment.yml        # Conda ç¯å¢ƒ
â””â”€â”€ makefile               # æ„å»ºè„šæœ¬
```

### ç¯å¢ƒè®¾ç½®

```bash
# ä½¿ç”¨ Conda
conda env create -f environment.yml
conda activate xfintech

# æˆ–ä½¿ç”¨ pip
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -e .[dev]
```

### ä»£ç è§„èŒƒ

```bash
# æ ¼å¼åŒ–å’Œ Lint
ruff check xfintech/
ruff format xfintech/

# ç±»å‹æ£€æŸ¥
mypy xfintech/
```

### æ„å»ºå’Œå‘å¸ƒ

```bash
# æ„å»ºåŒ…
python -m build

# å‘å¸ƒåˆ° PyPI
twine upload dist/*
```

## é…ç½®

### ç¯å¢ƒå˜é‡

```bash
# Tushare é…ç½®
export TUSHARE_TOKEN="your_token_here"

# AWS é…ç½®ï¼ˆç”¨äº S3Connectï¼‰
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
export AWS_DEFAULT_REGION="us-east-1"

# ç¼“å­˜ç›®å½•
export XFINTECH_CACHE_DIR="/custom/cache/path"
```

### é…ç½®æ–‡ä»¶

åˆ›å»º `~/.xfintech/config.toml`:

```toml
[tushare]
token = "your_tushare_token"

[cache]
path = "/data/cache"
ttl = 86400  # 24 hours

[s3]
bucket = "my-data-bucket"
region = "us-east-1"
```

## æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

```python
# 1. å§‹ç»ˆä½¿ç”¨ç¼“å­˜
from xfintech.data.common import Cache
job = SomeJob(session=session, cache=Cache())

# 2. é…ç½®é‡è¯•ç­–ç•¥
from xfintech.data.common import Retry
job = SomeJob(session=session, retry=Retry(retry=3, wait=1.0, rate=2.0))

# 3. ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with connect:
    artifact.write(connect, serialiser, format)

# 4. éªŒè¯æ•°æ®æ¨¡å¼
schema.validate(dataframe)

# 5. ç›‘æ§æ€§èƒ½
result = job.run()
print(f"è€—æ—¶: {job.metric.duration} ç§’")
```

### âŒ é¿å…çš„åšæ³•

```python
# 1. ä¸ä½¿ç”¨ç¼“å­˜ï¼ˆé‡å¤ API è°ƒç”¨ï¼‰
job = SomeJob(session=session)  # ç¼ºå°‘ cache

# 2. ç¡¬ç¼–ç è·¯å¾„
path = "/Users/john/data/file.csv"  # ä½¿ç”¨ ConnectRef

# 3. å¿½ç•¥é”™è¯¯
result = job.run()  # ä¸æ£€æŸ¥ job.metric.errors

# 4. ä¸éªŒè¯æ•°æ®
artifact.write(connect, serialiser, format)  # å…ˆéªŒè¯
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©å­˜å‚¨åç«¯ï¼Ÿ

**A**: 
- æœ¬åœ°å¼€å‘/å°æ•°æ®é›† â†’ `MacOSConnect`
- ç”Ÿäº§ç¯å¢ƒ/å¤§æ•°æ®é›†/å›¢é˜Ÿåä½œ â†’ `S3Connect`

### Q: å“ªç§åºåˆ—åŒ–æ ¼å¼æœ€é€‚åˆï¼Ÿ

**A**:
- æ€§èƒ½ä¼˜å…ˆï¼ˆå¤§æ•°æ®ï¼‰â†’ Parquet
- äººç±»å¯è¯» â†’ CSV
- API äº¤äº’ â†’ JSON
- Python å¯¹è±¡ â†’ Pickle

### Q: å¦‚ä½•æé«˜æ•°æ®è·å–æ€§èƒ½ï¼Ÿ

**A**:
1. å¯ç”¨ç¼“å­˜: `Cache()`
2. è°ƒå¤§åˆ†é¡µ: `Paginate(pagesize=5000)`
3. å¹¶å‘è·å–: ä½¿ç”¨ `ThreadPoolExecutor`
4. ä½¿ç”¨ä¸­ç»§: `Session(mode="relay")`

### Q: æµ‹è¯•å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**:
```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
pytest -v

# åªè¿è¡Œå¤±è´¥çš„æµ‹è¯•
pytest --lf

# æŸ¥çœ‹æ ‡å‡†è¾“å‡º
pytest -s
```

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æµç¨‹ï¼š

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing`)
5. åˆ›å»º Pull Request

## è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ä½œè€…

**Xing Xing** - [x.xing.work@gmail.com](mailto:x.xing.work@gmail.com)

## ç›¸å…³é¡¹ç›®

- `lib-x17-cloudcdk` - AWS CDK åŸºç¡€è®¾æ–½
- `lib-x17-cloudsdk` - äº‘æœåŠ¡ SDK
- `lib-x17-cloudmeta` - äº‘å…ƒæ•°æ®ç®¡ç†

## æ›´æ–°æ—¥å¿—

### v2.0.0 (2026-01-09)
- âœ¨ å®Œæ•´çš„å››æ¨¡å—æ¶æ„ï¼ˆconnect, data, fabric, serdeï¼‰
- ğŸ“š å®Œå–„çš„æ–‡æ¡£å’Œç¤ºä¾‹
- âœ… 2,467 æµ‹è¯•ï¼Œ98% è¦†ç›–ç‡
- ğŸš€ æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶

---

**è®©é‡‘èæ•°æ®å¤„ç†å˜å¾—ç®€å•ã€é«˜æ•ˆã€å¯é ï¼** ğŸš€
