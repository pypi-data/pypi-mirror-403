# =============================================================================
# L7 Federation Router Configuration
# arifOS v41.3Omega
# =============================================================================
#
# This file configures the multi-endpoint SEA-LION Federation Router.
# The router intelligently routes requests to specialized model endpoints
# based on task intent, while enforcing arifOS constitutional governance.
#
# Architecture:
#   User (Port 9000)
#       |
#   [000] SENTINEL GATE (SEA-Guard)
#       |
#   [111] INTENT ROUTER
#       |
#   @GEOX ‚Üí @RIF ‚Üí @WEALTH ‚Üí @WELL
#       |
#   [888] COOLING LEDGER
#       |
#   [999] VERDICT
#
# =============================================================================

version: "1.0"
arifos_version: "v41.3Omega"

# -----------------------------------------------------------------------------
# FEDERATION ENDPOINTS (ORGANS)
# -----------------------------------------------------------------------------
# Each organ is a specialized model endpoint with a specific role.
# Ports should match your local vLLM/Ollama deployment.

organs:
  # Safety Pre-filter (Layer 0)
  SENTINEL:
    name: "sea-guard"
    model: "openai/sea-guard"
    api_base: "http://localhost:8005/v1"
    port: 8005
    role: "sentinel"
    symbol: "üõ°Ô∏è"
    provider: "vllm"
    capabilities:
      - "safety"
      - "classification"
    description: "SEA-Guard safety classifier for F9 (Anti-Hantu) enforcement"

  # Vision/Multimodal (Eyes)
  GEOX:
    name: "gemma-vision"
    model: "ollama/gemma-sea-lion-v4-27b-it"
    api_base: "http://localhost:11434"
    port: 11434
    role: "vision"
    symbol: "üëÅÔ∏è"
    provider: "ollama"
    capabilities:
      - "multimodal"
      - "instruction"
      - "regional"
    description: "Gemma-based vision model for image understanding"

  # Deep Reasoning (Brain)
  RIF:
    name: "llama-reasoning"
    model: "openai/llama-sea-lion-v3.5-70b-r"
    api_base: "http://localhost:8001/v1"
    port: 8001
    role: "reasoning"
    symbol: "üß†"
    provider: "vllm"
    capabilities:
      - "reasoning"
      - "instruction"
      - "regional"
      - "thinking_mode"
    description: "Llama-R model with explicit reasoning mode"

  # Long Context (Memory)
  WEALTH:
    name: "qwen-context"
    model: "openai/qwen-sea-lion-v4-32b-it"
    api_base: "http://localhost:8002/v1"
    port: 8002
    role: "context"
    symbol: "üìö"
    provider: "vllm"
    capabilities:
      - "long_context"
      - "instruction"
      - "regional"
    description: "Qwen model with 128k native context window"

  # General Chat (Heart)
  WELL:
    name: "llama-chat"
    model: "openai/llama-sea-lion-v3-70b-it"
    api_base: "http://localhost:8003/v1"
    port: 8003
    role: "chat"
    symbol: "‚ù§Ô∏è"
    provider: "vllm"
    capabilities:
      - "instruction"
      - "chat"
      - "regional"
    description: "General-purpose instruction-tuned model"

# -----------------------------------------------------------------------------
# ROUTING CONFIGURATION
# -----------------------------------------------------------------------------

routing:
  # Default organ for low-confidence or unclassified requests
  default_organ: "WELL"

  # Sentinel organ for safety checks
  guard_organ: "SENTINEL"

  # Routing strategy: "intent", "round_robin", "fixed"
  strategy: "intent"

  # Minimum confidence to route to non-default organ
  confidence_floor: 0.55

  # Guard timeout in milliseconds
  guard_timeout_ms: 500

  # Whether guard check is required (set false only for testing)
  guard_required: true

  # Intent classification rules
  rules:
    # Rule 1: Multimodal content ‚Üí Vision
    - condition: "has_images"
      route_to: "GEOX"
      confidence: 1.0
      priority: 1

    # Rule 2: Long context ‚Üí Deep context model
    - condition: "text_length_gt_30000"
      route_to: "WEALTH"
      confidence: 0.95
      priority: 2

    # Rule 3: Reasoning keywords ‚Üí Reasoning model
    - condition: "reasoning_keywords"
      route_to: "RIF"
      base_confidence: 0.6
      boost_per_match: 0.1
      keywords:
        - "reason"
        - "plan"
        - "analyze"
        - "think"
        - "solve"
        - "step-by-step"
        - "why"
        - "explain"
        - "compare"
        - "contrast"
        - "prove"
        - "derive"
        - "calculate"
        - "logic"
        - "deduce"
      priority: 3

    # Rule 4: Default fallback
    - condition: "default"
      route_to: "WELL"
      confidence: 0.50
      priority: 99

# -----------------------------------------------------------------------------
# THERMODYNAMIC THRESHOLDS
# -----------------------------------------------------------------------------

floors:
  # F6: Empathy floor (Œ∫·µ£)
  kappa_r: 0.95

  # Entropy threshold for PARTIAL verdict
  delta_s_minimum: 1.0

  # Maximum acceptable latency before warning (ms)
  tau_warning_ms: 5000

# -----------------------------------------------------------------------------
# COOLING LEDGER CONFIGURATION
# -----------------------------------------------------------------------------

ledger:
  # Maximum entries to keep in memory
  max_entries: 10000

  # Whether to persist to disk
  persist: false

  # Persistence path (if enabled)
  persist_path: "cooling_ledger/federation_ledger.jsonl"

# -----------------------------------------------------------------------------
# MOCK MODE (Development)
# -----------------------------------------------------------------------------

# Set to false when real GPUs are connected
mock_mode: true

# Mock response template
mock_response_template: "[{organ} MOCK] Processed via {organ}. Intent validated."

# -----------------------------------------------------------------------------
# HEALTH CHECK CONFIGURATION
# -----------------------------------------------------------------------------

health_check:
  # Enable periodic health checks
  enabled: true

  # Interval between checks (seconds)
  interval_seconds: 30

  # Timeout for health check requests (ms)
  timeout_ms: 2000

  # Number of failures before marking unhealthy
  failure_threshold: 3

# -----------------------------------------------------------------------------
# VRAM OPTIMIZATION PROFILES
# -----------------------------------------------------------------------------

profiles:
  # Full federation (multi-GPU, 120GB+)
  full:
    description: "All models loaded simultaneously"
    vram_required: "120GB+"
    latency: "fastest"
    organs: ["SENTINEL", "GEOX", "RIF", "WEALTH", "WELL"]

  # Hybrid (2x GPU, 75GB)
  hybrid:
    description: "@WELL downgraded to 8B variant"
    vram_required: "75GB"
    latency: "fast"
    organs: ["SENTINEL", "GEOX", "RIF", "WEALTH", "WELL_8B"]
    substitutions:
      WELL: "aisingapore/Llama-SEA-LION-v3-8B-IT"

  # Swap mode (single GPU, 24GB)
  swap:
    description: "Only one model loaded at a time"
    vram_required: "24GB"
    latency: "slow"
    organs: ["SENTINEL", "GEOX", "RIF", "WEALTH", "WELL"]
    swap_enabled: true

# Active profile (override via ARIFOS_FEDERATION_PROFILE env var)
active_profile: "full"
