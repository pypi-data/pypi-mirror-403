{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534cb992",
   "metadata": {},
   "source": [
    "# Epistemic Nearest Neighbors (ENN)\n",
    "\n",
    "ENN is a non-parametric surrogate with $O(N)$ computation-time scaling, where $N$ is the number of observations in the data set. ENN can be used in Bayesian optimization as a scalable alternative to a GP (which scales as $O(N^2)$.)\n",
    "\n",
    "**Sweet, D., & Jadhav, S. A. (2025).** Taking the GP Out of the Loop. *arXiv preprint arXiv:2506.12818*.  \n",
    "   https://arxiv.org/abs/2506.12818\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from enn import EpistemicNearestNeighbors, enn_fit\n",
    "from enn.enn.enn_params import PosteriorFlags\n",
    "\n",
    "\n",
    "def make_enn_demo_data(num_samples: int, k: int, noise: float, m: int = 1):\n",
    "    x = np.sort(np.random.rand(num_samples + 4))\n",
    "    x[-3] = x[-4]\n",
    "    x[-2] = x[-4]\n",
    "    x[-1] = x[-4]\n",
    "    x[1] = x[0] + 0.03\n",
    "    eps = np.random.randn(num_samples + 4)\n",
    "    y = np.sin(2 * m * np.pi * x) + noise * eps\n",
    "    yvar = (noise**2) * np.ones_like(y)\n",
    "    train_x = x[:, None]\n",
    "    train_y = y[:, None]\n",
    "    train_yvar = yvar[:, None]\n",
    "    model = EpistemicNearestNeighbors(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        train_yvar,\n",
    "    )\n",
    "    rng = np.random.default_rng(0)\n",
    "    params = enn_fit(\n",
    "        model,\n",
    "        k=k,\n",
    "        num_fit_candidates=100,\n",
    "        num_fit_samples=min(10, num_samples),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return x, y, model, params\n",
    "\n",
    "\n",
    "def plot_enn_posterior(ax, x, y, model, params, num_samples: int, noise: float) -> None:\n",
    "    x_hat = np.linspace(0.0, 1.0, 30)\n",
    "    x_hat_2d = x_hat[:, None]\n",
    "    posterior = model.posterior(x_hat_2d, params=params, flags=PosteriorFlags())\n",
    "    mu = posterior.mu[:, 0]\n",
    "    se = posterior.se[:, 0]\n",
    "    marker_size = 3 if num_samples >= 100 else 15\n",
    "    ax.scatter(x, y, s=marker_size, color=\"black\", alpha=0.5)\n",
    "    ax.plot(x_hat, mu, linestyle=\"--\", color=\"tab:blue\", alpha=0.7)\n",
    "    ax.fill_between(x_hat, mu - 2 * se, mu + 2 * se, color=\"tab:blue\", alpha=0.2)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_title(f\"n={num_samples}, noise={noise}\")\n",
    "\n",
    "\n",
    "def plot_enn_function_samples(\n",
    "    ax, x, y, model, params, num_samples: int, noise: float, num_functions: int = 10\n",
    ") -> None:\n",
    "    x_hat = np.linspace(0.0, 1.0, 100)\n",
    "    x_hat_2d = x_hat[:, None]\n",
    "    posterior = model.posterior(x_hat_2d, params=params, flags=PosteriorFlags())\n",
    "    mu = posterior.mu[:, 0]\n",
    "    se = posterior.se[:, 0]\n",
    "    ax.fill_between(x_hat, mu - 2 * se, mu + 2 * se, color=\"tab:blue\", alpha=0.2)\n",
    "    for i in range(num_functions):\n",
    "        f_sample = model.posterior_function_draw(\n",
    "            x_hat_2d, params, function_seeds=[i], flags=PosteriorFlags()\n",
    "        )[0]\n",
    "        ax.plot(\n",
    "            x_hat,\n",
    "            f_sample[:, 0],\n",
    "            color=\"black\",\n",
    "            alpha=0.5,\n",
    "            linewidth=0.8,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "    marker_size = 3 if num_samples >= 100 else 15\n",
    "    ax.scatter(x, y, s=marker_size, color=\"black\", alpha=0.5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_title(f\"n={num_samples}, noise={noise} (samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd1dc6",
   "metadata": {},
   "source": [
    "## Function Estimation & Function Draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Row 1: Posterior mean + uncertainty band\n",
    "# Row 2: 10 function samples from the same model\n",
    "\n",
    "k = 5\n",
    "num_samples = 5\n",
    "noise_list = [0.0, 0.1, 0.3]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Build models once per noise level, reuse for both rows\n",
    "models_and_data = []\n",
    "for noise in noise_list:\n",
    "    np.random.seed(4)\n",
    "    x, y, model, params = make_enn_demo_data(num_samples=num_samples, k=k, noise=noise)\n",
    "    models_and_data.append((x, y, model, params, noise))\n",
    "\n",
    "# Row 1: posterior mean + uncertainty\n",
    "for col_idx, (x, y, model, params, noise) in enumerate(models_and_data):\n",
    "    ax = axes[0, col_idx]\n",
    "    plot_enn_posterior(ax, x, y, model, params, num_samples=num_samples, noise=noise)\n",
    "\n",
    "# Row 2: function samples\n",
    "for col_idx, (x, y, model, params, noise) in enumerate(models_and_data):\n",
    "    ax = axes[1, col_idx]\n",
    "    plot_enn_function_samples(\n",
    "        ax, x, y, model, params, num_samples=num_samples, noise=noise\n",
    "    )\n",
    "\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel(\"x\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"y\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb810c7",
   "metadata": {},
   "source": [
    "## Large Data Set\n",
    "\n",
    "ENN fits and queries are very fast. This data set uses $N=10^6$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84968071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "t_0 = time.time()\n",
    "x, y, model, params = make_enn_demo_data(num_samples=1_000_000, k=5, noise=0.3, m=3)\n",
    "plot_enn_posterior(ax, x, y, model, params, num_samples=1_000_000, noise=0.3)\n",
    "t_1 = time.time()\n",
    "print(f\"Time taken: {t_1 - t_0:.2f} seconds\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fab69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08ef98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd20fd48",
   "metadata": {},
   "source": [
    "## Aggregation: Function of Function Draws\n",
    "\n",
    "When computing a function, $g(\\{f(x_i)\\})$, over estimates of $f(x_i)$ from multiple $x_i$, it's important to account for the\n",
    "correlations between the  $g(\\{f(x_i)\\})$ when calculating the uncertainty in $g(\\{f(x_i)\\})$. One way to do this is by generating multiple function (joint) draws at $x_i$ and using them to make a Monte Carlo estimate of the distribution of $g(\\{f(x_i)\\})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 1\n",
    "num_obs = 5\n",
    "num_metrics = 1\n",
    "num_func_draw = 100\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "function_seeds = list(range(num_func_draw))\n",
    "train_x = rng.random((num_obs, num_dim))\n",
    "noise = 0.1\n",
    "train_y = np.sin(2 * np.pi * train_x) + noise * rng.standard_normal(\n",
    "    (num_obs, num_metrics)\n",
    ")\n",
    "train_yvar = (noise**2) * np.ones((num_obs, num_metrics))\n",
    "\n",
    "model = EpistemicNearestNeighbors(train_x, train_y, train_yvar)\n",
    "params = enn_fit(\n",
    "    model,\n",
    "    k=min(3, num_obs),\n",
    "    num_fit_candidates=50,\n",
    "    num_fit_samples=min(10, num_obs),\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "x_hat = np.linspace(0.0, 1.0, 30)\n",
    "x_hat_2d = x_hat[:, None]\n",
    "\n",
    "\n",
    "def g(y):\n",
    "    # Some function of y = draw of f(x)\n",
    "    return (y**2 - 1).mean()\n",
    "\n",
    "\n",
    "f_samples_raw = model.posterior_function_draw(\n",
    "    x_hat_2d, params=params, function_seeds=function_seeds, flags=PosteriorFlags()\n",
    ")\n",
    "# `posterior_function_draw` may return either a list of arrays or a single ndarray.\n",
    "if isinstance(f_samples_raw, list):\n",
    "    f_samples = np.stack(f_samples_raw, axis=0)\n",
    "else:\n",
    "    f_samples = np.asarray(f_samples_raw)\n",
    "\n",
    "# Shape: (num_func_draw, len(x_hat), num_metrics)\n",
    "y_vals = f_samples[:, :, 0]\n",
    "means = (y_vals**2 - 1).mean(axis=1)\n",
    "\n",
    "fig, (ax_fns, ax_hist) = plt.subplots(1, 2, figsize=(9, 3.5))\n",
    "ax_fns.plot(x_hat, y_vals.T, color=\"black\", alpha=0.5, linewidth=0.9)\n",
    "\n",
    "ax_fns.scatter(\n",
    "    train_x[:, 0],\n",
    "    train_y[:, 0],\n",
    "    color=\"tab:blue\",\n",
    "    s=40,\n",
    "    zorder=3,\n",
    "    label=\"data\",\n",
    ")\n",
    "ax_fns.set_title(\"ENN posterior function samples\")\n",
    "ax_fns.set_xlabel(\"x\")\n",
    "ax_fns.set_ylabel(\"y\")\n",
    "ax_fns.set_xlim(0, 1)\n",
    "\n",
    "ax_hist.hist(means, bins=25, color=\"tab:gray\", edgecolor=\"white\", alpha=0.9)\n",
    "\n",
    "mu = float(np.mean(means))\n",
    "std = float(np.std(means))\n",
    "for x_val in [mu, mu - 2 * std, mu + 2 * std]:\n",
    "    ax_hist.plot(\n",
    "        [x_val],\n",
    "        [0.0],\n",
    "        marker=\"|\",\n",
    "        markersize=16,\n",
    "        markeredgewidth=2,\n",
    "        color=\"tab:orange\",\n",
    "        transform=ax_hist.get_xaxis_transform(),\n",
    "        clip_on=False,\n",
    "    )\n",
    "\n",
    "true_y = np.sin(2 * np.pi * x_hat)\n",
    "true_g = g(true_y)\n",
    "ax_hist.axvline(\n",
    "    true_g,\n",
    "    color=\"tab:blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=\"true g\",\n",
    ")\n",
    "ax_hist.legend(frameon=False)\n",
    "\n",
    "ax_hist.set_title(\"Histogram of sample means\")\n",
    "ax_hist.set_xlabel(\"mean(y) over x grid\")\n",
    "ax_hist.set_ylabel(\"count\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d38369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb3c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d0a1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
