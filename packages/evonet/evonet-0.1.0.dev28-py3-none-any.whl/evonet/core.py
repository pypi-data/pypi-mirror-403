# SPDX-License-Identifier: MIT
"""
Core class for evolvable neural networks.

Manages neurons, layers, and connections with explicit topology. Supports forward passes
with optional recurrent connections across time steps, mutation/crossover hooks, and
export interfaces.
"""

from __future__ import annotations

import random
from pathlib import Path
from typing import Literal, Optional

import graphviz
import numpy as np

from evonet.activation import softmax as softmax_vec
from evonet.connection import Connection
from evonet.enums import ConnectionType, NeuronRole, RecurrentKind
from evonet.layer import Layer
from evonet.neuron import Neuron
from evonet.utils import connection_init_value


class Nnet:
    """
    Evolvable neural network with explicit layered topology.

    Attributes:
        layers (list[Layer]): Ordered list of network layers.
    """

    def __init__(self) -> None:
        self.layers: list[Layer] = []

    @property
    def num_weights(self) -> int:
        """Number of connections in the network (no allocation)."""
        return len(self.get_all_connections())

    @property
    def num_biases(self) -> int:
        """
        Return the number of trainable biases (excludes input neurons).

        Inputs are feature holders and have no trainable bias in this design.
        """
        count = 0
        for layer in self.layers:
            for neuron in layer.neurons:
                if neuron.role is not NeuronRole.INPUT:
                    count += 1
        return count

    @property
    def num_hidden(self) -> int:
        """Return the number of hidden neurons."""
        count = 0
        for layer in self.layers:
            for neuron in layer.neurons:
                if (
                    neuron.role is not NeuronRole.INPUT
                    and neuron.role is not NeuronRole.OUTPUT
                ):
                    count += 1
        return count

    @property
    def num_params(self) -> int:
        """Total parameter count = weights + biases."""
        return self.num_weights + self.num_biases

    def add_layer(self, count: int = 1) -> int:
        """
        Append one or more empty layers to the network.

        Args:
            count (int): Number of layers to add (must be > 0).

        Returns:
            int: Index of the last added layer.

        Raises:
            ValueError: If count is not positive.
        """

        if count <= 0:
            raise ValueError("Number of layers must be greater then zero")

        for _ in range(count):
            self.layers.append(Layer())

        return len(self.layers) - 1

    def insert_layer(self, index: int) -> None:
        """
        Insert an empty layer at a given index.

        Args:
            index (int): Position to insert the new layer (0 = before input).

        Raises:
            ValueError: If index is out of bounds.
        """

        if not (0 <= index <= len(self.layers)):
            raise ValueError(f"insert_layer: index {index} out of bounds.")
        self.layers.insert(index, Layer())

    def add_neuron(
        self,
        layer_idx: int | None = None,
        activation: str = "tanh",
        bias: float = 0.0,
        label: str = "",
        role: NeuronRole = NeuronRole.HIDDEN,
        count: int = 1,
        dynamics_name: str = "standard",
        dynamics_params: dict[str, float] | None = None,
        connection_init: Literal["random", "zero", "near_zero", "none"] = "zero",
        recurrent: Optional[set[RecurrentKind]] = None,
        connection_scope: Literal["adjacent", "crosslayer"] = "adjacent",
        connection_density: float = 1.0,
        max_connections: int = 2**63 - 1,
        require_io_connections: bool = True,
    ) -> list[Neuron]:
        """
        Add one or more neurons to the network.

        Args:
            layer_idx: Target layer index. Defaults to last layer.
            activation: Activation function name.
            bias: Initial bias value.
            label: Optional textual label.
            role: Role of the neuron (INPUT, HIDDEN, OUTPUT).
            count: Number of neurons to add.
            connection_init:
                "random"    - connect with random weights
                "zero"      - weight = 0.0
                "near_zero" - small uniform weights in (-0.05, 0.05)
                "none"      - do not create any connections
            recurrent: Optional recurrent connection types.
            connection_scope:
                - "adjacent": only neighbor layers
                - "crosslayer": connect to all earlier/later layers
            connection_density:
                Fraction of all possible feed-forward + recurrent connections
                that should actually be created. Must be in [0, 1].
            max_connections:
                Hard limit for the number of created connections per neuron.
            require_io_connections:
                If True, ensure that each newly added neuron has at least
                one incoming and one outgoing connection (if structurally possible).

        Returns:
            list[Neuron]: The created neuron objects.
        """

        if layer_idx is None:
            layer_idx = len(self.layers) - 1  # Add neuron to last layer

        if not (0 <= layer_idx < len(self.layers)):
            raise ValueError(f"Layer index out of bounds: {layer_idx}")

        if not (0.0 < connection_density <= 1.0):
            raise ValueError("connection_density must be in (0, 1].")

        if not (0 <= max_connections <= 2**63 - 1):
            raise ValueError("max_connections must be >= 0 ")

        target_layer = self.layers[layer_idx]
        new_neurons: list[Neuron] = []

        # Create neurons without any connections
        for _ in range(count):
            neuron = Neuron(activation=activation, bias=bias)
            neuron.role = role
            neuron.label = label

            neuron.dynamics_name = dynamics_name
            neuron.dynamics_params = dict(dynamics_params) if dynamics_params else {}

            target_layer.neurons.append(neuron)
            new_neurons.append(neuron)

        # No connections
        if connection_init == "none":
            return new_neurons

        weight = connection_init_value(connection_init)

        # Build connections for every new neuron
        for n in new_neurons:

            # --------------------------------------------------------------
            # FEED-FORWARD candidates
            # --------------------------------------------------------------
            ff_candidates: list[tuple[Neuron, Neuron, ConnectionType]] = []

            if connection_scope == "adjacent":
                # previous layer -> new neuron
                if layer_idx > 0:
                    for src in self.layers[layer_idx - 1].neurons:
                        ff_candidates.append((src, n, ConnectionType.STANDARD))

                # new neuron -> next layer (only hidden neurons produce FF forward)
                if role == NeuronRole.HIDDEN and layer_idx < len(self.layers) - 1:
                    for dst in self.layers[layer_idx + 1].neurons:
                        ff_candidates.append((n, dst, ConnectionType.STANDARD))

            elif connection_scope == "crosslayer":
                # all earlier layers -> new neuron
                for i in range(0, layer_idx):
                    for src in self.layers[i].neurons:
                        ff_candidates.append((src, n, ConnectionType.STANDARD))

                # new neuron -> all later layers
                for j in range(layer_idx + 1, len(self.layers)):
                    for dst in self.layers[j].neurons:
                        ff_candidates.append((n, dst, ConnectionType.STANDARD))

            # --------------------------------------------------------------
            # RECURRENT candidates
            # --------------------------------------------------------------
            rec_candidates: list[tuple[Neuron, Neuron, ConnectionType]] = []

            if recurrent:

                # DIRECT: self-loop
                if RecurrentKind.DIRECT in recurrent:
                    if n.role == NeuronRole.HIDDEN:
                        rec_candidates.append((n, n, ConnectionType.RECURRENT))

                # LATERAL: same-layer recurrent edges
                if RecurrentKind.LATERAL in recurrent:
                    full_layer = list(self.layers[layer_idx].neurons)
                    for src in full_layer:
                        if src is n:
                            continue
                        if src.role != NeuronRole.HIDDEN:
                            continue
                        rec_candidates.append((src, n, ConnectionType.RECURRENT))

                # INDIRECT: across earlier and later layers
                if RecurrentKind.INDIRECT in recurrent:

                    # From new neuron -> lower layers
                    for lower in self.layers[1:layer_idx]:
                        for dst in lower.neurons:
                            if n.role == NeuronRole.HIDDEN:
                                rec_candidates.append(
                                    (n, dst, ConnectionType.RECURRENT)
                                )

                    # From higher layers -> new neuron
                    for higher in self.layers[layer_idx + 1 :]:
                        for src in higher.neurons:
                            if src.role == NeuronRole.HIDDEN:
                                rec_candidates.append(
                                    (src, n, ConnectionType.RECURRENT)
                                )

            # --------------------------------------------------------------
            # GLOBAL CANDIDATE LIST (FF + RECURRENT)
            # --------------------------------------------------------------
            all_candidates = ff_candidates + rec_candidates

            if not all_candidates:
                continue

            forced: list[tuple[Neuron, Neuron, ConnectionType]] = []

            # --------------------------------------------------------------
            # IO enforcement
            # --------------------------------------------------------------

            if require_io_connections and max_connections > 0:
                incoming_candidates = [
                    (src, dst, ctype) for src, dst, ctype in all_candidates if dst is n
                ]

                outgoing_candidates = [
                    (src, dst, ctype) for src, dst, ctype in all_candidates if src is n
                ]

                # Enforce incoming if possible
                if incoming_candidates:
                    forced.append(random.choice(incoming_candidates))

                # Enforce outgoing if possible and budget allows
                if outgoing_candidates and len(forced) < max_connections:
                    forced.append(random.choice(outgoing_candidates))

            # --------------------------------------------------------------
            # Remaining
            # --------------------------------------------------------------

            total = len(all_candidates)
            k = int(np.ceil(total * connection_density))
            k = min(k, max_connections - len(forced))
            k = max(k, 0)

            remaining_candidates = [c for c in all_candidates if c not in forced]

            # Create forced connections
            for src, dst, ctype in forced:
                self.add_connection(src, dst, weight=weight, conn_type=ctype)

            k = min(k, len(remaining_candidates))

            if k == 0:
                continue

            sampled = random.sample(remaining_candidates, k)

            # --------------------------------------------------------------
            # Create connections
            # --------------------------------------------------------------
            for src, dst, ctype in sampled:
                self.add_connection(src, dst, weight=weight, conn_type=ctype)

        return new_neurons

    def add_connection(
        self,
        source: Neuron,
        target: Neuron,
        weight: float | None = None,
        conn_type: ConnectionType = ConnectionType.STANDARD,
        delay: int = 0,
    ) -> Connection | None:
        """
        Create a directed connection between two neurons.

        Args:
            source (Neuron): Source neuron.
            target (Neuron): Target neuron.
            weight (float | None): Initial weight. If None, 0.0 is used.
            conn_type (ConnectionType): Type of connection (e.g. standard, recurrent).
            delay (int): Delay in discrete time steps.

        Returns:
            Connection: Added Connection.
        """

        if any(c.target == target for c in source.outgoing):
            return None  # duplicate

        if weight is None:
            weight = 0.0

        # Normalize recurrent delays
        if conn_type is ConnectionType.RECURRENT and delay <= 0:
            delay = 1
        if conn_type is not ConnectionType.RECURRENT:
            delay = 0

        conn = Connection(
            source, target, weight=weight, conn_type=conn_type, delay=delay
        )
        source.outgoing.append(conn)
        target.incoming.append(conn)

        return conn

    def reset(self, full: bool = False) -> None:
        """Reset all neurons (clears input, output, and caches)."""
        for layer in self.layers:
            for neuron in layer.neurons:
                neuron.reset(full=full)

        if full:
            # Clear delayed history buffers on connections.
            for conn in self.get_all_connections():
                conn.reset_buffer()

    def calc(self, input_values: list[float]) -> list[float]:
        """
        Perform a forward pass through the network.

        Args:
            input_values (list[float]): Input vector (must match input layer size).

        Returns:
            list[float]: Output values from the last layer.

        Raises:
            AssertionError: If input size does not match input layer.
        """

        # Save LAST OUTPUT
        for layer in self.layers:
            for neuron in layer.neurons:
                neuron.last_output = neuron.output

        self.reset()

        # Set inputs
        input_layer = self.layers[0]
        assert len(input_layer.neurons) == len(input_values)
        for i, n in enumerate(input_layer.neurons):
            n.input = float(input_values[i])

        # Preload recurrent contributions from delay buffers.
        for layer in self.layers:
            for n in layer.neurons:
                for c in n.incoming:
                    if c.type is ConnectionType.RECURRENT:
                        n.input += c.weight * c.delayed_source_output()

        # Feed-forward by layers: activate first, then propagate non-recurrent edges
        for layer in self.layers:

            # Apply softmax to all neurons with activation_name == "softmax"
            softmax_neurons = [
                n for n in layer.neurons if n.activation_name == "softmax"
            ]
            if softmax_neurons:
                if len(softmax_neurons) >= 2:
                    # Normal softmax behaviour
                    totals = [n.input + n.bias for n in softmax_neurons]
                    probabilities = softmax_vec(totals)
                    for n, p in zip(softmax_neurons, probabilities):
                        n.output = float(p)
                else:
                    # Fallback: single softmax neuron acts like identity
                    n = softmax_neurons[0]
                    n.output = n.input + n.bias

            # Activate all neurons in this layer
            for n in layer.neurons:
                if n.activation_name != "softmax":
                    total = n.input + n.bias
                    n.output = n.compute_output(total)

            # Propagate to targets (exclude recurrent edges)
            for n in layer.neurons:
                for c in n.outgoing:
                    if c.type is not ConnectionType.RECURRENT:
                        c.target.input += c.weight * n.output

        # Update recurrent delay buffers once per time step.
        for c in self.get_all_connections():
            if c.type is ConnectionType.RECURRENT:
                c.push_source_output(c.source.output)

        return [n.output for n in self.layers[-1].neurons]

    def get_all_neurons(self) -> list[Neuron]:
        """Return all neurons in all layers (flattened)."""
        return [n for layer in self.layers for n in layer.neurons]

    def get_all_connections(self) -> list[Connection]:
        """Return all outgoing connections in the network."""
        return [c for n in self.get_all_neurons() for c in n.outgoing]

    def __repr__(self) -> str:
        total_neurons = sum(len(layer.neurons) for layer in self.layers)
        input_neurons = len(self.layers[0].neurons) if self.layers else 0
        output_neurons = len(self.layers[-1].neurons) if len(self.layers) > 1 else 0
        hidden_neurons = total_neurons - input_neurons - output_neurons

        total_connections = len(self.get_all_connections())

        return (
            f"<Nnet | {len(self.layers)} layers, "
            f"{total_neurons} neurons (I:{input_neurons} H:{hidden_neurons} "
            f"O:{output_neurons}), "
            f"{total_connections} connections "
        )

    def plot(
        self,
        name: str,
        engine: str = "dot",
        labels_on: bool = True,
        colors_on: bool = True,
        thickness_on: bool = False,
        fillcolors_on: bool = False,
        delay_on: bool = True,
    ) -> None:
        """
        Render a visual representation of the network using Graphviz.

        Args:
            name (str): Output file name (without extension).
            engine (str): Graphviz layout engine (e.g., 'dot', 'neato').
            labels_on (bool): Whether to show edge weights as labels.
            colors_on (bool): Whether to color edges by sign.
            thickness_on (bool): Whether to scale edge thickness by weight.
            fillcolors_on (bool): Whether to color neurons by role.
            delay_on (bool): Whether to show delay values on recurrent connections.
        """

        if not self.layers:
            print("No layers to visualize.")
            return

        dot = graphviz.Digraph(name=name, format="png", engine=engine)
        dot.graph_attr.update(
            bgcolor="white",
            rankdir="LR",
            overlap="prism",
            sep="15",
            ratio="fill",
            splines="spline",
            size="6.68,5!",
            dpi="600",
        )
        dot.node_attr.update(
            shape="circle", style="filled", fixedsize="shape", width="1.8"
        )
        dot.edge_attr.update(arrowsize="0.8")

        # Add neurons with coordinates (x = layer, y = index)
        for layer_idx, layer in enumerate(self.layers):
            for neuron_idx, neuron in enumerate(layer.neurons):
                if neuron.role.name == "INPUT":
                    fillcolor = "lightblue" if fillcolors_on else "white"
                elif neuron.role.name == "OUTPUT":
                    fillcolor = "orange" if fillcolors_on else "white"
                else:
                    fillcolor = "lightgreen" if fillcolors_on else "white"

                label = (
                    f"{neuron.label or neuron.role.name}({layer_idx})\n"
                    f"In: {neuron.input:.3f}\n"
                    f"Out: {neuron.output:.3f}\n"
                    f"LastOut: {neuron.last_output:.3f}\n"
                    f"Bias: {neuron.bias:.3f}\n"
                    f"{neuron.activation_name}"
                )

                pos = f"{layer_idx},{-neuron_idx}!"
                dot.node(
                    name=neuron.id,
                    label=label,
                    fillcolor=fillcolor,
                    pos=pos,
                )

        # Add edges
        for conn in self.get_all_connections():
            label = f"{conn.weight:.2f}" if labels_on else ""

            # Optional delay annotation (only meaningful for recurrent connections)
            if delay_on and labels_on and conn.type.name == "RECURRENT":
                if label:
                    label = f"{label}\\nd={conn.delay}"
                else:
                    label = f"d={conn.delay}"

            color = (
                "green"
                if colors_on and conn.weight >= 0
                else "red" if colors_on else "black"
            )

            penwidth = (
                str(max(1, min(5, abs(conn.weight * 5)))) if thickness_on else "1"
            )

            style = "dashed" if conn.type.name == "RECURRENT" else "solid"

            dot.edge(
                conn.source.id,
                conn.target.id,
                label=label,
                color=color,
                penwidth=penwidth,
                style=style,
            )

        dot.render(name, cleanup=True)

    def _build_index_map(self) -> dict[Neuron, tuple[int, int]]:
        """Build a mapping from neuron -> (layer_idx, neuron_idx) for O(1) lookups."""
        index_map: dict[Neuron, tuple[int, int]] = {}
        for layer_idx, layer in enumerate(self.layers):
            for neuron_idx, neuron in enumerate(layer.neurons):
                index_map[neuron] = (layer_idx, neuron_idx)
        return index_map

    def get_weights(self) -> np.ndarray:
        """
        Return all connection weights as a flat vector in a deterministic order.

        Returns:
            np.ndarray: 1D array of connection weights.

        Order key:
            (src_layer_idx, src_neuron_idx, dst_layer_idx,
            dst_neuron_idx, connection_type)
        """
        conns = self.get_all_connections()
        if not conns:
            return np.empty(0, dtype=float)

        index_map = self._build_index_map()

        def sort_key(c: Connection) -> tuple[int, int, int, int, int]:
            src_layer_idx, src_neuron_idx = index_map[c.source]
            dst_layer_idx, dst_neuron_idx = index_map[c.target]
            return (
                src_layer_idx,
                src_neuron_idx,
                dst_layer_idx,
                dst_neuron_idx,
                int(c.type.value),
            )

        conns_sorted = sorted(conns, key=sort_key)
        return np.array([c.weight for c in conns_sorted], dtype=float)

    def set_weights(self, flat: np.ndarray) -> None:
        """
        Set all connection weights from a flat vector using the same deterministic order
        as `get_weights()`.

        Args:
            flat (np.ndarray): Flat array of weights (must match number of connections).

        Raises:
            ValueError: If the length of the array does not match.
        """
        flat = np.asarray(flat, dtype=float).ravel()

        conns = self.get_all_connections()
        if not conns and flat.size == 0:
            return

        index_map = self._build_index_map()

        def sort_key(c: Connection) -> tuple[int, int, int, int, int]:
            src_layer_idx, src_neuron_idx = index_map[c.source]
            dst_layer_idx, dst_neuron_idx = index_map[c.target]
            return (
                src_layer_idx,
                src_neuron_idx,
                dst_layer_idx,
                dst_neuron_idx,
                int(c.type.value),
            )

        conns_sorted = sorted(conns, key=sort_key)

        if flat.size != len(conns_sorted):
            raise ValueError(
                f"Length mismatch for weights: expected {len(conns_sorted)}, "
                f"got {flat.size}."
            )

        for weight_value, conn in zip(flat, conns_sorted):
            conn.weight = float(weight_value)

    def get_biases(self) -> np.ndarray:
        """
        Return all trainable biases as a flat vector (excluding input neurons).

        Returns:
            np.ndarray: Bias vector.

        Order:
            (layer_index, neuron_index) over all non-input neurons.
        """

        if not self.layers:
            return np.empty(0, dtype=float)

        biases: list[float] = []
        for _, layer in enumerate(self.layers):
            for _, neuron in enumerate(layer.neurons):
                if neuron.role is not NeuronRole.INPUT:
                    biases.append(neuron.bias)
        return np.asarray(biases, dtype=float)

    def set_biases(self, flat: np.ndarray) -> None:
        """
        Set all neuron biases (excluding input neurons) from a flat vector using the
        same ordering as `get_biases()`.

        Args:
            flat (np.ndarray): Bias values in deterministic order.

        Raises:
            ValueError: If length does not match the number of biases.
        """
        flat = np.asarray(flat, dtype=float).ravel()

        # Collect non-input neurons in deterministic order
        targets: list[Neuron] = []
        for _, layer in enumerate(self.layers):
            for _, neuron in enumerate(layer.neurons):
                if neuron.role is not NeuronRole.INPUT:
                    targets.append(neuron)

        if flat.size != len(targets):
            raise ValueError(
                f"Length mismatch for biases: expected {len(targets)}, got {flat.size}."
            )

        for b, n in zip(flat, targets):
            n.bias = float(b)

    def save(self, path: str) -> None:
        """
        Save this network to a file.

        The file format is chosen automatically based on the extension:
        - .yaml / .yml --> YAML (human-readable, recommended)
        - .json        --> JSON (machine-friendly)

        Args:
            path (str): Output file path.
        """

        from . import serialization  # local import to avoid circular import

        suffix = Path(path).suffix.lower()
        if suffix in (".yaml", ".yml"):
            serialization.save_yaml(self, path)
        elif suffix == ".json":
            serialization.save_json(self, path)
        else:
            raise ValueError(
                f"Unsupported file extension '{suffix}'. Use .yaml, .yml or .json"
            )

    @classmethod
    def load(cls, path: str) -> "Nnet":
        """
        Load a network from a file.

        The file format is chosen automatically based on the extension:
        - .yaml / .yml --> YAML
        - .json        --> JSON

        Args:
            path (str): Path to the serialized network file.

        Returns:
            Nnet: The reconstructed network.
        """

        from . import serialization  # local import to avoid circular import

        suffix = Path(path).suffix.lower()
        if suffix in (".yaml", ".yml"):
            return serialization.load_yaml(path)
        elif suffix == ".json":
            return serialization.load_json(path)
        else:
            raise ValueError(
                f"Unsupported file extension '{suffix}'. Use .yaml, .yml or .json"
            )
