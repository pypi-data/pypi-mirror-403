import asyncio
from typing import List, Dict, Any
from ..config.config import Config
from ..utils.llm import create_chat_completion
from ..utils.logger import get_formatted_logger
from ..prompts import PromptFamily, get_prompt_by_report_type
from ..utils.enum import Tone

logger = get_formatted_logger()


async def write_report_introduction(
    query: str,
    context: str,
    agent_role_prompt: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> str:
    """
    Generate an introduction for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        str: The generated introduction.
    """
    try:
        introduction = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {"role": "user", "content": prompt_family.generate_report_introduction(
                    question=query,
                    research_summary=context,
                    language=config.language
                )},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return introduction
    except Exception as e:
        logger.error(f"Error in generating report introduction: {e}")
    return ""


async def write_conclusion(
    query: str,
    context: str,
    agent_role_prompt: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> str:
    """
    Write a conclusion for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        str: The generated conclusion.
    """
    try:
        conclusion = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {
                    "role": "user",
                    "content": prompt_family.generate_report_conclusion(query=query,
                                                                        report_content=context,
                                                                        language=config.language),
                },
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return conclusion
    except Exception as e:
        logger.error(f"Error in writing conclusion: {e}")
    return ""


async def summarize_url(
    url: str,
    content: str,
    role: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    **kwargs
) -> str:
    """
    Summarize the content of a URL.

    Args:
        url (str): The URL to summarize.
        content (str): The content of the URL.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.

    Returns:
        str: The summarized content.
    """
    try:
        summary = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{role}"},
                {"role": "user", "content": f"Summarize the following content from {url}:\n\n{content}"},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return summary
    except Exception as e:
        logger.error(f"Error in summarizing URL: {e}")
    return ""


async def generate_draft_section_titles(
    query: str,
    current_subtopic: str,
    context: str,
    role: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> List[str]:
    """
    Generate draft section titles for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        List[str]: A list of generated section titles.
    """
    try:
        section_titles = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{role}"},
                {"role": "user", "content": prompt_family.generate_draft_titles_prompt(
                    current_subtopic, query, context)},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=None,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return section_titles.split("\n")
    except Exception as e:
        logger.error(f"Error in generating draft section titles: {e}")
    return []


async def generate_report(
    query: str,
    context,
    agent_role_prompt: str,
    report_type: str,
    tone: Tone,
    report_source: str,
    websocket,
    cfg,
    main_topic: str = "",
    existing_headers: list = [],
    relevant_written_contents: list = [],
    cost_callback: callable = None,
    custom_prompt: str = "", # This can be any prompt the user chooses with the context
    headers=None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
):
    """
    generates the final report
    Args:
        query:
        context:
        agent_role_prompt:
        report_type:
        websocket:
        tone:
        cfg:
        main_topic:
        existing_headers:
        relevant_written_contents:
        cost_callback:
        prompt_family: Family of prompts

    Returns:
        report:

    """
    import time

    start_time = time.time()
    logger.info(f"========== generate_report å¼€å§‹ ==========")
    logger.info(f"ğŸ“‹ æŠ¥å‘Šé…ç½®:")
    logger.info(f"   - æŠ¥å‘Šç±»å‹: {report_type}")
    logger.info(f"   - ç›®æ ‡å­—æ•°: {cfg.total_words}")
    logger.info(f"   - è¯­æ°”: {tone}")
    logger.info(f"   - Contexté•¿åº¦: {len(str(context))} å­—ç¬¦")

    # Step 1: ç”Ÿæˆ prompt
    prompt_start = time.time()
    generate_prompt = get_prompt_by_report_type(report_type, prompt_family)
    report = ""

    if report_type == "subtopic_report":
        content = f"{generate_prompt(query, existing_headers, relevant_written_contents, main_topic, context, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}"
    elif custom_prompt:
        content = f"{custom_prompt}\n\nContext: {context}"
    else:
        content = f"{generate_prompt(query, context, report_source, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}"

    prompt_duration = time.time() - prompt_start
    content_length = len(content)
    logger.info(f"âœ… æ­¥éª¤1: Promptç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {prompt_duration:.2f}ç§’")
    logger.info(f"   - Prompté•¿åº¦: {content_length} å­—ç¬¦")

    # Step 2: è°ƒç”¨ LLM
    logger.info(f"ğŸ¤– æ­¥éª¤2: å¼€å§‹è°ƒç”¨ LLM (create_chat_completion)...")
    logger.info(f"   - æ¨¡å‹: {cfg.smart_llm_model}")
    logger.info(f"   - Provider: {cfg.smart_llm_provider}")
    logger.info(f"   - Max Tokens: {cfg.smart_token_limit}")
    logger.info(f"   - Temperature: 0.35")

    llm_start = time.time()
    try:
        report = await create_chat_completion(
            model=cfg.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {"role": "user", "content": content},
            ],
            temperature=0.35,
            llm_provider=cfg.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=cfg.smart_token_limit,
            llm_kwargs=cfg.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        llm_duration = time.time() - llm_start
        logger.info(f"âœ… æ­¥éª¤2: LLMè°ƒç”¨æˆåŠŸï¼Œè€—æ—¶: {llm_duration:.2f}ç§’ ({llm_duration/60:.2f}åˆ†é’Ÿ)")
        logger.info(f"   - ç”Ÿæˆå†…å®¹é•¿åº¦: {len(report)} å­—ç¬¦")
        logger.info(f"   - ç”Ÿæˆé€Ÿåº¦: {len(report)/llm_duration:.1f} å­—ç¬¦/ç§’")
    except:
        logger.warning(f"âš ï¸ ç¬¬ä¸€æ¬¡LLMè°ƒç”¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        try:
            fallback_start = time.time()
            report = await create_chat_completion(
                model=cfg.smart_llm_model,
                messages=[
                    {"role": "user", "content": f"{agent_role_prompt}\n\n{content}"},
                ],
                temperature=0.35,
                llm_provider=cfg.smart_llm_provider,
                stream=True,
                websocket=websocket,
                max_tokens=cfg.smart_token_limit,
                llm_kwargs=cfg.llm_kwargs,
                cost_callback=cost_callback,
                **kwargs
            )
            fallback_duration = time.time() - fallback_start
            llm_duration = time.time() - llm_start
            logger.info(f"âœ… æ­¥éª¤2: LLMå¤‡ç”¨æ–¹æ¡ˆæˆåŠŸï¼Œè€—æ—¶: {fallback_duration:.2f}ç§’")
            logger.info(f"   - æ€»LLMè€—æ—¶ï¼ˆå«é‡è¯•ï¼‰: {llm_duration:.2f}ç§’")
        except Exception as e:
            llm_duration = time.time() - llm_start
            logger.error(f"âŒ æ­¥éª¤2: LLMè°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶: {llm_duration:.2f}ç§’")
            logger.error(f"Error in generate_report: {e}")

    total_duration = time.time() - start_time
    logger.info(f"========== generate_report å®Œæˆ ==========")
    logger.info(f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ ({total_duration/60:.2f}åˆ†é’Ÿ)")
    logger.info(f"  - Promptç”Ÿæˆ: {prompt_duration:.2f}ç§’ ({prompt_duration/total_duration*100:.1f}%)")
    logger.info(f"  - LLMè°ƒç”¨: {llm_duration:.2f}ç§’ ({llm_duration/total_duration*100:.1f}%) âš ï¸")

    return report
