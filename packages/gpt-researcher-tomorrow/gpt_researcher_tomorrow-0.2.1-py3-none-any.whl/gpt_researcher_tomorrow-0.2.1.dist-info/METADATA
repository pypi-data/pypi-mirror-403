Metadata-Version: 2.4
Name: gpt-researcher-tomorrow
Version: 0.2.1
Summary: GPT Researcher Tomorrow - A fork of GPT Researcher, an autonomous agent for comprehensive online research on a variety of tasks.
License: MIT
License-File: LICENSE
Author: maowei
Author-email: kcevxqfchswn@gmail.com
Requires-Python: >=3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Provides-Extra: requirements-txt
Requires-Dist: aiofiles (>=23.2.1)
Requires-Dist: aiohappyeyeballs (>=2.6.1)
Requires-Dist: aiohttp (>=3.12.0)
Requires-Dist: aiosignal (>=1.3.2)
Requires-Dist: annotated-types (>=0.7.0)
Requires-Dist: anyio (>=4.9.0)
Requires-Dist: arxiv (>=2.0.0)
Requires-Dist: arxiv_client ; extra == "requirements-txt"
Requires-Dist: attrs (>=25.3.0)
Requires-Dist: azure-storage-blob ; extra == "requirements-txt"
Requires-Dist: backoff (>=2.2.1)
Requires-Dist: beautifulsoup4 (>=4.12.2)
Requires-Dist: brotli (>=1.1.0)
Requires-Dist: certifi (>=2025.4.26)
Requires-Dist: cffi (>=1.17.1)
Requires-Dist: chardet (>=5.2.0)
Requires-Dist: charset-normalizer (>=3.4.2)
Requires-Dist: click (>=8.1.0)
Requires-Dist: colorama (>=0.4.6)
Requires-Dist: cryptography (>=45.0.2)
Requires-Dist: cssselect2 (>=0.8.0)
Requires-Dist: dataclasses-json (>=0.6.7)
Requires-Dist: distro (>=1.9.0)
Requires-Dist: docopt (>=0.6.2)
Requires-Dist: duckduckgo-search (>=4.1.1)
Requires-Dist: duckduckgo_search ; extra == "requirements-txt"
Requires-Dist: emoji (>=2.14.1)
Requires-Dist: exa_py ; extra == "requirements-txt"
Requires-Dist: fastapi (>=0.104.1)
Requires-Dist: feedparser (>=6.0.11)
Requires-Dist: filelock (>=3.18.0)
Requires-Dist: filetype (>=1.2.0)
Requires-Dist: firecrawl ; extra == "requirements-txt"
Requires-Dist: fonttools (>=4.58.0)
Requires-Dist: frozenlist (>=1.6.0)
Requires-Dist: fsspec (>=2025.5.1)
Requires-Dist: greenlet (>=3.2.2)
Requires-Dist: h11 (>=0.16.0)
Requires-Dist: html5lib (>=1.1)
Requires-Dist: htmldocx (>=0.0.6)
Requires-Dist: httpcore (>=1.0.9)
Requires-Dist: httpx (>=0.28.1)
Requires-Dist: httpx-aiohttp (>=0.1.4)
Requires-Dist: httpx-sse (>=0.4.0)
Requires-Dist: huggingface-hub (>=0.32.0)
Requires-Dist: idna (>=3.10)
Requires-Dist: importlib-metadata (>=8.7.0)
Requires-Dist: jinja2 (>=3.1.6)
Requires-Dist: jiter (>=0.10.0)
Requires-Dist: joblib (>=1.5.1)
Requires-Dist: json-repair (>=0.44.0)
Requires-Dist: json5 (>=0.12.0)
Requires-Dist: jsonpatch (>=1.33)
Requires-Dist: jsonpointer (>=3.0.0)
Requires-Dist: jsonschema (>=4.23.0)
Requires-Dist: jsonschema-specifications (>=2025.4.1)
Requires-Dist: kiwisolver (>=1.4.5)
Requires-Dist: langchain (>=1.0.0)
Requires-Dist: langchain-anthropic ; extra == "requirements-txt"
Requires-Dist: langchain-classic (>=1.0.0)
Requires-Dist: langchain-cohere ; extra == "requirements-txt"
Requires-Dist: langchain-community (>=0.4.0)
Requires-Dist: langchain-core (>=1.0.0)
Requires-Dist: langchain-dashscope ; extra == "requirements-txt"
Requires-Dist: langchain-fireworks ; extra == "requirements-txt"
Requires-Dist: langchain-gigachat ; extra == "requirements-txt"
Requires-Dist: langchain-google-genai ; extra == "requirements-txt"
Requires-Dist: langchain-google-vertexai ; extra == "requirements-txt"
Requires-Dist: langchain-groq ; extra == "requirements-txt"
Requires-Dist: langchain-huggingface ; extra == "requirements-txt"
Requires-Dist: langchain-mistralai ; extra == "requirements-txt"
Requires-Dist: langchain-ollama (>=1.0.0)
Requires-Dist: langchain-openai (>=1.0.0)
Requires-Dist: langchain-text-splitters (>=1.0.0)
Requires-Dist: langchain-together ; extra == "requirements-txt"
Requires-Dist: langchain-xai ; extra == "requirements-txt"
Requires-Dist: langdetect (>=1.0.9)
Requires-Dist: langgraph (>=0.2.76)
Requires-Dist: langgraph-checkpoint (>=2.0.26)
Requires-Dist: langgraph-cli (>=0.2.10)
Requires-Dist: langgraph-sdk (>=0.1.70)
Requires-Dist: langsmith (>=0.3.42)
Requires-Dist: litellm (>=1.71.0)
Requires-Dist: loguru (>=0.7.3)
Requires-Dist: lxml (>=5.4.0)
Requires-Dist: markdown (>=3.8)
Requires-Dist: markdown2 (>=2.5.3)
Requires-Dist: markupsafe (>=3.0.2)
Requires-Dist: marshmallow (>=3.26.1)
Requires-Dist: mcp (>=1.9.1)
Requires-Dist: md2pdf (>=1.0.1)
Requires-Dist: mistune (>=3.1.3)
Requires-Dist: multidict (>=6.4.4)
Requires-Dist: mypy-extensions (>=1.1.0)
Requires-Dist: nest-asyncio (>=1.6.0)
Requires-Dist: nltk (>=3.9.1)
Requires-Dist: numpy (>=2.0.0,<2.3.0)
Requires-Dist: olefile (>=0.47)
Requires-Dist: ollama (>=0.4.8)
Requires-Dist: openai (>=1.82.0)
Requires-Dist: orjson (>=3.10.18)
Requires-Dist: ormsgpack (>=1.10.0)
Requires-Dist: packaging (>=24.2)
Requires-Dist: pillow (>=11.2.1)
Requires-Dist: playwright ; extra == "requirements-txt"
Requires-Dist: primp (>=0.15.0)
Requires-Dist: propcache (>=0.3.1)
Requires-Dist: psutil (>=6.0.0)
Requires-Dist: pycparser (>=2.22)
Requires-Dist: pydantic (>=2.11.5)
Requires-Dist: pydantic-core (>=2.33.2)
Requires-Dist: pydantic-settings (>=2.9.1)
Requires-Dist: pydyf (>=0.11.0)
Requires-Dist: pymupdf (>=1.26.0)
Requires-Dist: pypdf (>=5.5.0)
Requires-Dist: pyphen (>=0.17.2)
Requires-Dist: python-docx (>=1.1.2)
Requires-Dist: python-dotenv (>=1.1.0)
Requires-Dist: python-iso639 (>=2025.2.18)
Requires-Dist: python-magic (>=0.4.27)
Requires-Dist: python-multipart (>=0.0.20)
Requires-Dist: python-oxmsg (>=0.0.2)
Requires-Dist: pyyaml (>=6.0.2)
Requires-Dist: rapidfuzz (>=3.13.0)
Requires-Dist: referencing (>=0.36.2)
Requires-Dist: regex (>=2024.11.6)
Requires-Dist: requests (>=2.32.3)
Requires-Dist: requests-toolbelt (>=1.0.0)
Requires-Dist: rpds-py (>=0.25.1)
Requires-Dist: scrapy ; extra == "requirements-txt"
Requires-Dist: selenium ; extra == "requirements-txt"
Requires-Dist: sgmllib3k (>=1.0.0)
Requires-Dist: six (>=1.17.0)
Requires-Dist: sniffio (>=1.3.1)
Requires-Dist: soupsieve (>=2.7)
Requires-Dist: sqlalchemy (>=2.0.41)
Requires-Dist: sse-starlette (>=2.3.5)
Requires-Dist: starlette (>=0.46.2)
Requires-Dist: tenacity (>=9.1.2)
Requires-Dist: tiktoken (>=0.9.0)
Requires-Dist: tinycss2 (>=1.4.0)
Requires-Dist: tinyhtml5 (>=2.0.0)
Requires-Dist: tokenizers (>=0.21.1)
Requires-Dist: tqdm (>=4.67.1)
Requires-Dist: typing-extensions (>=4.13.2)
Requires-Dist: typing-inspect (>=0.9.0)
Requires-Dist: typing-inspection (>=0.4.1)
Requires-Dist: unstructured (>=0.17.2)
Requires-Dist: unstructured-client (>=0.35.0)
Requires-Dist: urllib3 (>=2.4.0)
Requires-Dist: uvicorn (>=0.34.2)
Requires-Dist: weasyprint (>=65.1) ; sys_platform != "win32"
Requires-Dist: webencodings (>=0.5.1)
Requires-Dist: websockets (>=15.0.1)
Requires-Dist: win32-setctime (>=1.2.0)
Requires-Dist: wrapt (>=1.17.2)
Requires-Dist: yarl (>=1.20.0)
Requires-Dist: zipp (>=3.21.0)
Requires-Dist: zopfli (>=0.2.3.post1)
Requires-Dist: zstandard (>=0.23.0)
Description-Content-Type: text/markdown

<div align="center" id="top">

<img src="https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3" alt="Logo" width="80">

####

[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord](https://img.shields.io/discord/1127851779011391548?logo=discord&logoColor=white&label=Discord&color=34b76a&style=for-the-badge)](https://discord.gg/QgZXvJAccX)


[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)
![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)
[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)
[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)
[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)

[English](README.md) | [‰∏≠Êñá](README-zh_CN.md) | [Êó•Êú¨Ë™û](README-ja_JP.md) | [ÌïúÍµ≠Ïñ¥](README-ko_KR.md)

</div>

# üîé GPT Researcher Tomorrow

> **Note**: This is a fork of the original [GPT Researcher](https://github.com/assafelovic/gpt-researcher) project by [Assaf Elovic](https://github.com/assafelovic). All credit for the original work goes to the original author and contributors.

---

**GPT Researcher is an open deep research agent designed for both web and local research on any given task.** 

The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.

**Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.**

## Why GPT Researcher?

- Objective conclusions for manual research can take weeks, requiring vast resources and time.
- LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.
- Current LLMs have token limitations, insufficient for generating long research reports.
- Limited web sources in existing services lead to misinformation and shallow results.
- Selective web sources can introduce bias into research tasks.

## Demo
<a href="https://www.youtube.com/watch?v=f60rlc_QCxE" target="_blank" rel="noopener">
  <img src="https://github.com/user-attachments/assets/ac2ec55f-b487-4b3f-ae6f-b8743ad296e4" alt="Demo video" width="800" target="_blank" />
</a>


## Architecture

The core idea is to utilize 'planner' and 'execution' agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.

<div align="center">
<img align="center" height="600" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527">
</div>

Steps:
* Create a task-specific agent based on a research query.
* Generate questions that collectively form an objective opinion on the task.
* Use a crawler agent for gathering information for each question.
* Summarize and source-track each resource.
* Filter and aggregate summaries into a final research report.

## Tutorials
 - [How it Works](https://docs.gptr.dev/blog/building-gpt-researcher)
 - [How to Install](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)
 - [Live Demo](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)

## Features

- üìù Generate detailed research reports using web and local documents.
- üñºÔ∏è Smart image scraping and filtering for reports.
- üìú Generate detailed reports exceeding 2,000 words.
- üåê Aggregate over 20 sources for objective conclusions.
- üñ•Ô∏è Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.
- üîç JavaScript-enabled web scraping.
- üìÇ Maintains memory and context throughout research.
- üìÑ Export reports to PDF, Word, and other formats.

## üìñ Documentation

See the [Documentation](https://docs.gptr.dev/docs/gpt-researcher/getting-started) for:
- Installation and setup guides
- Configuration and customization options
- How-To examples
- Full API references

## ‚öôÔ∏è Getting Started

### Installation

1. Install Python 3.11 or later. [Guide](https://www.tutorialsteacher.com/python/install-python).
2. Clone the project and navigate to the directory:

    ```bash
    git clone https://github.com/assafelovic/gpt-researcher.git
    cd gpt-researcher
    ```

3. Set up API keys by exporting them or storing them in a `.env` file.

    ```bash
    export OPENAI_API_KEY={Your OpenAI API Key here}
    export TAVILY_API_KEY={Your Tavily API Key here}
    ```

    For custom OpenAI-compatible APIs (e.g., local models, other providers), you can also set:
    
    ```bash
    export OPENAI_BASE_URL={Your custom API base URL here}
    ```

4. Install dependencies and start the server:

    ```bash
    pip install -r requirements.txt
    python -m uvicorn main:app --reload
    ```

Visit [http://localhost:8000](http://localhost:8000) to start.

For other setups (e.g., Poetry or virtual environments), check the [Getting Started page](https://docs.gptr.dev/docs/gpt-researcher/getting-started).

## Run as PIP package
```bash
pip install gpt-researcher-tomorrow

```
### Example Usage:
```python
...
from gpt_researcher import GPTResearcher

query = "why is Nvidia stock going up?"
researcher = GPTResearcher(query=query)
# Conduct research on the given query
research_result = await researcher.conduct_research()
# Write the report
report = await researcher.write_report()
...
```

**For more examples and configurations, please refer to the [PIP documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package) page.**

### üîß MCP Client
GPT Researcher supports MCP integration to connect with specialized data sources like GitHub repositories, databases, and custom APIs. This enables research from data sources alongside web search.

```bash
export RETRIEVER=tavily,mcp  # Enable hybrid web + MCP research
```

```python
from gpt_researcher import GPTResearcher
import asyncio
import os

async def mcp_research_example():
    # Enable MCP with web search
    os.environ["RETRIEVER"] = "tavily,mcp"
    
    researcher = GPTResearcher(
        query="What are the top open source web research agents?",
        mcp_configs=[
            {
                "name": "github",
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")}
            }
        ]
    )
    
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    return report
```

> For comprehensive MCP documentation and advanced examples, visit the [MCP Integration Guide](https://docs.gptr.dev/docs/gpt-researcher/retrievers/mcp-configs).

## ‚ú® Deep Research

GPT Researcher now includes Deep Research - an advanced recursive research workflow that explores topics with agentic depth and breadth. This feature employs a tree-like exploration pattern, diving deeper into subtopics while maintaining a comprehensive view of the research subject.

- üå≥ Tree-like exploration with configurable depth and breadth
- ‚ö°Ô∏è Concurrent processing for faster results
- ü§ù Smart context management across research branches
- ‚è±Ô∏è Takes ~5 minutes per deep research
- üí∞ Costs ~$0.4 per research (using `o3-mini` on "high" reasoning effort)

[Learn more about Deep Research](https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research) in our documentation.

## Run with Docker

> **Step 1** - [Install Docker](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)

> **Step 2** - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'

> **Step 3** - Within the docker-compose file comment out services that you don't want to run with Docker.

```bash
docker-compose up --build
```

If that doesn't work, try running it without the dash:
```bash
docker compose up --build
```

> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:
 - the Python server running on localhost:8000<br>
 - the React app running on localhost:3000<br>

Visit localhost:3000 on any browser and enjoy researching!


## üìÑ Research on Local Documents

You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.

Step 1: Add the env variable `DOC_PATH` pointing to the folder where your documents are located.

```bash
export DOC_PATH="./my-docs"
```

Step 2: 
 - If you're running the frontend app on localhost:8000, simply select "My Documents" from the "Report Source" Dropdown Options.
 - If you're running GPT Researcher with the [PIP package](https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package), pass the `report_source` argument as "local" when you instantiate the `GPTResearcher` class [code sample here](https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research).


## ü§ñ MCP Server

We've moved our MCP server to a dedicated repository: [gptr-mcp](https://github.com/assafelovic/gptr-mcp).

The GPT Researcher MCP Server enables AI applications like Claude to conduct deep research. While LLM apps can access web search tools with MCP, GPT Researcher MCP delivers deeper, more reliable research results.

Features:
- Deep research capabilities for AI assistants
- Higher quality information with optimized context usage
- Comprehensive results with better reasoning for LLMs
- Claude Desktop integration

For detailed installation and usage instructions, please visit the [official repository](https://github.com/assafelovic/gptr-mcp).


## üë™ Multi-Agent Assistant
As AI evolves from prompt engineering and RAG to multi-agent systems, we're excited to introduce our new multi-agent assistant built with [LangGraph](https://python.langchain.com/v0.1/docs/langgraph/).

By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent [STORM](https://arxiv.org/abs/2402.14207) paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.

An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.

Check it out [here](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents) or head over to our [documentation](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph) for more information.

## üñ•Ô∏è Frontend Applications

GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:

- An intuitive interface for inputting research queries
- Real-time progress tracking of research tasks
- Interactive display of research findings
- Customizable settings for tailored research experiences

Two deployment options are available:
1. A lightweight static frontend served by FastAPI
2. A feature-rich NextJS application for advanced functionality

For detailed setup instructions and more information about the frontend features, please visit our [documentation page](https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction).

## üöÄ Contributing
We highly welcome contributions! Please check out [contributing](https://github.com/assafelovic/gpt-researcher/blob/master/CONTRIBUTING.md) if you're interested.

Please check out our [roadmap](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) page and reach out to us via our [Discord community](https://discord.gg/QgZXvJAccX) if you're interested in joining our mission.
<a href="https://github.com/assafelovic/gpt-researcher/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=assafelovic/gpt-researcher" />
</a>
## ‚úâÔ∏è Support / Contact us
- [Community Discord](https://discord.gg/spBgZmm3Xe)
- Author Email: assaf.elovic@gmail.com

## üõ° Disclaimer

This project, GPT Researcher, is an experimental application and is provided "as-is" without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.

Our view on unbiased research claims:
1. The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.
2. We do not aim to eliminate biases; we aim to reduce it as much as possible. **We are here as a community to figure out the most effective human/llm interactions.**
3. In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.

---

<p align="center">
<a href="https://star-history.com/#assafelovic/gpt-researcher">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date" />
  </picture>
</a>
</p>


<p align="right">
  <a href="#top">‚¨ÜÔ∏è Back to Top</a>
</p>

