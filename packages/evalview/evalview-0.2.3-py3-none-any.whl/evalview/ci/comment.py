"""PR comment generation for CI integration.

Generates clean, scannable PR comments with:
- Overall status (PASSED / REGRESSION / TOOLS_CHANGED / OUTPUT_CHANGED)
- Summary metrics (tests, pass rate, cost, latency)
- Top changes when using diff mode
"""

import json
import os
import subprocess
from pathlib import Path
from typing import Optional, List, Dict, Any

# Signature used to identify EvalView comments for updates
COMMENT_SIGNATURE = "*Generated by [EvalView](https://github.com/hidai25/eval-view)*"

# Status display configuration
STATUS_CONFIG = {
    "passed": {"emoji": "\u2705", "label": "PASSED"},           # ✅
    "regression": {"emoji": "\u274c", "label": "REGRESSION"},   # ❌
    "tools_changed": {"emoji": "\u26a0\ufe0f", "label": "TOOLS CHANGED"},  # ⚠️
    "output_changed": {"emoji": "\u26a0\ufe0f", "label": "OUTPUT CHANGED"},  # ⚠️
}

DEFAULT_STATUS = {"emoji": "\u2753", "label": "UNKNOWN"}  # ❓


def load_latest_results(results_dir: str = ".evalview/results") -> Optional[Dict[str, Any]]:
    """Load the most recent results file.

    Args:
        results_dir: Directory containing result JSON files

    Returns:
        Parsed JSON data or None if no results found
    """
    results_path = Path(results_dir)
    if not results_path.exists():
        return None

    json_files = sorted(
        results_path.glob("*.json"),
        key=lambda x: x.stat().st_mtime,
        reverse=True
    )
    if not json_files:
        return None

    try:
        with open(json_files[0], encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return None


def get_status_display(status: str) -> Dict[str, str]:
    """Get display configuration for a status."""
    return STATUS_CONFIG.get(status.lower(), DEFAULT_STATUS)


def format_cost(cost: float) -> str:
    """Format cost value for display.

    Args:
        cost: Cost in dollars

    Returns:
        Formatted string like "$0.00" or "$0.0023"
    """
    if cost == 0:
        return "$0.00"
    elif cost < 0.01:
        return f"${cost:.4f}"
    return f"${cost:.2f}"


def format_latency(ms: float) -> str:
    """Format latency in human-readable form.

    Args:
        ms: Latency in milliseconds

    Returns:
        Formatted string like "250ms" or "1.5s"
    """
    if ms < 1000:
        return f"{ms:.0f}ms"
    return f"{ms/1000:.1f}s"


def _extract_pr_number() -> Optional[int]:
    """Extract PR number from GitHub Actions environment.

    Returns:
        PR number or None if not in PR context
    """
    github_ref = os.environ.get("GITHUB_REF", "")
    # Format: refs/pull/123/merge
    if "/pull/" not in github_ref:
        return None

    try:
        return int(github_ref.split("/pull/")[1].split("/")[0])
    except (IndexError, ValueError):
        return None


def _get_run_url() -> Optional[str]:
    """Get URL to the current GitHub Actions run."""
    server = os.environ.get("GITHUB_SERVER_URL", "https://github.com")
    repo = os.environ.get("GITHUB_REPOSITORY", "")
    run_id = os.environ.get("GITHUB_RUN_ID", "")

    if repo and run_id:
        return f"{server}/{repo}/actions/runs/{run_id}"
    return None


def _determine_overall_status(
    results: List[Dict[str, Any]],
    diff_results: Optional[List[Dict[str, Any]]] = None
) -> str:
    """Determine overall status from results.

    Args:
        results: List of evaluation results
        diff_results: Optional list of diff results

    Returns:
        Status string: "passed", "regression", "tools_changed", or "output_changed"
    """
    if diff_results:
        statuses = [d.get("overall_severity", "passed") for d in diff_results]
        # Priority: regression > tools_changed > output_changed > passed
        if "regression" in statuses:
            return "regression"
        if "tools_changed" in statuses:
            return "tools_changed"
        if "output_changed" in statuses:
            return "output_changed"
        return "passed"

    # No diff mode - check pass/fail
    failed = sum(1 for r in results if not r.get("passed", False))
    return "passed" if failed == 0 else "regression"


def _build_summary_table(results: List[Dict[str, Any]]) -> List[str]:
    """Build the summary metrics table.

    Args:
        results: List of evaluation results

    Returns:
        List of markdown lines for the table
    """
    total = len(results)
    passed = sum(1 for r in results if r.get("passed", False))
    pass_rate = (passed / total * 100) if total > 0 else 0

    # Safely extract metrics with defaults
    total_cost = sum(
        r.get("trace", {}).get("metrics", {}).get("total_cost", 0)
        for r in results
    )
    total_latency = sum(
        r.get("trace", {}).get("metrics", {}).get("total_latency", 0)
        for r in results
    )
    avg_score = sum(r.get("score", 0) for r in results) / total if total > 0 else 0

    return [
        "| Metric | Value |",
        "|--------|-------|",
        f"| Tests | {passed}/{total} passed ({pass_rate:.0f}%) |",
        f"| Avg Score | {avg_score:.1f}/100 |",
        f"| Total Cost | {format_cost(total_cost)} |",
        f"| Total Latency | {format_latency(total_latency)} |",
    ]


def _build_changes_section(diff_results: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
    """Build the changes from baseline section.

    Args:
        diff_results: List of diff results
        max_items: Maximum number of changes to show

    Returns:
        List of markdown lines
    """
    changes = [d for d in diff_results if d.get("has_differences", False)]
    if not changes:
        return []

    lines = ["### Changes from Baseline", ""]

    for diff in changes[:max_items]:
        test_name = diff.get("test_name", "Unknown")
        status = diff.get("overall_severity", "passed")
        status_display = get_status_display(status)

        summary_parts = []

        # Score change
        score_diff = diff.get("score_diff", 0)
        if abs(score_diff) > 1:
            sign = "+" if score_diff > 0 else ""
            summary_parts.append(f"score {sign}{score_diff:.1f}")

        # Tool changes
        tool_diffs = diff.get("tool_diffs", [])
        if tool_diffs:
            summary_parts.append(f"{len(tool_diffs)} tool change(s)")

        # Latency change (only if significant: >100ms)
        latency_diff = diff.get("latency_diff", 0)
        if abs(latency_diff) > 100:
            sign = "+" if latency_diff > 0 else ""
            summary_parts.append(f"latency {sign}{latency_diff:.0f}ms")

        summary = ", ".join(summary_parts) if summary_parts else "minor changes"
        lines.append(f"- {status_display['emoji']} **{test_name}**: {summary}")

    remaining = len(changes) - max_items
    if remaining > 0:
        lines.append(f"- ... and {remaining} more")

    lines.append("")
    return lines


def _build_failed_tests_section(results: List[Dict[str, Any]], max_items: int = 5) -> List[str]:
    """Build the failed tests section.

    Args:
        results: List of evaluation results
        max_items: Maximum number of failures to show

    Returns:
        List of markdown lines
    """
    failed_tests = [r for r in results if not r.get("passed", False)]
    if not failed_tests:
        return []

    lines = ["### Failed Tests", ""]

    for r in failed_tests[:max_items]:
        name = r.get("test_case", "Unknown")
        score = r.get("score", 0)
        min_score = r.get("min_score", 70)
        lines.append(f"- \u274c **{name}**: score {score:.1f} (min: {min_score})")

    remaining = len(failed_tests) - max_items
    if remaining > 0:
        lines.append(f"- ... and {remaining} more")

    lines.append("")
    return lines


def generate_pr_comment(
    results: List[Dict[str, Any]],
    diff_results: Optional[List[Dict[str, Any]]] = None,
    run_url: Optional[str] = None,
) -> str:
    """Generate markdown PR comment from results.

    Args:
        results: List of EvaluationResult dicts
        diff_results: Optional list of TraceDiff dicts (from --diff mode)
        run_url: Optional link to the GitHub Actions run

    Returns:
        Markdown string for PR comment
    """
    if not results:
        return "## EvalView Results\n\nNo test results found."

    # Determine overall status
    overall_status = _determine_overall_status(results, diff_results)
    status_display = get_status_display(overall_status)

    # Build comment sections
    lines = []

    # Header
    lines.append(f"## {status_display['emoji']} EvalView: {status_display['label']}")
    lines.append("")

    # Summary table
    lines.extend(_build_summary_table(results))
    lines.append("")

    # Changes section (diff mode) or failed tests section
    if diff_results:
        lines.extend(_build_changes_section(diff_results))
    else:
        lines.extend(_build_failed_tests_section(results))

    # Footer
    if run_url:
        lines.append(f"[View full report]({run_url})")
        lines.append("")

    lines.append("---")
    lines.append(COMMENT_SIGNATURE)

    return "\n".join(lines)


def post_pr_comment(comment: str, pr_number: Optional[int] = None) -> bool:
    """Post comment to PR using gh CLI.

    Args:
        comment: Markdown comment to post
        pr_number: PR number (auto-detected from GITHUB_REF if not provided)

    Returns:
        True if comment was posted successfully
    """
    if pr_number is None:
        pr_number = _extract_pr_number()
        if pr_number is None:
            return False

    # Check if gh CLI is available
    try:
        subprocess.run(
            ["gh", "--version"],
            capture_output=True,
            check=True,
            timeout=10
        )
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False

    # Post comment
    try:
        subprocess.run(
            ["gh", "pr", "comment", str(pr_number), "--body", comment],
            capture_output=True,
            text=True,
            check=True,
            timeout=30
        )
        return True
    except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
        return False


def update_or_create_comment(comment: str, pr_number: Optional[int] = None) -> bool:
    """Update existing EvalView comment or create new one.

    This prevents comment spam by updating the existing comment instead
    of creating multiple comments on the same PR.

    Args:
        comment: Markdown comment to post
        pr_number: PR number (auto-detected if not provided)

    Returns:
        True if comment was posted/updated successfully
    """
    if pr_number is None:
        pr_number = _extract_pr_number()
        if pr_number is None:
            return False

    # Try to find and update existing comment
    try:
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number), "--json", "comments"],
            capture_output=True,
            text=True,
            check=True,
            timeout=30
        )
        comments_data = json.loads(result.stdout)
        comments = comments_data.get("comments", [])

        # Find our comment by signature
        for c in comments:
            body = c.get("body", "")
            if COMMENT_SIGNATURE in body:
                comment_url = c.get("url", "")
                # Extract comment ID from URL: .../pull/123#issuecomment-456
                if "#issuecomment-" in comment_url:
                    comment_id = comment_url.split("#issuecomment-")[1]
                    subprocess.run(
                        [
                            "gh", "api", "-X", "PATCH",
                            f"/repos/:owner/:repo/issues/comments/{comment_id}",
                            "-f", f"body={comment}"
                        ],
                        capture_output=True,
                        check=True,
                        timeout=30
                    )
                    return True

        # No existing comment found, create new one
        return post_pr_comment(comment, pr_number)

    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, json.JSONDecodeError):
        # Fall back to creating new comment
        return post_pr_comment(comment, pr_number)
