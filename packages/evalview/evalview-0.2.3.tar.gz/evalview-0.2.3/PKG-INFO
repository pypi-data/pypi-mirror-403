Metadata-Version: 2.4
Name: evalview
Version: 0.2.3
Summary: Pytest-style testing framework for AI agents ‚Äî LangGraph, CrewAI, OpenAI, Anthropic
Author-email: EvalView Team <hidai@evalview.com>
License-Expression: Apache-2.0
Project-URL: Homepage, https://github.com/hidai25/eval-view
Project-URL: Documentation, https://github.com/hidai25/eval-view#readme
Project-URL: Repository, https://github.com/hidai25/eval-view.git
Project-URL: Issue Tracker, https://github.com/hidai25/eval-view/issues
Project-URL: Changelog, https://github.com/hidai25/eval-view/blob/main/CHANGELOG.md
Keywords: ai,agents,testing,evaluation,llm,langchain,langgraph,crewai,openai,anthropic,claude,pytest-ai,ai-agent-testing,llm-testing,agent-evaluation,regression-testing,ci-cd-testing,yaml-testing,tool-calling
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Testing
Classifier: Topic :: Software Development :: Quality Assurance
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: NOTICE
Requires-Dist: click>=8.1.0
Requires-Dist: pydantic>=2.5.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: openai>=1.12.0
Requires-Dist: anthropic>=0.39.0
Requires-Dist: rich>=13.7.0
Requires-Dist: prompt_toolkit>=3.0.0
Requires-Dist: httpx>=0.26.0
Requires-Dist: python-dateutil>=2.8.2
Requires-Dist: python-dotenv>=1.0.0
Provides-Extra: reports
Requires-Dist: jinja2>=3.0; extra == "reports"
Requires-Dist: plotly>=5.0; extra == "reports"
Provides-Extra: watch
Requires-Dist: watchdog>=3.0; extra == "watch"
Provides-Extra: telemetry
Requires-Dist: posthog>=3.0.0; extra == "telemetry"
Provides-Extra: all
Requires-Dist: jinja2>=3.0; extra == "all"
Requires-Dist: plotly>=5.0; extra == "all"
Requires-Dist: watchdog>=3.0; extra == "all"
Requires-Dist: posthog>=3.0.0; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black==24.10.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: jinja2>=3.0; extra == "dev"
Requires-Dist: plotly>=5.0; extra == "dev"
Requires-Dist: watchdog>=3.0; extra == "dev"
Requires-Dist: anthropic>=0.39.0; extra == "dev"
Requires-Dist: fastapi>=0.109.0; extra == "dev"
Requires-Dist: uvicorn>=0.27.0; extra == "dev"
Dynamic: license-file

# EvalView ‚Äî Pytest for AI Agents

> Your agent worked yesterday. Today it's broken. EvalView catches why.

**CI/CD for agent behavior.** Detect tool changes, output drift, cost spikes, and latency regressions ‚Äî before users complain.

<p align="center">
  <img src="assets/demo.gif" alt="EvalView Demo" width="700">
</p>

```bash
pip install evalview && evalview demo   # No API key needed
```

[![PyPI downloads](https://img.shields.io/pypi/dm/evalview.svg?label=downloads)](https://pypi.org/project/evalview/)
[![GitHub stars](https://img.shields.io/github/stars/hidai25/eval-view?style=social)](https://github.com/hidai25/eval-view/stargazers)
[![CI](https://github.com/hidai25/eval-view/actions/workflows/ci.yml/badge.svg)](https://github.com/hidai25/eval-view/actions/workflows/ci.yml)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

---

**You changed a prompt.** Now your agent calls wrong tools, hallucinates, costs 3x more, or times out. You find out when users complain.

**EvalView catches this in CI ‚Äî before you deploy.**

```bash
evalview golden save .evalview/results/xxx.json   # Save working run as baseline
evalview run --diff                                # Fail CI on regression
```

[Get started in 60 seconds ‚Üí](#quick-start)

---

## Why EvalView?

|  | Observability Tools | Generic Eval Frameworks | **EvalView** |
|---|:---:|:---:|:---:|
| Blocks bad deploys in CI | ‚ùå | ‚ö†Ô∏è Manual | ‚úÖ Built-in |
| Detects tool call changes | ‚ùå | ‚ùå | ‚úÖ |
| Tracks cost/latency regressions | ‚ö†Ô∏è Alerts only | ‚ùå | ‚úÖ Fails CI |
| Golden baseline diffing | ‚ùå | ‚ùå | ‚úÖ |
| Free & open source | ‚ùå | ‚úÖ | ‚úÖ |
| Works offline (Ollama) | ‚ùå | ‚ö†Ô∏è Some | ‚úÖ |

**Use observability tools to see what happened. Use EvalView to block it from shipping.**

---

## What EvalView Catches

| Status | Meaning | Action |
|--------|---------|--------|
| **REGRESSION** | Score dropped | Fix before deploy |
| **TOOLS_CHANGED** | Different tools called | Review before deploy |
| **OUTPUT_CHANGED** | Same tools, different output | Review before deploy |
| **PASSED** | Matches baseline | Ship it |

---

## Quick Start

```bash
pip install evalview

export OPENAI_API_KEY='your-key'   # For LLM-as-judge
evalview quickstart                 # Creates test + runs it
```

**Want free local evaluation?**
```bash
evalview run --judge-provider ollama --judge-model llama3.2
```

[Full getting started guide ‚Üí](docs/GETTING_STARTED.md)

---

## Add to CI in 60 Seconds

```yaml
# .github/workflows/evalview.yml
name: Agent Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: hidai25/eval-view@v0.2.3
        with:
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          diff: true
          fail-on: 'REGRESSION'
```

PRs with regressions get blocked. Add a PR comment showing exactly what changed:

```yaml
      - run: evalview ci comment  # Posts diff to PR
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

[Full CI/CD setup ‚Üí](docs/CI_CD.md)

---

## Interactive Chat Mode

Talk to your tests. Debug failures. Compare runs.

```bash
evalview chat
```

```
You: run the calculator test
ü§ñ Running calculator test...
‚úÖ Passed (score: 92.5)

You: what tools did it use?
ü§ñ The agent used: calculator, search

You: compare to yesterday
ü§ñ Score: 92.5 ‚Üí 87.2 (-5.3)
   Tools: +1 added (validator)
   Cost: $0.003 ‚Üí $0.005 (+67%)
```

Slash commands: `/run`, `/test`, `/compare`, `/traces`, `/adapters`

[Chat mode docs ‚Üí](docs/CHAT_MODE.md)

---

## Features

| Feature | Description | Docs |
|---------|-------------|------|
| **Golden Traces** | Save baselines, detect regressions with `--diff` | [‚Üí](docs/GOLDEN_TRACES.md) |
| **Tool Categories** | Match by intent, not exact tool names | [‚Üí](docs/TOOL_CATEGORIES.md) |
| **Statistical Mode** | Handle flaky LLMs with `--runs N` and pass@k | [‚Üí](docs/STATISTICAL_MODE.md) |
| **Chat Mode** | AI assistant: `/run`, `/test`, `/compare` | [‚Üí](docs/CHAT_MODE.md) |
| **Skills Testing** | Validate Claude Code / OpenAI Codex skills | [‚Üí](docs/SKILLS_TESTING.md) |
| **Test Generation** | Generate 1000 tests from 1 | [‚Üí](docs/TEST_GENERATION.md) |
| **Suite Types** | Separate capability vs regression tests | [‚Üí](docs/SUITE_TYPES.md) |
| **Difficulty Levels** | Filter by `--difficulty hard`, benchmark by tier | [‚Üí](docs/STATISTICAL_MODE.md) |
| **Behavior Coverage** | Track tasks, tools, paths tested | [‚Üí](docs/BEHAVIOR_COVERAGE.md) |
| **Cost & Latency** | Automatic threshold enforcement | [‚Üí](docs/EVALUATION_METRICS.md) |
| **HTML Reports** | Interactive Plotly charts | [‚Üí](docs/CLI_REFERENCE.md) |

---

## Who Uses EvalView?

- **Teams shipping LangGraph / CrewAI agents** who need CI gates
- **Solo developers** tired of "it worked yesterday" bugs
- **Platform teams** building internal agent tooling

---

## Supported Frameworks

LangGraph ‚Ä¢ CrewAI ‚Ä¢ OpenAI Assistants ‚Ä¢ Anthropic Claude ‚Ä¢ AutoGen ‚Ä¢ Dify ‚Ä¢ Ollama ‚Ä¢ Any HTTP API

[Compatibility details ‚Üí](docs/FRAMEWORK_SUPPORT.md)

---

## Documentation

| | |
|---|---|
| [Getting Started](docs/GETTING_STARTED.md) | [CLI Reference](docs/CLI_REFERENCE.md) |
| [Golden Traces](docs/GOLDEN_TRACES.md) | [CI/CD Integration](docs/CI_CD.md) |
| [Tool Categories](docs/TOOL_CATEGORIES.md) | [Statistical Mode](docs/STATISTICAL_MODE.md) |
| [Skills Testing](docs/SKILLS_TESTING.md) | [Evaluation Metrics](docs/EVALUATION_METRICS.md) |
| [FAQ](docs/FAQ.md) | [Debugging](docs/DEBUGGING.md) |

**Guides:** [Testing LangGraph in CI](guides/pytest-for-ai-agents-langgraph-ci.md) ‚Ä¢ [Detecting Hallucinations](guides/detecting-llm-hallucinations-in-ci.md)

---

## Examples

| Framework | Link |
|-----------|------|
| LangGraph | [examples/langgraph/](examples/langgraph/) |
| CrewAI | [examples/crewai/](examples/crewai/) |
| Anthropic Claude | [examples/anthropic/](examples/anthropic/) |
| Dify | [examples/dify/](examples/dify/) |
| Ollama (Local) | [examples/ollama/](examples/ollama/) |

**Node.js?** See [@evalview/node](sdks/node/)

---

## Get Help

- **Questions?** [GitHub Discussions](https://github.com/hidai25/eval-view/discussions)
- **Bugs?** [GitHub Issues](https://github.com/hidai25/eval-view/issues)
- **Want setup help?** Email hidai@evalview.com ‚Äî happy to help configure your first tests

---

## Roadmap

**Shipped:** Golden traces ‚Ä¢ Tool categories ‚Ä¢ Statistical mode ‚Ä¢ Difficulty levels ‚Ä¢ Partial sequence credit ‚Ä¢ Skills testing ‚Ä¢ MCP servers ‚Ä¢ HTML reports

**Coming:** Multi-turn conversations ‚Ä¢ Grounded hallucination detection ‚Ä¢ Error compounding metrics

[Vote on features ‚Üí](https://github.com/hidai25/eval-view/discussions)

---

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md).

**License:** Apache 2.0

---

<p align="center">
  <b>Stop shipping regressions.</b><br>
  <a href="#quick-start">Get started in 60 seconds ‚Üí</a>
</p>

---

*EvalView is an independent open-source project, not affiliated with LangGraph, CrewAI, OpenAI, Anthropic, or any other third party.*
