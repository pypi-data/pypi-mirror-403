import json
import logging
from typing import Any, Iterable, Optional
from typing import List, Optional, Union
import pandas as pd
import vertica_python
from sqlalchemy import create_engine
from mnemosynecore.vault import get_secret
HAS_AIRFLOW = False
try:
    from airflow.operators.python import get_current_context
    from airflow import DAG
    from airflow.utils.task_group import TaskGroup
    from airflow.operators.dummy import DummyOperator
    from os import listdir, path
    HAS_AIRFLOW = True
except ImportError:
    pass


def load_sql_tasks_from_dir(dir_sql: str, vertica_conn_id: str):
    if not HAS_AIRFLOW:
        raise ImportError("Для использования load_sql_tasks_from_dir необходимо установить Airflow")

    from airflow.providers.vertica.operators.vertica import VerticaOperator

    context = get_current_context()
    dag = context['dag']

    tasks = {}
    files_list = listdir(dir_sql)
    for filename in files_list:
        if not filename.endswith(".sql"):
            continue
        path_file = path.join(dir_sql, filename)
        sql = read_sql_file(path_file)
        if sql:
            sql_statements = sql.split(';')
            task_name = 'task_' + filename.replace('.sql', '_vertica')
            tasks[task_name] = VerticaOperator(
                sql=sql_statements,
                task_id=task_name,
                vertica_conn_id=vertica_conn_id,
                dag=dag,
            )
    return tasks


def read_sql_file(file_path: str):
    if not path.exists(file_path):
        print('Error: no file ' + file_path)
        return None
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


def vertica_dedupe(
        table_name: str,
        unique_keys: Union[str, List[str]],
        conn: Optional[vertica_python.Connection] = None,
        conn_id: Optional[str] = None,
        date_col: Optional[str] = None,
        keep: str = "last",
        commit: bool = True
):

    if isinstance(unique_keys, str):
        unique_keys = [unique_keys]

    close_conn = False
    if conn is None:
        if not conn_id:
            raise ValueError("Нужно указать conn или conn_id")
        conn = vertica_conn(conn_id)
        close_conn = True

    order_clause = f"ORDER BY {date_col} DESC" if date_col and keep == "last" else ""
    unique_cols = ", ".join(unique_keys)

    dedupe_sql = f"""
        DELETE FROM {table_name} t
        USING (
            SELECT {unique_cols}, ROW_NUMBER() OVER (PARTITION BY {unique_cols} {order_clause}) AS rn
            FROM {table_name}
        ) x
        WHERE t.{unique_keys[0]} = x.{unique_keys[0]}
          AND x.rn > 1
    """

    vertica_sql(conn=conn, sql=dedupe_sql, commit=commit)

    if close_conn:
        conn.close()

    print(f"Удаление дубликатов из {table_name} завершено по ключам: {unique_keys}")


def get_connection_as_json(conn_name):
    import json
    from airflow.hooks.base import BaseHook

    conn = BaseHook.get_connection(conn_name)
    ci = {'host': conn.host,
          'password': conn.password,
          'login': conn.login,
          'port': conn.port,
          'schema': conn.schema,
          'extra': conn.extra}
    return json.dumps(ci)


def vertica_upsert(
        df: pd.DataFrame,
        table_name: str,
        unique_keys: Union[str, List[str]],
        conn=None,
        date_col: Optional[str] = None,
        days_back: Optional[int] = None,
        commit: bool = True
):
    if df.empty:
        print("Нет данных для обновления.")
        return

    if isinstance(unique_keys, str):
        unique_keys = [unique_keys]

    close_conn = False
    if conn is None:
        raise ValueError("conn или conn_id обязателен")

    temp_table = f"{table_name}_tmp"
    create_temp_sql = f"""
        DROP TABLE IF EXISTS {temp_table};
        CREATE LOCAL TEMP TABLE {temp_table} AS
        SELECT * FROM {table_name} WHERE 1=0;
    """
    vertica_sql(conn, create_temp_sql, commit=False)

    rows = [tuple(x) for x in df.to_numpy()]
    cols = ", ".join(df.columns)
    placeholders = ", ".join(["%s"] * len(df.columns))

    insert_sql = f"INSERT INTO {temp_table} ({cols}) VALUES ({placeholders})"
    vertica_sql(conn, insert_sql, params=rows, many=True, commit=False)

    if date_col and days_back:
        delete_sql = f"""
            DELETE FROM {table_name}
            WHERE {date_col} >= CURRENT_DATE - INTERVAL '{days_back} day';
        """
        vertica_sql(conn, delete_sql, commit=False)

    merge_conditions = " AND ".join([f"t.{k} = s.{k}" for k in unique_keys])
    update_assignments = ", ".join([f"{c} = s.{c}" for c in df.columns if c not in unique_keys])

    merge_sql = f"""
        MERGE INTO {table_name} t
        USING {temp_table} s
        ON {merge_conditions}
        WHEN MATCHED THEN UPDATE SET {update_assignments}
        WHEN NOT MATCHED THEN INSERT ({cols}) VALUES ({cols});
    """
    vertica_sql(conn, merge_sql, commit=False)

    if commit:
        conn.commit()

    if close_conn:
        conn.close()

    print(f"Обновление таблицы {table_name} завершено. {len(df)} строк обработано.")


def vertica_conn(conn_id: str) -> vertica_python.Connection:
    cfg = get_secret(conn_id)

    return vertica_python.connect(
        host=cfg["host"],
        port=int(cfg["port"]),
        user=cfg["login"],
        password=cfg["password"],
        database=cfg.get("schema") or cfg.get("database"),
        autocommit=False,
    )


def get_vertica_engine(conn_id: str):
    cfg = json.loads(get_connection_as_json(conn_id))

    vertica_url = (
        f"vertica+vertica_python://{cfg['login']}:{cfg['password']}@"
        f"{cfg['host']}:{cfg['port']}/{cfg['schema']}"
    )

    return create_engine(vertica_url, pool_pre_ping=True)


def vertica_sql(
        *,
        conn_id: Optional[str] = None,
        conn: Optional[vertica_python.Connection] = None,
        sql: str,
        params: Optional[Iterable[Any]] = None,
        many: bool = False,
        commit: bool = True,
) -> None:

    if not conn and not conn_id:
        raise ValueError("Нужно указать conn или conn_id")

    close_conn = False
    if not conn:
        conn = vertica_conn(conn_id)
        close_conn = True

    try:
        with conn.cursor() as cur:
            if many:
                cur.executemany(sql, params)
            else:
                cur.execute(sql, params)

        if commit:
            conn.commit()

    except Exception:
        conn.rollback()
        logging.exception("Ошибка выполнения SQL в Vertica")
        raise

    finally:
        if close_conn:
            conn.close()


def vertica_select(
        *,
        conn_id: Optional[str] = None,
        conn: Optional[vertica_python.Connection] = None,
        sql: str,
        params: Optional[Iterable[Any]] = None,
) -> pd.DataFrame:

    if not conn and not conn_id:
        raise ValueError("Нужно указать conn или conn_id")

    close_conn = False
    if not conn:
        conn = vertica_conn(conn_id)
        close_conn = True

    try:
        with conn.cursor() as cur:
            cur.execute(sql, params)
            columns = [desc[0] for desc in cur.description]
            rows = cur.fetchall()

        return pd.DataFrame(rows, columns=columns)

    finally:
        if close_conn:
            conn.close()