"""
CLI æ•°æ®æ¸…æ´—å’Œå»é‡ç›¸å…³å‘½ä»¤
"""

import os
import shutil
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..core import DataTransformer
from ..storage.io import save_data
from ..streaming import load_stream
from ..utils.field_path import get_field_with_spec
from .common import (
    _check_file_format,
    _get_value_len,
    _is_empty_value,
    _is_streaming_supported,
    _parse_field_list,
)


def dedupe(
    filename: str,
    key: Optional[str] = None,
    similar: Optional[float] = None,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®å»é‡ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ç²¾ç¡®å»é‡ï¼ˆé»˜è®¤ï¼‰ï¼šå®Œå…¨ç›¸åŒçš„æ•°æ®æ‰å»é‡
    2. ç›¸ä¼¼åº¦å»é‡ï¼šä½¿ç”¨ MinHash+LSH ç®—æ³•ï¼Œç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼åˆ™å»é‡

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        key: å»é‡ä¾æ®å­—æ®µï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•ï¼š
            - meta.source        åµŒå¥—å­—æ®µ
            - messages[0].role   æ•°ç»„ç´¢å¼•
            - messages[-1].content  è´Ÿç´¢å¼•
            - messages.#         æ•°ç»„é•¿åº¦
            - messages[*].role:join  å±•å¼€æ‰€æœ‰å…ƒç´ 
            å¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ã€‚ä¸æŒ‡å®šåˆ™å…¨é‡å»é‡
        similar: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼ŒæŒ‡å®šåå¯ç”¨ç›¸ä¼¼åº¦å»é‡æ¨¡å¼ï¼Œéœ€è¦æŒ‡å®š --key
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt dedupe data.jsonl                       # å…¨é‡ç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=text            # æŒ‰ text å­—æ®µç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=user,timestamp  # æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=meta.id         # æŒ‰åµŒå¥—å­—æ®µå»é‡
        dt dedupe data.jsonl --key=messages[0].content   # æŒ‰ç¬¬ä¸€æ¡æ¶ˆæ¯å†…å®¹å»é‡
        dt dedupe data.jsonl --key=text --similar=0.8    # ç›¸ä¼¼åº¦å»é‡
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼å¿…é¡»æŒ‡å®š key
    if similar is not None and not key:
        print("é”™è¯¯: ç›¸ä¼¼åº¦å»é‡éœ€è¦æŒ‡å®š --key å‚æ•°")
        return

    if similar is not None and (similar <= 0 or similar > 1):
        print("é”™è¯¯: --similar å‚æ•°å¿…é¡»åœ¨ 0-1 ä¹‹é—´")
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # æ‰§è¡Œå»é‡
    if similar is not None:
        # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼
        print(f"ğŸ”‘ ç›¸ä¼¼åº¦å»é‡: å­—æ®µ={key}, é˜ˆå€¼={similar}")
        print("ğŸ”„ æ‰§è¡Œå»é‡ï¼ˆMinHash+LSHï¼‰...")
        try:
            result = dt.dedupe_similar(key, threshold=similar)
        except ImportError as e:
            print(f"é”™è¯¯: {e}")
            return
    else:
        # ç²¾ç¡®å»é‡æ¨¡å¼
        dedupe_key: Any = None
        if key:
            keys = [k.strip() for k in key.split(",")]
            if len(keys) == 1:
                dedupe_key = keys[0]
                print(f"ğŸ”‘ æŒ‰å­—æ®µç²¾ç¡®å»é‡: {dedupe_key}")
            else:
                dedupe_key = keys
                print(f"ğŸ”‘ æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡: {', '.join(dedupe_key)}")
        else:
            print("ğŸ”‘ å…¨é‡ç²¾ç¡®å»é‡")

        print("ğŸ”„ æ‰§è¡Œå»é‡...")
        result = dt.dedupe(dedupe_key)

    dedupe_count = len(result)
    removed_count = original_count - dedupe_count

    # ä¿å­˜ç»“æœ
    output_path = output or str(filepath)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        result.save(output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å»é™¤ {removed_count} æ¡é‡å¤æ•°æ®ï¼Œå‰©ä½™ {dedupe_count} æ¡")


def clean(
    filename: str,
    drop_empty: Optional[str] = None,
    min_len: Optional[str] = None,
    max_len: Optional[str] = None,
    keep: Optional[str] = None,
    drop: Optional[str] = None,
    strip: bool = False,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®æ¸…æ´—ï¼ˆé»˜è®¤æµå¼å¤„ç†ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        drop_empty: åˆ é™¤ç©ºå€¼è®°å½•ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„è¯­æ³•
            - ä¸å¸¦å€¼ï¼šåˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•
            - æŒ‡å®šå­—æ®µï¼šåˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•ï¼ˆé€—å·åˆ†éš”ï¼‰
        min_len: æœ€å°é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼Œå­—æ®µæ”¯æŒåµŒå¥—è·¯å¾„
        max_len: æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼Œå­—æ®µæ”¯æŒåµŒå¥—è·¯å¾„
        keep: åªä¿ç•™æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼Œä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        drop: åˆ é™¤æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼Œä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        strip: å»é™¤æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µçš„é¦–å°¾ç©ºç™½
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt clean data.jsonl --drop-empty                    # åˆ é™¤ä»»æ„ç©ºå€¼è®°å½•
        dt clean data.jsonl --drop-empty=text,answer        # åˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•
        dt clean data.jsonl --drop-empty=meta.source        # åˆ é™¤åµŒå¥—å­—æ®µä¸ºç©ºçš„è®°å½•
        dt clean data.jsonl --min-len=text:10               # text å­—æ®µæœ€å°‘ 10 å­—ç¬¦
        dt clean data.jsonl --min-len=messages.#:2          # è‡³å°‘ 2 æ¡æ¶ˆæ¯
        dt clean data.jsonl --max-len=messages[-1].content:500  # æœ€åä¸€æ¡æ¶ˆæ¯æœ€å¤š 500 å­—ç¬¦
        dt clean data.jsonl --keep=question,answer          # åªä¿ç•™è¿™äº›å­—æ®µ
        dt clean data.jsonl --drop=metadata,timestamp       # åˆ é™¤è¿™äº›å­—æ®µ
        dt clean data.jsonl --strip                         # å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # è§£æå‚æ•°
    min_len_field, min_len_value = _parse_len_param(min_len) if min_len else (None, None)
    max_len_field, max_len_value = _parse_len_param(max_len) if max_len else (None, None)
    keep_fields = _parse_field_list(keep) if keep else None
    drop_fields_set = set(_parse_field_list(drop)) if drop else None
    keep_set = set(keep_fields) if keep_fields else None

    # æ„å»ºæ¸…æ´—é…ç½®
    empty_fields = None
    if drop_empty is not None:
        if drop_empty == "" or drop_empty is True:
            print("ğŸ”„ åˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•...")
            empty_fields = []
        else:
            empty_fields = _parse_field_list(drop_empty)
            print(f"ğŸ”„ åˆ é™¤å­—æ®µä¸ºç©ºçš„è®°å½•: {', '.join(empty_fields)}")

    if strip:
        print("ğŸ”„ å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½...")
    if min_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {min_len_field} é•¿åº¦ < {min_len_value} çš„è®°å½•...")
    if max_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {max_len_field} é•¿åº¦ > {max_len_value} çš„è®°å½•...")
    if keep_fields:
        print(f"ğŸ”„ åªä¿ç•™å­—æ®µ: {', '.join(keep_fields)}")
    if drop_fields_set:
        print(f"ğŸ”„ åˆ é™¤å­—æ®µ: {', '.join(drop_fields_set)}")

    output_path = output or str(filepath)

    # æ£€æŸ¥è¾“å…¥è¾“å‡ºæ˜¯å¦ç›¸åŒï¼ˆæµå¼å¤„ç†éœ€è¦ä¸´æ—¶æ–‡ä»¶ï¼‰
    input_resolved = filepath.resolve()
    output_resolved = Path(output_path).resolve()
    use_temp_file = input_resolved == output_resolved

    # å¯¹äº JSONL æ–‡ä»¶ä½¿ç”¨æµå¼å¤„ç†
    if _is_streaming_supported(filepath):
        print(f"ğŸ“Š æµå¼åŠ è½½: {filepath}")

        # å¦‚æœè¾“å…¥è¾“å‡ºç›¸åŒï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        if use_temp_file:
            print("âš  æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ä¸è¾“å…¥æ–‡ä»¶ç›¸åŒï¼Œå°†ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶")
            temp_fd, temp_path = tempfile.mkstemp(
                suffix=output_resolved.suffix,
                prefix=".tmp_",
                dir=output_resolved.parent,
            )
            os.close(temp_fd)
            actual_output = temp_path
        else:
            actual_output = output_path

        try:
            count = _clean_streaming(
                str(filepath),
                actual_output,
                strip=strip,
                empty_fields=empty_fields,
                min_len_field=min_len_field,
                min_len_value=min_len_value,
                max_len_field=max_len_field,
                max_len_value=max_len_value,
                keep_set=keep_set,
                drop_fields_set=drop_fields_set,
            )

            # å¦‚æœä½¿ç”¨äº†ä¸´æ—¶æ–‡ä»¶ï¼Œç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
            if use_temp_file:
                shutil.move(temp_path, output_path)

            print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
            print(f"\nâœ… å®Œæˆ! æ¸…æ´—å {count} æ¡æ•°æ®")
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if use_temp_file and os.path.exists(temp_path):
                os.unlink(temp_path)
            print(f"é”™è¯¯: æ¸…æ´—å¤±è´¥ - {e}")
            import traceback

            traceback.print_exc()
        return

    # é JSONL æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œ
    data, step_stats = _clean_data_single_pass(
        dt.data,
        strip=strip,
        empty_fields=empty_fields,
        min_len_field=min_len_field,
        min_len_value=min_len_value,
        max_len_field=max_len_field,
        max_len_value=max_len_value,
        keep_fields=keep_fields,
        drop_fields=drop_fields_set,
    )

    # ä¿å­˜ç»“æœ
    final_count = len(data)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")

    try:
        save_data(data, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    # æ‰“å°ç»Ÿè®¡
    removed_count = original_count - final_count
    print(f"\nâœ… å®Œæˆ!")
    print(f"   åŸå§‹: {original_count} æ¡ -> æ¸…æ´—å: {final_count} æ¡ (åˆ é™¤ {removed_count} æ¡)")
    if step_stats:
        print(f"   æ­¥éª¤: {' | '.join(step_stats)}")


def _parse_len_param(param: str) -> tuple:
    """è§£æé•¿åº¦å‚æ•°ï¼Œæ ¼å¼ 'field:length'"""
    if ":" not in param:
        raise ValueError(f"é•¿åº¦å‚æ•°æ ¼å¼é”™è¯¯: {param}ï¼Œåº”ä¸º 'å­—æ®µ:é•¿åº¦'")
    parts = param.split(":", 1)
    field = parts[0].strip()
    try:
        length = int(parts[1].strip())
    except ValueError:
        raise ValueError(f"é•¿åº¦å¿…é¡»æ˜¯æ•´æ•°: {parts[1]}")
    return field, length


def _clean_data_single_pass(
    data: List[Dict],
    strip: bool = False,
    empty_fields: Optional[List[str]] = None,
    min_len_field: Optional[str] = None,
    min_len_value: Optional[int] = None,
    max_len_field: Optional[str] = None,
    max_len_value: Optional[int] = None,
    keep_fields: Optional[List[str]] = None,
    drop_fields: Optional[set] = None,
) -> tuple:
    """
    å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œã€‚

    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        strip: æ˜¯å¦å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
        empty_fields: æ£€æŸ¥ç©ºå€¼çš„å­—æ®µåˆ—è¡¨ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰å­—æ®µï¼ŒNone è¡¨ç¤ºä¸æ£€æŸ¥
        min_len_field: æœ€å°é•¿åº¦æ£€æŸ¥çš„å­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        min_len_value: æœ€å°é•¿åº¦å€¼
        max_len_field: æœ€å¤§é•¿åº¦æ£€æŸ¥çš„å­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        max_len_value: æœ€å¤§é•¿åº¦å€¼
        keep_fields: åªä¿ç•™çš„å­—æ®µåˆ—è¡¨ï¼ˆä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰
        drop_fields: è¦åˆ é™¤çš„å­—æ®µé›†åˆï¼ˆä»…æ”¯æŒé¡¶å±‚å­—æ®µï¼‰

    Returns:
        (æ¸…æ´—åçš„æ•°æ®, ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨)
    """
    result = []
    stats = {
        "drop_empty": 0,
        "min_len": 0,
        "max_len": 0,
    }

    # é¢„å…ˆè®¡ç®— keep_fields é›†åˆï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    keep_set = set(keep_fields) if keep_fields else None

    for item in data:
        # 1. strip å¤„ç†ï¼ˆåœ¨è¿‡æ»¤å‰æ‰§è¡Œï¼Œè¿™æ ·ç©ºå€¼æ£€æµ‹æ›´å‡†ç¡®ï¼‰
        if strip:
            item = {k: v.strip() if isinstance(v, str) else v for k, v in item.items()}

        # 2. ç©ºå€¼è¿‡æ»¤
        if empty_fields is not None:
            if len(empty_fields) == 0:
                # æ£€æŸ¥æ‰€æœ‰å­—æ®µ
                if any(_is_empty_value(v) for v in item.values()):
                    stats["drop_empty"] += 1
                    continue
            else:
                # æ£€æŸ¥æŒ‡å®šå­—æ®µï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
                if any(_is_empty_value(get_field_with_spec(item, f)) for f in empty_fields):
                    stats["drop_empty"] += 1
                    continue

        # 3. æœ€å°é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if min_len_field is not None:
            if _get_value_len(get_field_with_spec(item, min_len_field, default="")) < min_len_value:
                stats["min_len"] += 1
                continue

        # 4. æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if max_len_field is not None:
            if _get_value_len(get_field_with_spec(item, max_len_field, default="")) > max_len_value:
                stats["max_len"] += 1
                continue

        # 5. å­—æ®µç®¡ç†ï¼ˆkeep/dropï¼‰
        if keep_set is not None:
            item = {k: v for k, v in item.items() if k in keep_set}
        elif drop_fields is not None:
            item = {k: v for k, v in item.items() if k not in drop_fields}

        result.append(item)

    # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²åˆ—è¡¨
    step_stats = []
    if strip:
        step_stats.append("strip")
    if stats["drop_empty"] > 0:
        step_stats.append(f"drop-empty: -{stats['drop_empty']}")
    if stats["min_len"] > 0:
        step_stats.append(f"min-len: -{stats['min_len']}")
    if stats["max_len"] > 0:
        step_stats.append(f"max-len: -{stats['max_len']}")
    if keep_fields:
        step_stats.append(f"keep: {len(keep_fields)} å­—æ®µ")
    if drop_fields:
        step_stats.append(f"drop: {len(drop_fields)} å­—æ®µ")

    return result, step_stats


def _clean_streaming(
    input_path: str,
    output_path: str,
    strip: bool = False,
    empty_fields: Optional[List[str]] = None,
    min_len_field: Optional[str] = None,
    min_len_value: Optional[int] = None,
    max_len_field: Optional[str] = None,
    max_len_value: Optional[int] = None,
    keep_set: Optional[set] = None,
    drop_fields_set: Optional[set] = None,
) -> int:
    """
    æµå¼æ¸…æ´—æ•°æ®ã€‚

    Returns:
        å¤„ç†åçš„æ•°æ®æ¡æ•°
    """

    def clean_filter(item: Dict) -> bool:
        """è¿‡æ»¤å‡½æ•°ï¼šè¿”å› True ä¿ç•™ï¼ŒFalse è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰"""
        # ç©ºå€¼è¿‡æ»¤
        if empty_fields is not None:
            if len(empty_fields) == 0:
                if any(_is_empty_value(v) for v in item.values()):
                    return False
            else:
                # æ”¯æŒåµŒå¥—è·¯å¾„
                if any(_is_empty_value(get_field_with_spec(item, f)) for f in empty_fields):
                    return False

        # æœ€å°é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if min_len_field is not None:
            if _get_value_len(get_field_with_spec(item, min_len_field, default="")) < min_len_value:
                return False

        # æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼ˆæ”¯æŒåµŒå¥—è·¯å¾„ï¼‰
        if max_len_field is not None:
            if _get_value_len(get_field_with_spec(item, max_len_field, default="")) > max_len_value:
                return False

        return True

    def clean_transform(item: Dict) -> Dict:
        """è½¬æ¢å‡½æ•°ï¼šstrip + å­—æ®µç®¡ç†"""
        # strip å¤„ç†
        if strip:
            item = {k: v.strip() if isinstance(v, str) else v for k, v in item.items()}

        # å­—æ®µç®¡ç†
        if keep_set is not None:
            item = {k: v for k, v in item.items() if k in keep_set}
        elif drop_fields_set is not None:
            item = {k: v for k, v in item.items() if k not in drop_fields_set}

        return item

    # æ„å»ºæµå¼å¤„ç†é“¾
    st = load_stream(input_path)

    # å¦‚æœéœ€è¦ stripï¼Œå…ˆæ‰§è¡Œ strip è½¬æ¢ï¼ˆåœ¨è¿‡æ»¤ä¹‹å‰ï¼Œè¿™æ ·ç©ºå€¼æ£€æµ‹æ›´å‡†ç¡®ï¼‰
    if strip:
        st = st.transform(
            lambda x: {k: v.strip() if isinstance(v, str) else v for k, v in x.items()}
        )

    # æ‰§è¡Œè¿‡æ»¤
    if empty_fields is not None or min_len_field is not None or max_len_field is not None:
        st = st.filter(clean_filter)

    # æ‰§è¡Œå­—æ®µç®¡ç†ï¼ˆå¦‚æœæ²¡æœ‰ stripï¼Œä¹Ÿéœ€è¦åœ¨è¿™é‡Œå¤„ç†ï¼‰
    if keep_set is not None or drop_fields_set is not None:

        def field_transform(item):
            if keep_set is not None:
                return {k: v for k, v in item.items() if k in keep_set}
            elif drop_fields_set is not None:
                return {k: v for k, v in item.items() if k not in drop_fields_set}
            return item

        st = st.transform(field_transform)

    return st.save(output_path)
