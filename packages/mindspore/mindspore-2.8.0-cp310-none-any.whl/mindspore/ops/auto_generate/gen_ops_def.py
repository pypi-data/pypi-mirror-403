# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

"""Operators definition generated by gen_ops.py, includes functions."""

from .gen_ops_prim import *
from .pyboost_inner_prim import *
from mindspore.ops.operations.manually_defined.ops_def import *
from mindspore.ops._primitive_cache import _get_cache_prim


def masked_select(input, mask):
    r"""
    Return a new 1-D tensor which indexes the `input` tensor according to the boolean `mask`.

    Support broadcast.

    Args:
        input (Tensor): The input tensor.
        mask (Tensor[bool]): The input mask.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([1, 2, 3, 4], mindspore.int64)
        >>> mask = mindspore.tensor([1, 0, 1, 0], mindspore.bool)
        >>> output = mindspore.ops.masked_select(x, mask)
        >>> print(output)
        [1 3]
    """
    return masked_select_op(input, mask)


def sub_scalar_(input, other, alpha=1):
    r"""
    
    """
    return inplace_sub_scalar_op(input, other, alpha)


def prelu(input, weight):
    r"""
    Parametric Rectified Linear Unit activation function.

    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on
    ImageNet Classification <https://arxiv.org/abs/1502.01852>`_. Defined as follows:

    .. math::
        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),

    where :math:`x_i` is an element of a channel of the input, :math:`w` is the weight of the channel.

    PReLU Activation Function Graph:

    .. image:: ../images/PReLU2.png
        :align: center

    .. note::
        - Channel dim is the 2nd dim of input. When input has dims < 2, then there is
          no channel dim and the number of channels = 1.
        - In GE mode, the rank of the input tensor must be greater than 1;
          otherwise, an error will be triggered.

    Args:
        input (Tensor): The input Tensor of the activation function.
        weight (Tensor):  Weight Tensor. The size of the weight should be 1 or the number of channels at Tensor `input`.

    Returns:
        Tensor, with the same shape and dtype as `input`.
        For detailed information, please refer to :class:`mindspore.mint.nn.PReLU`.

    Raises:
        TypeError: If the `input` or the `weight` is not a Tensor.
        ValueError: If the `weight` is not a 0-D or 1-D Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)
        >>> weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)
        >>> output = ops.prelu(x, weight)
        >>> print(output)
        [[[-0.60 -0.50]
          [-2.40 -1.80]
          [ 0.60  0.30]]
         [[ 0.00  1.00]
          [ 2.00  3.00]
          [ 4.0   5.00]]]
    """
    return prelu_op(input, weight)


def assign(variable, value):
    r"""
    Assigns a parameter or tensor with a value.

    Support implicit type conversion and type promotion.

    Args:
        variable (Union[Parameter, Tensor]): The input parameter or tensor.
        value (Tensor): The value to be assigned.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> value = mindspore.tensor([2.0], mindspore.float32)
        >>> variable = mindspore.Parameter(mindspore.tensor([1.0], mindspore.float32), name="variable")
        >>> mindspore.ops.assign(variable, value)
        >>> print(variable.asnumpy())
        [2.]
    """
    return assign_op(variable, value)


def mla_preprocess(input1, gamma1, beta1, quant_scale1, quant_offset1, wdqkv, bias1, gamma2, beta2, quant_scale2, quant_offset2, gamma3, sin1, cos1, sin2, cos2, key_cache, slot_mapping, wuq, bias2, slot_wuk, de_scale1, de_scale2, ctkv_scale, qnope_scale, krope_cache, param_cache_mode=0):
    r"""
    
    """
    return mla_preprocess_op(input1, gamma1, beta1, quant_scale1, quant_offset1, wdqkv, bias1, gamma2, beta2, quant_scale2, quant_offset2, gamma3, sin1, cos1, sin2, cos2, key_cache, slot_mapping, wuq, bias2, slot_wuk, de_scale1, de_scale2, ctkv_scale, qnope_scale, krope_cache, param_cache_mode)


def acos(input):
    r"""
    Compute arccosine of each element in input tensors.

    .. math::

        out_i = \cos^{-1}(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.acos(mindspore.tensor([0.74, 0.04, 0.30, 0.56]))
        >>> print(output)
        [0.737726  1.5307857 1.2661036 0.9764105]
    """
    return acos_op(input)


def batch_norm_gather_stats_with_counts(input, mean, invstd, running_mean=None, running_var=None, momentum=1e-1, eps=1e-5, counts=None):
    r"""
    
    """
    return batch_norm_gather_stats_with_counts_op(input, mean, invstd, running_mean, running_var, momentum, eps, counts)


def cross_entropy_loss_grad(grad_loss, log_prob, target, weight=None, grad_zloss=None, lse_for_zloss=None, reduction='mean', ignore_index=-100, label_smoothing=0.0, lse_square_scale_for_zloss=0.0):
    r"""
    
    """
    return cross_entropy_loss_grad_op(grad_loss, log_prob, target, weight, grad_zloss, lse_for_zloss, reduction, ignore_index, label_smoothing, lse_square_scale_for_zloss)


def get_mem(target, target_offset, src, src_offset, size, target_pe=0, non_blocking=False):
    r"""
    
    """
    return get_mem_op(target, target_offset, src, src_offset, size, target_pe, non_blocking)


def selu_grad(gradient, result):
    r"""
    
    """
    return selu_grad_op(gradient, result)


def inplace_fill_tensor(input, value):
    r"""
    
    """
    return inplace_fill_tensor_op(input, value)


def outer_ext(input, vec2):
    r"""
    Return outer product of `input` and `vec2`. If `input` is a vector of size :math:`n`
    and `vec2` is a vector of size :math:`m` , then output must be a matrix of shape :math:`(n, m)` .

    .. note::
        This function does not broadcast.

    Args:
        input (Tensor): 1-D input vector.
        vec2 (Tensor): 1-D input vector.

    Returns:
        out, 2-D matrix, the outer product of two vectors.

    Raises:
        TypeError: If `input` or `vec2` is not a Tensor.
        TypeError: The implicitly converted data types of `input` and `vec2` are not one of float16, float32, float64, bool, uint8, int8, int16, int32, int64, complex64, complex128, bfloat16
        ValueError: If the dimension of `input` or `vec2` is not equal to 1.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> input = Tensor(np.array([7, 8, 9]), mindspore.int32)
        >>> vec2 = Tensor(np.array([7, 10, 11]), mindspore.int32)
        >>> out = ops.outer(input, vec2)
        >>> print(out)
        [[49 70 77]
         [56 80 88]
         [63 90 99]]
    """
    return outer_op(input, vec2)


def inplace_muls(input, other):
    r"""
    
    """
    return inplace_muls_op(input, other)


def sin(input):
    r"""
    Compute sine of the input tensor element-wise.

    .. math::

        output_i = \sin(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0.62, 0.28, 0.43, 0.62], mindspore.float32)
        >>> output = mindspore.ops.sin(input)
        >>> print(output)
        [0.58103514 0.27635565 0.4168708 0.58103514]
    """
    return sin_op(input)


def signal_op(signal, signal_offset, signal_value, signal_op='set', target_pe=0):
    r"""
    
    """
    return signal_op_op(signal, signal_offset, signal_value, signal_op, target_pe)


def topprouter(input, capacity, expert_num, drop_type=0, threshold=0.0, router_prob=0.0):
    r"""
    TopPRouter implementation in MOE.

    Inputs:
        - **x** (Tensor) - Input Tensor of 3D, supporting types:[int32, int64]
        - **capacity** (Int64) - The maximum number of tokens each expert can handle.
        - **expert_num** (Int64) - The number of expert.
        - **drop_type** (Int64) - S-Drop/K-Drop, 0 means S-Drop, 1 means K-Drop, default 0.
        - **threshold** (float32) - Expert threshold, default 0.
        - **router_prob** (Tensor) - Topk prob Tensor of 2D, supporting types:[float32], default 0.

    Outputs:
        tuple(Tensor), tuple of 2 tensors, `dispatch_index` and `combine_inex`.

        - dispatch_index (Tensor) - Token ID processed by each expert.
        - combine_index (Tensor) - The combine index of each token.

    Supported Platforms:
        ``Ascend``
    """
    return topprouter_op(input, capacity, expert_num, drop_type, threshold, router_prob)


def argsort_ext(input, dim=-1, descending=False, stable=False):
    r"""
    Return the indices that sort the tensor along the specified dimension.

    .. warning::
            This is an experimental API that may change or be removed.

    Args:
        input (Tensor): The input tensor.
        dim (int, optional): Specify the dimension to sort along. Default ``-1``.
            The Ascend backend only supports sorting the last dimension.
        descending (bool, optional): Sort order(ascending or descending).Default ``False``.
        stable (bool, optional): Control the relative order of equivalent elements. Default ``False``.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([[8, 2, 1], [5, 9, 3], [4, 6, 7]], mindspore.float16)
        >>> sort = mindspore.ops.auto_generate.argsort_ext(x)
        >>> print(sort)
        [[2 1 0]
         [2 0 1]
         [0 1 2]]
    """
    return argsort_op(input, dim, descending, stable)


def gather(input_params, input_indices, axis, batch_dims=0):
    r"""
    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.

    The following figure shows the calculation process of Gather commonly:

    .. image:: ../images/Gather.png

    where params represents the input `input_params`, and indices represents the index to be sliced `input_indices`.

    .. note::
        - The value of input_indices must be in the range of `[0, input_param.shape[axis])`.
          On CPU and GPU, an error is raised if an out of bound indice is found. On Ascend, the results may be
          undefined.
        - The data type of input_params cannot be `mindspore.bool` .
        - The shape of returned tensor is :math:`input\_params.shape[:axis] + input\_indices.shape[batch\_dims:] + input\_params.shape[axis + 1:]` .

    Args:
        input_params (Tensor): The input Tensor.
        input_indices (Tensor): The specified indices.
        axis (Union(int, Tensor[int])): The specified axis.
        batch_dims (int): The number of batch dimensions. Default ``0`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case1: input_indices is a Tensor with shape (5, ).
        >>> input_params = mindspore.tensor([1, 2, 3, 4, 5, 6, 7], mindspore.float32)
        >>> input_indices = mindspore.tensor([0, 2, 4, 2, 6], mindspore.int32)
        >>> axis = 0
        >>> output = mindspore.ops.gather(input_params, input_indices, axis)
        >>> print(output)
        [1. 3. 5. 3. 7.]
        >>> # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,
        >>> # the output shape is equal to the input_indices shape.
        >>> input_indices = mindspore.tensor([[0, 2], [2, 6]], mindspore.int32)
        >>> axis = 0
        >>> output = mindspore.ops.gather(input_params, input_indices, axis)
        >>> print(output)
        [[1. 3.]
         [3. 7.]]
        >>> # case3: input_indices is a Tensor with shape (2, ) and
        >>> # input_params is a Tensor with shape (3, 4) and axis is 0.
        >>> input_params = mindspore.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], mindspore.float32)
        >>> input_indices = mindspore.tensor([0, 2], mindspore.int32)
        >>> axis = 0
        >>> output = mindspore.ops.gather(input_params, input_indices, axis)
        >>> print(output)
        [[ 1.  2.  3.  4.]
         [ 9. 10. 11. 12.]]
        >>> # case4: input_indices is a Tensor with shape (2, ) and
        >>> # input_params is a Tensor with shape (3, 4) and axis is 1, batch_dims is 1.
        >>> input_params = mindspore.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], mindspore.float32)
        >>> input_indices = mindspore.tensor([0, 2, 1], mindspore.int32)
        >>> axis = 1
        >>> batch_dims = 1
        >>> output = mindspore.ops.gather(input_params, input_indices, axis, batch_dims)
        >>> print(output)
        [ 1.  7. 10.]
    """
    gather_op = _get_cache_prim(Gather)(batch_dims)
    return gather_op(input_params, input_indices, axis)


def clamp_scalar(input, min=None, max=None):
    r"""
    Clamps tensor values between the specified minimum value and maximum value.

    Limits the value of :math:`input` to a range, whose lower limit is `min` and upper limit is `max` .

    .. math::

        out_i= \left\{
        \begin{array}{align}
            max & \text{ if } input_i\ge max \\
            input_i & \text{ if } min \lt input_i \lt max \\
            min & \text{ if } input_i \le min \\
        \end{array}\right

    Note:
        - `min` and `max` cannot be None at the same time;
        - When `min` is None and `max` is not None, the elements in Tensor larger than `max` will become `max`;
        - When `min` is not None and `max` is None, the elements in Tensor smaller than `min` will become `min`;
        - If `min` is greater than `max`, the value of all elements in Tensor will be set to `max`;
        - The data type of `input`, `min` and `max` should support implicit type conversion and cannot be bool type.

    Args:
          input (Tensor): Input data, which type is Tensor. Tensors of arbitrary dimensions are supported.
          min (Union(float, int), optional): The minimum value. Default: ``None`` .
          max (Union(float, int), optional): The maximum value. Default: ``None`` .

    Returns:
          Tensor, a clipped Tensor.
          The data type and shape are the same as input.

    Raises:
          ValueError: If both `min` and `max` are None.
          TypeError: If the type of `input` is not Tensor.
          TypeError: If the type of `min` is not in None, float or int.
          TypeError: If the type of `max` is not in None, float or int.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> # case 1: the data type of input is number
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore.ops.auto_generate import clamp_scalar
        >>> import numpy as np
        >>> min_value = 5
        >>> max_value = 20
        >>> input = Tensor(np.array([[1., 25., 5., 7.], [4., 11., 6., 21.]]), mindspore.float32)
        >>> output = clamp_scalar(input, min_value, max_value)
        >>> print(output)
        [[ 5. 20.  5.  7.]
        [ 5. 11.  6. 20.]]
    """
    return clamp_scalar_op(input, min, max)


def cumsum_ext(input, dim, dtype=None):
    r"""
    Computes the cumulative sum of input Tensor along `dim`.

    .. math::

        y_i = x_1 + x_2 + x_3 + ... + x_i

    Args:
        input (Tensor): The input Tensor.
        dim (int): Dim along which the cumulative sum is computed.
        dtype (:class:`mindspore.dtype`, optional): The desired dtype of returned Tensor. If specified,
            the input Tensor will be cast to `dtype` before the computation. This is useful for preventing overflows.
            If not specified, stay the same as original Tensor. Default: ``None`` .

    Returns:
        Tensor, the shape of the output Tensor is consistent with the input Tensor's.

    Raises:
        TypeError: If `input` is not a Tensor.
        ValueError: If the `dim` is out of range.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>> import mindspore.ops as ops
        >>> x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))
        >>> # case 1: along the dim 0
        >>> y = ops.auto_generate.cumsum_ext(x, 0)
        >>> print(y)
        [[ 3.  4.  6. 10.]
        [ 4. 10. 13. 19.]
        [ 8. 13. 21. 26.]
        [ 9. 16. 28. 35.]]
        >>> # case 2: along the dim 1
        >>> y = ops.auto_generate.cumsum_ext(x, 1)
        >>> print(y)
        [[ 3.  7. 13. 23.]
        [ 1.  7. 14. 23.]
        [ 4.  7. 15. 22.]
        [ 1.  4. 11. 20.]]
    """
    return cumsum_ext_op(input, dim, dtype)


def count_nonzero(input, dim=None):
    r"""
    Count the number of non-zero elements in the Tensor `input` on a given dimension `dim`. If no dim is specified then all non-zeros in the tensor are counted.

    Args:
        input (Tensor): Input data is used to count non-zero numbers. With shape
            :math:`(*)` where :math:`*` means, any number of additional dimensions.
        dim (Union[None, int, tuple(int), list(int)], optional): Count the dimension of the number of non-zero elements.
            Default value: ``None``, which indicates that the number of non-zero elements is calculated.
            If `dim` is ``None``, all elements in the tensor are summed up.

    Returns:
        Tensor, number of nonzero element across dim specified by `dim`.

    Raises:
        TypeError: If `input` is not tensor.
        TypeError: If `dim` is not int, tuple(int), list(int) or None.
        ValueError: If any value in `dim` is not in range :math:`[-input.ndim, input.ndim)`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> import mindspore
        >>> # case 1: each value specified.
        >>> x = Tensor(np.array([[0, 1, 0], [1, 1, 0]]).astype(np.float32))
        >>> nonzero_num = ops.count_nonzero(input=x, dim=[0, 1])
        >>> print(nonzero_num)
        3
        >>> # case 2: all value is default.
        >>> nonzero_num = ops.count_nonzero(input=x)
        >>> print(nonzero_num)
        3
        >>> # case 3: dim value was specified 0.
        >>> nonzero_num = ops.count_nonzero(input=x, dim=[0,])
        >>> print(nonzero_num)
        [1 2 0]
        >>> # case 4: dim value was specified 1.
        >>> nonzero_num = ops.count_nonzero(input=x, dim=[1,])
        >>> print(nonzero_num)
        [1 2]
    """
    return count_nonzero_op(input, dim)


def log_softmax_ext(input, dim=None, dtype=None):
    r"""
    Applies the Log Softmax function to the input tensor on the specified axis.
    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,
    the Log Softmax function is shown as follows:

    .. math::
        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),

    where :math:`N` is the length of the Tensor.

    Args:
        input (Tensor): The input Tensor.
        dim (int, optional): The axis to perform the Log softmax operation. Default: ``None`` .

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired dtype of returned Tensor. If not set to None, the input
            Tensor will be cast to `dtype` before the operation is performed. This is useful for preventing overflows.
            If set to None, stay the same as original Tensor. Default: ``None`` . Supported data type is {float16, float32, double, bfloat16}.

    Returns:
        Tensor, with the same shape as the input.

    Raises:
        TypeError: If `dim` is not an int.
        ValueError: If `dim` is not in range [-len(input.shape), len(input.shape)).

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)
        >>> output = ops.auto_generate.log_softmax(logits, dim=-1)
        >>> print(output)
        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]
    """
    return log_softmax_ext_op(input, dim, dtype)


def rsqrt(input):
    r"""
    Compute reciprocal of square root of input tensor element-wise.

    .. math::

        out_{i} =  \frac{1}{\sqrt{input_{i}}}

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([-0.0370,  0.2970,  1.5420, -0.9105])
        >>> output = mindspore.ops.rsqrt(input)
        >>> print(output)
        [       nan 1.8349396  0.8053002        nan]
    """
    return rsqrt_op(input)


def ceil(input):
    r"""
    Rounds a tensor up to the closest integer element-wise.

    .. math::
        out_i = \lceil input_i \rceil

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor(([1.1, 2.5, -1.5]))
        >>> output = mindspore.ops.ceil(input)
        >>> print(output)
        [ 2.  3. -1.]
    """
    return ceil_op(input)


def clone(input):
    r"""
    Returns a copy of the input tensor.

    Note:
        This function is differentiable, and gradients will flow back directly from the calculation
        result of the function to the `input`.

    Args:
        input (Tensor): A tensor to be copied.

    Returns:
        Tensor, with the same data, shape and type as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.ones((3,3)).astype("float32"))
        >>> output = ops.auto_generate.clone(input)
        >>> print(output)
        [[1. 1. 1.]
         [1. 1. 1.]
         [1. 1. 1.]]
    """
    return clone_op(input)


def select(condition, input, other):
    r"""
    The conditional tensor determines whether the corresponding element in the output must be
    selected from `input` (if True) or `other` (if False) based on the value of each
    element.

    It can be defined as:

    .. math::
        out_i = \begin{cases}
        input_i, & \text{if } condition_i \\
        other_i, & \text{otherwise}
        \end{cases}

    Args:
        condition (Tensor[bool]): The condition tensor.
        input (Union[Tensor, int, float]): The first tensor or number to be selected.
        other (Union[Tensor, int, float]): The second tensor or number to be selected.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case1: Both `input` and `other` are tensor
        >>> cond = mindspore.tensor([True, False])
        >>> x = mindspore.tensor([2,3], mindspore.float32)
        >>> y = mindspore.tensor([1,2], mindspore.float32)
        >>> output1 = mindspore.ops.select(cond, x, y)
        >>> print(output1)
        [2. 2.]
        >>> # case2: Both `input` and `other` are number
        >>> output2 = mindspore.ops.select(cond, input=1, other=2)
        >>> print(output2)
        [1 2]
        >>> # case3: `input` is tensor and `other` is number
        >>> output3 = mindspore.ops.select(cond, x, other=3)
        >>> print(output3)
        [2. 3.]
    """
    return select_op(condition, input, other)


def range(start, end, step, maxlen=1000000):
    r"""
    Returns a tensor with a step length of `step` in the interval [ `start` , `end` ).

    .. note::
        - The types of all 3 inputs must be all integers or floating-point numbers.
        - When the input is a tensor, the tensor must contain only one element, whose dtype is Number.

    Args:
        start (Union[Number, Tensor]): The start value of the interval.
        end (Union[Number, Tensor]): The end value of the interval.
        step (Union[Number, Tensor]): The interval between each value.
        maxlen (int, optional): Memory that can fit `maxlen` many elements
            will be allocated for the output. Optional, must be positive. Default: 1000000.
            If the output has more than `maxlen` elements, a runtime error will occur.

    Returns:
        Tensor

    Supported Platforms:
        ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> mindspore.ops.range(0, 6, 2)
        Tensor(shape=[3], dtype=Int64, value= [0, 2, 4])
    """
    range_op = _get_cache_prim(Range)(maxlen)
    return range_op(start, end, step)


def irfft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Calculates the inverse of `rfft2()`.

    Note:
        - `irfft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `irfft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `irfft2`.
            Default: ``None`` , the dim[-1] of the `input` will be zero-padded to :math:`2*(input.shape[dim[-1]]-1)`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `irfft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `irfft2()` function, result.shape[dim[i]] is s[i].
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is float32.
        When the input is float64 or complex128, the return value type is float64.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> ops.irfft2(input, s=(4, 4), dim=(0, 1), norm="backward")
        Tensor(shape=[4, 4], dtype=Float32, value=
        [[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])
    """
    return irfft2_op(input, s, dim, norm)


def index_fill_scalar(input, dim, index, value):
    r"""
    
    """
    return index_fill_scalar_op(input, dim, index, value)


def hfftn(input, s=None, dim=None, norm=None):
    r"""
    Calculates the N dimensional discrete Fourier transform of of a Hermitian symmetric `input`.

    Note:
        - `hfftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `hfftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `hfftn`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `hfftn`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `hfftn()` function.
        If `s` is given, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`(s[-1] - 1) * 2`, otherwise :math:`(input.shape[dim[-1]] - 1) * 2`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.hfftn(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[16.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]]
    """
    return hfftn_op(input, s, dim, norm)


def cat(tensors, axis=0):
    r"""
    Connect input tensors along with the given axis.

    Args:
        tensors (Union[tuple[Tensor], list[Tensor]]): The input tensors.
            The shapes of all axes except the specified concatenation `axis` should be equal.
        axis (int): The specified axis. Default ``0`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input_x1 = mindspore.tensor([[0, 1], [2, 1]], mindspore.float32)
        >>> input_x2 = mindspore.tensor([[0, 1], [2, 1]], mindspore.float32)
        >>> output = mindspore.ops.cat((input_x1, input_x2))
        >>> print(output)
        [[0. 1.]
         [2. 1.]
         [0. 1.]
         [2. 1.]]
        >>> output = mindspore.ops.cat((input_x1, input_x2), 1)
        >>> print(output)
        [[0. 1. 0. 1.]
         [2. 1. 2. 1.]]
    """
    return concat_impl(tensors, axis)


def add_rms_norm(x1, x2, gamma, epsilon=1e-6):
    r"""
    The AddRmsNorm is a fusion operator that fusing RmsNorm and its preceding Add operator, reducing the time for
    moving data in and out.
    It computes the following expression:

    .. math::
        \begin{array}{ll} \\
            x_i = x1_i + x2_i \\
            y_i=RmsNorm(x_i)=\frac{x_i}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}{ x_i^2}+\varepsilon}}\gamma_i
        \end{array}

    .. warning::
        This is an experimental API that is subject to change or deletion. This API is only supported in Atlas A2
        training series for now.

    Args:
        x1 (Tensor): Input data of AddRmsNorm. Support data type: float16, float32, bfloat16.
        x2 (Tensor): Input data of AddRmsNorm. Support data type: float16, float32, bfloat16.
        gamma (Tensor): Learnable parameter :math:`\gamma` . Support data type: float16, float32, bfloat16.
        epsilon (float, optional): A float number ranged in (0, 1] to prevent division by 0. Default value is `1e-6`.

    Returns:
        - Tensor, denotes the normalized result, has the same type and shape as `x1`.
        - Tensor, with the float data type, denotes the reciprocal of the input standard deviation, used by gradient
          calculation.
        - Tensor, the sum of `x1` and `x2`.

    Raises:
        TypeError: If data type of `x1` or `x2` is not one of the following: float16, float32, bfloat16.
        TypeError: If data type of `gamma` is not one of the following: float16, float32, bfloat16.
        ValueError: If `epsilon` is not a float between 0 and 1.
        ValueError: If the rank of `gamma` is greater than the rank of `x1` or `x2`.
        RuntimeError: If the shapes of `x1` and `x2` are not same.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x1 = Tensor(np.array([[0.5, 1.0, 1.5], [0.5, 1.0, 1.5]]), mindspore.float32)
        >>> x2 = Tensor(np.array([[0.5, 1.0, 1.5], [0.5, 1.0, 1.5]]), mindspore.float32)
        >>> gamma = Tensor(np.ones([3]), mindspore.float32)
        >>> y, rstd = ops.add_rms_norm(x1, x2, gamma)
        >>> print(y)
        [[0.46290997  0.92581993  1.3887299]
         [0.46290997  0.92581993  1.3887299]]
        >>> print(rstd)
        [[0.46290997]
         [0.46290997]]
    """
    return add_rms_norm_op(x1, x2, gamma, epsilon)


def extract_image_patches(input_x, ksizes, strides, rates, padding='VALID'):
    r"""
    Extracts patches from images.
    The input tensor must be a 4-D tensor and the data format is NCHW.

    Args:
        input_x (Tensor): A 4-D tensor whose shape is :math:`(in\_batch, in\_depth, in\_row, in\_col)`.
        ksizes (Union[tuple[int], list[int]]): The size of sliding window, must be a tuple or a list of integers,
            the size must be 4, and the format is [1, 1, ksize_row, ksize_col].
        strides (Union[tuple[int], list[int]]): Distance between the centers of the two consecutive patches,
            must be a tuple or list of int, the size must be 4, and the format is [1, 1, stride_row, stride_col].
        rates (Union[tuple[int], list[int]]): In each extracted patch, the gap between the corresponding dimension
            pixel positions, must be a tuple or a list of integers, the size must be 4, and the format is [1, 1, rate_row, rate_col].
        padding (str): The type of padding algorithm, is a string whose value is "same" or "valid",
            not case sensitive. Default: "valid".

            - same: Means that the patch can take the part beyond the original image, and this part is filled with 0.

            - valid: Means that the taken patch area must be completely covered in the original image.

    Outputs:
        Tensor, a 4-D tensor whose data type is same as 'input_x', and the shape
        is :math:`(out\_batch, out\_depth, out\_row, out\_col)`,where the out_batch is the same as the in_batch
        and

        .. math::
            out_depth=ksize\_row * ksize\_col * in\_depth

        and
        if 'padding' is "valid":

        .. math::
            out\_row=floor((in\_row - (ksize\_row + (ksize\_row - 1) * (rate\_row - 1))) / stride\_row) + 1
            out\_col=floor((in\_col - (ksize\_col + (ksize\_col - 1) * (rate\_col - 1))) / stride\_col) + 1

        if 'padding' is "same":

        .. math::
            out\_row=floor((in\_row - 1) / stride\_row) + 1
            out\_col=floor((in\_col - 1) / stride\_col) + 1

    Supported Platforms:
        ``Ascend`` ``GPU``
    """
    extract_image_patches_op = _get_cache_prim(ExtractImagePatches)(ksizes, strides, rates, padding)
    return extract_image_patches_op(input_x)


def moe_token_unpermute_grad(permuted_tokens, unpermuted_tokens_grad, sorted_indices, probs=None, padded_mode=False, restore_shape=None):
    r"""
    
    """
    return moe_token_unpermute_grad_op(permuted_tokens, unpermuted_tokens_grad, sorted_indices, probs, padded_mode, restore_shape)


def div_scalar_(input, other):
    r"""
    
    """
    return inplace_divs_op(input, other)


def batch_norm_stats(input, eps):
    r"""
    
    """
    return batch_norm_stats_op(input, eps)


def moe_token_permute_grad(permuted_tokens_grad, sorted_indices, num_topk=1, padded_mode=False):
    r"""
    
    """
    return moe_token_permute_grad_op(permuted_tokens_grad, sorted_indices, num_topk, padded_mode)


def atan2(input, other):
    r"""
    Returns arctangent of input/other element-wise.

    It returns :math:`\theta\ \in\ [-\pi, \pi]`
    such that :math:`input = r*\sin(\theta), other = r*\cos(\theta)`, 
    where :math:`r = \sqrt{input^2 + other^2}`.

    Args:
        input (Tensor, Number.number): The input tensor or scalar.
        other (Tensor, Number.number): The input tensor or scalar.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.atan2(mindspore.tensor([0., 1.]), mindspore.tensor([1., 1.]))
        >>> print(output)
        [0.        0.7853982]
    """
    return atan2_op(input, other)


def ihfft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the inverse of `hfft()`.

    Note:
        - `ihfft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ihfft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64.

        n (int, optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim` axis will be zero-padded or truncated to `n` before calculating `ihfft`.
            Default: ``None`` , which does not need to process `input`.
        dim (int, optional): The dimension along which to take the one dimensional `ihfft`.
            Default: ``-1`` , which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as,

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1*n`).
            - ``"ortho"`` (normalize by :math:`1*\sqrt{n}`).

    Returns:
        Tensor, The result of `ihfft()` function.
        If `n` is given, result.shape[dim] is :math:`n // 2 + 1`, otherwise math:`input.shape[dim] // 2 + 1`.
        When the input is int16, int32, int64, float16, float32, the return value type is complex64.
        When the input is float64, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([ 1.6243454, -0.6117564, -0.5281718, -1.0729686])
        >>> out = ops.ihfft(input, n=4, dim=-1, norm="backward")
        >>> print(out)
        [-0.14713785-0.j          0.5381293 +0.11530305j  0.69522464-0.j        ]
    """
    return ihfft_op(input, n, dim, norm)


def narrow_view(input, dim, start, length):
    r"""
    Obtains a tensor of a specified length at a specified start position along a specified axis.

    Args:
        input (Tensor): the tensor to narrow.
        dim (int): the axis along which to narrow.
        start (Union[int, Tensor[int]]): the starting dimension.
        length (int): the distance to the ending dimension.

    Returns:
        output (Tensors) - The narrowed tensor.

    Raises:
        ValueError: the rank of `input` is 0.
        ValueError: the value of `dim` is out the range [-input.ndim, input.ndim).
        ValueError: the value of `start` is out the range [-input.shape[dim], input.shape[dim]].
        ValueError: the value of `length` is out the range [0, input.shape[dim]-start].

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import ops
        >>> from mindspore import Tensor
        >>> x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mindspore.int32)
        >>> output = ops.auto_generate.narrow_view(x, 0, 0, 2)
        >>> print(output)
        [[ 1 2 3]
         [ 4 5 6]]
        >>> output = ops.auto_generate.narrow_view(x, 1, 1, 2)
        >>> print(output)
        [[ 2 3]
         [ 5 6]
         [ 8 9]]
    """
    return narrow_view_op(input, dim, start, length)


def rfftfreq(n, d=1.0, dtype=None):
    r"""
    Computes the sample frequencies for `rfft` with a signal of size `n`.
    For instance, Given a length `n` and a sample spacing `d` , the returned result `f` is:

    .. math::
        f = [0, 1, ..., n // 2] / (d * n)

    Note:
        - `rfftfreq` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `rfftfreq` is not supported on Windows platform yet.

    Args:
        n (int): Window length.
        d (float, optional): Sample spacing (inverse of the sampling rate). Default: ``1.0`` .
        dtype (mindspore.dtype, optional): The dtype of the returned frequencies. Default: ``None`` represents float32.

    Returns:
        Tensor, Array of length ``n`` containing the sample frequencies.

    Raises:
        ValueError: If `n` is less than 1.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import ops
        >>> out = ops.rfftfreq(n=4, d=1.0)
        >>> print(out)
        [0.   0.25 0.5 ]
    """
    return rfftfreq_op(n, d, dtype)


def nan_to_num(input, nan=None, posinf=None, neginf=None):
    r"""
    Replace the `NaN`, positive infinity and negative infinity values in `input` with the
    specified values in `nan`, `posinf` and `neginf` respectively.

    .. warning::
        For Ascend, it is only supported on Atlas A2 Training Series Products.
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.
        nan (number, optional): The replace value of `NaN`. Default ``None``.
        posinf (number, optional): the value to replace `posinf` values with. Default ``None``,
            replacing `posinf` with the maximum value supported by the data type of `input`.
        neginf (number, optional): the value to replace `neginf` values with. Default ``None``,
            replacing `neginf` with the minimum value supported by the data type of `input`.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([float('nan'), float('inf'), -float('inf'), 5.0], mindspore.float32)
        >>> output = mindspore.ops.nan_to_num(input)
        >>> print(output)
        [ 0.0000000e+00  3.4028235e+38 -3.4028235e+38  5.0000000e+00]
        >>> output = mindspore.ops.nan_to_num(input, 1.0)
        >>> print(output)
        [ 1.0000000e+00  3.4028235e+38 -3.4028235e+38  5.0000000e+00]
        >>> output = mindspore.ops.nan_to_num(input, 1.0, 2.0)
        >>> print(output)
        [ 1.0000000e+00  2.0000000e+00 -3.4028235e+38  5.0000000e+00]
        >>> output = mindspore.ops.nan_to_num(input, 1.0, 2.0, 3.0)
        >>> print(output)
        [1.  2.  3.  5.0]
    """
    return nan_to_num_impl(input, nan, posinf, neginf)


def floor_(input):
    r"""
    
    """
    return inplace_floor_op(input)


def get_data(input):
    r"""
    
    """
    return get_data_op(input)


def fftshift(input, dim=None):
    r"""
    Shift the zero-frequency component to the center of the spectrum.

    Note:
        - `fftshift` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `fftshift` is not supported on Windows platform yet.

    Args:
        input (Tensor): Input tensor.
        dim (Union[int, list(int), tuple(int)], optional): The dimensions which to shift.
            Default is ``None``, which shifts all dimensions.

    Returns:
        output (Tensor), the shifted tensor with the same shape and dtype as `input`.

    Raises:
        TypeError: If `input` is not a tensor.
        TypeError: If the type/dtype of `dim` is not int.
        ValueError: If `dim` is out of the range of :math:`[-input.ndim, input.ndim)`.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> from mindspore.ops import fftshift
        >>> from mindspore import Tensor
        >>> from mindspore import dtype as mstype
        >>> input = Tensor([0, 1, 2, 3, 4, -5, -4, -3, -2, -1], dtype=mstype.int32)
        >>> fftshift(input)
        Tensor(shape=[10], dtype=Int32, value= [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4])
    """
    return fftshift_op(input, dim)


def mul(input, other):
    r"""
    Multiplies two tensors element-wise.

    .. math::

        out_{i} = input_{i} * other_{i}

    Note:
        - When the two inputs have different shapes,
          they must be able to broadcast to a common shape.
        - The two inputs can not be bool type at the same time,
          [True, Tensor(True), Tensor(np.array([True]))] are all considered bool type.
        - Support implicit type conversion and type promotion.

    Args:
        input (Union[Tensor, number.Number, bool]): The first input.
        other (Union[Tensor, number.Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> # case 1: The shape of two inputs are different
        >>> import mindspore
        >>> x = mindspore.tensor([1.0, 2.0, 3.0], mindspore.float32)
        >>> output = mindspore.ops.mul(x, 100)
        >>> print(output)
        [100. 200. 300.]
        >>> # case 2: The shape of two inputs are the same
        >>> import mindspore
        >>> x = mindspore.tensor([1.0, 2.0, 3.0], mindspore.float32)
        >>> y = mindspore.tensor([4.0, 5.0, 6.0], mindspore.float32)
        >>> output = mindspore.ops.mul(x, y)
        >>> print(output)
        [ 4. 10. 18.]
    """
    return mul_op(input, other)


def atanh(input):
    r"""
    Computes inverse hyperbolic tangent of the input element-wise.

    .. math::

        out_i = \tanh^{-1}(input_{i})

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.atanh(mindspore.tensor([0, -0.5]))
        >>> print(output)
        [ 0.         -0.54930615]
    """
    return atanh_op(input)


def strided_slice(input_x, begin, end, strides, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0):
    r"""
    Extracts a strided slice of a Tensor based on `begin/end` index and `strides`.

    Note:
        - `begin` , `end` and `strides` must have the same shape.
        - `begin` , `end` and `strides` are all 1-D Tensor,  and their shape size
          must not greater than the dim of `input_x`.

    Example: For `input_x` with shape :math:`(5, 6, 7)`,
    set `begin`, `end` and `strides` to (1, 3, 2), (3, 5, 6),
    (1, 1, 2) respectively, then elements from index 1 to 3 are extrected for dim 0, index 3 to 5
    are extrected for dim 1 and index 2 to 6 with a `stirded` of 2 are extrected for dim 2, this
    process is equivalent to a pythonic slice `input_x[1:3, 3:5, 2:6:2]`.

    If the length of `begin`, `end` and `strides` is smaller than the dim of `input_x`,
    then all elements are extracted from the missing dims, it behaves like all the
    missing dims are filled with zeros, size of that missing dim and ones.

    Example: For Tensor `input_x` with shape :math:`(5, 6, 7)`,
    set `begin`, `end` and `strides` to (1, 3),
    (3, 5), (1, 1) respectively, then elements from index 1 to 3 are extrected
    for dim 0, index 3 to 5 are extrected for dim 1 and index 3 to 5 are extrected
    for dim 2, this process is equivalent to a pythonic slice `input_x[1:3, 3:5, 0:7]`.

    Here's how a mask works:
    For each specific mask, it will be converted to a binary representation internally, and then
    reverse the result to start the calculation. For Tensor `input_x` with
    shape :math:`(5, 6, 7)`. Given mask value of 3 which
    can be represented as 0b011. Reverse that we get 0b110, which implies the first and second dim of the
    original Tensor will be effected by this mask. See examples below, for simplicity all mask mentioned
    below are all in their reverted binary form:

    - `begin_mask` and `end_mask`

      If the ith bit of `begin_mask` is 1, `begin[i]` is ignored and the fullest
      possible range in that dimension is used instead. `end_mask` is analogous,
      except with the end range. For Tensor `input_x` with shape :math:`(5, 6, 7, 8)`, if `begin_mask`
      is 0b110, `end_mask` is 0b011, the slice `input_x[0:3, 0:6, 2:7:2]` is produced.

    - `ellipsis_mask`

      If the ith bit of `ellipsis_mask` is 1, as many unspecified dimensions as needed
      will be inserted between other dimensions. Only one non-zero bit is allowed
      in `ellipsis_mask`. For Tensor `input_x` with shape :math:`(5, 6, 7, 8)`,  `input_x[2:,...,:6]`
      is equivalent to `input_x[2:5,:,:,0:6]` ,  `input_x[2:,...]` is equivalent
      to `input_x[2:5,:,:,:]`.

    - `new_axis_mask`

      If the ith bit of `new_axis_mask` is 1, `begin`, `end` and `strides` are
      ignored and a new length 1 dimension is added at the specified position
      in the output Tensor. For Tensor `input_x` with shape :math:`(5, 6, 7)`, if `new_axis_mask`
      is 0b110,  a new dim is added to the second dim, which will produce
      a Tensor with shape :math:`(5, 1, 6, 7)`.

    - `shrink_axis_mask`

      If the ith bit of `shrink_axis_mask` is 1, `begin`, `end` and `strides`
      are ignored and dimension i will be shrunk to 0.
      For Tensor `input_x` with shape :math:`(5, 6, 7)`,
      if `shrink_axis_mask` is 0b010, it is equivalent to slice `x[:, 5, :]`
      and results in an output shape of :math:`(5, 7)`.

    Note:
        `new_axis_mask` and  `shrink_axis_mask` are not recommended to
        use at the same time, it might incur unexpected result.

    Args:
        input_x (Tensor): The input tensor.
        begin (tuple[int]): Specify the index to start slicing.
        end (tuple[int]): Specify the index to end slicing.
        strides (tuple[int]): Specify the step size for slicing in each dimension.
        begin_mask (int, optional): Starting index of the slice. Default ``0`` .
        end_mask (int, optional): Ending index of the slice. Default ``0`` .
        ellipsis_mask (int, optional): An int mask, ignore slicing operation when set to 1. Default ``0`` .
        new_axis_mask (int, optional): An int mask for adding new dims. Default ``0`` .
        shrink_axis_mask (int, optional): An int mask for shrinking dims. Default ``0`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[[1, 1, 1], 
        ...                            [2, 2, 2]],
        ...                           [[3, 3, 3], 
        ...                            [4, 4, 4]],
        ...                           [[5, 5, 5], 
        ...                            [6, 6, 6]]])
        >>> mindspore.ops.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) 
        Tensor(shape=[1, 1, 3], dtype=Int64, value=
        [[[3, 3, 3]]])
        >>> mindspore.ops.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) 
        Tensor(shape=[1, 2, 3], dtype=Int64, value=
        [[[3, 3, 3],
          [4, 4, 4]]])
        >>> mindspore.ops.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) 
        Tensor(shape=[1, 2, 3], dtype=Int64, value=
        [[[4, 4, 4],
          [3, 3, 3]]])
    """
    strided_slice_op = _get_cache_prim(StridedSlice)(begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)
    return strided_slice_op(input_x, begin, end, strides)


def ifft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Computes the two dimensional inverse discrete Fourier transform of `input`.

    Note:
        - `ifft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ifft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `ifft2`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `ifft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `ifft2()` function. The default is the same shape as `input`.
        If `s` is given, the size of the `dim[i]` axis is changed to `s[i]`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.ifft2(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[1.+0.j 0.+0.j 0.+0.j 0.+0.j]
         [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
         [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
         [0.+0.j 0.+0.j 0.+0.j 0.+0.j]]
    """
    return ifft2_op(input, s, dim, norm)


def matmul_ext(input, other):
    r"""
    
    """
    return matmul_ext_op(input, other)


def transpose(input, dims):
    r"""
    Transpose dimensions of the input tensor according to input permutation.

    Note:
        On GPU and CPU, if the value of `dims` is negative, its actual value is `dims[i] + rank(input)`.
        Negative value of `dims` is not supported on Ascend.

    Args:
        input (Tensor): The input tensor.
        dims (Union[tuple[int], list[int]]): Specify the new axis ordering.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], mindspore.float32)
        >>> output = mindspore.ops.transpose(input, (0, 2, 1))
        >>> print(output)
        [[[ 1.  4.]
          [ 2.  5.]
          [ 3.  6.]]
         [[ 7. 10.]
          [ 8. 11.]
          [ 9. 12.]]]
    """
    return transpose_op(input, dims)


def expm1(input):
    r"""
    Compute exponential of the input tensor, then minus 1, element-wise.

    .. math::

      out_i = e^{x_i} - 1

    Args:
      input (Tensor): The input tensor.

    Returns:
      Tensor

    Supported Platforms:
      ``Ascend`` ``GPU`` ``CPU``

    Examples:
      >>> import mindspore
      >>> input = mindspore.tensor([0.0, 1.0, 3.0], mindspore.float32)
      >>> output = mindspore.ops.expm1(input)
      >>> print(output)
      [ 0.         1.7182817 19.085537 ]
    """
    return expm1_op(input)


def fused_add_topk_div(x, add_num, group_num, group_topk, n, k, activate_type=0, is_norm=True, scale=2.5, mapping_num=None, mapping_table=None, enable_expert_mapping=False):
    r"""
    
    """
    return fused_add_topk_div_op(x, add_num, group_num, group_topk, n, k, activate_type, is_norm, scale, mapping_num, mapping_table, enable_expert_mapping)


def view_dtype(input, dtype):
    r"""
    
    """
    return view_dtype_op(input, dtype)


def argmax_ext(input, dim=None, keepdim=False):
    r"""
    argmax(input) -> Tensor

    Return the indices of the maximum values of a tensor.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> x = mindspore.tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))
        >>> output = mindspore.ops.auto_generate.argmax_ext(x)
        >>> print(output)
        6

    .. function:: argmax(input, dim, keepdim=False) -> Tensor
        :noindex:

    Return the indices of the maximum values of a tensor across a dimension.

    Args:
        input (Tensor): The input tensor.
        dim (int): The dimension to reduce. 
        keepdim (bool, optional): Whether the output tensor retains the specified
            dimension. Default ``False`` .

    Returns:
        Tensor, indices of the maximum values across a dimension.

    Raises:
        ValueError: If `dim` is out of range.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> x = mindspore.tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))
        >>> output = mindspore.ops.auto_generate.argmax_ext(x, dim=-1)
        >>> print(output)
        [1 0 0]
    """
    return argmax_ext_op(input, dim, keepdim)


def create_symmetric_memory(shape, dtype, group):
    r"""
    
    """
    return create_symmetric_memory_op(shape, dtype, group)


def stack_ext(tensors, dim=0):
    r"""
    Stacks a list of tensors in specified dim.

    Stacks the list of input tensors with the same rank `R`, output is a tensor of rank `(R+1)`.

    Given input tensors of shape :math:`(x_1, x_2, ..., x_R)`. Set the number of input tensors as `N`.
    If :math:`dim \ge 0`, the shape of the output tensor is
    :math:`(x_1, x_2, ..., x_{dim}, N, x_{dim+1}, ..., x_R)`.

    Args:
        tensors (Union[tuple, list]): A Tuple or list of Tensor objects with the same shape.
        dim (int, optional): Dimension to stack. The range is [-(R+1), R+1). Default: ``0`` .

    Returns:
        A stacked Tensor.

    Raises:
        ValueError: If `dim` is out of the range [-(R+1), R+1);
                    or if the shapes of elements in `tensors` are not the same.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> data1 = Tensor(np.array([0, 1]).astype(np.float32))
        >>> data2 = Tensor(np.array([2, 3]).astype(np.float32))
        >>> output = ops.auto_generate.stack_ext([data1, data2], 0)
        >>> print(output)
        [[0. 1.]
         [2. 3.]]
    """
    return stack_ext_impl(tensors, dim)


def take(input, index):
    r"""
    Select the input element at the given index.

    .. warning::

        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): Input tensor.
        index (LongTensor): The index tensor of input tensor.

    Returns:
        Tensor, has the same data type as index tensor.

    Raises:
        TypeError: If `input` is not tensor.
        TypeError: If the dtype of `index` is not long type.

    Examples:
        >>> import mindspore as ms
        >>> from mindspore import ops, Tensor        
        >>> input = Tensor([[4, 3, 5],[6, 7, 8]], ms.float32)
        >>> index = Tensor([0, 2, 5], ms.int64)
        >>> output = ops.take(input, index)
        >>> print(output)
        [4, 5, 8]
    """
    return take_op(input, index)


def frac_ext(input):
    r"""
    Calculates the fractional part of each element in the input.

    .. math::
        out_i = input_i - \lfloor |input_i| \rfloor * sgn(input_i)

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input Tensor.

    Returns:
        Tensor, has the same shape and type as input.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor([2, 4.2, -2.5], mindspore.float16)
        >>> output = ops.frac_ext(x)
        >>> print(output)
          [ 0.      0.1992 -0.5   ]
    """
    return frac_op(input)


def argmin_ext(input, dim=None, keepdim=False):
    r"""
    Return the indices of the minimum values of a tensor across a dimension.

    Args:
        input (Tensor): Input tensor.
        dim (Union[int, None], optional): Specify the dimension for computation. If ``None``, compute all elements in the `input` . Default ``None``.
        keepdim (bool, optional): Whether the output tensor has dim retained. Default ``False``.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> x = mindspore.tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]), mindspore.float32)
        >>> output = mindspore.ops.auto_generate.argmin_ext(x, dim=-1, keepdim=False)
        >>> print(output)
        [0 1 2]
    """
    return argmin_ext_op(input, dim, keepdim)


def erfinv(input):
    r"""
    Compute the inverse error of input tensor element-wise.

    It is defined in the range `(-1, 1)` as:

    .. math::

        erfinv(erf(x)) = x

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # When the `input` is int8, int16, int32, int64, uint8 or bool, the return value type is float32.
        >>> input = mindspore.tensor([0, 0.5, -0.9], mindspore.int64)
        >>> mindspore.ops.erfinv(input)
        Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00])
        >>> # Otherwise, the return value type is the same as the input type.
        >>> input = mindspore.tensor([0, 0.5, -0.9], mindspore.float32)
        >>> mindspore.ops.erfinv(input)
        Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  4.76936132e-01, -1.16308689e+00])
    """
    return erfinv_op(input)


def gather_nd(input_x, indices):
    r"""
    Gathers slices from the input tensor by specified indices.

    Suppose `indices` is an K-dimensional integer tensor, follow the formula below:

    .. math::

        output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]

    Must be satisfied :math:`indices.shape[-1] <= input\_x.rank`.

    Args:
        input_x (Tensor): The input tensor.
        indices (Tensor): The specified indices.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> input_x = mindspore.tensor([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]], mindspore.float32)
        >>> indices = mindspore.tensor([[0, 0], [1, 1]], mindspore.int32)
        >>> output = mindspore.ops.gather_nd(input_x, indices)
        >>> print(output)
        [-0.1  0.5]
    """
    return gather_nd_op(input_x, indices)


def inplace_bernoulli_scalar(input, p, seed, offset):
    r"""
    
    """
    return inplace_bernoulli_scalar_op(input, p, seed, offset)


def square(input):
    r"""
    Return square of a tensor element-wise.

    .. math::

        y_i = input_i ^ 2

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([1.0, 2.0, 3.0], mindspore.float32)
        >>> output = mindspore.ops.square(input)
        >>> print(output)
        [1. 4. 9.]
    """
    return square_op(input)


def erf(input):
    r"""
    Compute the Gauss error of input tensor element-wise.

    .. math::

        \text{erf}(x)=\frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # The datatype of output will be float32 when datatype of input is in [int64, bool](Datatype only supported on Ascend).
        >>> input = mindspore.tensor([-1, 0, 1, 2, 3], mindspore.int64)
        >>> mindspore.ops.erf(input)
        Tensor(shape=[5], dtype=Float32, value= [-8.42700793e-01,  0.00000000e+00,  8.42700793e-01,  9.95322265e-01,  9.99977910e-01])
        >>>
        >>> # Otherwise output has the same dtype as the input.
        >>> input = mindspore.tensor([-1, 0, 1, 2, 3], mindspore.float64)
        >>> mindspore.ops.erf(input)
        Tensor(shape=[5], dtype=Float64, value= [-8.42700793e-01,  0.00000000e+00,  8.42700793e-01,  9.95322265e-01,  9.99977910e-01])
    """
    return erf_op(input)


def conj(input):
    r"""
    Returns a tensor of complex numbers that are the complex conjugate of each element in input.
    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.

    The complex conjugate returned by this operation is of the form a - bj.

    If `input` is real, it is returned unchanged.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor(1.3+0.4j, mindspore.complex64)
        >>> output = mindspore.ops.conj(x)
        >>> output
        Tensor(shape=[], dtype=Complex64, value= 1.3-0.4j)
    """
    return conj_op(input)


def prod_ext(input, dim=None, keepdim=False, dtype=None):
    r"""
    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can
    reduce a dimension of `input` along the `dim`. Determine whether the dimensions of the output and input are the
    same by controlling `keepdim`.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
        dim (int): The dimensions to reduce. Default: ``None`` , reduce all dimensions.
            Only constant value is allowed. Assume the rank of `input` is r, and the value range is [-r,r).
        keepdim (bool): If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .
        dtype (:class:`mindspore.dtype`): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor, has the same data type as input tensor.

        - If `dim` is ``None`` , and `keepdim` is  ``False`` ,
          the output is a 0-D tensor representing the product of all elements in the input tensor.
        - If `dim` is int, set as 1, and `keepdim` is  ``False`` ,
          the shape of output is :math:`(input_0, input_2, ..., input_R)`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not one of the following: int or None.
        TypeError: If `keepdim` is not a bool.
        ValueError: If `dim` is out of range.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))
        >>> output = ops.prod_ext(x, 1, keepdim=True)
        >>> result = output.shape
        >>> print(result)
        (3, 1, 5, 6)
        >>> # case 1: Reduces a dimension by multiplying all elements in the dimension.
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)
        >>> output = ops.prod_ext(x)
        >>> print(output)
        2.2833798e+33
        >>> print(output.shape)
        ()
        >>> # case 2: Reduces a dimension along dim 0.
        >>> output = ops.prod_ext(x, 0, True)
        >>> print(output)
        [[[ 28.  28.  28.  28.  28.  28.]
        [ 80.  80.  80.  80.  80.  80.]
        [162. 162. 162. 162. 162. 162.]]]
        >>> # case 3: Reduces a dimension along dim 1.
        >>> output = ops.prod_ext(x, 1, True)
        >>> print(output)
        [[[  6.   6.   6.   6.   6.   6.]]
        [[120. 120. 120. 120. 120. 120.]]
        [[504. 504. 504. 504. 504. 504.]]]
        >>> # case 4: Reduces a dimension along dim 2.
        >>> output = ops.prod_ext(x, 2, True)
        >>> print(output)
        [[[1.00000e+00]
        [6.40000e+01]
        [7.29000e+02]]
        [[4.09600e+03]
        [1.56250e+04]
        [4.66560e+04]]
        [[1.17649e+05]
        [2.62144e+05]
        [5.31441e+05]]]
    """
    return prod_ext_op(input, dim, keepdim, dtype)


def grouped_matmul_add_(x, weight, group_list, out):
    r"""
    Fusion Operator of Transpose, GroupedMatmul, and InplaceAdd.

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - This API is only supported in Atlas A2 training series for now.
        - This API is only supported on KBK mode.

    Args:
        x (Tensor): Tensor with shape :math:`(m, k)`, whose type should be float16 or bfloat16.
        weight (Tensor): Tensor with shape :math:`(m, n)`, whose type should be float16 or bfloat16.
        group_list (Tensor): 1D Tensor, grouping positions for the m-axis of `x` and `weight`,
            whose type should be int64. It must be a non-negative ascending sequence .
        out (Tensor): A Tensor acting as both input and output, with type of float32.
            It's shape should be :math:`group_list.shape[0], k, n` or :math:`group_list.shape[0] * k, n` .

    Returns:
        Tensor, has the same shape and data type as `out`.

    Raises:
        TypeError: If the dtype of `weight` is not the same as `x`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops, nn, context
        >>> context.set_context(mode=context.GRAPH_MODE, jit_config={"jit_level": "O0"})
        >>> class Net(nn.Cell):
        ...     def construct(self, x, weight, group_list, out):
        ...        return ops.auto_generate.grouped_matmul_add_(x, weight, group_list, out)
        >>> x = Tensor(np.random.randn(10, 20), mindspore.float16)
        >>> weight = Tensor(np.random.randn(10, 8), mindspore.float16)
        >>> group_list = Tensor([2, 6, 8, 10], mindspore.int64)
        >>> out = Tensor(np.random.randn(4, 20, 8), mindspore.float32)
        >>> output = Net()(x, weight, group_list, out)
        >>> print(output.shape)
        [4, 20, 8]
    """
    return inplace_grouped_matmul_add_op(x, weight, group_list, out)


def broadcast_to_view(input, shape):
    r"""
    Broadcasts input tensor to a given shape. The dim of input must be smaller
    than or equal to that of target. Suppose input shape is :math:`(x_1, x_2, ..., x_m)`,
    target shape is :math:`(*, y_1, y_2, ..., y_m)`, where :math:`*` means any additional dimension.
    The broadcast rules are as follows:

    Compare the value of :math:`x_m` and :math:`y_m`, :math:`x_{m-1}` and :math:`y_{m-1}`, ...,
    :math:`x_1` and :math:`y_1` consecutively and
    decide whether these shapes are broadcastable and what the broadcast result is.

    If the value pairs at a specific dim are equal, then that value goes right into that dim of output shape.
    With an input shape :math:`(2, 3)`, target shape :math:`(2, 3)` , the inferred output shape is :math:`(2, 3)`.

    If the value pairs are unequal, there are three cases:

    Case 1: If the value of the target shape in the dimension is -1, the value of the
    output shape in the dimension is the value of the corresponding input shape in the dimension.
    With an input shape :math:`(3, 3)`, target
    shape :math:`(-1, 3)`, the output shape is :math:`(3, 3)`.

    Case 2: If the value of target shape in the dimension is not -1, but the corresponding
    value in the input shape is 1, then the corresponding value of the output shape
    is that of the target shape. With an input shape :math:`(1, 3)`, target
    shape :math:`(8, 3)`, the output shape is :math:`(8, 3)`.

    Case 3: If the corresponding values of the two shapes do not satisfy the above cases,
    it means that broadcasting from the input shape to the target shape is not supported.

    So far we got the last m dims of the outshape, now focus on the first :math:`*` dims, there are
    two cases:

    If the first :math:`*` dims of output shape does not have -1 in it, then fill the input
    shape with ones until their length are the same, and then refer to
    Case 2 mentioned above to calculate the output shape. With target shape :math:`(3, 1, 4, 1, 5, 9)`,
    input shape :math:`(1, 5, 9)`, the filled input shape will be :math:`(1, 1, 1, 1, 5, 9)` and thus the
    output shape is :math:`(3, 1, 4, 1, 5, 9)`.

    If the first :math:`*` dims of output shape have -1 in it, it implies this -1 is corresponding to
    a non-existing dim so they're not broadcastable. With target shape :math:`(3, -1, 4, 1, 5, 9)`,
    input shape :math:`(1, 5, 9)`, instead of operating the dim-filling process first, it raises errors directly.

    Args:
        input (Tensor): The input tensor.
        shape (tuple[int]): The target shape.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> shape = (2, 3)
        >>> x = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.auto_generate.broadcast_to_view(x, shape)
        >>> print(output)
        [[1. 2. 3.]
         [1. 2. 3.]]
        >>> shape = (-1, 2)
        >>> x = mindspore.tensor([[1], [2]], mindspore.float32)
        >>> output = mindspore.ops.auto_generate.broadcast_to_view(x, shape)
        >>> print(output)
        [[1. 1.]
         [2. 2.]]
    """
    return broadcast_to_view_op(input, shape)


def mla(query, q_rope, kv_cache, k_rope, block_tables, attn_mask=None, deq_scale_qk=None, deq_scale_pv=None, q_seq_lens=None, context_lens=None, head_num=32, scale_value=0.0, kv_head_num=1, mask_mode='MASK_NONE', is_ring=0):
    r"""
    
    """
    return mla_op(query, q_rope, kv_cache, k_rope, block_tables, attn_mask, deq_scale_qk, deq_scale_pv, q_seq_lens, context_lens, head_num, scale_value, kv_head_num, mask_mode, is_ring)


def celu(x, alpha=1.0):
    r"""
    celu activation function, computes celu (Continuously differentiable exponential
    linear units) of input tensors element-wise. The formula is defined as follows:

    .. math::

        \text{CeLU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))

    For more details, please refer to `celu <https://arxiv.org/abs/1704.07483>`_.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    CELU Activation Function Graph:

    .. image:: ../images/CELU.png
        :align: center

    Args:
        x (Tensor): The input of celu with data type of float16 or float32.
        alpha (float, optional): The :math:`\alpha` value for the Celu formulation. Default: 1.0

    Returns:
        Tensor, has the same data type and shape as the input.

    Raises:
        TypeError: If `alpha` is not a float.
        TypeError: If `x` is not a Tensor.
        TypeError: If dtype of `x` is neither float16 nor float32.
        ValueError: If `alpha` has the value of 0.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)
        >>> output = ops.celu(x, alpha=1.0)
        >>> print(output)
        [-0.86466473 -0.63212055  1.          2.        ]
    """
    celu_op = _get_cache_prim(CeLU)(alpha)
    return celu_op(x)


def apply_rotary_pos_emb_(query, key, cos, sin, position_ids, cos_format=0):
    r"""
    
    """
    return apply_rotary_pos_emb_impl(query, key, cos, sin, position_ids, cos_format)


def threshold(input, threshold, value):
    r"""
    
    """
    return threshold_op(input, threshold, value)


def trace(input):
    r"""
    Return the sum of the elements along the diagonal of the input tensor.

    Note:
        Input must be matrix, and complex number is not supported at present.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[0, 1, 2], 
        ...                           [3, 4, 5], 
        ...                           [6, 7, 8]])
        >>> mindspore.ops.trace(input)
        Tensor(shape=[], dtype=Int64, value= 12)
    """
    return trace_op(input)


def hfft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Calculates the two dimensional discrete Fourier transform of of a Hermitian symmetric `input`.

    Note:
        - `hfft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `hfft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `hfft2`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `hfft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `hfft2()` function.
        If `s` is given, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`(s[-1] - 1) * 2`, otherwise :math:`(input.shape[dim[-1]] - 1) * 2`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.hfft2(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[16.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]]
    """
    return hfft2_op(input, s, dim, norm)


def fill_diagonal_(input, fill_value, wrap=False):
    r"""
    
    """
    return inplace_fill_diagonal_op(input, fill_value, wrap)


def inplace_clamp_tensor(input, min=None, max=None):
    r"""
    
    """
    return inplace_clamp_tensor_op(input, min, max)


def update_to_device(x, sync=False):
    r"""
    
    """
    return update_to_device_op(x, sync)


def angle(input):
    r"""
    Returns the element-wise angle of the given complex tensor.

    .. math::
        output_i = angle(input_i)

    Args:
      input (Tensor):
        The input tensor.

    Returns:
      Tensor

    Supported Platforms:
      ``Ascend`` ``GPU`` ``CPU``

    Examples:
      >>> import mindspore
      >>> input = mindspore.tensor([-1 + 1j, -2 + 2j, 3 - 3j], mindspore.complex64)
      >>> output = mindspore.ops.angle(input)*180/3.14159
      >>> print(output)
      [135.0001  135.0001  -45.00004]
    """
    return angle_op(input)


def isneginf_ext(input):
    r"""
    Return whether each element in the input is a negative infinity number.

    .. warning::
        - This API can be used only on the Atlas A2 training series.

    Args:
        input (Tensor):  The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.isneginf(mindspore.tensor([[-float("inf"), float("inf")], [1, -float("inf")]], dtype=mindspore.float32))
        >>> print(output)
        [[ True False]
         [False  True]]
    """
    return isneginf_op(input)


def nsa_compress_attention(query, key, value, scale_value, head_num, compress_block_size, compress_stride, select_block_size, select_block_count, topk_mask=None, atten_mask=None, actual_seq_qlen=None, actual_cmp_seq_kvlen=None, actual_sel_seq_kvlen=None):
    r"""
    
    """
    return nsa_compress_attention_op(query, key, value, scale_value, head_num, compress_block_size, compress_stride, select_block_size, select_block_count, topk_mask, atten_mask, actual_seq_qlen, actual_cmp_seq_kvlen, actual_sel_seq_kvlen)


def relu(input):
    r"""
    Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.

    It returns :math:`\max(input,\  0)` element-wise. Specially, the neurons with the negative output
    will be suppressed and the active neurons will stay the same.

    .. math::

        ReLU(input) = (input)^+ = \max(0, input)

    ReLU Activation Function Graph:

    .. image:: ../images/ReLU.png
        :align: center

    Args:
        input (Tensor): The input Tensor.
        inplace (bool, optional): Whether to use inplace mode, Defaults to ``False``.

    Returns:
        Tensor, with the same dtype and shape as the `input`.

    Raises:
        TypeError: If dtype of `input` is not Number type.
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.relu(input)
        >>> print(output)
        [[0. 4. 0.]
         [2. 0. 9.]]
    """
    return relu_op(input)


def all(input, axis=None, keep_dims=False):
    r"""
    Tests if all element in `input` evaluates to `True` along the given axes.

    Args:
        input (Tensor): The input Tensor.
        axis (Union[int, tuple(int), list(int), Tensor], optional): The dimensions to reduce. If ``None`` ,
            all dimensions are reduced. Default ``None`` .
        keep_dims (bool, optional): Whether the output tensor has dim retained or not. Default ``False`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[True, False], [True, True]])
        >>>
        >>> # case 1:  By default, mindspore.ops.all tests along all the axes.
        >>> mindspore.ops.all(input)
        Tensor(shape=[], dtype=Bool, value= False)
        >>> 
        >>> # case 2: Reduces a dimension along axis 1, with keep_dims False.
        >>> mindspore.ops.all(input, axis=1)
        Tensor(shape=[2], dtype=Bool, value= [False,  True])
        >>>
        >>> # case 3: Reduces a dimension along axis (0,1), with keep_dims False.
        >>> mindspore.ops.all(input, axis=(0,1))
        Tensor(shape=[], dtype=Bool, value= False)
        >>>
        >>> # case 4: Reduces a dimension along axis [0,1], with keep_dims True.
        >>> mindspore.ops.all(input, axis=[0,1], keep_dims=True)
        Tensor(shape=[1, 1], dtype=Bool, value=
        [[False]])
    """
    return reduce_all_impl(input, axis, keep_dims)


def sigmoid(input):
    r"""
    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:

    .. math::

        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}

    where :math:`x_i` is an element of `x`.

    Sigmoid Function Graph:

    .. image:: ../images/Sigmoid.png
        :align: center

    Args:
        input (Tensor): `input` is :math:`x` in the preceding formula. Tensor of any dimension,
            the data type is float16, float32, float64, complex64 or complex128.

    Returns:
        Tensor, with the same type and shape as the input.

    Raises:
        TypeError: If dtype of `input` is not float16, float32, float64, complex64 or complex128.
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)
        >>> output = ops.sigmoid(input)
        >>> print(output)
        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]
    """
    return sigmoid_op(input)


def floor_div_scalar(input, other):
    r"""
    
    """
    return floor_div_scalar_op(input, other)


def divmod_tensor_(input, other, rounding_mode=None):
    r"""
    
    """
    return inplace_divmod_op(input, other, rounding_mode)


def histc_ext(input, bins=100, min=0, max=0):
    r"""
    Compute the histogram of a tensor.

    The elements are sorted into equal width bins between `min` and `max`.
    If `min` and `max` are both zero, the minimum and maximum values of the data are used.

    Elements lower than `min` or higher than `max` are ignored.

    .. warning::
        If `input` is mindspore.int64, valid values fit within mindspore.int32; exceeding this may cause precision errors.

    Args:
        input (Tensor): The input tensor.
        bins (int, optional): Number of histogram bins. Default ``100``.
        min (int, float, optional): Minimum value of the histogram data range. Default ``0``.
        max (int, float, optional): Maximum value of the histogram data range. Default ``0``.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([1., 2, 1])
        >>> y = mindspore.ops.histc_ext(x, bins=4, min=0, max=3)
        >>> print(y)
        [0. 2. 1. 0.]
    """
    return histc_ext_op(input, bins, min, max)

type_as_op=TypeAs()

def type_as(input, other):
    r"""
    Returns input cast to the type of the with the other.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Note:
        When converting complex numbers to boolean type, the imaginary part of the complex number is not
        taken into account. As long as the real part is non-zero, it returns True; otherwise, it returns False.

    Args:
        input (Tensor): The shape of tensor is :math:`(x_0, x_1, ..., x_R)`. The tensor whose data type is to be converted.
        other (Tensor): The shape of tensor is :math:`(x_0, x_1, ..., x_R)`. The tensor whose data type is specified.

    Returns:
        Tensor, the shape of tensor is the same as `input`, :math:`(x_0, x_1, ..., x_R)`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `other` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)
        >>> input = Tensor(input_np)
        >>> other_np = np.random.randn(2, 3, 4).astype(np.int32)
        >>> other = Tensor(other_np)
        >>> output = ops.type_as(input, other)
        >>> print(output.dtype)
        Int32
        >>> print(output.shape)
        (2, 3, 4, 5)
    """
    return type_as_op(input, other)


def sinh(input):
    r"""
    Compute hyperbolic sine of the input element-wise.

    .. math::

        output_i = \sinh(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0.62, 0.28, 0.43, 0.62], mindspore.float32)
        >>> output = mindspore.ops.sinh(input)
        >>> print(output)
        [0.6604918  0.28367308 0.44337422 0.6604918 ]
    """
    return sinh_op(input)


def dot(input, other):
    r"""
    Computes the dot product of two 1D tensor.

    Args:
        input (Tensor): The first input in the dot product, must be 1D.
        other (Tensor): The second input in the dot product, must be 1D.

    Returns:
        Tensor, the shape is [] and the data type is same as `input`.

    Raises:
        TypeError: If dtype of `input`, `other` is not tensor.
        TypeError: If dtype of `input`, `other` are not in float16, float32 or bfloat16.
        RuntimeError: If dtypes of `input` and `other` are not same.
        RuntimeError: If shapes of `input` and `other` are not same.
        RuntimeError: If shapes of `input` and `other` are not 1D.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> x = Tensor([2.0, 3.0], mindspore.float32)
        >>> y = Tensor([2.0, 1.0], mindspore.float32)
        >>> output = ops.auto_generate.dot(x, y)
        >>> print(output)
        7.0
        >>> print(output.dtype)
        Float32
    """
    return dot_op(input, other)


def max_(input):
    r"""
    Calculates the maximum value of the input tensor.

    Also see :func:`mindspore.ops.extend.max`.
    """
    return max_op(input)


def log(input):
    r"""
    Compute the natural logarithm of the input tensor element-wise.

    .. math::
        y_i = \log_e(x_i)

    .. warning::
        If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may
        be affacted.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([1.0, 2.0, 4.0], mindspore.float32)
        >>> output = mindspore.ops.log(x)
        >>> print(output)
        [0.        0.6931472 1.3862944]
    """
    return log_op(input)


def irfftn(input, s=None, dim=None, norm=None):
    r"""
    Calculates the inverse of `rfftn()`.

    Note:
        - `irfftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `irfftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `irfftn`.
            Default: ``None`` , the dim[-1] of the `input` will be zero-padded to :math:`2*(input.shape[dim[-1]]-1)`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `irfftn`.
            Default: ``None`` , which means transform the all dimension of `input`, or the last `len(s)` dimensions if s is given.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `irfftn()` function, result.shape[dim[i]] is s[i].
        When the input is int16, int32, int64, float16, float32 the return value type is float32.
        When the input is float64, the return value type is float64.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((2, 2, 2))
        >>> ops.irfftn(input, s=(2, 2, 2), dim=(0, 1, 2), norm="backward")
        Tensor(shape=[2, 2, 2], dtype=Float32, value=
        [[[ 1.00000000e+00,  0.00000000e+00],
          [ 0.00000000e+00,  0.00000000e+00]],
         [[ 0.00000000e+00,  0.00000000e+00],
          [ 0.00000000e+00,  0.00000000e+00]]])
    """
    return irfftn_op(input, s, dim, norm)


def sign(input):
    r"""
    Return an element-wise indication of the sign of a number.

    .. math::
        \text{out}_{i} = \begin{cases}
                          -1 & \text{input}_{i} < 0 \\
                           0 & \text{input}_{i} = 0 \\
                           1 & \text{input}_{i} > 0
                         \end{cases}

    .. note::
        When the input is NaN and dtype is float64, the output of this operator is NaN.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[-1, 0, 2, 4, 6], [2, 3, 5, -6, 0]])
        >>> output = mindspore.ops.sign(input)
        >>> print(output)
        [[-1  0  1  1  1]
         [ 1  1  1 -1  0]]
        >>> mindspore.set_device(device_target="CPU")
        >>> x = mindspore.tensor([[-1, 0, float('inf'), 4, float('nan')], [2, 3, float('-inf'), -6, 0]])
        >>> output = mindspore.ops.sign(x)
        >>> print(output)
        [[-1.  0.  1.  1.  0.]
         [ 1.  1. -1. -1.  0.]]
    """
    return sign_op(input)


def log10_ext(input):
    r"""
    Returns the logarithm to the base 10 of a tensor element-wise.

    .. math::
        y_i = \log_{10}(x_i)

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - If the input value of operator Log10 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy
          may be affacted.

    Args:
        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.

    Returns:
        Tensor, has the same shape as the `input`, and the dtype changes according to the `input.dtype`.
        
        - if `input.dtype` is in [float16, float32, float64, bfloat16], the output dtype is the same as the `input.dtype`.
        - if `input.dtype` is integer or boolean type, the output dtype is float32.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([3.0, 5.0, 7.0]), mindspore.float32)
        >>> output = ops.auto_generate.log10_ext(x)
        >>> print(output)
        [0.47712136 0.69897    0.845098  ]
    """
    return log10_op(input)


def kthvalue(input, k, dim=-1, keepdim=False):
    r"""
    Calculates the kth smallest value along given dim specified by `dim` of the input
    tensor, and returns a tuple of (`values`, `indices`) where `values` contains the k-th smallest element
    and `indices` provides the index of each corresponding element.

    Args:
        input (Tensor): The input tensor, can be any dimension. Set the shape of input tensor as
            :math:`(input_1, input_2, ..., input_N)`.
        k (int): Specifies the k-th smallest element to retrieve.
        dim (int, optional): The dimension along which to find the k-th smallest value. Default: ``-1`` .
        keepdim (bool, optional): Whether to reduce dimension, if ``True`` , the output will keep same dimension with the
            input, the output will reduce dimension if ``False`` . Default: ``False`` .

    Returns:
        A tuple consisting of `values` and `indices`.

        - **values** (Tensor) - The k-th smallest value of input tensor, with the same dtype as `input`.

          -If `keepdim` is ``True`` , the shape of output tensors is :math:`(input_1, input_2, ..., input_{dim-1}, 1, input_{dim+1}, ..., input_N)`.
          -If `keepdim` is ``False`` , the shape is :math:`(input_1, input_2, ..., input_{dim-1}, input_{dim+1}, ..., input_N)` .

        - **indices** (Tensor) - The `indices` for the k-th smallest value of the input tensor, it has the same shape as `values` with dtype of int64.
            
    Raises:
        TypeError: If `k` or `dim` is not an int.
        TypeError: If `keepdim` is not a bool.
        TypeError: If dtype of `input` is not supported.
        ValueError: If `input` is an empty Tensor.
        RuntimeError: If `k` is not in the proper range.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_x = Tensor(np.array([[1.01, 2.02, 3.03], [1.04, 2.05, 3.06]]), mindspore.float32)
        >>> out = ops.auto_generate.kthvalue(input_x, 2, 1, False)
        >>> print(out)
        (Tensor(shape=[2], dtype=Float32, value= [ 2.01999998e+00,  2.04999995e+00]), Tensor(shape=[2], dtype=Int64, value= [1, 1]))
        >>> out1 = ops.auto_generate.kthvalue(input_x, 2, 1, True)
        >>> print(out1)
        (Tensor(shape=[2, 1], dtype=Float32, value=
        [[ 2.01999998e+00],
         [ 2.04999995e+00]]), Tensor(shape=[2, 1], dtype=Int64, value=
        [[1],
         [1]]))
    """
    return kthvalue_op(input, k, dim, keepdim)

cast_op=Cast()

def cast(input, dtype):
    r"""
    Returns a tensor with the new specified data type.

    Note:
        When converting complex numbers to boolean type, the imaginary part of the complex number is not
        taken into account. As long as the real part is non-zero, it returns ``True``; otherwise, it returns ``False``.

    Args:
        input (Union[Tensor, Number]): The input tensor or number.
        dtype (dtype.Number): The dtype after conversion. Only constant value is allowed.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> dtype = mindspore.float64
        >>> output = mindspore.ops.cast(input, dtype)
        >>> print(output.dtype)
        Float64
        >>> print(output)
        [1. 2. 3.]
    """
    return cast_op(input, dtype)


def softshrink_grad(input_grad, input_x, lambd=0.5):
    r"""
    Computes gradients for SoftShrinkGrad operation.

    Args:
        input_grad (Tensor): the gradients of loss to output of SoftShrink function. Supported dtypes:

            - Ascend: float16, float32, bfloat16.
            - CPU/GPU: float16, float32.
        input_x (Tensor): Must be the input `input` of the forward operator SoftSHrink. Supported dtypes:

            - Ascend: float16, float32, bfloat16.
            - CPU/GPU: float16, float32.
        lambd (float): the lambda value for the Softshrink formulation. Default: ``0.5`` .

    Returns:
        backprops, a Tensor with the same shape and data type as `input_x`.

    Rasise:
        ValueError: If `lambd` is not a float.
        ValueError: If shape of `input_grad` is not the same as `input_x`.
        TypeError: If dtype of `input_grad` is not the same as `input_x`.
        TypeError: If dtype of `input_grad` or `input_x` is not float16, float32 or bfloat16.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``
    """
    return softshrink_grad_impl(input_grad, input_x, lambd)


def view_as(input, other):
    r"""
    Change the shape of the input tensor based on the shape of other.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.
        other (Tensor): The shape of return tensor is same as the shape of other.

    Returns:
        Tensor, which has the same shape of other.

    Raises:
        TypeError: If `input` is not a tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float32))
        >>> other = Tensor(np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32))
        >>> output = ops.view_as(input, other)
        >>> print(output)
        [[1. 2.]
         [3. 2.]
         [3. 4.]]
    """
    return view_as_op(input, other)


def reshape(input, shape):
    r"""
    Reshape the input tensor based on the given shape.

    .. note::
        The -1 in the parameter `shape` indicates that the size of that dimension is inferred from the other
        dimensions and the total number of elements in input tensor.

    Args:
        input (Tensor): The input tensor.
        shape (Union[tuple[int], list[int], Tensor[int]]): New shape.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]], mindspore.float32)
        >>> # case1: Parameter `shape` does not contain -1.
        >>> output = mindspore.ops.reshape(input, (3, 2))
        >>> print(output)
        [[-0.1  0.3]
         [ 3.6  0.4]
         [ 0.5 -3.2]]
        >>> # case2: Parameter `shape` contains -1.
        >>> output = mindspore.ops.reshape(input, (-1, 6))
        >>> print(output)
        [[-0.1  0.3  3.6  0.4  0.5 -3.2]]
    """
    return reshape_op(input, shape)


def inplace_elu(input, alpha=1.0):
    r"""
    Exponential Linear Unit activation function.

    Applies the exponential linear unit function of input tensors inplace element-wise.
    The activation function is defined as:

    .. math::

        \text{ELU}(x)= \left\{
        \begin{array}{align}
            \alpha(e^{x}  - 1) & \text{if } x \le 0\\
            x & \text{if } x \gt 0\\
        \end{array}\right.

    Where :math:`x` is the element of input Tensor `input`, :math:`\alpha` is param `alpha`,
    it determines the smoothness of ELU.

    ELU Activation function graph:

    .. image:: ../images/ELU.png
        :align: center

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input of ELU is a Tensor of any dimension.
        alpha (float, optional): The alpha value of ELU, the data type is float. Default: ``1.0``.

    Returns:
        Tensor, has the same shape and data type as `input`.

    Raises:
        RuntimeError: If the dtype of `input` is not float16, float32 or bfloat16.
        TypeError: If the dtype of `alpha` is not float.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> ops.auto_generate.inplace_elu(input)
        >>> print(input)
        [[-0.63212055  4.         -0.99966455]
         [ 2.         -0.99326205  9.        ]]
    """
    return inplace_elu_op(input, alpha)


def triu(input, diagonal=0):
    r"""
    Zero the input tensor below the diagonal specified.

    Args:
        input (Tensor): The input tensor.
        diagonal (int, optional): The diagonal specified of 2-D tensor. Default ``0`` represents the main diagonal.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[ 1,  2,  3,  4],
        ...                           [ 5,  6,  7,  8],
        ...                           [10, 11, 12, 13],
        ...                           [14, 15, 16, 17]])
        >>> mindspore.ops.triu(input)
        Tensor(shape=[4, 4], dtype=Int64, value=
        [[ 1,  2,  3,  4],
         [ 0,  6,  7,  8],
         [ 0,  0, 12, 13],
         [ 0,  0,  0, 17]])
        >>> mindspore.ops.triu(input, 1)
        Tensor(shape=[4, 4], dtype=Int64, value=
        [[ 0,  2,  3,  4],
         [ 0,  0,  7,  8],
         [ 0,  0,  0, 13],
         [ 0,  0,  0,  0]])
        >>> mindspore.ops.triu(input, -1)
        Tensor(shape=[4, 4], dtype=Int64, value=
        [[ 1,  2,  3,  4],
         [ 5,  6,  7,  8],
         [ 0, 11, 12, 13],
         [ 0,  0, 16, 17]])
        >>> input = mindspore.tensor([[[ 1,  2,  3],
        ...                            [ 5,  6,  7],
        ...                            [10, 11, 12]],
        ...                           [[ 1,  2,  3],
        ...                            [ 5,  6,  7],
        ...                            [10, 11, 12]]])
        >>> mindspore.ops.triu(input)
        Tensor(shape=[2, 3, 3], dtype=Int64, value=
        [[[ 1,  2,  3],
          [ 0,  6,  7],
          [ 0,  0, 12]],
         [[ 1,  2,  3],
          [ 0,  6,  7],
          [ 0,  0, 12]]])
    """
    return triu_impl(input, diagonal)


def softplus_ext(input, beta=1, threshold=20):
    r"""
    Applies softplus function to `input` element-wise.

    The softplus function is shown as follows, x is the element of `input` :

    .. math::

        \text{output} = \frac{1}{beta}\log(1 + \exp(\text{beta * x}))

    where :math:`input * beta > threshold`, the implementation converts to the linear function to ensure numerical stability.

    Args:
        input (Tensor): Tensor of any dimension. Supported dtypes: 

            - Ascend: float16, float32, bfloat16.
        beta (number.Number, optional): Scaling parameters in the softplus function. Default: ``1`` .
        threshold (number.Number, optional): For numerical stability, the softplus function is converted 
            to a threshold parameter of a linear function. Default: ``20`` .

    Returns:
        Tensor, with the same type and shape as the input.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If dtype of `input` is not float16, float32, bfloat16.

    Supported Platforms:
        ``Ascend`` 

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([0.1, 0.2, 30, 25]), mindspore.float32)
        >>> output = ops.auto_generate.softplus_ext(input)
        >>> print(output)
        [0.74439657 0.7981388 30. 25.]
    """
    return softplus_ext_op(input, beta, threshold)


def sub_tensor_(input, other, alpha=1):
    r"""
    
    """
    return inplace_sub_ext_op(input, other, alpha)


def sum_ext(input, dim=None, keepdim=False, dtype=None):
    r"""
    Calculate sum of Tensor elements over a given dim.

    Note:
        The `dim` with tensor type is only used for compatibility with older versions and is not recommended.

    Args:
        input (Tensor): The input tensor.
        dim (Union[None, int, tuple(int), list(int), Tensor]): Dimensions along which a sum is performed.
            If ``None`` , sum all the elements of the input tensor.
            If the `dim` is a tuple or list of ints, a sum is performed on all the dimensions specified in the tuple.
            Must be in the range :math:`[-input.ndim, input.ndim)` . Default: ``None`` .
        keepdim (bool): Whether the output tensor has `dim` retained or not.
            If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .
        dtype (:class:`mindspore.dtype`): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        A Tensor, sum of elements over a given `dim` in `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not an int, tulpe(int), list(int), Tensor or None.
        ValueError: If `dim` is not in the range :math:`[-input.ndim, input.ndim)` .
        TypeError: If `keepdim` is not a bool.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> from mindspore import dtype as mstype
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mstype.float32)
        >>> out = ops.sum_ext(x)
        >>> print(out)
        270.0
        >>> out = ops.sum_ext(x, dim=2)
        >>> print(out)
        [[ 6. 12. 18.]
        [24. 30. 36.]
        [42. 48. 54.]]
        >>> out = ops.sum_ext(x, dim=2, keepdim=True)
        >>> print(out)
        [[[ 6.]
        [12.]
        [18.]]
        [[24.]
        [30.]
        [36.]]
        [[42.]
        [48.]
        [54.]]]
    """
    return sum_ext_op(input, dim, keepdim, dtype)


def convolution(input, weight, bias=None, stride=1, padding=0, dilation=1, transposed=False, output_padding=0, groups=1):
    r"""
    
    """
    return convolution_op(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups)


def inner_moe_token_unpermute(permuted_tokens, sorted_indices, probs=None, padded_mode=False, restore_shape=None):
    r"""
    
    """
    return inner_moe_token_unpermute_op(permuted_tokens, sorted_indices, probs, padded_mode, restore_shape)


def copy_to_device(x, sync=False):
    r"""
    
    """
    return copy_to_device_op(x, sync)


def trace_ext(input):
    r"""
    Return the sum of the elements along the diagonal of the input tensor.

    Args:
        input (Tensor): 2-D input tensor.

    Returns:
        Tensor, when the data type of `input` is integer or bool, its data type is mindspore.int64, otherwise it is the same as `input`, and size equals to 1.

    Raises:
        ValueError: If the dimension of `input` is not equal to 2.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor(mindspore.mint.arange(1, 13).reshape(3, 4), mindspore.float32)
        >>> print(input)
        [[ 1.  2.  3.  4.]
         [ 5.  6.  7.  8.]
         [ 9. 10. 11. 12.]]
        >>> output = mindspore.mint.trace(input)
        >>> print(output)
        18.0
    """
    return trace_ext_op(input)


def topk_ext(input, k, dim=-1, largest=True, sorted=True):
    r"""
    Finds values and indices of the `k` largest or smallest entries along a given dimension.

    .. warning::
        - If sorted is set to False, due to different memory layout and traversal methods on different platforms,
          the display order of calculation results may be inconsistent when `sorted` is False.

    If the `input` is a one-dimensional Tensor, finds the `k` largest  or smallest entries in the Tensor,
    and outputs its value and index as a Tensor. values[`k`] is the `k` largest item in `input`,
    and its index is indices [`k`].

    For a multi-dimensional matrix,
    calculates the first or last `k` entries in a given dimension, therefore:

    .. math::

        values.shape = indices.shape

    If the two compared elements are the same, the one with the smaller index value is returned first.

    Args:
        input (Tensor): Input to be computed.
        k (int): The number of top or bottom elements to be computed along the last dimension.
        dim (int, optional): The dimension to sort along. Default: ``-1`` .
        largest (bool, optional): If largest is ``False``  then the k smallest elements are returned.
            Default: ``True`` .
        sorted (bool, optional): If ``True`` , the obtained elements will be sorted by the values in descending
            order or ascending order according to `largest`. If ``False`` , the obtained elements will not be
            sorted. Default: ``True`` .

    Returns:
        A tuple consisting of `values` and `indices`.

        - values (Tensor) - The `k` largest or smallest elements in each slice of the given dimension.
        - indices (Tensor) - The indices of values within the last dimension of input.

    Raises:
        TypeError: If `sorted` is not a bool.
        TypeError: If `input` is not a Tensor.
        TypeError: If `k` is not an int.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> from mindspore import ops
        >>> x = ms.Tensor([[0.5368, 0.2447, 0.4302, 0.9673],
        ...                [0.4388, 0.6525, 0.4685, 0.1868],
        ...                [0.3563, 0.5152, 0.9675, 0.8230]], dtype=ms.float32)
        >>> output = ops.topk_ext(x, 2, dim=1)
        >>> print(output)
        (Tensor(shape=[3, 2], dtype=Float32, value=
        [[ 9.67299998e-01,  5.36800027e-01],
         [ 6.52499974e-01,  4.68499988e-01],
         [ 9.67499971e-01,  8.23000014e-01]]), Tensor(shape=[3, 2], dtype=Int64, value=
        [[3, 0],
         [1, 2],
         [2, 3]]))
        >>> output2 = ops.topk_ext(x, 2, dim=1, largest=False)
        >>> print(output2)
        (Tensor(shape=[3, 2], dtype=Float32, value=
        [[ 2.44700000e-01,  4.30200011e-01],
         [ 1.86800003e-01,  4.38800007e-01],
         [ 3.56299996e-01,  5.15200019e-01]]), Tensor(shape=[3, 2], dtype=Int64, value=
        [[1, 2],
         [3, 0],
         [0, 1]]))
    """
    return topk_ext_op(input, k, dim, largest, sorted)


def sub_scalar(input, other, alpha=1):
    r"""
    
    """
    return sub_scalar_op(input, other, alpha)


def cummin_ext(input, dim):
    r"""
    Return the cumulative minimum values and their indices along the given dimension of the tensor.

    .. math::
        \begin{array}{ll} \\
            y_{i} = \min(x_{1}, x_{2}, ... , x_{i})
        \end{array}

    .. note::
        GE backend is not supported in Ascend.

    Args:
        input (Tensor): The input tensor.
        dim (int): Specify the dimension to compute.

    Returns:
        Tuple of two tensors, tuple(min, min_indices).

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> a = mindspore.tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)
        >>> output = mindspore.ops.cummin_ext(a, dim=0)
        >>> print(output[0])
        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]
        >>> print(output[1])
        [0 1 1 1 4 4]
    """
    return cummin_ext_op(input, dim)


def cross_entropy_loss(input, target, weight=None, reduction='mean', ignore_index=-100, label_smoothing=0.0, lse_square_scale_for_zloss=0.0, return_zloss=False):
    r"""
    Computes the cross entropy loss between input and target.

    Assume the number of classes :math:`C` in the range :math:`[0, C)`,
    the loss with reduction=none can be described as:

    .. math::

        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}
        \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}

    where :math:`x` is the inputs, :math:`y` is the target, :math:`w` is the weight, :math:`N` is the batch size,
    :math:`c` belonging to :math:`[0, C-1]` is class index, where :math:`C` is the number of classes.

    If `reduction` is not ``None`` (default ``'mean'`` ), then

    .. math::

        \ell(x, y) = \begin{cases}
            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}} l_n, &
            \text{if reduction} = \text{'mean',}\\
            \sum_{n=1}^N l_n,  &
            \text{if reduction} = \text{'sum'.}
            \end{cases}

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Inputs:
        - **input** (Tensor) - Tensor of shape of :math:`(N, C)` where `C = number of classes`, data type must be bfloat16, float16 or float32.
        - **target** (Tensor) - For class indices, tensor of shape :math:`(N)`, data type must be int64. The value must be in range [0, C).
        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.
          If not None, the shape is :math:`(C,)`, data type must be float32. Default: ``None`` .
        - **reduction** (str, optional) - Apply specific reduction method to the output: ``'none'`` , ``'mean'`` ,
          ``'sum'`` . Default: ``'mean'`` .

          - ``'none'``: no reduction will be applied.
          - ``'mean'``: compute and return the weighted mean of elements in the output.
          - ``'sum'``: the output elements will be summed.

        - **ignore_index** (int, optional) - Specifies a target value that is ignored and does not contribute to the input
          gradient. When set to negative values, no target value is ignored. It should be int64.
          Default: ``-100`` .
        - **label_smoothing** (float, optional) - Label smoothing values, a regularization tool used to prevent the model
          from overfitting when calculating Loss. This value must be 0.0 currently. Default: ``0.0`` .
        - **lse_square_scale_for_zloss** (float, optional) - The value range is [0.0, 1.0), not enabled for now, can only be 0.0. Default: ``0.0`` .
        - **return_zloss** (float, optional) - Not enabled for now, can only be ``False``. Default: ``False`` .

    Outputs:
        A tuple consisting of 4 Tensors.

        - **loss** (Tensor) - loss between `input` and `target`, the dtype is the same as `input`.

          - If `reduction` is ``'none'`` , the shape is :math:`(N,)` .
          - If `reduction` is ``'sum'` or ``'mean'`, the shape is :math:`(1,)` .

        - **log_prob** (Tensor) - the shape is :math:`(N, C)` with the same dtype as `input`.
        - **zloss** (Tensor) - the shape is :math:`(N,)` if `return_zloss` is True, or the shape is :math:`(0,)` with the same dtype as `input`. This parameter is disabled for now.
        - **lse_for_zloss** (Tensor) - the shape is :math:`(N,)` if `lse_square_scale_for_zloss` is not 0.0, or the shape is :math:`(0,)` with the same dtype as `input`. This parameter is disabled for now.


    Raises:
        ValueError: If `reduction` is not one of ``'none'``, ``'mean'`` or ``'sum'``.
        TypeError: If `input`, `target` or `weight` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, nn, ops
        >>> 
        >>> 
        >>> class Net(nn.Cell):
        ...     def __init__(self):
        ...         super(Net, self).__init__()
        ...         self.cross_entropy_loss = ops.auto_generate.CrossEntropyLoss()
        ... 
        ...     def construct(self, input, target, weight):
        ...         result = self.cross_entropy_loss(input, target, weight)
        ...         return result
        ... 
        >>> 
        >>> net = Net()
        >>> input = Tensor(np.array([[0.2, 0.7, 0.1], [0.2, 0.7, 0.1]]), mindspore.float32)
        >>> target = Tensor(np.array([0, 1]), mindspore.int64)
        >>> weight = Tensor(np.array([1, 0.5, 0.5]), mindspore.float32)
        >>> output = net(input, target, weight)
        >>> print(output[:2])
        (Tensor(shape=[1], dtype=Float32, value= [ 1.10128295e+00]), Tensor(shape=[2, 3], dtype=Float32, value=
        [[-1.26794958e+00, -7.67949641e-01, -1.36794960e+00],
         [-1.26794958e+00, -7.67949641e-01, -1.36794960e+00]]))
    """
    return cross_entropy_loss_op(input, target, weight, reduction, ignore_index, label_smoothing, lse_square_scale_for_zloss, return_zloss)


def log1p(input):
    r"""
    Compute the natural logarithm of (tensor + 1) element-wise.

    .. math::
        out_i = \log_e(input_i + 1)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([1.0, 2.0, 4.0], mindspore.float32)
        >>> output = mindspore.ops.log1p(x)
        >>> print(output)
        [0.6931472 1.0986123 1.609438 ]
    """
    return log1p_op(input)


def rotary_position_embedding(x, cos, sin, mode=0):
    r"""
    Implements the Rotary Position Embedding algorithm.
    Refer to paper `Enhanced Transformer with Rotary Position Embedding <https://arxiv.org/pdf/2104.09864.pdf>`_.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        x (Tensor): 4D tensor, with float16, bfloat16 or float32 data type.
        cos (Tensor): 4D constant, has the same type as `x` , in range of [-1, 1].
        sin (Tensor): Same with `cos` .
        mode (int): An optional attribute. Used to select a calculation mode. 0: rotate_half(GPT-NeoX style); 1: rotate_interleaved(GPT-J style). Defaults to ``0`` .

    .. list-table:: Config layout constraints
        :widths: 5 20 20
        :header-rows: 1

        * - Args
          - RotateHalf(mode:0)
          - RotateInterleaved(mode:1)
        * - x
          - Supported layout:

            11SD, B1SD, BNSD; D < 896 and D is an Even. B, N < 1000;

          - Supported layout: 11SD, B1SD, BNSD;

            D < 896 and D is an Even.

            B, N < 1000;
        * - cos
          - Support layout for different values of `x`:

            `x` is BNSD: 11SD, B1SD, BNSD;

            `x` is BSND: 1S1D, BS1D, BSND;

            `x` is SBND: S11D, SB1D, SBND
          - Support layout for different values of `x`:

            `x` is BNSD: 11SD;

            `x` is BSND: 1S1D;

            `x` is SBND: S11D
        * - sin
          - Same with `cos` .
          - Same with `cos` .

    .. note::
        When the layout is BNSD, B * N > 8S and D is 32-bytes alignment, the performance is poor. Therefore, this interface cannot be called.

    Returns:
        Tensor, has the same dtype and shape as the `x`.

    Raises:
        TypeError: If `x` is not a Tensor.
        TypeError: If `cos` is not a Tensor.
        TypeError: If `sin` is not a Tensor.
        TypeError: If `mode` is not an int.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.random.uniform(-2, 2, (4, 8192, 4, 128)))
        >>> cos = Tensor(np.random.uniform(-1, 1, (1, 8192, 1, 128)))
        >>> sin = Tensor(np.random.uniform(-1, 1, (1, 8192, 1, 128)))
        >>> output = ops.rotary_position_embedding(x, cos, sin, 0)
        >>> print(output.shape)
        (4, 8192, 4, 128)
    """
    return rotary_position_embedding_op(x, cos, sin, mode)


def acosh_ext(input):
    r"""
    Computes inverse hyperbolic cosine of the inputs element-wise.

    .. math::

        out_i = \cosh^{-1}(input_i)

    .. note::
        Given an input tensor input, the function computes inverse hyperbolic cosine of every element.
        Input range is [1, inf].

    Args:
        input (Tensor): The input tensor of inverse hyperbolic cosine function.

    Returns:
        Tensor, has the same shape as `input`. The dtype of output is float32 when dtype of `input` is in [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)
        >>> output = ops.acosh_ext(input)
        >>> print(output)
        [0.        0.9624236 1.7627472 5.298292 ]
    """
    return acosh_ext_op(input)


def selu_ext(input):
    r"""
    Activation function SELU (Scaled exponential Linear Unit).

    The activation function is defined as:

    .. math::
        E_{i} =
        scale *
        \begin{cases}
        x_{i}, &\text{if } x_{i} \geq 0; \cr
        \text{alpha} * (\exp(x_i) - 1), &\text{otherwise.}
        \end{cases}

    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`
    and :math:`scale=1.05070098`).

    See more details in `Self-Normalizing Neural Networks <https://arxiv.org/abs/1706.02515>`_.

    SELU Activation Function Graph:

    .. image:: ../images/SeLU.png
        :align: center

    Args:
        input (Tensor): Tensor of any dimension.
            The data type is float16, float32, bfloat16.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Raises:
        TypeError: If dtype of `input` is not float16, float32, bfloat16.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> input = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.auto_generate.selu_ext(input)
        >>> print(output)
        [[-1.1113307 4.202804 -1.7575096]
         [ 2.101402 -1.7462534 9.456309 ]]
    """
    return selu_ext_op(input)


def tuple_to_tensor(input_tuple, dtype=None):
    r"""
    
    """
    return tuple_to_tensor_op(input_tuple, dtype)


def add(input, other):
    r"""
    Compute the element-wise sum of the two input tensors.

    .. math::

        out_{i} = input_{i} + other_{i}

    Note:
        - The two inputs can not be bool type at the same time,
          [True, Tensor(True), Tensor(np.array([True]))] are all considered bool type.
        - Support broadcast, support implicit type conversion and type promotion.
        - When the input is a tensor, the dimension should be greater than or equal to 1.

    Args:
        input (Union[Tensor, number.Number, bool]): The first input tensor.
        other (Union[Tensor, number.Number, bool]): The second input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: x and y are both tensor.
        >>> x = mindspore.tensor([1., 2., 3.])
        >>> y = mindspore.tensor([4., 5., 6.])
        >>> output = mindspore.ops.add(x, y)
        >>> print(output)
        [5. 7. 9.]
        >>> # case 2: x is a scalar and y is a tensor
        >>> x = mindspore.tensor(1, mindspore.int32)
        >>> y = mindspore.tensor([4., 5., 6.])
        >>> output = mindspore.ops.add(x, y)
        >>> print(output)
        [5. 6. 7.]
        >>> # the data type of x is int32, the data type of y is float32,
        >>> # and the output is the data format of higher precision float32.
        >>> print(output.dtype)
        Float32
    """
    return add_op(input, other)


def logsigmoid_grad(dy, input, buffer):
    r"""
    
    """
    return logsigmoid_grad_op(dy, input, buffer)


def rfft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Calculates the two dimensional discrete Fourier transform for real input `input`.

    Note:
        - `rfft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `rfft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `rfft2`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `rfft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `rfft2()` function, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`s[-1] // 2 + 1`.
        When the input is int16, int32, int64, float16, float32, the return value type is complex64.
        When the input is float64, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((2, 2))
        >>> ops.rfft2(input, s=(2, 2), dim=(0, 1), norm="backward")
        Tensor(shape=[2, 2], dtype=Complex64, value=
        [[4+0j, 0+0j],
         [0+0j, 0+0j]])
    """
    return rfft2_op(input, s, dim, norm)


def isinf(input):
    r"""
    Return a boolean tensor indicating which elements are +/- inifnity.

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - For Ascend, it is only supported on platforms above Atlas A2.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``CPU`` ``GPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([-1, 3, float("inf"), float("-inf"), float("nan")])
        >>> mindspore.ops.isinf(input)
        Tensor(shape=[5], dtype=Bool, value= [False, False,  True,  True, False])
    """
    return isinf_op(input)


def logsumexp_ext(input, dim, keepdim=False):
    r"""
    Calculate the logarithm of the sum of exponentiations of all elements along the specified `dim` dimension of the input tensor.

    .. math::

        logsumexp(input) = \log(\sum(e^{input-input_{max}})) + input_{max}

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.
        dim (Union[int, tuple(int), list(int)]): Specify the dimension for computation. If `dim` is `()`, compute all elements in the `input`.
        keepdim (bool, optional): Whether the output tensor has dim retained. Default ``False``.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> x = mindspore.tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))
        >>> output = mindspore.ops.auto_generate.logsumexp_ext(x, 1, keepdim=True)
        >>> print(output.shape)
        (3, 1, 5, 6)
    """
    return logsumexp_op(input, dim, keepdim)


def index(input, indices):
    r"""
    Index the Tensor using an `indices`.

    .. warning::
        This is an experimental optimizer API that is subject to change.

    Args:
        input (Tensor): The input Tensor.
        indices (tuple[Tensor], list[Tensor]): the indices of type is bool, uint8, int32 or int64, used to index into the `input`.
            The size of indices should <= the rank of `input` and the tensors in indices should be broadcastable.
            When the tensor types are bool and uint8, shape will match the input dimensions in turn. For example: the first tensor of `indices` is of type bool, 
            Shape(x, y), `input` Shape(a, b, c), and (x, y) needs to match (a, b).


    Returns:
        Tensor, has the same dtype as input Tensor.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If the dtype of `indices` is not tuple[Tensor], list[Tensor].
        TypeError: If the dtype of tensors in `indices` is not bool, uint8, int32 or int64.
        ValueError: If the tensors in `indices` is not be broadcastable.
        ValueError: If size(`indices`) > rank(`input`).
        ValueError: If rank of `input` = 0.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input1 = Tensor(np.array([[1, 2, 3], [4, 5, 6]]), mindspore.int32)
        >>> indices1 = Tensor(np.array([0, 1, 1]), mindspore.int32)
        >>> indices2 = Tensor(np.array([1, 2, 1]), mindspore.int32)
        >>> output = ops.auto_generate.index(input1, [indices1, indices2])
        >>> print(output)
        [2 6 5]
        >>> input2 = Tensor(np.arange(4 * 3 * 3).reshape(4, 3, 3), mindspore.int32)
        >>> indices3 = Tensor(np.array([1, 0]), mindspore.int32)
        >>> indices4 = Tensor(np.array([1, 1, 0]), mindspore.bool)
        >>> output2 = ops.auto_generate.index(input2, [indices3, indices4])
        >>> print(output2)
        [[ 9 10 11]
         [ 3  4  5]]
    """
    return index_op(input, indices)


def hardswish(input):
    r"""
    Hard Swish activation function. The input is a Tensor with any valid shape.

    Hard swish is defined as:

    .. math::
        \text{HardSwish}(input) =
        \begin{cases}
        0, & \text{ if } input \leq -3, \\
        input, & \text{ if } input \geq +3, \\
        input*(input + 3)/6, & \text{ otherwise }
        \end{cases}

    HardSwish Activation Function Graph:

    .. image:: ../images/Hardswish.png
        :align: center

    Args:
        input (Tensor): The input Tensor.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `input` is neither int nor float.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)
        >>> output = ops.hardswish(input)
        >>> print(output)
        [-0.3333  -0.3333  0  1.667  0.6665]
    """
    return hswish_op(input)


def equal(input, other):
    r"""
    Compute the equivalence of the two inputs element-wise.

    .. math::

        out_{i} =\begin{cases}
            & \text{True,    if } input_{i} = other_{i} \\
            & \text{False,   if } input_{i} \ne other_{i}
            \end{cases}

    Note:
        - Support implicit type conversion.
        - The input must be two Tensors, or a Tensor and a Scalar.
        - The shapes of the inputs can be broadcasted to each other.

    Args:
        input (Union[Tensor, Number]): The first input.
        other (Union[Tensor, Number]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: The shape of two inputs are different
        >>> input = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.equal(input, 2.0)
        >>> print(output)
        [False  True False]
        >>> # case 2: The shape of two inputs are the same
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> output = mindspore.ops.equal(input, other)
        >>> print(output)
        [ True  True False]
    """
    return equal_op(input, other)


def log2_ext(input):
    r"""
    Returns the logarithm to the base 2 of a tensor element-wise.

    .. math::
        y_i = \log_2(x_i)

    .. warning::
        - If the input value of operator Log2 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy
          may be affacted.

    Args:
        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.

    Returns:
        Tensor, has the same shape as the `input`. If `input.dtype` is of integer or boolean type, the output dtype
        will be float32. Otherwise, the output dtype will be the same as `input.dtype`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([3.0, 5.0, 7.0]), mindspore.float32)
        >>> output = ops.auto_generate.log2_ext(x)
        >>> print(output)
        [1.5849625 2.321928  2.807355 ]
    """
    return log2_op(input)


def put_(input, index, source, accumulate=False):
    r"""
    
    """
    return inplace_put_op(input, index, source, accumulate)


def ihfftn(input, s=None, dim=None, norm=None):
    r"""
    Computes the N dimensional inverse discrete Fourier transform of real `input`.

    Note:
        - `ihfftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ihfftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `ihfftn`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `ihfftn`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

          - ``"backward"`` (normalize by :math:`1/n`).
          - ``"forward"`` (no normalization).
          - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `ihfftn()` function.
        If `s` is given, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`s[-1] // 2 + 1`, otherwise :math:`input.shape[dim[-1]] // 2 + 1`.
        When the input is int16, int32, int64, float16, float32, the return value type is complex64.
        When the input is float64, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.ihfftn(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[16.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]
         [ 0.  0.  0.  0.]]
    """
    return ihfftn_op(input, s, dim, norm)


def masked_fill_scalar_(input, mask, value):
    r"""
    
    """
    return inplace_masked_fill_scalar_op(input, mask, value)


def group_topk(token, idx_arr, group_num, k, k_inner=1):
    r"""
    
    """
    return group_topk_op(token, idx_arr, group_num, k, k_inner)


def muls(input, other):
    r"""
    
    """
    return muls_op(input, other)


def flatten_ext(input, start_dim=0, end_dim=-1):
    r"""
    Flatten a tensor along dimensions from `start_dim` to `end_dim`.

    Args:
        input (Tensor): The input Tensor.
        start_dim (int, optional): The first dimension to flatten. Default: ``0`` .
        end_dim (int, optional): The last dimension to flatten. Default: ``-1`` .

    Returns:
        Tensor. If no dimensions are flattened, returns the original `input`, otherwise return the flattened Tensor.
        If `input` is a 0-dimensional Tensor, a 1-dimensional Tensor will be returned.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `start_dim` or `end_dim` is not int.
        ValueError: If `start_dim` is greater than `end_dim` after canonicalized.
        ValueError: If `start_dim` or `end_dim` is not in range of [-input.dim, input.dim-1].

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)
        >>> output = ops.auto_generate.flatten_ext(input_x)
        >>> print(output.shape)
        (24,)
    """
    return flatten_ext_op(input, start_dim, end_dim)


def nextafter(input, other):
    r"""
    Returns the next representable floating-point value after `input` towards `other` element-wise.

    .. math::
        out_i = \begin{cases}
            & input_i + eps, & \text{if } input_i < other_i \\
            & input_i - eps, & \text{if } input_i > other_i \\
            & input_i, & \text{if } input_i = other_i
        \end{cases}

    Where eps is the smallest representable increment value for the input tensor's dtype.

    Args:
        input (Tensor): The first input tensor.
        other (Tensor): The second input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> eps = np.finfo(np.float32).eps
        >>> input = mindspore.tensor([1.0], mindspore.float32)
        >>> other = mindspore.tensor([2.0], mindspore.float32)
        >>> output = mindspore.ops.nextafter(input, other)
        >>> print(output == eps + 1)
        [ True]
        >>> input = mindspore.tensor([1.0, 2.0], mindspore.float32)
        >>> other = mindspore.tensor([2.0, 1.0], mindspore.float32)
        >>> output = mindspore.ops.nextafter(input, other)
        >>> print(output == mindspore.tensor([eps + 1, 2 - eps], mindspore.float32))
        [ True True]
    """
    return next_after_op(input, other)


def asinh(input):
    r"""
    Computes inverse hyperbolic sine of the input element-wise.

    .. math::

        out_i = \sinh^{-1}(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.asinh(mindspore.tensor([-5.0, 1.5, 3.0, 100.0]))
        >>> print(output)
        [-2.3124382  1.1947632  1.8184465  5.298342 ]
    """
    return asinh_op(input)


def diag(input):
    r"""
    Return a tensor with the `input` as its diagonal elements and ``0`` elsewhere.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: When input is a 1-D tensor:
        >>> input = mindspore.ops.randn(3)
        >>> output = mindspore.ops.diag(input)
        >>> print(output)
        [[ 1.7477764  0.         0.       ]
         [ 0.        -1.2616369  0.       ]
         [ 0.         0.         2.3283238]]
        >>>
        >>> # case 2: When input is a multi-dimensional tensor:
        >>> input = mindspore.ops.randn(2, 3)
        >>> print(input)
        [[ 0.21546374 -0.0120403  -0.7330481 ]
         [-2.5405762   0.44775972 -1.4063131 ]]
        >>> output = mindspore.ops.diag(input)
        >>> print(output)
        [[[[ 0.21546374  0.          0.        ]
           [ 0.          0.          0.        ]]
          [[ 0.         -0.0120403   0.        ]
           [ 0.          0.          0.        ]]
          [[ 0.          0.         -0.7330481 ]
           [ 0.          0.          0.        ]]]
         [[[ 0.          0.          0.        ]
           [-2.5405762   0.          0.        ]]
          [[ 0.          0.          0.        ]
           [ 0.          0.44775972  0.        ]]
          [[ 0.          0.          0.        ]
           [ 0.          0.         -1.4063131 ]]]]
        >>> # Assume input has dimensions (D_1,... D_k), the output is a tensor of rank 2k with dimensions
            (D_1,..., D_k, D_1,..., D_k).
        >>> print(output.shape)
        (2, 3, 2, 3)
    """
    return diag_op(input)


def inplace_fill_scalar(input, value):
    r"""
    
    """
    return inplace_fill_scalar_op(input, value)


def moe_token_permute(tokens, indices, num_out_tokens=None, padded_mode=False):
    r"""
    Permute the `tokens` based on the `indices`. Token with the same index will be grouped together.

    .. warning::
        - It is only supported on Atlas A2 Training Series Products.
        - When `indices` is 2-D, the size of the second dim must be less than or equal to 512.

    Args:
        tokens (Tensor): The input token tensor to be permuted. The dtype is bfloat16, float16 or float32.
            The shape is :math:`(num\_tokens, hidden\_size)` , where `num_tokens` and `hidden_size` are positive integers.
        indices (Tensor): The tensor specifies indices used to permute the tokens. The dtype is int32 or int64.
            The shape is :math:`(num\_tokens, topk)` or :math:`(num\_tokens,)`, where `num_tokens` and `topk` are positive integers.
            If the shape is the latter case, `topk` is implied to be 1.
        num_out_tokens (int, optional): The effective output token count, when enabling the capacity factor, should equal the number of tokens not dropped. It should be non-negative integer. Default: ``None``, meaning no tokens are dropped.
        padded_mode (bool, optional): If ``True``, indicating the indices are padded to denote selected tokens per expert. It can only be False currently. Default: ``False`` .

    Returns:
        tuple (Tensor), tuple of 2 tensors, containing the permuted tokens and sorted indices.

        - **permuted_tokens** (Tensor) - The permuted tensor of the same dtype as `tokens`.
        - **sorted_indices** (Tensor) - The indices Tensor of dtype int32, corresponds to permuted tensor.

    Raises:
        TypeError: If `tokens` or `indices` is not a Tensor.
        TypeError: If dtype of `indices` is not int32 or int64.
        TypeError: If specified `num_out_tokens` is not an integer.
        TypeError: If specified `padded_mode` is not a bool.
        ValueError: If second dim of `indices` is greater than 512 when exists.
        ValueError: If `padded_node` is set to True.
        ValueError: If `tokens` is not 2-D or `indices` is not 1-D or 2-D Tensor.
        RuntimeError: If first dimensions of `tokens` and `indices` are not consistent.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> tokens = Tensor([[1, 1, 1],
        ...                  [7, 7, 7],
        ...                  [2, 2, 2],
        ...                  [1, 1, 1],
        ...                  [2, 2, 2],
        ...                  [3, 3, 3]], dtype=mindspore.bfloat16)
        >>> indices = Tensor([5, 0, 3, 1, 2, 4], dtype=mindspore.int32)
        >>> out = ops.moe_token_permute(tokens, indices)
        >>> print(out)
        (Tensor(shape=[6, 3], dtype=BFloat16, value=
        [[7, 7, 7],
         [1, 1, 1],
         [2, 2, 2],
         [2, 2, 2],
         [3, 3, 3],
         [1, 1, 1]]), Tensor(shape=[6], dtype=Int32, value= [5, 0, 3, 1, 2, 4]))
    """
    return moe_token_permute_op(tokens, indices, num_out_tokens, padded_mode)


def logaddexp_ext(input, other):
    r"""
    Computes the logarithm of the sum of exponentiations of the inputs.
    This function is useful in statistics where the calculated probabilities of events may be
    so small as to exceed the range of normal floating point numbers.

    .. math::

        out_i = \log(exp(input_i) + \exp(other_i))

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): Input Tensor. The dtype of `input` must be float.
        other (Tensor): Input Tensor. The dtype of `other` must be float.
            If the shape of `input` is not equal to the shape of `other`,
            they must be broadcastable to a common shape.

    Returns:
        Tensor, with the same dtype as `input` and `other`.

    Raises:
        TypeError: If `input` or `other` is not a Tensor.
        TypeError: The dtype of `input` or `other` is not float.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x1 = Tensor(np.array([1, 2, 3]).astype(np.float16))
        >>> x2 = Tensor(np.array(2).astype(np.float16))
        >>> output = ops.logaddexp_ext(x1, x2)
        >>> print(output)
        [2.312 2.693 3.312]
    """
    return logaddexp_op(input, other)


def inplace_mul(input, other):
    r"""
    
    """
    return inplace_mul_op(input, other)


def as_strided(input, size, stride, storage_offset=0):
    r"""
    
    """
    return as_strided_op(input, size, stride, storage_offset)


def inplace_bernoulli_tensor(input, p, seed, offset):
    r"""
    
    """
    return inplace_bernoulli_tensor_op(input, p, seed, offset)


def sort_ext(input, dim=-1, descending=False, stable=False):
    r"""
    
    """
    return sort_ext_op(input, dim, descending, stable)


def matmul_add_(x, weight, C):
    r"""
    Fusion Operator of Transpose, Matmul, and InplaceAdd.

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - This API is only supported in Atlas A2 training series for now.
        - This API is only supported on GRAPH mode.

    Args:
        x (Tensor): Matrix A in matrix multiplication, with shape :math:`(k, m)` or :math:`(batch, k, m)`,
            whose type should be float16 or bfloat16.
        weight (Tensor): Matrix B in matrix multiplication, with shape :math:`(k, n)` or :math:`(batch, k, n)`,
            whose type should be float16 or bfloat16.
        C (Tensor): A Tensor acting as both input and output, with type of float32.
            It's shape should be :math:`(m, n)` or :math:`(batch, m, n)`.

    Returns:
        Tensor, has the same shape and data type as `C`.

    Raises:
        TypeError: If the dtype of `weight` is not the same as `x`.
        ValueError: If the ranks of `x` , `weight` and `C` are not the same.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops, nn, context
        >>> context.set_context(mode=context.GRAPH_MODE, jit_config={"jit_level": "O0"})
        >>> class Net(nn.Cell):
        ...     def construct(self, x, weight, C):
        ...        return ops.auto_generate.inplace_matmul_add_op(x, weight, C)
        >>> x = Tensor(np.random.randn(10, 20), mindspore.float16)
        >>> weight = Tensor(np.random.randn(10, 8), mindspore.float16)
        >>> C = Tensor(np.random.randn(20, 8), mindspore.float32)
        >>> output = Net()(x, weight, C)
        >>> print(output.shape)
        (20, 8)
    """
    return inplace_matmul_add_op(x, weight, C)


def less(input, other):
    r"""
    Compute the value of :math:`input < other` element-wise.

    .. math::
        out_{i} =\begin{cases}
            & \text{True,    if } input_{i}<other_{i} \\
            & \text{False,   if } input_{i}>=other_{i}
            \end{cases}

    .. note::
       Support implicit type conversion.

    Args:
        input (Union[Tensor, Number, bool]): The first input.
        other (Union[Tensor, Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: The shape of two inputs are different
        >>> input = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.less(input, 2.0)
        >>> print(output)
        [True  False False]
        >>> # case 2: The shape of two inputs are the same
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> output = mindspore.ops.less(input, other)
        >>> print(output)
        [ False  False  True]
    """
    return less_op(input, other)


def inplace_sign(input):
    r"""
    
    """
    return inplace_sign_op(input)


def floor_mod(x, y):
    r"""
    Compute the remainder of element-wise flooring division of first input by second input.

    If two input have different data types, implicit type conversion rules are followed.
    Inputs must be two tensors or one tensor and one scalar.
    When the inputs are two tensors, their shapes must be broadcastable, and their data types cannot both be bool
    simultaneously.

    .. math::

        out_{i} =\text{floor}(x_{i} // y_{i})

    .. warning::
        - Data of input `y` should not be 0, or the maximum value of its dtype will be returned.
        - When the elements of input exceed 2048, the accuracy of operator cannot guarantee the requirement of
          double thousandths in the mini form.
        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.
        - If shape is expressed as :math:`(D1, D2 ..., Dn)`, then D1\*D2... \*DN<=1000000,n<=8.

    Args:
        x (Union[Tensor, Number, bool]): The first input tensor.
        y (Union[Tensor, Number, bool]): The second input tensor.

    Returns:
        Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: Two tensors with boolean and integer data type.
        >>> input = mindspore.tensor([True, True, False])
        >>> other = mindspore.tensor([1, 2, 4])
        >>> output = mindspore.ops.floor_mod(input, other)
        >>> print(output)
        [0 1 0]
        >>>
        >>> # case 2: One tensor and one scalar.
        >>> input = mindspore.tensor([1, 2, 4])
        >>> other = mindspore.tensor(1.5)
        >>> output = mindspore.ops.floor_mod(input, other)
        >>> print(output)
        [1.  0.5 1. ]
        >>>
        >>> # case 3: When inputs have different data types, type promotion rules are followed.
        >>> input = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> other = mindspore.tensor([1.1, 2.5, -1.5], mindspore.float32)
        >>> output = mindspore.ops.floor_mod(input, other)
        >>> print(output)
        [ 1.   2.  -0.5]
    """
    return floor_mod_op(x, y)


def softshrink(input, lambd=0.5):
    r"""
    Soft Shrink activation function. Calculates the output according to the input elements.

    The formula is defined as follows:

    .. math::
        \text{SoftShrink}(x) =
        \begin{cases}
        x - \lambda, & \text{ if } x > \lambda \\
        x + \lambda, & \text{ if } x < -\lambda \\
        0, & \text{ otherwise }
        \end{cases}

    SoftShrink Activation Function Graph:

    .. image:: ../images/Softshrink.png
        :align: center

    Args:
        input (Tensor): The input of Soft Shrink. Supported dtypes: 

            - Ascend: float16, float32, bfloat16.
            - CPU/GPU: float16, float32.
        lambd (number, optional): The threshold :math:`\lambda` defined by the Soft Shrink formula.
            It should be greater than or equal to 0, default: ``0.5`` .

    Returns:
        Tensor, has the same data type and shape as the input `input`.

    Raises:
        TypeError: If `lambd` is not a float, int or bool.
        TypeError: If `input` is not a tensor.
        TypeError: If dtype of `input` is not float16, float32 or bfloat16.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> import numpy as np
        >>> x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float32)
        >>> output = ops.softshrink(x)
        >>> print(output)
        [[ 0.02979  0.287    0.676  ]
        [ 0.2837   0.1216  -0.6543 ]]
    """
    return softshrink_impl(input, lambd)


def deepcopy(input_x):
    r"""
    Return a deepcopy of input tensor.

    Args:
        input_x (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[0, 1], [2, 1]], dtype=mindspore.int32)
        >>> output = mindspore.ops.deepcopy(input)
        >>> print(output)
        [[0 1]
        [2 1]]
    """
    return identity_op(input_x)


def inplace_clamp_scalar(input, min=None, max=None):
    r"""
    
    """
    return inplace_clamp_scalar_op(input, min, max)


def smooth_l1_loss(prediction, target, beta=1.0, reduction='none'):
    r"""
    Calculate the smooth L1 loss, and the L1 loss function has robustness.

    Refer to :func:`mindspore.ops.smooth_l1_loss` for more details.

    .. warning::
        This API has poor performance on CPU and it is recommended to run it on the Ascend/GPU.

    Args:
        beta (number, optional): A parameter used to control the point where the function will change between
            L1 to L2 loss. Default: ``1.0`` .

            - Ascend: The value should be equal to or greater than zero.
            - CPU/GPU: The value should be greater than zero.
        reduction (str, optional): Apply specific reduction method to the output: ``'none'`` , ``'mean'`` ,
            ``'sum'`` . Default: ``'none'`` .

            - ``'none'``: no reduction will be applied.
            - ``'mean'``: compute and return the mean of elements in the output.
            - ``'sum'``: the output elements will be summed.

    Inputs:
        - **logits** (Tensor) - Input Tensor of any dimension. Supported dtypes: 

          - Ascend: float16, float32, bfloat16.
          - CPU/GPU: float16, float32, float64.
        - **labels** (Tensor) - Ground truth data.
            
          - CPU/Ascend: has the same shape as the `logits`, `logits` and `labels` comply with the implicit type conversion rules to make the data types consistent.
          - GPU: has the same shape and dtype as the `logits`.

    Outputs:
        Tensor, if `reduction` is ``'none'``, then output is a tensor with the same shape as `logits`. Otherwise the shape of output tensor is :math:`()`.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> loss = ops.SmoothL1Loss()
        >>> logits = Tensor(np.array([1, 2, 3]), mindspore.float32)
        >>> labels = Tensor(np.array([1, 2, 2]), mindspore.float32)
        >>> output = loss(logits, labels)
        >>> print(output)
        [0.  0.  0.5]
    """
    return smooth_l1_loss_impl(prediction, target, beta, reduction)


def sub(input, other):
    r"""
    Subtract the second input from the first input element-wise.

    .. math::

        out_{i} = input_{i} - other_{i}

    Note:
        - When the two inputs have different shapes, they must be able to broadcast to a common shape.
        - The two inputs can not be bool type at the same time,
          [True, Tensor(True), Tensor(np.array([True]))] are all considered bool type.
        - Support implicit type conversion and type promotion.

    Args:
        input (Union[Tensor, number.Number, bool]): The first input.
        other (Union[Tensor, number.Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([4, 5, 6], mindspore.int32)
        >>> output = mindspore.ops.sub(input, other)
        >>> print(output)
        [-3 -3 -3]
    """
    return sub_op(input, other)


def detach(x, sync=False):
    r"""
    
    """
    return detach_op(x, sync)


def zero_(input):
    r"""
    
    """
    return inplace_zero_op(input)


def matrix_inverse_ext(input):
    r"""
    Compute the inverse of the input matrix.

    Args:
        input (Tensor): A matrix to be calculated. Input `input` must be at least two dimensions, and the size of
            the last two dimensions must be the same size.

    Returns:
        Tensor, has the same type and shape as input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        ValueError: If the size of the last two dimensions of `input` is not the same.
        ValueError: If the dimension of `input` is 1.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import Tensor, ops
        >>> from mindspore import dtype as mstype
        >>> x = Tensor([[1., 2.], [3., 4.]], mstype.float32)
        >>> print(ops.matrix_inverse_ext(x))
        [[-2.   1. ]
         [ 1.5 -0.5]]
    """
    return matrix_inverse_ext_op(input)


def elu(input_x, alpha=1.0):
    r"""
    Exponential Linear Unit activation function.

    Applies the exponential linear unit function element-wise.
    The activation function is defined as:

    .. math::

        \text{ELU}(x)= \left\{
        \begin{array}{align}
            \alpha(e^{x}  - 1) & \text{if } x \le 0\\
            x & \text{if } x \gt 0\\
        \end{array}\right.

    Where :math:`x` is the element of input Tensor `input_x`, :math:`\alpha` is param `alpha`,
    it determines the smoothness of ELU.

    ELU function graph:

    .. image:: ../images/ELU.png
        :align: center

    Args:
        input_x (Tensor): The input of ELU is a Tensor of any dimension with data type of float16 or float32.
        alpha (float, optional): The alpha value of ELU, the data type is float. Only support ``1.0`` currently.
            Default: ``1.0`` .

    Returns:
        Tensor, has the same shape and data type as `input_x`.

    Raises:
        TypeError: If `alpha` is not a float.
        TypeError: If dtype of `input_x` is neither float16 nor float32.
        ValueError: If `alpha` is not equal to 1.0.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.elu(x)
        >>> print(output)
        [[-0.63212055  4.         -0.99966455]
         [ 2.         -0.99326205  9.        ]]
    """
    elu_op = _get_cache_prim(Elu)(alpha)
    return elu_op(input_x)


def sqrt(x):
    r"""
    Return sqrt of a tensor element-wise.

    .. math::

        out_{i} = \sqrt{x_{i}}

    Args:
        x (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([1.0, 4.0, 9.0], mindspore.float32)
        >>> output = mindspore.ops.sqrt(x)
        >>> print(output)
        [1. 2. 3.]
    """
    return sqrt_op(x)


def add_ext(input, other, alpha=1):
    r"""
    Adds scaled other value to input Tensor.

    .. math::

        out_{i} = input_{i} + alpha \times other_{i}

    Note:
        - When the two inputs have different shapes,
          they must be able to broadcast to a common shape.
        - The two inputs and alpha comply with the implicit type conversion rules to make the data types
          consistent.

    Args:
        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or
            a bool or a tensor whose data type is
            `number <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_ or
            `bool <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_.
        other (Union[Tensor, number.Number, bool]): The second input, is a number.Number or
            a bool or a tensor whose data type is
            `number <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_ or
            `bool <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_.
        alpha (number.Number): A scaling factor applied to `other`, default 1.

    Returns:
        Tensor with a shape that is the same as the broadcasted shape of the input `input` and `other`,
        and the data type is the one with higher precision or higher digits among the two inputs and alpha.

    Raises:
        TypeError: If the type of `input`, `other`, or `alpha` is not one of the following: Tensor, number.Number, bool.
        TypeError: If `alpha` is of type float but `input` and `other` are not of type float.
        TypeError: If `alpha` is of type bool but `input` and `other` are not of type bool.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> x = Tensor(1, mindspore.int32)
        >>> y = Tensor(np.array([4, 5, 6]).astype(np.float32))
        >>> alpha = 0.5
        >>> output = ops.auto_generate.add_ext(x, y, alpha)
        >>> print(output)
        [3. 3.5 4.]
        >>> # the data type of x is int32, the data type of y is float32,
        >>> # alpha is a float, and the output is the data format of higher precision float32.
        >>> print(output.dtype)
        Float32
    """
    return add_ext_op(input, other, alpha)


def tanh(input):
    r"""
    Computes hyperbolic tangent of input tensor element-wise. The Tanh function is defined as:

    .. math::

        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1}

    where :math:`x_i` is an element of the input tensor.

    Tanh Activation Function Graph:

    .. image:: ../images/Tanh.png
        :align: center

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Supported Platforms:
        ``Ascend`` ``GPU``  ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> input = mindspore.tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)
        >>> output = mindspore.ops.tanh(input)
        >>> print(output)
        [0.7615942 0.9640276 0.9950548 0.9993293 0.9999092]
    """
    return tanh_op(input)


def narrow(input, dim, start, length):
    r"""
    Obtains a tensor of a specified length at a specified start position along a specified axis.

    Args:
        input (Tensor): the tensor to narrow.
        dim (int): the axis along which to narrow.
        start (Union[int, Tensor[int]]): the starting dimension.
        length (int): the distance to the ending dimension.

    Returns:
        output (Tensors) - The narrowed tensor.

    Raises:
        ValueError: the rank of `input` is 0.
        ValueError: the value of `dim` is out the range [-input.ndim, input.ndim).
        ValueError: the value of `start` is out the range [-input.shape[dim], input.shape[dim]].
        ValueError: the value of `length` is out the range [0, input.shape[dim]-start].

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import ops
        >>> from mindspore import Tensor
        >>> x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mindspore.int32)
        >>> output = ops.auto_generate.narrow(x, 0, 0, 2)
        >>> print(output)
        [[ 1 2 3]
         [ 4 5 6]]
        >>> output = ops.auto_generate.narrow(x, 1, 1, 2)
        >>> print(output)
        [[ 2 3]
         [ 5 6]
         [ 8 9]]
    """
    return narrow_op(input, dim, start, length)


def sinc(input):
    r"""
    Compute the normalized sinc of input.

    .. math::
        out_i = \begin{cases} \frac{sin(\pi input_i)}{\pi input_i} & input_i\neq 0\\ 
        1 & input_i=0 \end{cases}

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0.62, 0.28, 0.43, 0.62], mindspore.float32)
        >>> output = mindspore.ops.sinc(input)
        >>> print(output)
        [0.47735003 0.8759357  0.7224278  0.47735003]
    """
    return sinc_op(input)


def real(input):
    r"""
    Return a tensor that is the real part of the input. If input is real, it is returned unchanged.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor(1.3+0.4j, mindspore.complex64)
        >>> output = mindspore.ops.real(input)
        >>> print(output)
        1.3
    """
    return real_op(input)


def topkrouter(input, capacity, expert_num, drop_type=0):
    r"""
    TopkRouter implementation in MOE.

    Inputs:
        - **x** (Tensor) - Input Tensor of 3D, Supporting types:[int32, int64]
        - **capacity** (Int64) - The maximum number of tokens each expert can handle
        - **expert_num** (Int64) - The number of expert.
        - **drop_type** (Int64) - S-Drop/K-Drop, 0 means S-Drop, 1 means K-Drop, default 0.

    Outputs:
        tuple(Tensor), tuple of 2 tensors, `dispatch_index` and `combine_inex`.
        - dispatch_index (Tensor) - Token ID processed by each expert.
        - combine_index (Tensor) - The combine index of each token.

    Supported Platforms:
        ``Ascend``
    """
    return topkrouter_op(input, capacity, expert_num, drop_type)


def minimum(input, other):
    r"""
    Compute the minimum of the two input tensors element-wise.

    .. math::
        output_i = \min(input_i, other_i)

    Note:
        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types
          consistent.
        - When the inputs are two tensors, dtypes of them cannot be bool at the same time.
        - When the inputs are one tensor and one scalar, the scalar could only be a constant.
        - Shapes of them are supposed to be broadcast.
        - If one of the elements being compared is a NaN, then that element is returned.

    Args:
        input (Union[Tensor, Number, bool]): The first input.
        other (Union[Tensor, Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1 : same data type
        >>> input = mindspore.tensor([1.0, 5.0, 3.0], mindspore.float32)
        >>> other = mindspore.tensor([4.0, 2.0, 6.0], mindspore.float32)
        >>> mindspore.ops.minimum(input, other)
        Tensor(shape=[3], dtype=Float32, value= [ 1.00000000e+00,  2.00000000e+00,  3.00000000e+00])
        >>>
        >>> # case 2 : the data type is the one with higher precision or higher digits among the two inputs.
        >>> input = mindspore.tensor([1.0, 5.0, 3.0], mindspore.int64)
        >>> other = mindspore.tensor([4.0, 2.0, 6.0], mindspore.float64)
        >>> mindspore.ops.minimum(input, other)
        Tensor(shape=[3], dtype=Float64, value= [ 1.00000000e+00,  2.00000000e+00,  3.00000000e+00])
    """
    return minimum_op(input, other)


def acos_ext(input):
    r"""
    Computes arccosine of input tensors element-wise.

    .. math::

        out_i = \cos^{-1}(input_i)

    Args:
        input (Tensor): The shape of tensor is
            :math:`(N,*)`, where :math:`*` means any number of additional dimensions.

    Returns:
        Tensor, has the same shape as `input`. The dtype of output is float32 when dtype of `input` is in [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)
        >>> output = ops.acos_ext(input)
        >>> print(output)
        [0.7377037  1.5307857 1.2661037 0.9764114]
    """
    return acos_ext_op(input)


def exp2(input):
    r"""
    Calculates the base-2 exponent of the Tensor `input` element by element.

    .. math::

        out_i = 2^{input_i}

    Args:
        input (Tensor): The input Tensor.

    Returns:
        Tensor, which has the same shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)
        >>> output = ops.exp2(x)
        >>> print(output)
        [ 1. 2. 4. 16.]
    """
    return exp2_op(input)


def trunc(input):
    r"""
    Returns a tensor with the truncated integer values of the elements of the input tensor.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.trunc(mindspore.tensor([3.4742, 0.5466, -0.8008, -3.9079]))
        >>> print(output)
        [3. 0. 0. -3.]
    """
    return trunc_op(input)


def tan(input):
    r"""
    Computes tangent of `input` element-wise.

    .. math::

        out_i = \tan(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.tan(mindspore.tensor([-1.0, 0.0, 1.0]))
        >>> print(output)
        [-1.5574077 0. 1.5574077]
    """
    return tan_op(input)


def greater_equal(input, other):
    r"""
    Compute the value of :math:`input >= other` element-wise.

    Args:
        input (Union[Tensor, Number]): The first input.
        other (Union[Tensor, Number]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: The shape of two inputs are different
        >>> input = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.greater_equal(input, 2.0)
        >>> print(output)
        [False  True True]
        >>> # case 2: The shape of two inputs are the same
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> output = mindspore.ops.greater_equal(input, other)
        >>> print(output)
        [ True  True False]
    """
    return greater_equal_op(input, other)


def copy_to_host(x, sync=False):
    r"""
    
    """
    return copy_to_host_op(x, sync)


def moe_distribute_dispatch(x, expert_ids, ep_world_size, ep_rank_id, moe_expert_num, expert_scales=None, scales=None, x_active_mask=None, group_ep=None, group_tp=None, tp_world_size=0, tp_rank_id=0, expert_shard_type=0, shared_expert_num=0, shared_expert_rank_num=0, quant_mode=0, global_bs=0, expert_token_nums_type=0):
    r"""
    Performs token data quantization (optional) and parallel communication for Mixture of Experts (MoE).
    When Tensor Parallelism (TP) communication exists, it first performs Expert Parallelism (EP) AllToAllV
    communication followed by TP AllGatherV communication. Otherwise, only EP AllToAllV communication is performed.

    Notes:
        - A: Maximum tokens to dispatch per rank:
            - For shared experts: A = BS * ep_world_size * shared_expert_num / shared_expert_rank_num
            - For MoE experts:
                - When global_bs = 0: A >= BS * ep_world_size * min(local_expert_num, K)
                - When global_bs != 0: A >= global_bs * min(local_expert_num, K)
        - H (hidden size): Dimension of each token's hidden state
            - Ascend 910B: 0 < H <= 7168, must be multiple of 32
            - Ascend 910_93: H = 7168
        - BS (batch sequence size): Number of tokens processed per rank
            - Ascend 910B: 0 < BS <= 256
            - Ascend 910_93: 0 < BS <= 512
        - K: Number of experts selected per token (0 < K <= 8 and K <= moe_expert_num)
        - server_num: Number of server nodes (supports 2, 4, 8)
        - local_expert_num: Number of experts per rank:
            - Shared expert ranks: local_expert_num = 1
            - MoE expert ranks: local_expert_num = moe_expert_num / (ep_world_size - shared_expert_rank_num)
            (TP communication not supported when localExpertNum > 1)

    Inputs:
        - **x** (Tensor) - Input token data to be sent. 2D tensor with shape [BS, H].
            Supported dtypes: float16, bfloat16. Format: ND, non-contiguous allowed.
        - **expert_ids** (Tensor) - Top-K expert indices for each token. 2D int32 tensor with shape [BS, K].
            Format: ND, non-contiguous allowed.
        - **ep_world_size** (int64) - EP domain size.
            - Ascend 910B: Supports 16, 32, 64.
            - Ascend 910_93: Supports 8, 16, 32, 64, 128, 144, 256, 288.
        - **ep_rank_id** (int64) - Local rank ID in EP domain [0, ep_world_size), must be unique per domain.
        - **moe_expert_num** (int64) - Number of MoE experts (0 < moe_expert_num <= 256),
            must satisfy moe_expert_num % (ep_world_size-shared_expert_rank_num) = 0.
        - **expert_scales** (Tensor) - Top-K expert weights per token.
            - Ascend 910B: 2D float32 tensor [BS, K], ND format, non-contiguous allowed.
            - Ascend 910_93: Unsupported (pass nullptr).
        - **scales** (Tensor) - Expert weights. 2D float32 tensor with shape [shared_expert_num + moe_expert_num, H].
            Pass nullptr for non-quantized scenarios. Format: ND, non-contiguous allowed.
            Note: On Ascend 910B, must be nullptr when HCCL_INTRA_PCIE_ENABLE=1 and HCCL_INTRA_ROCE_ENABLE=0.
        - **x_active_mask** (Tensor) - Reserved parameter (pass nullptr in current version).
        - **group_ep** (str) - EP communication domain name (string length 1-127), must differ from group_tp.
        - **group_tp** (str) - TP communication domain name.
            - Ascend 910B: Unsupported (pass empty string).
            - Ascend 910_93: When TP communication exists, string length 1-127, must differ from group_ep.
        - **tp_world_size** (int64) - TP domain size.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: 0/1 means no TP communication; only 2 supported when TP exists.
        - **tp_rank_id** (int64) - Local rank ID in TP domain.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: [0,1], unique per domain; pass 0 when no TP communication.
        - **expert_shard_type** (int64) - Shared expert distribution type.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: Currently only 0 (shared experts precede MoE experts).
        - **shared_expert_num** (int64) - Number of shared experts.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: Currently 0 (none) or 1 (one shared expert).
        - **shared_expert_rank_num** (int64) - Number of ranks hosting shared experts.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: [0, ep_world_size-1), must satisfy ep_world_size % shared_expert_rank_num = 0 when non-zero.
        - **quant_mode** (int64) - Quantization mode: 0 (none), 2 (dynamic quantization).
        - **global_bs** (int64) - Global batch size across EP domain.
            - Ascend 910B: 256*ep_world_size when BS varies per rank; 0 or BS*ep_world_size when uniform.
            - Ascend 910_93: 0 or BS*ep_world_size.
        - **expert_token_nums_type** (int64) - Semantic meaning of expert_token_nums output:
            0 (prefix sums), 1 (raw counts).

    Outputs:
        - **expand_x** (Tensor) - Expanded token features. 2D tensor [A, H] with dtype matching input.
            Supported dtypes: float16, bfloat16, int8. Format: ND, non-contiguous allowed.
        - **dynamic_scales** (Tensor) - Dynamic quantization scales (when quant_mode=2).
            1D float32 tensor [A]. Format: ND, non-contiguous allowed.
        - **expand_idx** (Tensor) - Token counts per expert for combine operation.
            1D int32 tensor [BS*K]. Format: ND, non-contiguous allowed.
        - **expert_token_nums** (Tensor) - Tokens received per expert.
            1D int64 tensor [local_expert_num]. Format: ND, non-contiguous allowed.
        - **ep_recv_counts** (Tensor) - Tokens received from each EP rank.
            - Ascend 910B: 1D int32 tensor [moe_expert_num + 2 * global_bs * K * server_num]
            - Ascend 910_93: 1D int32 tensor [ep_world_size * max(tp_world_size,1) * local_expert_num]
            Format: ND, non-contiguous allowed.
        - **tp_recv_counts** (Tensor) - Tokens received from each TP rank (when TP exists).
            - Ascend 910B: Not supported.
            - Ascend 910_93: 1D int32 tensor [tp_world_size] when TP exists. Format: ND, non-contiguous allowed.
        - **expand_scales** (Tensor) - Output token weights for combine operation.
            - Ascend 910B: 1D float32 tensor [A]. Format: ND, non-contiguous allowed.
            - Ascend 910_93: Unsupported.

    Raises:
        TypeError: If input dtypes don't match specifications.
        ValueError: If input values violate constraints (e.g., invalid expert indices).
        RuntimeError: If communication domain configuration is invalid.

    Supported Platforms:
        ``Ascend``

    Examples:
    >>> # EP-only communication example (Ascend 910B)
    >>> import mindspore as ms
    >>> from mindspore import Tensor
    >>> from mindspore import ops
    >>> from mindspore.communication import init, get_rank, GlobalComm
    >>> from mindspore.ops.auto_generate import moe_distribute_dispatch
    >>> import numpy as np
    >>> bs = 8
    >>> h = 7168
    >>> k = 8
    >>> ep_world_size = 16
    >>> moe_expert_num = 16
    >>> global_bs = bs * ep_world_size
    >>> x = Tensor(np.random.randn(bs, h), ms.float16)
    >>> expert_ids = Tensor(np.random.randint(0, moe_expert_num, (bs, k)), ms.int32)
    >>> expert_scales = Tensor(np.random.randn(bs, k), ms.float32)
    >>> init()
    >>> rank_id = get_rank()
    >>> out = moe_distribute_dispatch(
    ...     x, expert_ids, ep_world_size, rank_id, moe_expert_num, expert_scales=expert_scales,
    ...     group_ep=GlobalComm.WORLD_COMM_GROUP)
    >>> print(out[0].shape)  # expand_x
    (128, 7168)
    """
    return moe_distribute_dispatch_op(x, expert_ids, ep_world_size, ep_rank_id, moe_expert_num, expert_scales, scales, x_active_mask, group_ep, group_tp, tp_world_size, tp_rank_id, expert_shard_type, shared_expert_num, shared_expert_rank_num, quant_mode, global_bs, expert_token_nums_type)


def diagonal(input, offset=0, dim1=0, dim2=1):
    r"""
    Returns diagonals of the input tensor along specified dimension.

    If `input` is 2-D, returns a 1-D tensor containing the diagonal of `input` with the given offset.

    If `input` has more than two dimensions, then the diagonals of specified 2-D sub-array determined by `dim1` and
    `dim2` is returned. The shape of returned tensor is the original shape with axis1 and axis2 removed and a new
    dimension inserted at the end corresponding to the diagonal.

    Args:
        input (Tensor): The input tensor with at least two dimensions.
        offset (int, optional): Diagonal offset. Default ``0`` .

          - When `offset` is a positive integer, shift the diagonal upward.
          - When `offset` is a negative integer, shift the diagonal downward.
        dim1 (int, optional): The first dimension specifying the 2D plane. Default ``0`` .
        dim2 (int, optional): The second dimension specifying the 2D plane. Default ``1`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[[1, 0, 0],
        ...                            [0, 2, 0],
        ...                            [0, 0, 3]],
        ...                           [[4, 0, 0],
        ...                            [0, 5, 0],
        ...                            [0, 0, 6]],
        ...                           [[7, 0, 0],
        ...                            [0, 8, 0],
        ...                            [0, 0, 9]]])
        >>> mindspore.ops.diagonal(input)
        Tensor(shape=[3, 3], dtype=Int64, value=
        [[1, 0, 0],
         [0, 5, 0],
         [0, 0, 9]])
        >>> mindspore.ops.diagonal(input, offset=1)
        Tensor(shape=[3, 2], dtype=Int64, value=
        [[0, 0],
         [2, 0],
         [0, 6]])
        >>> mindspore.ops.diagonal(input, offset=0, dim1=2, dim2=1)
        Tensor(shape=[3, 3], dtype=Int64, value=
        [[1, 2, 3],
         [4, 5, 6],
         [7, 8, 9]])
    """
    diagonal_op = _get_cache_prim(Diagonal)(offset, dim1, dim2)
    return diagonal_op(input)


def geqrf(input):
    r"""
    Perform a QR decomposition on the input tensor :math:`A = QR`. 

    The tensor is decomposed into the product of an orthogonal matrix `Q` and an upper triangular matrix `R`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tuple(`y`, `tau`) of 2 tensors.

        - **y** (Tensor) - Store the matrices `Q` and `R` implicitly. The matrix `Q` (Householder vectors) is
          stored below the diagonal, and the elements of matrix `R` are stored on and above the diagonal. 
        - **tau** (Tensor) - Store the scaling factors of each Householder transformation (Householder reflection 
          coefficients).

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[-2.0, -1.0], [1.0, 2.0]])
        >>> y, tau = mindspore.ops.geqrf(input)
        >>> print(y)
        [[ 2.236068   1.7888544]
         [-0.236068   1.3416407]]
        >>> print(tau)
        [1.8944271 0.       ]
    """
    return geqrf_op(input)


def shard_identity(input):
    r"""
    A intermediate operator only be created when using mindspore.shard or
    cell.shard during parallel procedure. Will not be exposed to the users.
    """
    return shard_identity_op(input)


def expand_dims_view(input, dim):
    r"""
    Adds an additional dimension to `input_x` at the given axis, the dimension
    of `input_x` should be greater than or equal to 1.

    Note:
        If the specified axis is a negative number, the index is counted
        backward from the end and starts at 1.

    Args:
        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.
        axis (int): Specifies the dimension index at which to expand
            the shape of `input_x`. The value of axis must be in the range
            `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.

    Returns:
        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the
        value of `axis` is 0. It has the same data type as `input_x`.

    Raises:
        TypeError: If `axis` is not an int.
        ValueError: If `axis` is not in the valid range :math:`[-a.ndim-1, a.ndim]`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> from mindspore.ops.auto_generate import ExpandDimsView
        >>> input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)
        >>> output = ExpandDimsView()(input_tensor, 0)
        >>> print(output)
        [[[2. 2.]
          [2. 2.]]]
    """
    return expand_dims_view_op(input, dim)


def abs(input):
    r"""
    Compute the absolute value of a tensor element-wise.

    .. math::

        out_i = |input_i|

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.abs(mindspore.tensor([-1.0, 1.0, 0.0]))
        >>> print(output)
        [1. 1. 0.]
    """
    return abs_op(input)


def inplace_stop_gradient(input):
    r"""
    
    """
    return inplace_stop_gradient_op(input)


def kv_scale_cache(key_scale, value_scale, key_value_scale_cache, batch_valid_length, cache_mode):
    r"""
    
    """
    return kv_scale_cache_op(key_scale, value_scale, key_value_scale_cache, batch_valid_length, cache_mode)


def inplace_add_ext(input, other, alpha=1):
    r"""
    
    """
    return inplace_add_ext_op(input, other, alpha)


def zeros(size, dtype=None):
    r"""
    Creates a tensor filled with value zeros.

    .. warning::
        For argument `size`, Tensor type input will be deprecated in the future version.

    Args:
        size (Union[tuple[int], list[int], int, Tensor]): The shape specified.
        dtype (:class:`mindspore.dtype`, optional): The data type specified. Default: ``None`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> mindspore.ops.zeros(4)
        Tensor(shape=[4], dtype=Float32, value= [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])
        >>> mindspore.ops.zeros((2, 3))
        Tensor(shape=[2, 3], dtype=Float32, value=
        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])
        >>> mindspore.ops.zeros(mindspore.tensor([1, 2, 3]))
        Tensor(shape=[1, 2, 3], dtype=Float32, value=
        [[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
          [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]])
    """
    return zeros_op(size, dtype)


def soft_margin_loss(input, target, reduction='mean'):
    r"""
    Calculate the soft margin loss of input and target.

    Creates a criterion that optimizes a two-class classification
    logistic loss between input tensor :math:`x` and target tensor :math:`y`
    (containing 1 or -1).

    .. math::
        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}

    where :math:`x.nelement()` is the number of elements of :math:`x`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): Predict data. Data type must be float16, float32, bfloat16 (Atlas training series products are not supported).
        target (Tensor): Ground truth data, with the same shape as `input`. In GE mode, the data type should be the same as `input`.
        reduction (str, optional): Apply specific reduction method to the output: ``'none'`` , ``'mean'`` , ``'sum'`` . Default: ``'mean'`` .

            - ``'none'``: no reduction will be applied.
            - ``'mean'``: compute and return the mean of elements in the output.
            - ``'sum'``: the output elements will be summed.

    Returns:
        Tensor or Scalar. If `reduction` is ``'none'``, its shape is the same as `input`.
        Otherwise, a scalar value will be returned.

    Raises:
        TypeError: If `input` or `target` is not a Tensor.
        TypeError: If dtype of `input` or `target` is not float16, float32, bfloat16 (Atlas training series products are not supported).
        ValueError: If shape of `input` is not the same as that of `target`.
        ValueError: If `reduction` is not one of ``'none'``, ``'mean'`` or ``'sum'``.

    Supported Platforms:
        ``Ascend`` ``GPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> logits = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)
        >>> labels = Tensor(np.array([[-1, 1], [1, -1]]), mindspore.float32)
        >>> output = ops.soft_margin_loss(logits, labels)
        >>> print(output)
        0.6764238
    """
    return soft_margin_loss_impl(input, target, reduction)


def cummax(input, axis):
    r"""
    Return the cumulative maximum values and their indices along the given axis of the tensor.

    .. math::
        \begin{array}{ll} \\
            y_{i} = \max(x_{1}, x_{2}, ... , x_{i})
        \end{array}

    Args:
        input (Tensor): The input tensor.
        axis (int): Specify the axis for computation.

    Returns:
        Tuple(max, max_indices) of 2 tensors.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[3, 4, 6, 10], 
        ...                           [1, 6, 7, 9], 
        ...                           [4, 3, 8, 7], 
        ...                           [1, 3, 7, 9]])
        >>> mindspore.ops.cummax(input, axis=0)
        (Tensor(shape=[4, 4], dtype=Int64, value=
         [[ 3,  4,  6, 10],
          [ 3,  6,  7, 10],
          [ 4,  6,  8, 10],
          [ 4,  6,  8, 10]]),
         Tensor(shape=[4, 4], dtype=Int64, value=
         [[0, 0, 0, 0],
          [0, 1, 1, 0],
          [2, 1, 2, 0],
          [2, 1, 2, 0]]))
    """
    return cummax_impl(input, axis)


def flip(input, axis):
    r"""
    Reverses specific dimensions of a tensor.

    .. warning::
        The value range of "axis" is [-dims, dims - 1]. "dims" is the dimension length of "input".

    Args:
        input (Tensor): The target tensor.
            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
        axis (Union[tuple(int), list(int)]): The indices of the dimensions to reverse.

    Outputs:
        Tensor, has the same shape and type as `input`.

    Raises:
        TypeError: If `axis` is neither list nor tuple.
        TypeError: If element of `axis` is not an int.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)
        >>> output = ops.flip(input_x, axis=[1])
        >>> print(output)
        [[4 3 2 1]
         [8 7 6 5]]
        >>> input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)
        >>> output = ops.flip(input_x, axis=[1, 0])
        >>> print(output)
        [[8 7 6 5]
         [4 3 2 1]]
    """
    return reverse_v2_impl(input, axis)


def inplace_hardtanh(input, min_val=-1, max_val=1):
    r"""
    Update the `input` tensor in-place by computing the hardtanh activation function `input`, The activation 
    function is defined as:

    .. math::
        \text{hardtanh}(input) = \begin{cases}
         max\_val, & \text{ if } input > max\_val \\
         min\_val, & \text{ if } input < min\_val \\
         input, & \text{ otherwise. }
        \end{cases}

    Linear region range :math:`[min\_val, max\_val]` can be adjusted using `min_val` and `max_val`.

    Hardtanh Activation Function Graph:

    .. image:: ../images/Hardtanh.png
        :align: center

    .. warning::
        This is an experimental optimizer API that is subject to change.

    Args:
        input (Tensor): Input Tensor.
        min_val (Union[bool, int, float], optional): Minimum value of the linear region range. Default: ``-1.0`` .
        max_val (Union[bool, int, float], optional): Maximum value of the linear region range. Default: ``1.0`` .

    Returns:
        Tensor.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If dtype of `input` is not one of: int8, int16, int32, int64, uint8, float16, float32, bfloat16.
        TypeError: If dtype of `min_val` is neither float nor int.
        TypeError: If dtype of `max_val` is neither float nor int.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> x = Tensor([-1, -2, 0, 2, 1], mindspore.float16)
        >>> ops.auto_generate.inplace_hardtanh(x, min_val=-1.0, max_val=1.0)
        >>> print(x)
        [-1. -1.  0.  1.  1.]
    """
    return inplace_hardtanh_op(input, min_val, max_val)


def inplace_adds_ext(input, other, alpha=1):
    r"""
    
    """
    return inplace_adds_ext_op(input, other, alpha)


def mv(input, vec):
    r"""
    Multiply matrix `input` and vector `vec`.
    If `input` is a tensor with shape :math:`(N, M)` and `vec` is a tensor with shape :math:`(M,)`,
    The output is a 1-D tensor which shape is :math:`(N,)`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input matrix which shape is :math:`(N,M)` and the rank must be 2-D.
        vec (Tensor): The input vector which shape is :math:`(M,)` and the rank is 1-D.

    Returns:
        Tensor, the shape is :math:`(N,)`.

    Raises:
        TypeError: If `input` or `vec` is not a tensor.
        TypeError: If the dtype of `input` or `vec` is not float16 or float32.
        TypeError: If the dtypes of `input` and `vec` are different.
        ValueError: If the `input` is not a 2-D tensor or the `vec` is not a 1-D tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[3., 4.], [1., 6.], [1., 3.]]).astype(np.float32))
        >>> vec = Tensor(np.array([1., 2.]).astype(np.float32))
        >>> output = ops.auto_generate.mv(input, vec)
        >>> print(output)
        [11. 13. 7.]
    """
    return mv_op(input, vec)


def inplace_silu(input):
    r"""
    Computes Sigmoid Linear Unit of input element-wise. The SiLU function is defined as:

    .. math::

        \text{SiLU}(x) = x * \sigma(x),

    where :math:`x` is an element of the input, :math:`\sigma(x)` is Sigmoid function.

    .. math::

        \text{sigma}(x_i) = \frac{1}{1 + \exp(-x_i)},

    SiLU Function Graph:

    .. image:: ../images/SiLU.png
        :align: center

    Args:
        input (Tensor): `input` is :math:`x` in the preceding formula. Input with the data type
            float16 or float32.
        inplace (bool, optional): If it is ``True``, enable the in place update function.
            Default value: ``False``.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Raises:
        TypeError: If dtype of `input` is neither float16 nor float32.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, mint
        >>> import numpy as np
        >>> input = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)
        >>> output = mint.nn.functional.silu(input, inplace=True)
        >>> print(output)
        [-0.269  1.762  -0.1423  1.762  -0.269]
    """
    return inplace_silu_op(input)


def matrix_exp(input):
    r"""
    Computes the exponential of a single or a batch of square matrices.

    .. math::

        matrix\_exp(x) = \sum_{k=0}^{\infty} \frac{1}{k !} x^{k} \in \mathbb{K}^{n \times n}

    where :math:`x` corresponds to `input` .

    Args:
        input (Tensor): The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.
            Must be one of the following types: float16, float32, float64, complex64, complex128.

    Returns:
        Tensor, has the same shape and dtype as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If the dtype of `input` is not one of the following dtype:
                float16, float32, float64, complex64, complex128.
        ValueError: If the rank of `input` is less than 2.
        ValueError: If the size of last two dimensions of `input` are not equal.

    Supported Platforms:


    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)
        >>> output = ops.matrix_exp(input)
        >>> print(output)
        [[2.7182817 5.436563 ]
        [0.        2.7182817]]
    """
    return matrix_exp_op(input)


def acosh(input):
    r"""
    Computes inverse hyperbolic cosine of each element in inputs tensors.

    .. math::

        out_i = \cosh^{-1}(input_i)

    .. note::
        Given an input tensor input, the function computes inverse hyperbolic cosine of every element.
        Input range is [1, inf].

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.acosh(mindspore.tensor([1.0, 1.5, 3.0, 100.0]))
        >>> print(output)
        [0.        0.9624237 1.7627472 5.298292 ]
    """
    return acosh_op(input)


def conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):
    r"""
    Applies a 2D transposed convolution operator over an input image composed of several input planes,
    sometimes also called deconvolution (although it is not an actual deconvolution).

    Refer to :class:`mindspore.mint.nn.ConvTranspose2d` for more details.

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - In the scenario where inputs are non-contiguous, `output_padding` must be less than `stride` .
        - For Atlas training products, when the dtype of input is float32, the `groups` only supports 1.

    Args:
        input (Tensor): input tensor of shape :math:`(minibatch, in\_channels, iH, iW)` or :math:`(in\_channels, iH, iW)` .
        weight (Tensor): filters of shape :math:`(in\_channels, \frac{out\_channels}{\text{groups}}, kH, kW)` .
        bias (Tensor, optional): bias of shape :math:`(out\_channels)` . Default: ``None`` .
        stride (Union[int, tuple(int), list[int]], optional): the stride of the convolving kernel. Can be a single number or a
            tuple :math:`(sH, sW)` . Default: ``1`` .
        padding (Union[int, tuple(int), list[int]], optional): :math:`dilation * (kernel\_size - 1) - padding` zero-padding will
            be added to both sides of each dimension in the input. Can be a single number or a tuple :math:`(padH, padW)` .
            Default: ``0`` .
        output_padding (Union[int, tuple(int), list[int]], optional): additional size added to one side of each dimension in the
            output shape. Can be a single number or a tuple :math:`(out\_padH, out\_padW)` . The value of `output_padding` must
            be less than `stride` or `dilation` . Default: ``0`` .
        groups (int, optional): split input into groups, :math:`in\_channels` should be divisible by the
            number of groups. Default: ``1`` .
        dilation (Union[int, tuple(int), list[int]], optional): the spacing between kernel elements. Can be a single number or
            a tuple :math:`(dH, dW)` . Default: ``1`` .

    Returns:
        Tensor of shape :math:`(minibatch, out\_channels, oH, oW)` or :math:`(out\_channels, oH, oW)` , where

        .. math::
            oH = (iH - 1) \times sH - 2 \times padH + dH \times (kH - 1) + out\_padH + 1
        .. math::
            oW = (iW - 1) \times sW - 2 \times padW + dW \times (kW - 1) + out\_padW + 1

    Raises:
        TypeError: If `stride`, `padding`, `output_padding` or `dilation` is neither an int nor a tuple or a list.
        TypeError: If `groups` is not an int.
        ValueError: If the shape of `bias` is not :math:`(out\_channels)` .
        ValueError: If `stride` or `dilation` is less than 1.
        ValueError: If `padding` or `output_padding` is less than 0.
        ValueError: If `stride`, `padding`, `output_padding` or `dilation` is a tuple whose length is not equal to 2.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.ones([1, 4, 5, 5]), mindspore.float32)
        >>> weight = Tensor(np.ones([4, 8, 3, 3]), mindspore.float32)
        >>> output = ops.conv_transpose2d(x, weight)
        >>> print(output.shape)
        (1, 8, 7, 7)
    """
    return conv_transpose2d_op(input, weight, bias, stride, padding, output_padding, groups, dilation)

scalar_to_tensor_op=ScalarToTensor()

def scalar_to_tensor(input_x, dtype=None):
    r"""
    
    """
    return scalar_to_tensor_op(input_x, dtype)


def ihfft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Computes the two dimensional inverse discrete Fourier transform of real `input`.

    Note:
        - `ihfft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ihfft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `ihfft2`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `ihfft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

          - ``"backward"`` (normalize by :math:`1/n`).
          - ``"forward"`` (no normalization).
          - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `ihfft2()` function.
        If `s` is given, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`s[-1] // 2 + 1`, otherwise :math:`input.shape[dim[-1]] // 2 + 1`.
        When the input is int16, int32, int64, float16, float32, the return value type is complex64.
        When the input is float64, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.ihfft2(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[1.-0.j 0.-0.j 0.-0.j]
         [0.-0.j 0.-0.j 0.-0.j]
         [0.-0.j 0.-0.j 0.-0.j]
         [0.-0.j 0.-0.j 0.-0.j]]
    """
    return ihfft2_op(input, s, dim, norm)


def mish_ext(input):
    r"""
    Computes MISH (A Self Regularized Non-Monotonic Neural Activation Function)
    of input tensors element-wise.

    The formula is defined as follows:

    .. math::
        \text{mish}(input) = input * \tanh(softplus(\text{input}))

    See more details in `A Self Regularized Non-Monotonic Neural Activation Function 
    <https://arxiv.org/abs/1908.08681>`_.

    Mish Activation Function Graph:

    .. image:: ../images/Mish.png
        :align: center

    Args:
        input (Tensor): The input of MISH. Supported dtypes: 

            - Ascend: float16, float32.

    Returns:
        Tensor, has the same type and shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If dtype of `input` is not float16 or float32.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> x = Tensor(np.array([[-1.1, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.mish(x)
        >>> print(output)
        [[-3.0764845e-01 3.9974124e+00 -2.6832507e-03]
         [ 1.9439589e+00 -3.3576239e-02 8.9999990e+00]]
    """
    return mish_ext_op(input)


def adaptive_avg_pool1d(input, output_size):
    r"""
    Performs 1D adaptive average pooling on a multi-plane input signal.
    That is, for any input size, the size of the specified output is L.
    The number of output features is equal to the number of input features.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input of adaptive_avg_pool1d, which is a 2D or 3D tensor,
            with float16 or float32 data type.
        output_size (int): The target output feature size. `output_size` is an integer.

    Returns:
        Tensor, with the same type as the `input`.

        Shape of the output is `input_shape[:len(input_shape) - 1] + [output_size]`.

    Raises:
        ValueError: If `output_size` is not integer.
        TypeError: If `input` is not a Tensor.
        TypeError: If dtype of `input` is not float16, float32.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([[2,3],[3,4]],dtype=mindspore.float16)
        >>> output = ops.auto_generate.adaptive_avg_pool1d(input, 3)
        >>> print(output)
        [[2.  2.5 3. ]
         [3.  3.5 4. ]]
    """
    return adaptive_avg_pool1d_op(input, output_size)


def neg(input):
    r"""
    Returns a tensor with negative values of the input tensor element-wise.

    .. math::
        out_{i} = - input_{i}

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([1, 2, -1, 2, 0, -3.5], mindspore.float32)
        >>> output = mindspore.ops.neg(input)
        >>> print(output)
        [-1.  -2.   1.  -2.   0.   3.5]
    """
    return neg_op(input)


def transpose_view(input, input_perm):
    r"""
    Permutes the dimensions of the input tensor according to input permutation.

    For a 1-D array this has no effect, as a transposed vector is simply the same vector.
    To convert a 1-D array into a 2D column vector please refer to :func:`mindspore.ops.expand_dims`.
    For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given,
    their order indicates how the axes are permuted (see Examples).
    If axes are not provided and a.shape is :math:`(i[0], i[1], ... i[n-2], i[n-1])`,
    then a.transpose().shape is :math:`(i[n-1], i[n-2], ... i[1], i[0])`.

    Note:
        On GPU and CPU, if the value of `input_perm` is negative, its actual value is `input_perm[i] + rank(input)`.
        Negative value of `input_perm` is not supported on Ascend.

    Args:
        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.
        input_perm (tuple[int]): The permutation to be converted. The elements in `input_perm` are composed of
            the indexes of each dimension of `input`. The length of `input_perm` and the shape of `input` must be
            the same. Only constant value is allowed. Must be in the range [-rank(input), rank(input)).

    Returns:
        Tensor, the type of output tensor is the same as `input` and the shape of output tensor is decided by the
        shape of `input` and the value of `input_perm`.

    Raises:
        TypeError: If `input_perm` is not a tuple.
        ValueError: If length of shape of `input` is not equal to length of shape of `input_perm`.
        ValueError: If the same element exists in `input_perm`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)
        >>> input_perm = (0, 2, 1)
        >>> output = ops.TransposeView()(input, input_perm)
        >>> print(output)
        [[[ 1.  4.]
          [ 2.  5.]
          [ 3.  6.]]
         [[ 7. 10.]
          [ 8. 11.]
          [ 9. 12.]]]
    """
    return transpose_view_op(input, input_perm)


def fft2(input, s=None, dim=(-2, -1), norm=None):
    r"""
    Calculates the two dimensional discrete Fourier transform of `input`.

    Note:
        - `fft2` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `fft2` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `fft2`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `fft2`.
            Default: ``(-2, -1)`` , which means transform the last two dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `fft2()` function. The default is the same shape as `input`.
        If `s` is given, the size of the `dim[i]` axis is changed to `s[i]`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((4, 4))
        >>> out = ops.fft2(input, s=(4, 4), dim=(0, 1), norm="backward")
        >>> print(out)
        [[16.+0.j  0.+0.j  0.+0.j  0.+0.j]
         [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]
         [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]
         [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]]
    """
    return fft2_op(input, s, dim, norm)


def transpose_ext_view(input, dim0, dim1):
    r"""
    Interchange two axes of a tensor.

    Args:
        input(Tensor): Input tensor.
        dim0 (int): First axis.
        dim1 (int): Second axis.

    Returns:
        Transposed tensor, has the same data type as `input`.

    Raises:
        TypeError: If argument `input` is not Tensor.
        TypeError: If `dim0` or `dim1` is not integer.
        ValueError: If `dim0` or `dim1` is not in the range of :math:`[-ndim, ndim-1]`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.ones((2, 3, 4), dtype=np.float32))
        >>> output = ops.auto_generate.transpose_ext_view(input, 0, 2)
        >>> print(output.shape)
        (4, 3, 2)
    """
    return transpose_ext_view_op(input, dim0, dim1)


def triangular_solve(b, A, upper=True, transpose=False, unitriangular=False):
    r"""
    Solves a system of equations with a square upper or lower triangular invertible matrix `A` and multiple right-hand sides `b`.

    In symbols, it solves :math:`A X = b` and assumes `A` is square upper-triangular (or lower-triangular if ``upper = False``) and does not have zeros on the diagonal.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        b (Tensor):  A Tensor of shape :math:`(*, M, K)` where `*` is zero of more batch dimensions.
        A (Tensor): A Tensor of shape :math:`(*, M, M)` where `*` is zero of more batch dimensions.
        upper (bool, optional): Whether `A` is upper or lower triangular. Default: ``True``.
        transpose (bool, optional): Solves :math:`op(A) X = b` where :math:`op(A) = A^T` if this flag is True, and :math:`op(A) = A` if it is False, Default: ``False``.
        unitriangular (bool, optional): Whether `A` is unit triangular. If True, the diagonal elements of `A` are assumed to be 1 and not referenced from `A`. Default: ``False``.

    Returns:
        A tuple of X and A.

    Raises:
        TypeError: If argument `b` is not Tensor.
        TypeError: If argument `A` is not Tensor.
        TypeError: If `upper` is not bool.
        TypeError: If `transpose` is not bool.
        TypeError: If `unitriangular` is not bool.
        ValueError: If the rank of `b` or `A` is not in the range of :math:`[2, 6]`.
        ValueError: If the shapes of `b` and `A` are not matched.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import ops
        >>> from mindspore import Tensor
        >>> b = Tensor(np.ones((2, 3, 4), dtype=np.float32))
        >>> A = Tensor(np.ones((2, 3, 3), dtype=np.float32))
        >>> output = ops.triangular_solve(b, A)
        >>> print(output[0])
        [[[ 0.  0.  0.  0.]
          [ 0.  0.  0.  0.]
          [ 1.  1.  1.  1.]]
         [[ 0.  0.  0.  0.]
          [ 0.  0.  0.  0.]
          [ 1.  1.  1.  1.]]]
    """
    return triangular_solve_op(b, A, upper, transpose, unitriangular)


def scalar_cast(input_x, input_y):
    r"""
    The interface is deprecated from version 2.3 and will be removed in a future version,
    please use the `int(x)` or `float(x)` instead.

    Casts the input scalar to another type.

    Args:
        input_x (scalar): The input scalar. Only constant value is allowed.
        input_y (mindspore.dtype): The type to be cast. Only constant value is allowed. And the value should only be mindspore.int64, mindspore.float64, or mindspore.bool.

    Returns:
        Scalar. The type is the same as the python type corresponding to `input_y`.

    Raises:
        ValueError: if input_y's value is invalid.

    Supported Platforms:
        Deprecated

    Examples:
        >>> import mindspore
        >>> from mindspore import ops
        >>> output = ops.scalar_cast(255.0, mindspore.int64)
        >>> print(output)
        255
    """
    return scalar_cast_op(input_x, input_y)


def adaptive_avg_pool2d_grad_ext(grad_output, x):
    r"""
    
    """
    return adaptive_avg_pool2d_grad_ext_op(grad_output, x)


def bmm_ext(input, mat2):
    r"""
    Performs batch matrix-matrix multiplication of two three-dimensional tensors.

    .. math::
        \text{output}= \text{input} @ \text{mat2}

    Args:
        input (Tensor): The first batch of matrices to be multiplied. Must be a three-dimensional tensor of shape `(b, n, m)`.
        mat2 (Tensor): The second batch of matrices to be multiplied. Must be a three-dimensional tensor of shape `(b, m, p)`.

    Returns:
        Tensor, the output tensor of shape `(b, n, p)`, where each matrix is the product of the corresponding matrices in the input batches.

    Raises:
        ValueError: If `input` or `mat2` is not three-dimensional tensors.
        ValueError: If the length of the third dimension of `input` is not equal to the length of the second dimension of `mat2`.
        ValueError: If the batch size of the inputs is not equal to the batch size of the mat2.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> a = Tensor(np.ones(shape=[2, 3, 4]), mindspore.float32)
        >>> b = Tensor(np.ones(shape=[2, 4, 5]), mindspore.float32)
        >>> output = ops.auto_generate.bmm_ext(a, b)
        >>> print(output)
        [[[4. 4. 4. 4. 4.]
          [4. 4. 4. 4. 4.]
          [4. 4. 4. 4. 4.]]
         [[4. 4. 4. 4. 4.]
          [4. 4. 4. 4. 4.]
          [4. 4. 4. 4. 4.]]]
    """
    return bmm_ext_op(input, mat2)


def asinh_ext(input):
    r"""
    Computes inverse hyperbolic sine of the input element-wise.

    .. math::

        out_i = \sinh^{-1}(input_i)

    Args:
        input (Tensor): The input tensor of inverse hyperbolic sine function.

    Returns:
        Tensor, has the same shape as `input`. The dtype of output is float32 when dtype of `input` is in [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)
        >>> output = ops.asinh_ext(input)
        >>> print(output)
        [-2.3124385  1.1947632  1.8184465  5.298342 ]
    """
    return asinh_ext_op(input)


def sub_ext(input, other, alpha=1):
    r"""
    Subtracts scaled other value from input Tensor.

    .. math::

        out_{i} = input_{i} - alpha \times other_{i}

    Note:
        - When the two inputs have different shapes,
          they must be able to broadcast to a common shape.
        - The two inputs and alpha comply with the implicit type conversion rules to make the data types
          consistent.

    Args:
        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or
            a bool or a tensor whose data type is
            `number <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_ or
            `bool <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_.
        other (Union[Tensor, number.Number, bool]): The second input, is a number.Number or
            a bool or a tensor whose data type is
            `number <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_ or
            `bool <https://www.mindspore.cn/docs/en/master/api_python/mindspore/mindspore.dtype.html>`_.
        alpha (number.Number): A scaling factor applied to `other`, default 1.

    Returns:
        Tensor with a shape that is the same as the broadcasted shape of the input `input` and `other`,
        and the data type is the one with higher precision or higher digits among the two inputs and alpha.

    Raises:
        TypeError: If the type of `input`, `other`, or `alpha` is not one of the following: Tensor, number.Number, bool.
        TypeError: If `alpha` is of type float but `input` and `other` are not of type float.
        TypeError: If `alpha` is of type bool but `input` and `other` are not of type bool.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> x = Tensor(np.array([4, 5, 6]).astype(np.float32))
        >>> y = Tensor(1, mindspore.int32)
        >>> alpha = 0.5
        >>> output = ops.auto_generate.sub_ext(x, y, alpha)
        >>> print(output)
        [3.5 4.5 5.5]
        >>> # the data type of x is float32, the data type of y is int32,
        >>> # alpha is a float, and the output is the data format of higher precision float32.
        >>> print(output.dtype)
        Float32
    """
    return sub_ext_op(input, other, alpha)


def ifftn(input, s=None, dim=None, norm=None):
    r"""
    Computes the N dimensional inverse discrete Fourier transform of `input`.

    Note:
        - `ifftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ifftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `ifftn`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `ifftn`.
            Default: ``None`` , which means transform the all dimension of `input`, or the last `len(s)` dimensions if s is given.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `ifftn()` function. The default is the same shape as `input`.
        If `s` is given, the size of the `dim[i]` axis is changed to `s[i]`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((2, 2, 2))
        >>> out = ops.ifftn(input, s=(2, 2, 2), dim=(0, 1, 2), norm="backward")
        >>> print(out)
        [[[1.+0.j 0.+0.j]
          [0.+0.j 0.+0.j]]
          [[0.+0.j 0.+0.j]
          [0.+0.j 0.+0.j]]]
    """
    return ifftn_op(input, s, dim, norm)


def baddbmm(input, batch1, batch2, beta=1, alpha=1):
    r"""
    The result is the sum of the input and a batch matrix-matrix product of matrices in batch1 and batch2.
    The formula is defined as follows:

    .. math::
        \text{out}_{i} = \beta \text{input}_{i} + \alpha (\text{batch1}_{i} \mathbin{@} \text{batch2}_{i})

    Args:
        input (Tensor): The input Tensor. When batch1 is a :math:`(C, W, T)` Tensor and batch2 is a
            :math:`(C, T, H)` Tensor, input must be broadcastable with :math:`(C, W, H)` Tensor.
        batch1 (Tensor): :math:`batch1` in the above formula. Must be 3-D Tensor, dtype is same as input.
        batch2 (Tensor): :math:`batch2` in the above formula. Must be 3-D Tensor, dtype is same as input.
        
    Keyword Args:
        beta (Union[float, int], optional): multiplier for input. Default: ``1`` .
        alpha (Union[float, int], optional): multiplier for :math:`batch1 @ batch2`. Default: ``1`` .

    Returns:
        Tensor, has the same dtype as input, shape will be :math:`(C, W, H)`.

    Raises:
        TypeError: If the type of `input`, `batch1`, `batch2` is not Tensor.
        TypeError: If the types of `input`, `batch1`, `batch2` are different.
        ValueError: If `batch1` and `batch2` are not 3-D tensors.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.ones([1, 3, 3]).astype(np.float32))
        >>> batch1 = Tensor(np.ones([1, 3, 4]).astype(np.float32))
        >>> batch2 = Tensor(np.ones([1, 4, 3]).astype(np.float32))
        >>> output =  ops.baddbmm_ext(input, batch1, batch2)
        >>> print(output)
        [[[5. 5. 5.]
        [5. 5. 5.]
        [5. 5. 5.]]]
    """
    return baddbmm_op(input, batch1, batch2, beta, alpha)


def tensor_scatter_add(input_x, indices, updates):
    r"""
    tensor_scatter_add(input_x, indices, updates)

    Return a new tensor by adding the values from `updates` in `input_x` indicated by `indices` .

    .. math::
        output\left [indices  \right ] = input\_x + update

    The figure below shows an example of the computational process of tensor_scatter_add:

    .. image:: ../images/TensorScatterAdd.png
        :align: center

    Note:
        If the values in `indices` are out of the index range of the input `input_x`:

        - On GPU, if some values of the `indices` are out of bound, instead of raising an index error,
          the corresponding `updates` will not be updated to self tensor.
        - On CPU, if some values of the `indices` are out of bound, raising an index error.
        - On Ascend, out of bound checking is not supported, if some values of the `indices` are out of bound,
          unknown errors may be caused.

    Args:
        input_x (Tensor): The target tensor. The dimension of input_x must be no less than indices.shape[-1].
        indices (Tensor): The index of input tensor whose data type is mindspore.int32 or mindspore.int64. The rank must be at least 2.
        updates (Tensor): The tensor to update the input tensor, has the same type as input, and updates.
            Shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input_x = mindspore.tensor([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]], mindspore.float32)
        >>> indices = mindspore.tensor([[0, 0], [0, 0]], mindspore.int32)
        >>> updates = mindspore.tensor([1.0, 2.2], mindspore.float32)
        >>> output = mindspore.ops.tensor_scatter_add(input_x, indices, updates)
        >>> print(output)
        [[ 3.1  0.3  3.6]
         [ 0.4  0.5 -3.2]]
    """
    return tensor_scatter_add_op(input_x, indices, updates)


def sequence_concat(x, axis=0):
    r"""
    Support sequence Concat operation.

    .. note::
        This is only for internal used.

    Args:
        axis (Int): The axis to be concat.

    Inputs:
        - **sequence** (Union[List, Tuple]) - A sequence of Tensor objects with same shape and type..

    Outputs:
        The concat of all input.

    Raises:
        TypeError: The 'sequence' is not list or tuple.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``
    """
    sequence_concat_op = _get_cache_prim(SequenceConcat)(axis)
    return sequence_concat_op(x)


def clamp_tensor(input, min=None, max=None):
    r"""
    Clamps tensor values between the specified minimum value and maximum value.

    Limits the value of :math:`input` to a range, whose lower limit is `min` and upper limit is `max` .

    .. math::

        out_i= \left\{
        \begin{array}{align}
            max & \text{ if } input_i\ge max \\
            input_i & \text{ if } min \lt input_i \lt max \\
            min & \text{ if } input_i \le min \\
        \end{array}\right

    Note:
        - `min` and `max` cannot be None at the same time;
        - When `min` is None and `max` is not None, the elements in Tensor larger than `max` will become `max`;
        - When `min` is not None and `max` is None, the elements in Tensor smaller than `min` will become `min`;
        - If `min` is greater than `max`, the value of all elements in Tensor will be set to `max`;
        - The data type of `input`, `min` and `max` should support implicit type conversion and cannot be bool type.

    Args:
          input (Tensor): Input data, which type is Tensor. Tensors of arbitrary dimensions are supported.
          min (Tensor, optional): The minimum value. Default: ``None`` .
          max (Tensor, optional): The maximum value. Default: ``None`` .

    Returns:
          Tensor, a clipped Tensor.
          The data type and shape are the same as input.

    Raises:
          ValueError: If both `min` and `max` are None.
          TypeError: If the type of `input` is not Tensor.
          TypeError: If the type of `min` is not in None, Tensor.
          TypeError: If the type of `max` is not in None, Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> # case 1: the data type of input is Tensor
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore.ops.auto_generate import clamp_tensor
        >>> import numpy as np
        >>> min_value = Tensor(5, mindspore.float32)
        >>> max_value = Tensor(20, mindspore.float32)
        >>> input = Tensor(np.array([[1., 25., 5., 7.], [4., 11., 6., 21.]]), mindspore.float32)
        >>> output = clamp_tensor(input, min_value, max_value)
        >>> print(output)
        [[ 5. 20.  5.  7.]
        [ 5. 11.  6. 20.]]
    """
    return clamp_tensor_op(input, min, max)


def divmod_scalar_(input, other, rounding_mode=None):
    r"""
    
    """
    return inplace_divmods_op(input, other, rounding_mode)


def fftfreq(n, d=1.0, dtype=None):
    r"""
    Computes the discrete Fourier Transform sample frequencies for a signal of size `n`.
    For instance, Given a length `n` and a sample spacing `d` , the returned result `f` is:

    .. math::
        f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)

    Note:
        - `fftfreq` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `fftfreq` is not supported on Windows platform yet.

    Args:
        n (int): Window length.
        d (float, optional): Sample spacing (inverse of the sampling rate). Default: ``1.0`` .
        dtype (mindspore.dtype, optional): The dtype of the returned frequencies. Default: ``None`` represents float32.

    Returns:
        Tensor, Array of length ``n`` containing the sample frequencies.

    Raises:
        ValueError: If `n` is less than 1.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import ops
        >>> out = ops.fftfreq(n=4, d=1.0)
        >>> print(out)
        [ 0.    0.25 -0.5  -0.25]
    """
    return fftfreq_op(n, d, dtype)


def tensor_scatter_elements(data, indices, updates, axis=0, reduce='none'):
    r"""
    
    """
    tensor_scatter_elements_op = _get_cache_prim(TensorScatterElements)(axis, reduce)
    return tensor_scatter_elements_op(data, indices, updates)


def fftn(input, s=None, dim=None, norm=None):
    r"""
    Computes the N dimensional discrete Fourier transform of `input`.

    Note:
        - `fftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `fftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `fftn`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `fftn`.
            Default: ``None`` , which means transform the all dimension of `input`, or the last `len(s)` dimensions if s is given.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `fftn()` function. The default is the same shape as `input`.
        If `s` is given, the size of the `dim[i]` axis is changed to `s[i]`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((2, 2, 2))
        >>> out = ops.fftn(input, s=(2, 2, 2), dim=(0, 1, 2), norm="backward")
        >>> print(out)
        [[[8.+0.j 0.+0.j]
          [0.+0.j 0.+0.j]]
          [[0.+0.j 0.+0.j]
          [0.+0.j 0.+0.j]]]
    """
    return fftn_op(input, s, dim, norm)


def kl_div_grad(grad_output, input, target, reduction='mean', log_target=False):
    r"""
    
    """
    return kl_div_grad_op(grad_output, input, target, reduction, log_target)


def hardsigmoid(input):
    r"""
    Hard Sigmoid activation function. Calculates the output according to the input elements.

    Hard Sigmoid is defined as:

    .. math::
        \text{HardSigmoid}(input) =
        \begin{cases}
        0, & \text{ if } input \leq -3, \\
        1, & \text{ if } input \geq +3, \\
        input/6 + 1/2, & \text{ otherwise }
        \end{cases}

    HardSigmoid Activation Function Graph:

    .. image:: ../images/Hardsigmoid.png
        :align: center

    Args:
        input (Tensor): The input Tensor.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `input` is neither int nor float.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)
        >>> output = ops.hardsigmoid(input)
        >>> print(output)
        [0.3333 0.1666 0.5    0.8335 0.6665]
    """
    return hsigmoid_op(input)


def linalg_qr(A, mode='reduced'):
    r"""
    Orthogonal decomposition of the input :math:`A = QR`.

    Where `A` is an input tensor, a dimension is at least 2, and `A` may be represented as a product
    form of an orthogonal matrix `Q` and an upper triangular matrix `R`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        A (Tensor): The calculated matrix, `A` is at least two-dimensional.
        mode (str, optional): Matrix decomposition mode. The options are ``reduced``,
            ``complete``, and ``r``. The default value is ``reduced``.

            - ``"reduced"``: For input :math:`A(*, m, n)` output simplified size :math:`Q(*, m, k)`, :math:`R(*, k, n)`, where k is the minimum value of m and n.
            - ``"complete"``: For input :math:`A(*, m, n)` output full-size :math:`Q(*, m, m)`, :math:`R(*, m, n)`.
            - ``"r"``: Only :math:`R(*, k, n)` in the reduced scenario is calculated, where k is the minimum value of m and n, and Q is returned as an empty tensor.

    Returns:
        - **Q** (Tensor) - The shape is :math:`Q(*, m, k)` or :math:`(*, m, n)`, has the same dtype as `A`.
        - **R** (Tensor) - The shape is :math:`Q(*, k, n)` or :math:`(*, m, n)`, has the same dtype as `A`.

    Raises:
        TypeError: If `A` is not a Tensor.
        TypeError: If the dtype of `A` is not the float32.
        ValueError: If `A` is not empty and its dimension is less than 2 dimensions.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[1.0, 1.0, 2.0, 4.0], [1.0, 1.0, 2.0, 4.0]]), mindspore.float32)
        >>> output = ops.auto_generate.linalg_qr(x)
        >>> print(output)
        (Tensor(shape=[2, 2], dtype=Float32, value=
        [[-7.07106829e-01, -7.07106769e-01],
        [-7.07106769e-01,  7.07106829e-01]]),
        Tensor(shape=[2, 4], dtype=Float32, value=
        [[-1.41421354e+00, -1.41421354e+00, -2.82842731e+00, -5.65685463e+00],
        [ 0.00000000e+00,  3.42285418e-08,  0.00000000e+00,  0.00000000e+00]]))
    """
    return linalg_qr_op(A, mode)


def scatter_nd(indices, updates, shape):
    r"""
    Scatters a tensor into a new tensor depending on the specified indices.

    Creates an empty tensor with the given `shape`, and set values by scattering the update tensor
    depending on indices. The empty tensor has rank :math:`P` and `indices` has rank :math:`Q`.

    The `shape` is :math:`(s_0, s_1, ..., s_{P-1})`, where :math:`P \ge 1`.

    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)`, where :math:`Q \ge 2` and :math:`N \le P`.

    The last dimension of `indices` (with length :math:`N` ) indicates slices along the :math:`N` th dimension of the
    empty tensor.

    `updates` is a tensor of rank :math:`Q-1+P-N`, and
    its shape is :math:`(i_0, i_1, ..., i_{Q-2}, s_N, s_{N+1}, ..., s_{P-1})`.

    If `indices` contains duplicates, the duplicate `updates` are summed.

    The following figure shows the calculation process of inserting two new value matrices into the first dimension
    with rank-3:

    .. image:: ScatterNd.png

    Args:
        indices (Tensor): The index of scattering in the new tensor.
            The rank of `indices` must be at least 2 and `indices.shape[-1] <= len(shape)`.
        updates (Tensor): The source tensor to be updated.
            It has shape `indices.shape[:-1] + shape[indices.shape[-1]:]`.
        shape (tuple[int]): The shape of the output tensor. `shape` can not be empty, and the elements
            in `shape` must be greater than or equal to 1.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> indices = mindspore.tensor([[0], [2]], mindspore.int32)
        >>> updates = mindspore.tensor([[[1, 1, 1, 1], [2, 2, 2, 2],
        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],
        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],
        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]], mindspore.float32)
        >>> shape = (4, 4, 4)
        >>> output = mindspore.ops.scatter_nd(indices, updates, shape)
        >>> print(output)
        [[[1. 1. 1. 1.]
          [2. 2. 2. 2.]
          [3. 3. 3. 3.]
          [4. 4. 4. 4.]]
         [[0. 0. 0. 0.]
          [0. 0. 0. 0.]
          [0. 0. 0. 0.]
          [0. 0. 0. 0.]]
         [[1. 1. 1. 1.]
          [2. 2. 2. 2.]
          [3. 3. 3. 3.]
          [4. 4. 4. 4.]]
         [[0. 0. 0. 0.]
          [0. 0. 0. 0.]
          [0. 0. 0. 0.]
          [0. 0. 0. 0.]]]
        >>> indices = mindspore.tensor([[0, 1], [1, 1]], mindspore.int32)
        >>> updates = mindspore.tensor([3.2, 1.1], mindspore.float32)
        >>> shape = (3, 3)
        >>> output = mindspore.ops.scatter_nd(indices, updates, shape)
        >>> # In order to facilitate understanding, explain the operator pseudo-operation process step by step:
        >>> # Step 1: Generate an empty Tensor of the specified shape according to the shape
        >>> # [
        >>> #     [0. 0. 0.]
        >>> #     [0. 0. 0.]
        >>> #     [0. 0. 0.]
        >>> # ]
        >>> # Step 2: Modify the data at the specified location according to the indicators
        >>> # 0th row of indices is [0, 1], 0th row of updates is 3.2.
        >>> # means that the empty tensor in the 0th row and 1st col set to 3.2
        >>> # [
        >>> #     [0. 3.2. 0.]
        >>> #     [0. 0.   0.]
        >>> #     [0. 0.   0.]
        >>> # ]
        >>> # 1th row of indices is [1, 1], 1th row of updates is 1.1.
        >>> # means that the empty tensor in the 1th row and 1st col set to 1.1
        >>> # [
        >>> #     [0. 3.2. 0.]
        >>> #     [0. 1.1  0.]
        >>> #     [0. 0.   0.]
        >>> # ]
        >>> # The final result is as follows:
        >>> print(output)
        [[0. 3.2 0.]
         [0. 1.1 0.]
         [0. 0.  0.]]
    """
    return scatter_nd_op(indices, updates, shape)


def cdist(x1, x2, p=2.0):
    r"""
    Computes p-norm distance between each pair of row vectors of two input Tensors.

    Note:
        - On Ascend, the supported dtypes are float16 and float32.
        - On CPU, the supported dtypes are float16 and float32.
        - On GPU, the supported dtypes are float32 and float64.

    Args:
        x1 (Tensor): Input tensor of shape :math:`(B, P, M)`.
            Letter :math:`B` represents 0 or positive int number.
            When :math:`B` is equal to 0, it means this dimension can be ignored,
            i.e. shape of the tensor is :math:`(P, M)`.
        x2 (Tensor): Input tensor of shape :math:`(B, R, M)`, has the same dtype as `x1`.
        p (float, optional): P value for the p-norm distance to calculate between each
            vector pair, P >= 0. Default: ``2.0`` .

    Returns:
        Tensor, p-norm distance, has the same dtype as `x1`, its shape is :math:`(B, P, R)`.

    Raises:
        TypeError: If `x1` or `x2` is not Tensor.
        TypeError: If dtype of `x1` or `x2` is not listed in the "Note" above.
        TypeError: If `p` is not float32.
        ValueError: If `p` is negative.
        ValueError: If dimension of `x1` is not the same as `x2`.
        ValueError: If dimension of `x1` or `x2` is neither 2 nor 3.
        ValueError: If the batch dim of `x1` and `x2` can not broadcast.
        ValueError: If the number of columns of `x1` is not the same as that of `x2`.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[[1.0, 1.0], [2.0, 2.0]]]).astype(np.float32))
        >>> y = Tensor(np.array([[[3.0, 3.0], [3.0, 3.0]]]).astype(np.float32))
        >>> output = ops.cdist(x, y, 2.0)
        >>> print(output)
        [[[2.8284273 2.8284273]
          [1.4142137 1.4142137]]]
    """
    cdist_op = _get_cache_prim(Cdist)(p)
    return cdist_op(x1, x2)


def not_equal(input, other):
    r"""
    Alias for :func:`mindspore.ops.ne` .

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``
    """
    return not_equal_op(input, other)


def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False):
    r"""
    Solve the linear system :math:`a x = b` for `x`, Assuming `a` is a triangular matrix.

    Note:
        - `solve_triangular` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `solve_triangular` is not supported on Windows platform yet.

    Args:
        a (Tensor): A triangular matrix of shape :math:`(*, M, M)` where :math:`*` is zero or more batch dimensions.
        b (Tensor): A Tensor of shape :math:`(*, M)` or :math:`(*, M, N)`. Right-hand side matrix in :math:`a x = b`.
        trans (Union[int, str], optional): Type of system to solve. Default: ``0``.

            ========  =========
            trans     system
            ========  =========
            0 or 'N'  a x  = b
            1 or 'T'  a^T x = b
            2 or 'C'  a^H x = b
            ========  =========

        lower (bool, optional): Use only data contained in the lower triangle of `a`. Default: ``False``.
        unit_diagonal (bool, optional): If ``True``, diagonal elements of :math:`a` are assumed to be 1 and
            will not be referenced. Default: ``False``.

    Returns:
        Tensor of shape :math:`(*, M)` or :math:`(*, M, N)`,
        which is the solution to the system :math:`a x = b`.
        Shape of :math:`x` matches :math:`b`.

    Raises:
        ValueError: If `a` is less than 2 dimension.
        ValueError: if `a` is not square matrix.
        TypeError: If dtype of `a` and `b` are not the same.
        ValueError: If the shape of `a` and `b` are not matched.
        ValueError: If `trans` is not in set {0, 1, 2, 'N', 'T', 'C'}.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import numpy as onp
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> from mindspore.ops import solve_triangular
        >>> a = Tensor(onp.array([[3, 0, 0, 0], [2, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]], onp.float32))
        >>> b = Tensor(onp.array([3, 1, 3, 4], onp.float32))
        >>> x = solve_triangular(a, b, lower=True, unit_diagonal=False, trans='N')
        >>> print(x)
        [ 1. -1.  2.  2.]
        >>> print(a @ x)  # Check the result
        [3. 1. 3. 4.]
    """
    return solve_triangular_op(a, b, trans, lower, unit_diagonal)


def broadcast_to(input, shape):
    r"""
    Broadcasts input tensor to a given shape. The dim of input must be smaller
    than or equal to that of target. Suppose input shape is :math:`(x_1, x_2, ..., x_m)`,
    target shape is :math:`(*, y_1, y_2, ..., y_m)`, where :math:`*` means any additional dimension.
    The broadcast rules are as follows:

    Compare the value of :math:`x_m` and :math:`y_m`, :math:`x_{m-1}` and :math:`y_{m-1}`, ...,
    :math:`x_1` and :math:`y_1` consecutively and
    decide whether these shapes are broadcastable and what the broadcast result is.

    If the value pairs at a specific dim are equal, then that value goes right into that dim of output shape.
    With an input shape :math:`(2, 3)`, target shape :math:`(2, 3)` , the inferred output shape is :math:`(2, 3)`.

    If the value pairs are unequal, there are three cases:

    Case 1: If the value of the target shape in the dimension is -1, the value of the
    output shape in the dimension is the value of the corresponding input shape in the dimension.
    With an input shape :math:`(3, 3)`, target
    shape :math:`(-1, 3)`, the output shape is :math:`(3, 3)`.

    Case 2: If the value of target shape in the dimension is not -1, but the corresponding
    value in the input shape is 1, then the corresponding value of the output shape
    is that of the target shape. With an input shape :math:`(1, 3)`, target
    shape :math:`(8, 3)`, the output shape is :math:`(8, 3)`.

    Case 3: If the corresponding values of the two shapes do not satisfy the above cases,
    it means that broadcasting from the input shape to the target shape is not supported.

    So far we got the last m dims of the outshape, now focus on the first :math:`*` dims, there are
    two cases:

    If the first :math:`*` dims of output shape does not have -1 in it, then fill the input
    shape with ones until their length are the same, and then refer to
    Case 2 mentioned above to calculate the output shape. With target shape :math:`(3, 1, 4, 1, 5, 9)`,
    input shape :math:`(1, 5, 9)`, the filled input shape will be :math:`(1, 1, 1, 1, 5, 9)` and thus the
    output shape is :math:`(3, 1, 4, 1, 5, 9)`.

    If the first :math:`*` dims of output shape have -1 in it, it implies this -1 is corresponding to
    a non-existing dim so they're not broadcastable. With target shape :math:`(3, -1, 4, 1, 5, 9)`,
    input shape :math:`(1, 5, 9)`, instead of operating the dim-filling process first, it raises errors directly.

    Args:
        input (Tensor): The input tensor.
        shape (tuple[int]): The target shape.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> shape = (2, 3)
        >>> x = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.broadcast_to(x, shape)
        >>> print(output)
        [[1. 2. 3.]
         [1. 2. 3.]]
        >>> shape = (-1, 2)
        >>> x = mindspore.tensor([[1], [2]], mindspore.float32)
        >>> output = mindspore.ops.broadcast_to(x, shape)
        >>> print(output)
        [[1. 1.]
         [2. 2.]]
    """
    return broadcast_to_impl(input, shape)


def add_scalar(input, other, alpha=1):
    r"""
    
    """
    return add_scalar_op(input, other, alpha)


def exp(input):
    r"""
    Compute exponential of the input tensor element-wise.

    .. math::
        out_i = e^{x_i}

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0.0, 1.0, 3.0], mindspore.float32)
        >>> output = mindspore.ops.exp(input)
        >>> print(output)
        [ 1.        2.7182817 20.085537]
    """
    return exp_op(input)


def moe_distribute_combine(expand_x, expert_ids, expand_idx, ep_send_counts, expert_scales, ep_world_size, ep_rank_id, moe_expert_num, tp_send_counts=None, x_active_mask=None, activate_scale=None, weight_scale=None, group_list=None, expand_scales=None, group_ep=None, group_tp=None, tp_world_size=0, tp_rank_id=0, expert_shard_type=0, shared_expert_num=0, shared_export_rank_num=0, global_bs=0, out_dtype=0, common_quant_mode=0, group_list_type=0):
    r"""
    Parallel communication for Mixture of Experts (MoE). When Tensor Parallelism (TP) communication exists,
    it first ReduceScatter performs communication followed by Expert Parallelism (EP) AllToAllV communication.
    Otherwise, only EP AllToAllV communication is performed. Finally multiply the received data by weight and
    add them up.

    Notes:
        This function must be used in conjunction with function `moe_distribute_dispatch`.
        - A: Maximum tokens to dispatch per rank:
            - For shared experts: A = BS * ep_world_size * shared_expert_num / shared_expert_rank_num
            - For MoE experts:
                - When global_bs = 0: A >= BS * ep_world_size * min(local_expert_num, K)
                - When global_bs != 0: A >= global_bs * min(local_expert_num, K)
        - H (hidden size): Dimension of each token's hidden state
            - Ascend 910B: 0 < H <= 7168, must be multiple of 32
            - Ascend 910_93: H = 7168
        - BS (batch sequence size): Number of tokens processed per rank
            - Ascend 910B: 0 < BS <= 256
            - Ascend 910_93: 0 < BS <= 512
        - K: Number of experts selected per token (0 < K <= 8 and K <= moe_expert_num)
        - server_num: Number of server nodes (supports 2, 4, 8)
        - local_expert_num: Number of experts per rank:
            - Shared expert ranks: local_expert_num = 1
            - MoE expert ranks: local_expert_num = moe_expert_num / (ep_world_size - shared_expert_rank_num)
            (TP communication not supported when localExpertNum > 1)

    Inputs:
        - **expand_x** (Tensor) - Expanded token features. 2D tensor [A, H] with dtype matching input.
            Supported dtypes: float16, bfloat16, int8. Format: ND, non-contiguous allowed.
        - **expert_ids** (Tensor) - Top-K expert indices for each token. 2D int32 tensor with shape [BS, K].
            Format: ND, non-contiguous allowed.
        - **expert_idx** (Tensor) - Token counts per expert, it's the output of dispatch operation. 
            1D int32 tensor [BS*K]. Format: ND, non-contiguous allowed.
        - **ep_send_counts** (Tensor) - Tokens that each EP rank needs to send, it's the output of dispatch operation.
            - Ascend 910B: 1D int32 tensor [moe_expert_num + 2 * global_bs * K * server_num]
            - Ascend 910_93: 1D int32 tensor [ep_world_size * max(tp_world_size,1) * local_expert_num]
            Format: ND, non-contiguous allowed.
        - **expert_scales** (Tensor) - Top-K expert weights per token.
        - **ep_world_size** (int) - EP domain size.
            - Ascend 910B: Supports 16, 32, 64.
            - Ascend 910_93: Supports 8, 16, 32, 64, 128, 144, 256, 288.
        - **ep_rank_id** (int) - Local rank ID in EP domain [0, ep_world_size), must be unique per domain.
        - **moe_expert_num** (int) - Number of MoE experts (0 < moe_expert_num <= 256), 
            must satisfy moe_expert_num % (ep_world_size-shared_expert_rank_num) = 0.
        - **tp_send_counts** (Tensor) - Tokens that each TP rank needs to send (when TP exists). It's the output of dispatch operation. Default: ``None``.
            - Ascend 910B: Not supported.
            - Ascend 910_93: 1D int32 tensor [tp_world_size] when TP exists. Format: ND, non-contiguous allowed.
        - **x_active_mask** (Tensor) - Reserved parameter. Default: ``None``.
        - **activate_scale** (Tensor) - Reserved parameter. Default: ``None``.
        - **weight_scale** (Tensor) - Reserved parameter. Default: ``None``.
        - **group_list** (Tensor) - Reserved parameter. Default: ``None``.
        - **expand_scales** (Tensor) - Output of dispatch operation. Default: ``None``.
            - Ascend 910B: 1D float32 tensor [A]. Format: ND, non-contiguous allowed.
            - Ascend 910_93: Unsupported.
        - **group_ep** (str) - EP communication domain name (string length 1-127), must differ from group_tp. Default: ``None``.
        - **group_tp** (str) - TP communication domain name. Default: ``None``.
            - Ascend 910B: Unsupported (pass empty string).
            - Ascend 910_93: When TP communication exists, string length 1-127, must differ from group_ep.
        - **tp_world_size** (int) - TP domain size. Default: ``0``.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: 0/1 means no TP communication; only 2 supported when TP exists.
        - **tp_rank_id** (int) - Local rank ID in TP domain. Default: ``0``.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: [0,1], unique per domain; pass 0 when no TP communication.
        - **expert_shard_type** (int) - Shared expert distribution type. Default: ``0``.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: Currently only 0 (shared experts precede MoE experts).
        - **shared_expert_num** (int) - Number of shared experts. Default: ``0``.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: Currently 0 (none) or 1 (one shared expert).
        - **shared_expert_rank_num** (int) - Number of ranks hosting shared experts. Default: ``0``.
            - Ascend 910B: Unsupported (pass 0).
            - Ascend 910_93: [0, ep_world_size-1), must satisfy ep_world_size % shared_expert_rank_num = 0 when non-zero.
        - **global_bs** (int) - Global batch size across EP domain. Default: ``0``.
            - Ascend 910B: 256*ep_world_size when BS varies per rank; 0 or BS*ep_world_size when uniform.
            - Ascend 910_93: 0 or BS*ep_world_size.
        - **out_dtype** (int) - Specify the type of output x. Reserved parameter (pass 0 in current version). Default: ``0``.
        - **common_quant_mode** (int) - Communication quantification type. Reserved parameter (pass 0 in current version). Default: ``0``.
        - **group_list_type** (int) - The format of group_list. Reserved parameter (pass 0 in current version). Default: ``0``.

    Outputs:
        - **x** (Tensor) - Processed tokens. 2D tensor [BS, H] with dtype matching input `expand_x`.

    Raises:
        TypeError: If input dtypes don't match specifications.
        ValueError: If input values violate constraints (e.g., invalid expert indices).
        RuntimeError: If communication domain configuration is invalid.

    Supported Platforms:
        ``Ascend``

    Examples:
    >>> # EP-only communication example (Ascend 910B)
    >>> import mindspore as ms
    >>> from mindspore import Tensor
    >>> from mindspore import ops
    >>> from mindspore.communication import init, get_rank, GlobalComm
    >>> from mindspore.ops.auto_generate import moe_distribute_dispatch, moe_distribute_combine
    >>> import numpy as np
    >>> bs = 8
    >>> h = 7168
    >>> k = 8
    >>> ep_world_size = 16
    >>> moe_expert_num = 16
    >>> global_bs = bs * ep_world_size
    >>> x = Tensor(np.random.randn(bs, h), ms.float16)
    >>> expert_ids = Tensor(np.random.randint(0, moe_expert_num, (bs, k)), ms.int32)
    >>> expert_scales = Tensor(np.random.randn(bs, k), ms.float32)
    >>> init()
    >>> rank_id = get_rank()
    >>> expand_x, _, expand_idx, _, ep_recv_count, _, expand_scale = moe_distribute_dispatch(
    ...     x, expert_ids, expert_scales, ep_world_size, rank_id, moe_expert_num,
    ...     group_ep=GlobalComm.WORLD_COMM_GROUP)
    >>> out_x = moe_distribute_combine(
    ...     expand_x, expert_ids, expand_idx, ep_recv_count, expert_scales, ep_world_size, rank_id,
    ...     moe_expert_num, group_ep=GlobalComm.WORLD_COMM_GROUP)
    >>> print(out_x.shape)
    (8, 7168)
    """
    return moe_distribute_combine_op(expand_x, expert_ids, expand_idx, ep_send_counts, expert_scales, ep_world_size, ep_rank_id, moe_expert_num, tp_send_counts, x_active_mask, activate_scale, weight_scale, group_list, expand_scales, group_ep, group_tp, tp_world_size, tp_rank_id, expert_shard_type, shared_expert_num, shared_export_rank_num, global_bs, out_dtype, common_quant_mode, group_list_type)


def bincount_ext(input, weights=None, minlength=0):
    r"""
    Count the occurrences of each value in the input.

    If `minlength` is not specified, the length of the output Tensor is the maximum value in the input plus one.
    If `minlength` is specified, the length of the output Tensor is the maximum value between `minlength` or
    the maximum value in the input plus one.

    Each value in the output Tensor represents the number of occurrences of that index value in the input.
    If `weights` is specified, the output results are weighted, 
    i.e., :math:`out[n] += weight[i]` instead of :math:`out[n] += 1`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): A one-dimensional Tensor.
        weights (Tensor, optional): Weights with the same shape as the input. Default: ``None``.
        minlength (int, optional): The minimum length of output Tensor. Should be non-negative. Default: ``0``.

    Returns:
        Tensor, If input is non-empty, the output shape is :math:`(max(max(input)+1, minlength), )`,
        otherwise the shape is :math:`(0, )`.

    Raises:
        TypeError: If `input` or `weights` is not a Tensor.
        ValueError: If `input` contains negative values.
        ValueError: If `input` is not one-dimensional or `input` and `weights` do not have the same shape.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import ops, Tensor
        >>> print(ops.auto_generate.bincount_ext(Tensor(np.arange(5))))
        [1 1 1 1 1]
        >>> print(ops.auto_generate.bincount_ext(Tensor(np.array([0, 1, 1, 3, 2, 1, 7]))))
        [1 3 1 1 0 0 0 1]
        >>> w = Tensor(np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6])) # weights
        >>> x = Tensor(np.array([0, 1, 1, 2, 2, 2]))
        >>> print(ops.auto_generate.bincount_ext(x,  weights=w, minlength=5))
        [0.3 0.7 1.1 0.  0. ]
    """
    return bincount_ext_op(input, weights, minlength)


def hardtanh(input, min_val=-1, max_val=1):
    r"""
    
    """
    return hardtanh_op(input, min_val, max_val)


def put_mem_signal(target, target_offset, src, src_offset, size, signal, signal_offset, signal_value, signal_op='set', target_pe=0, non_blocking=False):
    r"""
    
    """
    return put_mem_signal_op(target, target_offset, src, src_offset, size, signal, signal_offset, signal_value, signal_op, target_pe, non_blocking)


def irfft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the inverse of `rfft()`.

    Note:
        - `irfft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `irfft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
        n (int, optional): Length of the transformed `dim` of the result.
            If given, the input will either be zero-padded or trimmed to this length before computing `irfft`.
            If n is not given, it is taken to be :math:`2*(input.shape[dim]-1)`.
            Default: ``None``.
        dim (int, optional): The dimension along which to take the one dimensional `irfft`.
            Default: ``-1``, which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"``.
            Three modes are defined as,

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, the result of `irfft()` function, dtype of the result is float32/64, result.shape[dim] is :math:`n`.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int16, int32, int64, float32, float64, complex64, complex128.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"``.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([1, 2, 3, 4])
        >>> y = ops.irfft(input, n=6, dim=-1, norm='backward')
        >>> print(y)
        [ 2.5        -0.6666667   0.         -0.16666667  0.         -0.6666667 ]
    """
    return irfft_op(input, n, dim, norm)


def roll(input, shifts, dims=None):
    r"""
    Roll the elements of a tensor along a dimension.

    Args:
        input (Tensor): The input tensor.
        shifts (Union[list(int), tuple(int), int]): The amount of element shifting.
        dims (Union[list(int), tuple(int), int], optional): Specify the dimension to move. Default ``None`` ,
            which means the input tensor will be flattened before computation, and the result will be reshaped back
            to the original input shape.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0, 1, 2, 3, 4], mindspore.float32)
        >>> # case1: Parameter `shifts` is positive
        >>> output = mindspore.ops.roll(input, shifts=2, dims=0)
        >>> print(output)
        [3. 4. 0. 1. 2.]
        >>> # case2: Parameter `shifts` is negative
        >>> output = mindspore.ops.roll(input, shifts=-2, dims=0)
        >>> print(output)
        [2. 3. 4. 0. 1.]
    """
    return roll_impl(input, shifts, dims)


def index_put_(input, indices, values, accumulate=False):
    r"""
    Based on the indices in `indices`, replace the corresponding elements in Tensor `self` with the values
    in `values`. The expression `Tensor.index_put_(indices, values)` is equivalent to `tensor[indices] = values`.
    Update and return `self`.

    .. warning::
        The behavior is unpredictable in the following scenario:

        - If `accumulate` is `False` and `indices` contains duplicate elements.

    Args:
        indices (tuple[Tensor], list[Tensor]): the indices of type is bool, uint8, int32 or int64,
            used to index into the `self`. The size of indices should <=  the rank of `self`
            and the tensors in indices should be broadcastable.
        values (Tensor): Tensor with the same type as `self`. If size == 1, it will be broadcastable.
        accumulate (bool, optional): If `accumulate` is `True`, the elements in `values` will be added to `self`,
            otherwise the elements in `values` will replace the corresponding elements in the `self`.
            Default: ``False``.

    Returns:
        Tensor `self`.

    Raises:
        TypeError: If the dtype of the `self` is not equal to the dtype of `values`.
        TypeError: If the dtype of `indices` is not tuple[Tensor], list[Tensor].
        TypeError: If the dtype of tensors in `indices` are not bool, uint8, int32 or int64.
        TypeError: If the dtypes of tensors in `indices` are inconsistent.
        TypeError: If the dtype of `accumulate` is not bool.
        ValueError: If size(`values`) is not 1 or max size of the tensors in `indices` when
            rank(`self`) == size(`indices`).
        ValueError: If size(`values`) is not 1 or `self`.shape[-1] when rank(`self`) > size(`indices`).
        ValueError: If the tensors in `indices` is not be broadcastable.
        ValueError: If size(`indices`) > rank(`self`).

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor
        >>> x = Tensor(np.array([[1, 2, 3], [4, 5, 6]]).astype(np.int32))
        >>> values = Tensor(np.array([3]).astype(np.int32))
        >>> indices = [Tensor(np.array([0, 1, 1]).astype(np.int32)), Tensor(np.array([1, 2, 1]).astype(np.int32))]
        >>> accumulate = True
        >>> output = x.index_put_(indices, values, accumulate)
        >>> print(output)
        [[1 5 3]
         [4 8 9]]
    """
    return inplace_index_put_op(input, indices, values, accumulate)


def correlate(a, v, pad_mode='valid'):
    r"""
    Cross-correlation of two 1-dimensional sequences.

    This function computes the correlation as generally defined in signal processing texts:

    :math:`c_{av}[k] = \sum_{n}{a[n+k] * conj(v[n])}`

    with `a` and `v` sequences being zero-padded where necessary and conj being the conjugate.

    Note:
        - `correlate` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `correlate` is not supported on Windows platform yet.

    Args:
        a (Union[list, tuple, Tensor]): First input sequence.
        v (Union[list, tuple, Tensor]): Second input sequence.
        mode (str, optional): Specifies padding mode. The optional values are
            ``"same"`` , ``"valid"`` and ``"full"`` . Default: ``"valid"`` .

            - ``"same"``: it returns output of length :math:`max(M, N)`. Boundary
              effects are still visible.

            - ``"valid"``: it returns output of length :math:`max(M, N) - min(M, N) + 1`.
              The convolution product is only given for points where the signals overlap
              completely. Values outside the signal boundary have no effect.

            - ``"full"``: it returns the convolution at each point of overlap, with
              an output shape of :math:`(N + M - 1,)`.At the end-points of the convolution,
              the signals do not overlap completely, and boundary effects may be seen.

    Returns:
        Tensor, Discrete cross-correlation of `a` and `v`.

    Raises:
        TypeError: If `a` or `v` is not a tensor.
        TypeError: If `a` and `v` is of different dtype.
        ValueError: If `a` and `v` are empty or have wrong dimensions

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> from mindspore.ops.auto_generate import correlate
        >>> from mindspore import Tensor
        >>> output = correlate(Tensor([1., 2., 3.]), Tensor([0., 1., 0.5]))
        >>> print(output)
        [3.5]
        >>> output = correlate(Tensor([1., 2., 3.]), Tensor([0., 1., 0.5]), mode="same")
        >>> print(output)
        [2.  3.5 3. ]
        >>> output = correlate(Tensor([1., 2., 3., 4., 5.]), Tensor([1., 2.]), mode="full")
        >>> print(output)
        [ 2.  5.  8. 11. 14.  5.]
    """
    correlate_op = _get_cache_prim(Correlate)(pad_mode)
    return correlate_op(a, v)


def ffn_ext(x, weight1, weight2, expertTokens=None, bias1=None, bias2=None, scale=None, offset=None, deqScale1=None, deqScale2=None, antiquant_scale1=None, antiquant_scale2=None, antiquant_offset1=None, antiquant_offset2=None, activation='fastgelu', inner_precise=0):
    r"""
    
    """
    return ffn_ext_impl(x, weight1, weight2, expertTokens, bias1, bias2, scale, offset, deqScale1, deqScale2, antiquant_scale1, antiquant_scale2, antiquant_offset1, antiquant_offset2, activation, inner_precise)


def relu6(x):
    r"""
    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.

    .. math::

        \text{ReLU6}(x) = \min(\max(0,x), 6)

    It returns :math:`\min(\max(0,x), 6)` element-wise.

    ReLU6 Activation Function Graph:

    .. image:: ../images/ReLU6.png
        :align: center

    Args:
        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means any number of additional dimensions.
            Data type must be float16, float32.

    Returns:
        Tensor, with the same dtype and shape as the `x`.

    Raises:
        TypeError: If dtype of `x` is neither float16 nor float32.
        TypeError: If `x` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> result = ops.relu6(x)
        >>> print(result)
        [[0. 4. 0.]
         [2. 0. 6.]]
    """
    return relu6_op(x)


def expand_as(input, other):
    r"""
    Broadcast the shape of the input tensor to be the same as the another input tensor. The dim of the
    input shape must be smaller than or equal to that of another and the broadcast rules must be met.

    Args:
        input (Tensor): The input Tensor.
        other (Tensor): The target Tensor. It's shape is the target shape that input tensor need to be broadcasted.

    Returns:
        Tensor, with the given shape of `other` and the same data type as `input`.

    Raises:
        TypeError: If `other` is not a tensor.
        ValueError: If the shape of `other` and `input` are incompatible.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>>  from mindspore.ops.function.array_func import expand_as
        >>> x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]).astype(np.float32))
        >>> other = Tensor(np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]).astype(np.float32))
        >>> output = expand_as(x, other)
        >>> print(output)
        [[1. 2. 3.]
         [1. 2. 3.]
         [1. 2. 3.]]
        >>> shape = (3, 3)
    """
    return expand_as_op(input, other)


def copy(input):
    r"""
    
    """
    return copy_op(input)


def unsorted_segment_sum(input_x, segment_ids, num_segments):
    r"""
    Compute the sum of the input tensor along segments.

    The following figure shows the calculation process of unsorted_segment_sum:

    .. image:: UnsortedSegmentSum.png

    .. math::

        \text{output}[i] = \sum_{segment\_ids[j] == i} \text{data}[j, \ldots]

    where :math:`j,...` is a tuple describing the index of element in data. `segment_ids` selects which elements 
    in data to sum up. Segment_ids does not need to be sorted, and it does not need to cover all values in the 
    entire valid value range.

    Note:
        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 0.
        - On Ascend, if the value of segment_id is less than 0 or greater than the length of the input data shape, an
          execution error will occur.

    If the sum of the given segment_ids :math:`i` is empty, then :math:`\text{output}[i] = 0`. If the given segment_ids
    is negative, the value will be ignored. 'num_segments' must be equal to the number of different segment_ids.

    Args:
        input_x (Tensor): The input tensor.
        segment_ids (Tensor): Indicate the segment to which each element belongs.
        num_segments (Union[int, Tensor], optional): Number of segments, it can be an int or 0-D tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input_x = mindspore.tensor([1, 2, 3, 4])
        >>> segment_ids = mindspore.tensor([0, 0, 1, 2])
        >>> num_segments = 3
        >>> mindspore.ops.unsorted_segment_sum(input_x, segment_ids, num_segments)
        Tensor(shape=[3], dtype=Int64, value= [3, 3, 4])
        >>> input_x = mindspore.tensor([1, 2, 3, 4, 2, 5])
        >>> segment_ids = mindspore.tensor([0, 0, 1, 2, 3, 4])
        >>> num_segments = 6
        >>> mindspore.ops.unsorted_segment_sum(input_x, segment_ids, num_segments)
        Tensor(shape=[6], dtype=Int64, value= [3, 3, 4, 2, 5, 0])
    """
    return unsorted_segment_sum_op(input_x, segment_ids, num_segments)


def set_data(input, value):
    r"""
    
    """
    return set_data_op(input, value)


def cosh(input):
    r"""
    Computes hyperbolic cosine of input element-wise.

    .. math::

        out_i = \cosh(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([0.74, 0.04, 0.30, 0.56])
        >>> output = mindspore.ops.cosh(input)
        >>> print(output)
        [1.2865248 1.0008001 1.0453385 1.1609408]
    """
    return cosh_op(input)


def update_to_remote(x, sync=False):
    r"""
    
    """
    return update_to_remote_op(x, sync)


def swiglu(input, dim=-1):
    r"""
    Computes SwiGLU (Swish-Gated Linear Unit activation function) of input tensor.
    SwiGLU is a variant of the :class:`mindspore.ops.GLU` activation function, it is defined as:

    .. math::
        {SwiGLU}(a, b)= Swish(a) \otimes b

    where :math:`a` is the first half of the `input` matrices and :math:`b` is the second half,
    Swish(a)=a :math:`\sigma` (a), :math:`\sigma` is the :func:`mindspore.ops.sigmoid` activation function
    and :math:`\otimes` is the Hadamard product.

    .. warning::
        Only support on Atlas A2 training series.

    Args:
        input (Tensor): Tensor to be split. It has shape :math:`(\ast_1, N, \ast_2)`
            where `*` means, any number of additional dimensions. :math:`N` must be divisible by 2.
        dim (int, optional): the axis to split the input. It must be int. Default: ``-1`` , the last axis of `input`.

    Returns:
        Tensor, the same dtype as the `input`, with the shape :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`.

    Raises:
        TypeError: If dtype of `input` is not float16, float32 or bfloat16.
        TypeError: If `input` is not a Tensor.
        RuntimeError: If the dimension specified by `dim` is not divisible by 2.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([[-0.12, 0.123, 31.122], [2.1223, 4.1212121217, 0.3123]], dtype=mindspore.float32)
        >>> output = ops.swiglu(input, 0)
        >>> print(output)
        [[-0.11970687 0.2690224 9.7194 ]]
    """
    return swiglu_op(input, dim)


def masked_fill(input_x, mask, value):
    r"""
    Fills elements of tensor with value where mask is ``True``.

    Support broadcast.

    Args:
        input_x (Tensor): The input tensor.
        mask (Tensor[bool]): The input mask.
        value (Union[Number, Tensor]): The value to fill in with.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input_x = mindspore.tensor([1., 2., 3., 4.], mindspore.float32)
        >>> mask = mindspore.tensor([True, True, False, True], mindspore.bool)
        >>> output = mindspore.ops.masked_fill(input_x, mask, 0.5)
        >>> print(output)
        [0.5 0.5 3.  0.5]
    """
    return masked_fill_op(input_x, mask, value)


def masked_fill_tensor_(input, mask, value):
    r"""
    
    """
    return inplace_masked_fill_tensor_op(input, mask, value)


def ifft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the inverse of `fft()`.

    Note:
        - `ifft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ifft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        n (int, optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim` axis will be zero-padded or truncated to `n` before calculating `ifft`.
            Default: ``None`` , which does not need to process `input`.
        dim (int, optional): The dimension along which to take the one dimensional `ifft`.
            Default: ``-1`` , which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as,

            - ``"backward"`` (normalize by :math:`1/n`).
            - ``"forward"`` (no normalization).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `ifft()` function. The default is the same shape as `input`.
        If `n` is given, the size of the `dim` axis is changed to `n`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([ 1.6243454, -0.6117564, -0.5281718, -1.0729686])
        >>> out = ops.ifft(input, n=4, dim=-1, norm="backward")
        >>> print(out)
        [-0.14713785+0.j          0.5381293 +0.11530305j  0.69522464+0.j
          0.5381293 -0.11530305j]
    """
    return ifft_op(input, n, dim, norm)


def maximum(input, other):
    r"""
    Compute the maximum of the two input tensors element-wise.

    .. math::
        output_i = \max(input_i, other_i)

    Note:
        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types
          consistent.
        - When the inputs are two tensors,
          dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.
        - When the inputs are one tensor and one scalar,
          the scalar could only be a constant.
        - Broadcasting is supported.
        - If one of the elements being compared is a NaN, then that element is returned.

    .. warning::
        If all inputs are scalar of integers. In Graph mode, the output will be Tensor of int32, while in 
        PyNative mode, the output will be Tensor of int64.

    Args:
        input (Union[Tensor, Number, bool]): The first input.
        other (Union[Tensor, Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1 : same data type
        >>> input = mindspore.tensor([1.0, 5.0, 3.0], mindspore.float32)
        >>> other = mindspore.tensor([4.0, 2.0, 6.0], mindspore.float32)
        >>> mindspore.ops.maximum(input, other)
        Tensor(shape=[3], dtype=Float32, value= [ 4.00000000e+00,  5.00000000e+00,  6.00000000e+00])
        >>>
        >>> # case 2 : the data type is the one with higher precision or higher digits among the two inputs.
        >>> input = mindspore.tensor([1.0, 5.0, 3.0], mindspore.int64)
        >>> other = mindspore.tensor([4.0, 2.0, 6.0], mindspore.float64)
        >>> mindspore.ops.maximum(input, other)
        Tensor(shape=[3], dtype=Float64, value= [ 4.00000000e+00,  5.00000000e+00,  6.00000000e+00])
    """
    return maximum_op(input, other)


def erfc(input):
    r"""
    Compute the complementary error function of input tensor element-wise.

    .. math::

        \text{erfc}(x) = 1 - \frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt

    Args:
        input (Tensor): The input tensor. 

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # The datatype of output will be float32 when datatype of input is in [int64, bool](Datatype only supported on Ascend).
        >>> input = mindspore.tensor([-1, 0, 1, 2, 3], mindspore.int64)
        >>> mindspore.ops.erfc(input)
        Tensor(shape=[5], dtype=Float32, value= [ 1.84270079e+00,  1.00000000e+00,  1.57299207e-01,  4.67773498e-03,  2.20904970e-05])
        >>>
        >>> # Otherwise output has the same dtype as the input.
        >>> input = mindspore.tensor([-1, 0, 1, 2, 3], mindspore.float64)
        >>> mindspore.ops.erfc(input)
        Tensor(shape=[5], dtype=Float64, value= [ 1.84270079e+00,  1.00000000e+00,  1.57299207e-01,  4.67773498e-03,  2.20904970e-05])
    """
    return erfc_op(input)


def leaky_relu_ext(input, negative_slope=0.01):
    r"""
    leaky_relu activation function. The element of `input` less than 0 times `negative_slope` .

    The activation function is defined as:

    .. math::
        \text{leaky_relu}(input) = \begin{cases}input, &\text{if } input \geq 0; \cr
        \text{negative_slope} * input, &\text{otherwise.}\end{cases}

    where :math:`negative\_slope` represents the `negative_slope` parameter.

    For more details, see `Rectifier Nonlinearities Improve Neural Network Acoustic Models
    <https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf>`_.

    LeakyReLU Activation Function Graph:

    .. image:: ../images/LeakyReLU.png
        :align: center

    Args:
        input (Tensor): The input of leaky_relu is a Tensor of any dimension.
        negative_slope (Union[int, float], optional): Slope of the activation function when the element of `input` is less than 0.
          Default: ``0.01`` .

    Returns:
        Tensor, has the same type and shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `negative_slope` is not a float or an int.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> print(ops.extend.leaky_relu_ext(input, negative_slope=0.2))
        [[-0.2  4.  -1.6]
         [ 2.  -1.   9. ]]
    """
    return leaky_relu_ext_op(input, negative_slope)


def inplace_copy(input, src, non_blocking=False):
    r"""
    
    """
    return inplace_copy_op(input, src, non_blocking)


def hfft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the one dimensional discrete Fourier transform of of a Hermitian symmetric `input` signal.

    Note:
        - `hfft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `hfft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        n (int, optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim` axis will be zero-padded or truncated to `n` before calculating `hfft`.
            Default: ``None`` , which does not need to process `input`.
        dim (int, optional): The dimension along which to take the one dimensional `hfft`.
            Default: ``-1`` , which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as,

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `hfft()` function.
        If `n` is given, result.shape[dim] is :math:`(n - 1) * 2`, otherwise math:`(input.shape[dim] - 1) * 2`.
        When the `input` is int16, int32, int64, float16, float32, complex64, the return value type is float32.
        When the `input` is float64 or complex128, the return value type is float64.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([ 1.6243454, -0.6117564, -0.5281718, -1.0729686])
        >>> out = ops.hfft(input, n=4, dim=-1, norm="backward")
        >>> print(out)
        [-0.12733912  2.1525173   2.3196864   2.1525173 ]
    """
    return hfft_op(input, n, dim, norm)


def elu_ext(input, alpha=1.0):
    r"""
    Exponential Linear Unit activation function.

    Applies the exponential linear unit function element-wise.
    The activation function is defined as:

    .. math::

        \text{ELU}(x)= \left\{
        \begin{array}{align}
            \alpha(e^{x}  - 1) & \text{if } x \le 0\\
            x & \text{if } x \gt 0\\
        \end{array}\right.

    Where :math:`x` is the element of input Tensor `input`, :math:`\alpha` is param `alpha`,
    it determines the smoothness of ELU.

    ELU function graph:

    .. image:: ../images/ELU.png
        :align: center

    Args:
        input (Tensor): The input of ELU is a Tensor of any dimension.
        alpha (float, optional): The alpha value of ELU, the data type is float.
            Default: ``1.0`` .

    Returns:
        Tensor, has the same shape and data type as `input`.

    Raises:
        TypeError: If `alpha` is not a float.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.auto_generate.elu_ext(x)
        >>> print(output)
        [[-0.63212055  4.         -0.99966455]
         [ 2.         -0.99326205  9.        ]]
    """
    return elu_ext_impl(input, alpha)


def fast_gelu(x):
    r"""
    Fast Gaussian Error Linear Units activation function.

    FastGeLU is defined as follows:

    .. math::
        \text{output} = \frac {x} {1 + \exp(-1.702 * \left| x \right|)} * \exp(0.851 * (x - \left| x \right|)),

    where :math:`x` is the element of the input.

    FastGelu function graph:

    .. image:: ../images/FastGelu.png
        :align: center

    Args:
        x (Tensor): Input to compute the FastGeLU with data type of float16 or float32.

    Returns:
        Tensor, with the same type and shape as `x`.

    Raises:
        TypeError: If dtype of `x` is neither float16 nor float32.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)
        >>> output = ops.fast_gelu(x)
        >>> print(output)
        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]
        [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]
    """
    return fast_gelu_op(x)


def rfftn(input, s=None, dim=None, norm=None):
    r"""
    Computes the N dimensional discrete Fourier transform for real input `input`.

    Note:
        - `rfftn` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `rfftn` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64.

        s (tuple[int], optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim[i]` axis will be zero-padded or truncated to `s[i]` before calculating `rfftn`.
            Default: ``None`` , which does not need to process `input`.
        dim (tuple[int], optional): The dimension along which to take the one dimensional `rfftn`.
            Default: ``None`` , which means transform the all dimension of `input`, or the last `len(s)` dimensions if s is given.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as, where :math: `n = prod(s)`

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `rfftn()` function, result.shape[dim[i]] is s[i], and for the last transformed dim, 
        result.shape[dim[-1]] is :math:`s[-1] // 2 + 1`.
        When the input is int16, int32, int64, float16, float32 the return value type is complex64.
        When the input is float64, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64.
        TypeError: If the type/dtype of `s` and `dim` is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `dim` has duplicate values.
        ValueError: If `s` is less than 1.
        ValueError: If `s` and `dim` are given but have different shapes.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = ops.ones((2, 2, 2))
        >>> ops.rfftn(input, s=(2, 2, 2), dim=(0, 1, 2), norm="backward")
        Tensor(shape=[2, 2, 2], dtype=Complex64, value=
        [[[8+0j, 0+0j],
          [0+0j, 0+0j]],
         [[0+0j, 0+0j],
          [0+0j, 0+0j]]])
    """
    return rfftn_op(input, s, dim, norm)


def log_softmax(logits, axis=-1):
    r"""
    Applies the Log Softmax function to the input tensor on the specified axis.
    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,
    the Log Softmax function is shown as follows:

    .. math::
        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),

    where :math:`N` is the length of the Tensor.

    Args:
        logits (Tensor): The input Tensor, which is the :math:`x` in the formula above, it's shape is :math:`(N, *)`, 
            where :math:`*` means, any number of additional dimensions, with float16 or float32 data type.
        axis (int): The axis to perform the Log softmax operation. Default: ``-1`` .

    Returns:
        Tensor, with the same type and shape as the logits.

    Raises:
        TypeError: If `axis` is not an int.
        TypeError: If dtype of `logits` is neither float16 nor float32.
        ValueError: If `axis` is not in range [-len(logits.shape), len(logits.shape)).
        ValueError: If dimension of `logits` is less than 1.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)
        >>> output = ops.log_softmax(logits)
        >>> print(output)
        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]
    """
    return log_softmax_impl(logits, axis)


def batch_norm_elemt(input, weight=None, bias=None, mean=None, invstd=None, eps=1e-5):
    r"""
    
    """
    return batch_norm_elemt_op(input, weight, bias, mean, invstd, eps)


def lerp(input, end, weight):
    r"""
    Does a linear interpolation of two tensors start and end based on a float or tensor weight.

    If `weight` is a tensor, the shapes of three inputs need to be broadcast;
    If `weight` is a float, the shapes of `input` and `end` need to be broadcast.
    If `weight` is a float and platform is Ascend, the types of `input` and `end` need to be float32.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    .. math::
        output_{i} = input_{i} + weight_{i} * (end_{i} - input_{i})

    Args:
        input (Tensor): The tensor with the starting points. Data type must be float16 or float32.
        end (Tensor): The tensor with the ending points. Data type must be the same as `input`.
        weight (Union[float, Tensor]): The weight for the interpolation formula. Must be a float scalar
            or a tensor with float16 or float32 data type.

    Returns:
        Tensor, has the same type and shape as input `input`.

    Raises:
        TypeError: If `input` or `end` is not a tensor.
        TypeError: If `weight` is neither scalar(float) nor tensor.
        TypeError: If dtype of `input` or `end` is neither float16 nor float32.
        TypeError: If dtype of `weight` is neither float16 nor float32 when it is a tensor.
        TypeError: If `input` and `end` have different data types.
        TypeError: If `input`, `end` and `weight` have different data types when `weight` is a tensor.
        ValueError: If `end` could not be broadcast to a tensor with shape of `input`.
        ValueError: If `weight` could not be broadcast to tensors with shapes of `input` and `end` when it is a tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> start = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)
        >>> end = Tensor(np.array([10., 10., 10., 10.]), mindspore.float32)
        >>> output = ops.lerp(start, end, 0.5)
        >>> print(output)
        [5.5 6. 6.5 7. ]
    """
    return lerp_op(input, end, weight)


def cos(input):
    r"""
    Computes cosine of input element-wise.

    .. math::
        out_i = \cos(x_i)

    .. warning::
        Using float64 may cause a problem of missing precision.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # The datatype of output will be float32 when datatype of input is in [bool, int8, uint8, int16, int32, int64](Datatype only supported on Ascend).
        >>> input = mindspore.tensor([0, 1, 2], mindspore.int32)
        >>> mindspore.ops.cos(input)
        Tensor(shape=[3], dtype=Float32, value= [ 1.00000000e+00,  5.40302306e-01, -4.16146837e-01])
        >>>
        >>> # Otherwise output has the same dtype as the `input`.
        >>> input = mindspore.tensor([0.74, 0.04, 0.30, 0.56], mindspore.float64)
        >>> mindspore.ops.cos(input)
        Tensor(shape=[4], dtype=Float64, value= [ 7.38468559e-01,  9.99200107e-01,  9.55336489e-01,  8.47255111e-01])
    """
    return cos_op(input)


def floor_divide(input, other):
    r"""
    Compute element-wise division of `input` by `other` and floor the result.

    If `input` and `other` have different data types, the implicit type conversion rules are followed.
    Inputs must be two tensors or one tensor and one scalar.
    When the inputs are two tensors, their shapes must be broadcastable, and their data types cannot both be bool
    simultaneously.

    .. math::
        out_{i} = \text{floor}( \frac{input_i}{other_i})

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Union[Tensor, Number, bool]): The first input tensor.
        other (Union[Tensor, Number, bool]): The second input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: Two tensors with boolean and integer data type.
        >>> input = mindspore.tensor([True, True, False])
        >>> other = mindspore.tensor([1, 2, 4])
        >>> output = mindspore.ops.floor_divide(input, other)
        >>> print(output)
        [1 0 0]
        >>>
        >>> # case 2: One tensor and one scalar.
        >>> input = mindspore.tensor([1, 2, 4])
        >>> other = mindspore.tensor(1.5)
        >>> output = mindspore.ops.floor_divide(input, other)
        >>> print(output)
        [0. 1. 2.]
        >>>
        >>> # case 3: When inputs have different data types, type promotion rules are followed.
        >>> input = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> other = mindspore.tensor([1.1, 2.5, -1.5], mindspore.float32)
        >>> output = mindspore.ops.floor_divide(input, other)
        >>> print(output)
        [ 0.  0. -3.]
    """
    return floor_div_op(input, other)


def tril_ext(input, diagonal=0):
    r"""
    
    """
    return tril_ext_op(input, diagonal)


def t_ext(input):
    r"""
    Transpose the input tensor.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor, transpose 2D tensor, return 1D tensor as it is.

    Raises:
        ValueError: If the dimension of `input` is greater than 2.
        ValueError: If `input` is empty.
        TypeError: If `input` is not a tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[1, 2, 3], [4, 5, 6]]), mindspore.float32)
        >>> output = ops.t_ext(input)
        >>> print(output)
        [[ 1. 4.]
         [ 2. 5.]
         [ 3. 6.]]
    """
    return t_ext_op(input)


def select_ext_view(input, dim, index):
    r"""
    Slices the input tensor along the selected dimension at the given index.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): the input tensor.
        dim (int): the dimension to slice.
        index (int): the index to select with.

    Returns:
        Tensor.

    Raises:
        TypeError: If input is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([[2, 3, 4, 5],[3, 2, 4, 5]])
        >>> y = ops.auto_generate.select_ext_view(input, 0, 0)
        >>> print(y)
        [2 3 4 5]
        
    """
    return select_ext_view_op(input, dim, index)


def divs(input, other):
    r"""
    
    """
    return divs_op(input, other)


def atan_ext(input):
    r"""
    Computes the trigonometric inverse tangent of the input element-wise.

    .. math::

        out_i = \tan^{-1}(input_i)

    Args:
        input (Tensor): The shape of tensor is
            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.

    Returns:
        Tensor, has the same shape as `input`. The dtype of output is float32 when dtype of `input` is in [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([1.0, 0.0]), mindspore.float32)
        >>> output = ops.atan_ext(input)
        >>> print(output)
        [0.7853982 0.       ]
    """
    return atan_ext_op(input)


def pow(input, exponent):
    r"""
    Calculate the `exponent` power of each element in `input`.

    .. note::
        - Broadcasting is supported.
        - Support implicit type conversion and type promotion.

    .. math::

        out_{i} = input_{i} ^{ exponent_{i}}

    Args:
        input (Union[Tensor, Number]): The first input.
        exponent (Union[Tensor, Number]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([1.0, 2.0, 4.0], mindspore.float32)
        >>> output = mindspore.ops.pow(input, exponent=3.0)
        >>> print(output)
        [ 1.  8. 64.]
        >>>
        >>> input = mindspore.tensor([1.0, 2.0, 4.0], mindspore.float32)
        >>> exponent = mindspore.tensor([2.0, 4.0, 3.0], mindspore.float32)
        >>> output = mindspore.ops.pow(input, exponent)
        >>> print(output)
        [ 1. 16. 64.]
    """
    return pow_op(input, exponent)


def mean_ext(input, dim=None, keepdim=False, dtype=None):
    r"""
    Reduces all dimension of a tensor by averaging all elements in the dimension, by default.
    And reduce a dimension of `input` along the specified `dim`. `keepdim`
    determines whether the dimensions of the output and input are the same.

    Note:
        The `dim` with tensor type is only used for compatibility with older versions and is not recommended.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
        dim (Union[int, tuple(int), list(int), Tensor]): The dimensions to reduce. Default: ``None`` ,
            reduce all dimensions. Only constant value is allowed. Assume the rank of `input` is r,
            and the value range is [-r,r).
        keepdim (bool): If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .
        dtype (:class:`mindspore.dtype`): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor, has the same data type as input tensor.

        - If `dim` is ``None`` , and `keepdim` is ``False`` ,
          the output is a 0-D tensor representing the product of all elements in the input tensor.
        - If `dim` is int, set as 1, and `keepdim` is ``False`` ,
          the shape of output is :math:`(x_0, x_2, ..., x_R)`.
        - If `dim` is tuple(int), set as (1, 2), and `keepdim` is ``False`` ,
          the shape of output is :math:`(x_0, x_3, ..., x_R)`.
        - If `dim` is 1-D Tensor, set as [1, 2], and `keepdim` is ``False`` ,
          the shape of output is :math:`(x_0, x_3, ..., x_R)`.

    Raises:
        TypeError: If `x` is not a Tensor.
        TypeError: If `dim` is not one of the following: int, tuple, list or Tensor.
        TypeError: If `keepdim` is not a bool.
        ValueError: If `dim` is out of range.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))
        >>> output = ops.mean_ext(x, 1, keepdim=True)
        >>> result = output.shape
        >>> print(result)
        (3, 1, 5, 6)
        >>> # case 1: Reduces a dimension by averaging all elements in the dimension.
        >>> x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],
        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),
        ... mindspore.float32)
        >>> output = ops.mean_ext(x)
        >>> print(output)
        5.0
        >>> print(output.shape)
        ()
        >>> # case 2: Reduces a dimension along the dim 0
        >>> output = ops.mean_ext(x, 0, True)
        >>> print(output)
        [[[4. 4. 4. 4. 4. 4.]
        [5. 5. 5. 5. 5. 5.]
        [6. 6. 6. 6. 6. 6.]]]
        >>> # case 3: Reduces a dimension along the dim 1
        >>> output = ops.mean_ext(x, 1, True)
        >>> print(output)
        [[[2. 2. 2. 2. 2. 2.]]
        [[5. 5. 5. 5. 5. 5.]]
        [[8. 8. 8. 8. 8. 8.]]]
        >>> # case 4: Reduces a dimension along the dim 2
        >>> output = ops.mean_ext(x, 2, True)
        >>> print(output)
        [[[ 2.]
        [ 2.]
        [ 2.]]
        [[ 4.]
        [ 5.]
        [ 6.]]
        [[ 6.]
        [ 8.]
        [10.]]]
    """
    return mean_ext_op(input, dim, keepdim, dtype)


def atan(input):
    r"""
    Computes the trigonometric inverse tangent of the input element-wise.

    .. math::

        out_i = \tan^{-1}(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.atan(mindspore.tensor([1.0, 0.0]))
        >>> print(output)
        [0.7853982 0.       ]
    """
    return atan_op(input)


def logaddexp2(input, other):
    r"""
    Logarithm of the sum of exponentiations of the inputs in base of 2.

    .. math::

        out_i = \log_2(2^{input_i} + 2^{other_i})

    Args:
        input (Tensor): Input Tensor. The dtype of `input` must be float.
        other (Tensor): Input Tensor. The dtype of `other` must be float.
            If the shape of `input` is not equal to the shape of `other`,
            they must be broadcastable to a common shape (which becomes the shape of the output).

    Returns:
        Tensor, with the same dtype as `input` and `other`.

    Raises:
        TypeError: If `input` or `other` is not a Tensor.
        TypeError: The dtype of `input` or `other` is not float.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x1 = Tensor(np.array([1, 2, 3]).astype(np.float16))
        >>> x2 = Tensor(np.array(2).astype(np.float16))
        >>> output = ops.auto_generate.logaddexp2(x1, x2)
        >>> print(output)
        [2.586 3. 3.586]
    """
    return logaddexp2_op(input, other)


def expand_dims(input_x, axis):
    r"""
    Adds an additional axis to input tensor.

    .. note::
        - The dimension of `input_x` should be greater than or equal to 1.
        - If the specified axis is a negative number, the index is counted
          backward from the end and starts at 1.

    Args:
        input_x (Tensor): The input tensor.
        axis (int): The newly added axis. Only constant value is allowed.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input_tensor = mindspore.tensor([[2, 2], [2, 2]], mindspore.float32)
        >>> output = mindspore.ops.expand_dims(input_tensor, 0)
        >>> print(output)
        [[[2. 2.]
          [2. 2.]]]
    """
    return expand_dims_op(input_x, axis)


def l1_loss_ext(input, target, reduction='mean'):
    r"""
    Calculate the mean absolute error between the `input` value and the `target` value.

    Assuming that the :math:`x` and :math:`y` are the predicted value and target value,
    both are one-dimensional tensors of length :math:`N`, length :math:`N`, `reduction` is set to ``'none'`` ,
    then calculate the loss of :math:`x` and :math:`y` without dimensionality reduction.

    The formula is as follows:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad \text{with } l_n = \left| x_n - y_n \right|,

    where :math:`N` is the batch size.

    If `reduction` is ``'mean'`` or ``'sum'`` , then:

    .. math::
        \ell(x, y) =
        \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    Args:
        input (Tensor): Predicted value, Tensor of any dimension.
        target (Tensor): Target value, usually has the same shape as the `input`.
            If `input` and `target` have different shapes, make sure they can broadcast to each other.
        reduction (str, optional): Apply specific reduction method to the output: ``'none'`` , ``'mean'`` ,
            ``'sum'`` . Default: ``'mean'`` .

            - ``'none'``: no reduction will be applied.
            - ``'mean'``: compute and return the mean of elements in the output. Notice: At least one of the input and target is float type when the reduction is ``'mean'`` .
            - ``'sum'``: the output elements will be summed.

    Returns:
        Tensor or Scalar, if `reduction` is ``'none'`` , return a Tensor with same shape and dtype as `input`.
        Otherwise, a scalar value will be returned.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `target` is not a Tensor.
        ValueError: If `reduction` is not one of ``'none'`` , ``'mean'`` or ``'sum'`` .

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import Tensor, ops
        >>> from mindspore import dtype as mstype
        >>> x = Tensor([[1, 2, 3], [4, 5, 6]], mstype.float32)
        >>> target = Tensor([[6, 5, 4], [3, 2, 1]], mstype.float32)
        >>> output = ops.l1_loss_ext(x, target, reduction="mean")
        >>> print(output)
        3.0
    """
    return l1_loss_ext_op(input, target, reduction)


def adaptive_max_pool2d_grad(y_grad, x, argmax):
    r"""
    
    """
    return adaptive_max_pool2d_grad_op(y_grad, x, argmax)


def rfft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the one dimensional discrete Fourier transform for real input `input`.

    Note:
        - `rfft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `rfft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
        n (int, optional): Number of points along `dim` in the input to use. 
            If given, the input will either be zero-padded or trimmed to this length before computing `rfft`.
            Default: ``None``.
        dim (int, optional): The dimension along which to take the one dimensional `rfft`.
            Default: ``-1``, which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"``.
            Three modes are defined as,

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, the result of `rfft()` function, dtype of the result is complex64/128, result.shape[dim] 
        is :math:`n // 2 + 1`.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int16, int32, int64, float32, float64.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"``.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([1, 2, 3, 4])
        >>> y = ops.rfft(input, n=4, dim=-1, norm='backward')
        >>> print(y)
        [10.+0.j -2.+2.j -2.+0.j]
    """
    return rfft_op(input, n, dim, norm)


def ring_attention_update(prev_attn_out, prev_softmax_max, prev_softmax_sum, cur_attn_out, cur_softmax_max, cur_softmax_sum, actual_seq_qlen=None, layout='SBH'):
    r"""
    The RingAttentionUpdate operator updates the output of two FlashAttention operations based on their respective softmax max and softmax sum values.

    - S: Sequence length
    - B: Batch dimension
    - H: Hidden layer size, equals to N * D
    - T: time, equals to B*S
    - N: Number of attention heads
    - D: Head dimension

    .. warning::
        - It is only supported on Atlas A2 Training Series Products.
        - This is an experimental API that is subject to change or deletion.
        - When `layout` is ``"TND"``, the last dimension of `prev_attn_out` must be a multiple of 64.
        - When `layout` is ``"TND"``, `actual_seq_qlen` is mandatory.
        - When `layout` is ``"TND"``, N * D must satisfy the constraint:
          :math:`(\text{AlignUp}(N*D, 64)*(DataSize*6+8))+(\text{AlignUp}(N*8, 64)*56) <= 192*1024`.
          :math:`DataSize` is 4 bytes when `prev_attn_out` dtype is float32, 2 bytes when dtype is float16 / bfloat16.
        - When `layout` is ``"TND"``, if `actual_seq_qlen` is not a non-decreasing sequence from 0 to T, the result is undefined.

    Args:
        prev_attn_out (Tensor): Output of the first FlashAttention operation. The dtype is float16, float32, bfloat16.
            The shape is :math:`(S, B, H)` or :math:`(T, N, D)`.
        prev_softmax_max (Tensor): The max values from the first FlashAttention softmax computation. The dtype float32.
            The shape is :math:`(B, N, S, 8)` or :math:`(T, N, 8)`. The last dimension contains 8 identical values, which must be positive.
        prev_softmax_sum (Tensor): The sum values from the first FlashAttention softmax computation.
            It has the same shape and dtype as `prev_softmax_max`.
        cur_attn_out (Tensor): Output of the second FlashAttention operation. It has the same shape and dtype as `prev_attn_out`.
        cur_softmax_max (Tensor): The max values from the second FlashAttention softmax computation. It has the same shape and dtype as `prev_softmax_max`.
        cur_softmax_sum (Tensor):The sum values from the second FlashAttention softmax computation. It has the same shape and dtype as `prev_softmax_max`.
        actual_seq_qlen (Tensor, optional): Cumulative sequence length, starting from 0. Required if `layout` is ``"TND"``. Does not take effect if `layout` is ``"SBH"``.
            The tensor must be 1D and contain non-decreasing integer values starting from 0 to T. Default: ``None``.
        layout (str, optional): Indicates the input layout, currently support ``"TND"`` and ``"SBH"``. Default: ``"SBH"``.

    Returns:
        tuple (Tensor), tuple of 3 tensors.

        - **attn_out** (Tensor) - The updated attention out, with the same shape and dtype as `prev_attn_out`.
        - **softmax_max** (Tensor) - The updated softmax max values, with the same shape and dtype as `prev_softmax_max`.
        - **softmax_sum** (Tensor) - The updated softmax sum values, with the same shape and dtype as `prev_softmax_max`.

    Raises:
        RuntimeError: If `layout` is ``"TND"``, and `prev_attn_out`'s last dimension is not aligned to 64.
        RuntimeError: If `layout` is ``"TND"``, and `actual_seq_qlen` is not provided.
        RuntimeError: If `layout` is ``"TND"``, and `actual_seq_qlen` is not a non-decreasing sequence from 0 to T.
        RuntimeError: If `layout` is ``"TND"``, and `prev_attn_out` exceeds the size constraints.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> np.random.seed(123)
        >>> S, B, H, N= 4, 6, 16, 8
        >>> prev_attn_out = np.random.uniform(-1.0, 1.0, size=(S, B, H)).astype(np.float32)
        >>> prev_softmax_max = np.random.uniform(-1.0, 1.0, size=(B, N, S, 8)).astype(np.float32)
        >>> prev_softmax_sum = np.random.uniform(-1.0, 1.0, size=(B, N, S, 8)).astype(np.float32)
        >>> cur_attn_out = np.random.uniform(-1.0, 1.0, size=(S, B, H)).astype(np.float32)
        >>> cur_softmax_max = np.random.uniform(-1.0, 1.0, size=(B, N, S, 8)).astype(np.float32)
        >>> cur_softmax_sum = np.random.uniform(-1.0, 1.0, size=(B, N, S, 8)).astype(np.float32)
        >>> inputs_np = [prev_attn_out, prev_softmax_max, prev_softmax_sum, cur_attn_out, cur_softmax_max, cur_softmax_sum]
        >>> inputs_ms = [Tensor(item) for item in inputs_np]
        >>> out = ops.ring_attention_update(*inputs_ms)
        >>> print(out[0].shape)
        (4, 6, 16)
    """
    return ring_attention_update_op(prev_attn_out, prev_softmax_max, prev_softmax_sum, cur_attn_out, cur_softmax_max, cur_softmax_sum, actual_seq_qlen, layout)


def asin(input):
    r"""
    Computes arcsine of input tensors element-wise.

    .. math::

        out_i = \sin^{-1}(input_i)

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> output = mindspore.ops.asin(mindspore.tensor([0.74, 0.04, 0.30, 0.56]))
        >>> print(output)
        [0.8330704  0.04001067  0.30469266  0.5943858 ]
    """
    return asin_op(input)


def index_fill_tensor(input, dim, index, value):
    r"""
    
    """
    return index_fill_tensor_op(input, dim, index, value)


def addn(x):
    r"""
    
    """
    return addn_op(x)


def select_v2(condition, input, other):
    r"""
    
    """
    return select_v2_op(condition, input, other)


def swiglu_grad(grad_output, input, dim=-1):
    r"""
    
    """
    return swiglu_grad_op(grad_output, input, dim)


def adaptive_avg_pool3d_ext(input, output_size):
    r"""
    
    """
    return adaptive_avg_pool3d_ext_op(input, output_size)


def index_select_ext(input, dim, index):
    r"""
    Generates a new Tensor that accesses the values of `input` along the specified `dim` dimension
    using the indices specified in `index`. The new Tensor has the same number of dimensions as `input`,
    with the size of the `dim` dimension being equal to the length of `index`, and the size of all other
    dimensions will be unchanged from the original `input` Tensor.

    .. note::
        The value of index must be in the range of `[0, input.shape[dim])`, the result is undefined out of range.

    Args:
        input (Tensor): The input Tensor.
        dim (int): The dimension to be indexed.
        index (Tensor): A 1-D Tensor with the indices.

    Returns:
        Tensor, has the same dtype as input Tensor.

    Raises:
        TypeError: If `input` or `index` is not a Tensor.
        TypeError: If `dim` is not int number.
        ValueError: If the value of `dim` is out the range of `[-input.ndim, input.ndim - 1]`.
        ValueError: If the dimension of `index` is not equal to 1.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> input = Tensor(np.arange(16).astype(np.float32).reshape(2, 2, 4))
        >>> print(input)
        [[[ 0.  1.  2.  3.]
        [ 4.  5.  6.  7.]]
        [[ 8.  9. 10. 11.]
        [12. 13. 14. 15.]]]
        >>> index = Tensor([0,], mindspore.int32)
        >>> y = ops.auto_generate.index_select_ext(input, 1, index)
        >>> print(y)
        [[[ 0.  1.  2.  3.]]
        [[ 8.  9. 10. 11.]]]
    """
    return index_select_op(input, dim, index)


def min_(input):
    r"""
    Calculates the minimum value of the input tensor.

    Also see :func:`mindspore.ops.extend.min`.
    """
    return min_op(input)


def inplace_sigmoid(input):
    r"""
    sigmoid_() -> Tensor

    In-place version of sigmoid().

    .. warning::
        Only supports Ascend.
    """
    return inplace_sigmoid_op(input)


def put_mem(target, target_offset, src, src_offset, size, target_pe=0, non_blocking=False):
    r"""
    
    """
    return put_mem_op(target, target_offset, src, src_offset, size, target_pe, non_blocking)


def silu(input):
    r"""
    Computes Sigmoid Linear Unit of input element-wise, also known as Swish function. The SiLU function is defined as:

    .. math::

        \text{SiLU}(x) = x * \sigma(x),

    where :math:`x` is an element of the input, :math:`\sigma(x)` is Sigmoid function.

    .. math::

        \text{sigma}(x_i) = \frac{1}{1 + \exp(-x_i)},

    SiLU Function Graph:

    .. image:: ../images/SiLU.png
        :align: center

    Args:
        input (Tensor): `input` is :math:`x` in the preceding formula. Input with the data type
            float16 or float32.

    Returns:
        Tensor, with the same type and shape as the `input`.

    Raises:
        TypeError: If dtype of `input` is neither float16 nor float32.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> import numpy as np
        >>> input = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)
        >>> output = ops.silu(input)
        >>> print(output)
        [-0.269  1.762  -0.1423  1.762  -0.269]
    """
    return silu_op(input)


def mse_loss_ext(input, target, reduction='mean'):
    r"""
    Calculates the mean squared error between the predicted value and the label value.

    For detailed information, please refer to :class:`mindspore.nn.MSELoss`.

    Args:
        input (Tensor): Tensor of any dimension. The data type needs to be consistent with the `target`.
            It should also be broadcastable with the `target`.
        target (Tensor): The input label. Tensor of any dimension. The data type needs to be consistent with the `input`.
            It should also be broadcastable with the `input`.
        reduction (str, optional): Apply specific reduction method to the output: ``'mean'`` , ``'none'`` ,
            ``'sum'`` . Default: ``'mean'`` .

            - ``'none'``: no reduction will be applied.
            - ``'mean'``: compute and return the mean of elements in the output.
            - ``'sum'``: the output elements will be summed.

    Returns:
        - Tensor. If `reduction` is ``'mean'`` or ``'sum'``, the shape of output is `Tensor Scalar`.
        - If reduction is ``'none'``, the shape of output is the broadcasted shape of **input** and **target** .

    Raises:
        ValueError: If `reduction` is not one of ``'mean'`` , ``'sum'`` or ``'none'``.
        ValueError: If `input` and `target` are not broadcastable.
        TypeError: If `input` and `target` are in different data type.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> logits = Tensor(np.array([1, 2, 3]), mindspore.float32)
        >>> labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)
        >>> output = ops.mse_loss_ext(logits, labels, reduction='none')
        >>> print(output)
        [[0. 1. 4.]
         [0. 0. 1.]]
    """
    return mse_loss_ext_op(input, target, reduction)


def free(x, sync=False):
    r"""
    
    """
    return free_op(x, sync)


def dequant_swiglu_quant(x, weight_scale, activation_scale, bias=None, quant_scale=None, quant_offset=None, group_index=None, activate_left=False, quant_mode='static'):
    r"""
    
    """
    return dequant_swiglu_quant_op(x, weight_scale, activation_scale, bias, quant_scale, quant_offset, group_index, activate_left, quant_mode)


def diag_ext(input, diagonal=0):
    r"""
    If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal.

    If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.

    The argument diagonal controls which diagonal to consider:

    - If `diagonal` = 0, it is the main diagonal.

    - If `diagonal` > 0, it is above the main diagonal.

    - If `diagonal` < 0, it is below the main diagonal.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.
        diagonal (int, optional): the diagonal to consider. Defaults: ``0``.

    Returns:
        Tensor, has the same dtype as the `input`, its shape is up to `diagonal`.

        - If `input` shape is :math:`(x_0)` : then output shape is :math:`(x_0 + \left | diagonal \right | , x_0 + \left | diagonal \right | )` 2-D Tensor.

        - If `input` shape is :math:`(x_0, x_1)` : then output shape is main diagonal to move :math:`(\left | diagonal \right |)` elements remains elements' length 1-D Tensor.

    Raises:
        TypeError: If `input` is not a Tensor.
        ValueError: If shape of `input` is not 1-D and 2-D.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([1, 2, 3, 4]).astype('int32')
        >>> output = ops.auto_generate.diag_ext(input)
        >>> print(output)
        [[1 0 0 0]
         [0 2 0 0]
         [0 0 3 0]
         [0 0 0 4]]
    """
    return diag_ext_op(input, diagonal)


def signal_wait_until(depend_target, signal, signal_offset, compare_value, compare_op='eq'):
    r"""
    
    """
    return signal_wait_until_op(depend_target, signal, signal_offset, compare_value, compare_op)


def tanh_(input):
    r"""
    
    """
    return inplace_tanh_op(input)


def add_layernorm_v2(x1, x2, gamma, beta, epsilon=1e-5, additionalOut=False):
    r"""
    
    """
    return add_layernorm_v2_op(x1, x2, gamma, beta, epsilon, additionalOut)


def assign_sub(variable, value):
    r"""
    Updates a parameter or tensor by subtracting a value from it.

    Support implicit type conversion and type promotion.

    Args:
        variable (Union[Parameter, Tensor]): The input parameter or tensor.
        value (Tensor): The value to be subtracted from the `variable`.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> variable = mindspore.Parameter([1], name="global_step")
        >>> value = mindspore.tensor([100], dtype=mindspore.int32)
        >>> mindspore.ops.assign_sub(variable, value)
        >>> print(variable.asnumpy())
        [-99]
    """
    return assign_sub_op(variable, value)


def view(input, shape):
    r"""
    Reshape the tensor according to the input shape. It's the same as :func:`mindspore.Tensor.reshape`,
    implemented by the underlying reshape operator.

    Args:
        shape (Union[tuple(int), int]): Dimension of the output tensor.

    Returns:
        Tensor, which dimension is the input shape's value.

    Examples:
        >>> from mindspore import Tensor
        >>> import numpy as np
        >>> a = Tensor(np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float32))
        >>> output = a.view((3, 2))
        >>> print(output)
        [[1. 2.]
        [3. 2.]
        [3. 4.]]
    """
    return view_op(input, shape)


def isfinite(input):
    r"""
    Return a boolean tensor indicating which elements are finite. 

    An element is considered finite if it is not ``NaN`` , ``-INF`` , or ``INF`` .

    Args:
      input (Tensor): The input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([-1, 3, float("inf"), float("-inf"), float("nan")])
        >>> mindspore.ops.isfinite(input)
        Tensor(shape=[5], dtype=Bool, value= [ True,  True, False, False, False])
    """
    return isfinite_op(input)


def rms_norm(x, gamma, epsilon=1e-6):
    r"""
    The RmsNorm(Root Mean Square Layer Normalization) operator is a normalization operation. Compared to
    LayerNorm, it retains scaling invariance and removes translation invariance. Its formula is:

    .. math::
        y=\frac{x_i}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}{ x_i^2}+\varepsilon}}\gamma_i

    .. warning::
        This is an experimental API that is subject to change or deletion. This API is only supported in Atlas A2
        training series for now.

    Args:
        x (Tensor): Input data of RmsNorm. Support data type: float16, float32, bfloat16.
        gamma (Tensor): Learnable parameter :math:`\gamma` . Support data type: float16, float32, bfloat16.
        epsilon (float, optional): A float number ranged in (0, 1] to prevent division by 0. Default value is `1e-6`.

    Returns:
        - Tensor, denotes the normalized result, has the same type and shape as `x`.
        - Tensor, with the float data type, denotes the reciprocal of the input standard deviation, used by gradient
          calculation.

    Raises:
        TypeError: If data type of `x` is not one of the following: float16, float32, bfloat16.
        TypeError: If data type of `gamma` is not one of the following: float16, float32, bfloat16.
        TypeError: If data type of `x` is not the same with the data type of `gamma`.
        ValueError: If `epsilon` is not a float between 0 and 1.
        ValueError: If the rank of `gamma` is lagger than the rank of `x`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)
        >>> gamma = Tensor(np.ones([3]), mindspore.float32)
        >>> y, rstd = ops.rms_norm(x, gamma)
        >>> print(y)
        [[0.46290997  0.92581993  1.3887299]
         [0.46290997  0.92581993  1.3887299]]
        >>> print(rstd)
        [[0.46290997]
         [0.46290997]]
    """
    return rms_norm_impl(x, gamma, epsilon)


def fft(input, n=None, dim=-1, norm=None):
    r"""
    Calculates the one dimensional discrete Fourier transform of `input`.

    Note:
        - `fft` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `fft` is not supported on Windows platform yet.

    Args:
        input (Tensor): The input tensor.
            Supported dtypes:

            - Ascend/CPU: int16, int32, int64, float16, float32, float64, complex64, complex128.

        n (int, optional): Length of the transformed `dim` of the result.
            If given, the size of the `dim` axis will be zero-padded or truncated to `n` before calculating `fft`.
            Default: ``None`` , which does not need to process `input`.
        dim (int, optional): The dimension along which to take the one dimensional `fft`.
            Default: ``-1`` , which means transform the last dimension of `input`.
        norm (str, optional): Normalization mode. Default: ``None`` that means ``"backward"`` .
            Three modes are defined as,

            - ``"backward"`` (no normalization).
            - ``"forward"`` (normalize by :math:`1/n`).
            - ``"ortho"`` (normalize by :math:`1/\sqrt{n}`).

    Returns:
        Tensor, The result of `fft()` function. The default is the same shape as `input`.
        If `n` is given, the size of the `dim` axis is changed to `n`.
        When the input is int16, int32, int64, float16, float32, complex64, the return value type is complex64.
        When the input is float64 or complex128, the return value type is complex128.

    Raises:
        TypeError: If the `input` type is not Tensor.
        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.
        TypeError: If `n` or `dim` type is not int.
        ValueError: If `dim` is not in the range of "[ `-input.ndim` , `input.ndim` )".
        ValueError: If `n` is less than 1.
        ValueError: If `norm` is none of ``"backward"`` , ``"forward"`` or ``"ortho"`` .

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([ 1.6243454, -0.6117564, -0.5281718, -1.0729686])
        >>> out = ops.fft(input, n=4, dim=-1, norm="backward")
        >>> print(out)
        [-0.5885514+0.j          2.1525173-0.46121222j  2.7808986+0.j
          2.1525173+0.46121222j]
    """
    return fft_op(input, n, dim, norm)


def hardshrink(input, lambd=0.5):
    r"""
    Hard Shrink activation function. Calculates the output according to the input elements.

    The formula is defined as follows:

    .. math::
        \text{HardShrink}(x) =
        \begin{cases}
        x, & \text{ if } x > \lambda \\
        x, & \text{ if } x < -\lambda \\
        0, & \text{ otherwise }
        \end{cases}

    HardShrink Activation Function Graph:

    .. image:: ../images/Hardshrink.png
        :align: center

    Args:
        input (Tensor): The input of Hard Shrink. Supported dtypes: 

            - Ascend: float16, float32, bfloat16.
            - CPU/GPU: float16, float32.
        lambd (number, optional): The threshold :math:`\lambda` defined by the Hard Shrink formula.
            Default: ``0.5`` .

    Returns:
        Tensor, has the same data type and shape as the input `input`.

    Raises:
        TypeError: If `lambd` is not a float, int or bool.
        TypeError: If `input` is not a tensor.
        TypeError: If dtype of `input` is not float16, float32 or bfloat16.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[0.5, 1, 2.0], [0.0533, 0.0776, -2.1233]]), mindspore.float32)
        >>> output = ops.hardshrink(input)
        >>> print(output)
        [[ 0.      1.      2.    ]
         [ 0.      0.     -2.1233]]
    """
    return hshrink_impl(input, lambd)


def contiguous(input):
    r"""
    Converts a Tensor into a continuous-memory Tensor that contains the same data as the original Tensor.

    Returns:
        A contiguous in memory tensor containing the same data as self tensor.

    Examples:
        >>> import mindspore as ms
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor([[1, 2, 3], [4, 5, 6]], dtype=ms.float32)
        >>> y = ops.transpose(x, (1, 0))
        >>> y.contiguous()
        >>> y[:, 1] = 1
        >>> print(x)
        [[1. 2. 3.]
         [4. 5. 6.]]
    """
    return contiguous_op(input)


def gather_nd_ext(input, indices):
    r"""
    
    """
    return gather_nd_ext_op(input, indices)


def floor(input):
    r"""
    Rounds a tensor down to the closest integer element-wise.

    .. math::

        out_i = \lfloor input_i \rfloor

    Args:
        input (Tensor): The input tensor. 

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor(([1.1, 2.5, -1.5]))
        >>> output = mindspore.ops.floor(input)
        >>> print(output)
        [ 1.  2. -2.]
    """
    return floor_op(input)


def inplace_threshold(input, threshold, value):
    r"""
    Update the `input` tensor in-place by computing the Threshold activation function element-wise.

    The Threshold is defined as:

    .. math::
        y =
        \begin{cases}
        x, &\text{ if } x > \text{threshold} \\
        \text{value}, &\text{ otherwise }
        \end{cases}

    Args:
        input (Tensor): The input Tensor.
        threshold (Union[int, float]): The value of the threshold.
        value (Union[int, float]): The value to replace with when element is less than threshold.
        
    Returns:
        Tensor, the same shape and data type as the input.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `threshold` is not a float or an int.
        TypeError: If `value` is not a float or an int.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> inputs = mindspore.Tensor([0.0, 2, 3], mindspore.float32)
        >>> outputs = ops.threshold_(inputs, 1, 100)
        >>> print(outputs)
        [100.   2.   3.]
    """
    return inplace_threshold_op(input, threshold, value)


def fold_ext(input, output_size, kernel_size, dilation=1, padding=0, stride=1):
    r"""
    Combines an array of sliding local blocks into a large containing tensor.

    Consider a batched input tensor of shape :math:`(N, C \times \prod(\text{kernel_size}), L)` ,
    where :math:`N` is the batch dimension, :math:`C \times \prod(\text{kernel_size})` is the
    total number of values within each block (a block has :math:`\prod(\text{kernel_size})` spatial
    locations each containing a `C`-channeled vector), and :math:`L` is the total number of such blocks:

    .. math::
        L = \prod_d \left\lfloor\frac{\text{output_size}[d] + 2 \times \text{padding}[d] %
            - \text{dilation}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

    where :math:`d` is over all spatial dimensions.

    Therefore, `output_size` is the spatial shape of the large containing tensor of the sliding local blocks.

    The `dilation`, `padding` and `stride` arguments specify how the sliding blocks are retrieved.

    .. warning::
        Currently, only unbatched(3D) or batched(4D) image-like output tensors are supported.

    Args:
        input (Tensor): 2-D or 3-D Tensor.
        output_size (Union[int, tuple[int], list[int]]): The shape of the spatial dimensions of
            the output(i.e., output.shape[2:]).
        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int
            for height and width. If type is int, it means that height equal with width. Must be specified.
        dilation (Union[int, tuple[int], list[int]], optional): The size of the dilation, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``1`` .
        padding (Union[int, tuple[int], list[int]], optional): The size of the padding, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``0`` .
        stride (Union[int, tuple[int], list[int]], optional): The size of the stride, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``1`` .

    Returns:
        A Tensor, with same type as `input` .

    Shape:
        - Input: :math:`(N, C \times \prod(\text{kernel_size}), L)` or
          :math:`(C \times \prod(\text{kernel_size}), L)`
        - Output: :math:`(N, C, output\_size[0], output\_size[1], ...)` or
          :math:`(C, output\_size[0], output\_size[1], ...)`

    Raises:
        TypeError: If `output_size`, `kernel_size`, `stride`, `dilation`, `padding` data type is not int, tuple or list.
        ValueError: If `output_size`, `kernel_size`, `dilation`, `stride` value is not
            greater than zero or elements number invalid.
        ValueError: If `padding` value is less than zero or elements number invalid.
        ValueError: If input.shape[-2] can't be divisible by the product of kernel_size.
        ValueError: If `input.shape[-1]` is not equal to the calculated number of sliding blocks `L`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.random.rand(16, 64, 25).astype(np.float32))
        >>> output = ops.auto_generate.fold_ext(x, (8, 8), [2, 2], [2, 2], [2, 2], [2, 2])
        >>> print(output.shape)
        (16, 16, 8, 8)
    """
    return col2im_ext_op(input, output_size, kernel_size, dilation, padding, stride)


def mm_ext(input, mat2):
    r"""
    Returns the matrix product of two arrays.
    If `input` is a :math:`(n \times m)` Tensor, `mat2` is a
    :math:`(m \times p)` Tensor, `out` will be a :math:`(n \times p)` Tensor.

    Note:
        This function cannot support broadcasting.
        Refer to :func:`mindspore.ops.matmul` instead if you need a broadcastable function.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The first matrix of matrix multiplication.
            The last dimension of `input` must be the same size as the first dimension of `mat2`.
        mat2 (Tensor): The second matrix of matrix multiplication.
            The last dimension of `input` must be the same size as the first dimension of `mat2`.

    Returns:
        Tensor, the matrix product of the inputs.

    Raises:
        ValueError: If the last dimension of `input` is not the same size as the
            second-to-last dimension of `mat2`.
        TypeError: If `input` or `mat2` is not a Tensor.
        TypeError: If dtype of `input` or `mat2` is not float16, float32 or bfloat16.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> from mindspore import ops
        >>> import numpy as np
        >>> x1 = ms.Tensor(np.random.rand(2, 3), ms.float32)
        >>> x2 = ms.Tensor(np.random.rand(3, 4), ms.float32)
        >>> out = ops.mm_ext(x1, x2)
        >>> print(out.shape)
        (2, 4)
    """
    return mm_ext_op(input, mat2)


def div_tensor_(input, other):
    r"""
    
    """
    return inplace_div_op(input, other)


def ones(shape, dtype=None):
    r"""
    Creates a tensor filled with value ones.

    .. warning::
        For argument `shape`, Tensor type input will be deprecated in the future version.

    Args:
        shape (Union[tuple[int], list[int], int, Tensor]): The shape specified.
        dtype (:class:`mindspore.dtype`): The data type specified. Default ``None`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> mindspore.ops.ones(4)
        Tensor(shape=[4], dtype=Float32, value= [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.00000000e+00])
        >>> mindspore.ops.ones((2, 3))
        Tensor(shape=[2, 3], dtype=Float32, value=
        [[ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00],
         [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00]])
        >>> mindspore.ops.ones(mindspore.tensor([1, 2, 3]))
        Tensor(shape=[1, 2, 3], dtype=Float32, value=
        [[[ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00],
          [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00]]])
    """
    return ones_op(shape, dtype)


def assign_add(variable, value):
    r"""
    Updates a parameter or tensor by adding a value to it.

    Support implicit type conversion and type promotion.

    Args:
        variable (Union[Parameter, Tensor]): The input parameter or tensor.
        value (Union[Tensor, Number]): The value to be added to the `variable`.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> variable = mindspore.Parameter([1], name="global_step")
        >>> value = mindspore.tensor([100], dtype=mindspore.int32)
        >>> mindspore.ops.assign_add(variable, value)
        >>> print(variable.asnumpy())
        [101]
    """
    return assign_add_op(variable, value)


def gather_d(x, dim, index):
    r"""
    Gathers elements along an axis specified by dim.

    Refer to :func:`mindspore.ops.gather_elements` for more detail.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> x = mindspore.tensor([[1, 2], [3, 4]], mindspore.int32)
        >>> index = mindspore.tensor([[0, 0], [1, 0]], mindspore.int32)
        >>> dim = 1
        >>> output = mindspore.ops.gather_d(x, dim, index)
        >>> print(output)
        [[1 1]
         [4 3]]
    """
    return gather_d_op(x, dim, index)


def asin_ext(input):
    r"""
    Computes arcsine of input tensors element-wise.

    .. math::

        out_i = \sin^{-1}(input_i)

    Args:
        input (Tensor): The shape of tensor is
            :math:`(N,*)`, where :math:`*` means any number of additional dimensions.

    Returns:
        Tensor, has the same shape as `input`. The dtype of output is float32 when dtype of `input` is in [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)
        >>> output = ops.asin_ext(input)
        >>> print(output)
        [0.8330927  0.04001068  0.30469266  0.59438497 ]
    """
    return asin_ext_op(input)


def greater(input, other):
    r"""
    Compute the value of :math:`input > other` element-wise.

    Args:
        input (Union[Tensor, Number]): The first input.
        other (Union[Tensor, Number]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: The shape of two inputs are different
        >>> input = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.greater(input, 2.0)
        >>> print(output)
        [False False True]
        >>> # case 2: The shape of two inputs are the same
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> output = mindspore.ops.greater(input, other)
        >>> print(output)
        [ False False False]
    """
    return greater_op(input, other)


def lerp_scalar(input, end, weight):
    r"""
    
    """
    return lerp_scalar_op(input, end, weight)


def ifftshift(input, dim=None):
    r"""
    The inverse of :func:`mindspore.ops.fftshift` .

    Note:
        - `ifftshift` is currently only used in `mindscience` scientific computing scenarios and
          does not support other usage scenarios.
        - `ifftshift` is not supported on Windows platform yet.

    Args:
        input (Tensor): Input tensor.
        dim (Union[int, list(int), tuple(int)], optional): The dimensions which to shift.
            Default is ``None``, which shifts all dimensions.

    Returns:
        output (Tensor), the shifted tensor with the same shape and dtype as `input`.

    Raises:
        TypeError: If `input` is not a tensor.
        TypeError: If the type/dtype of `dim` is not int.
        ValueError: If `dim` is out of the range of :math:`[-input.ndim, input.ndim)`.

    Supported Platforms:
        ``Ascend`` ``CPU``

    Examples:
        >>> from mindspore.ops import fftshift, ifftshift
        >>> from mindspore import Tensor
        >>> from mindspore import dtype as mstype
        >>> input = Tensor([0, 1, 2, 3, 4, -5, -4, -3, -2, -1], dtype=mstype.int32)
        >>> ifftshift(fftshift(input))
        Tensor(shape=[10], dtype=Int32, value= [ 0, 1, 2, 3, 4, -5, -4, -3, -2, -1])
    """
    return ifftshift_op(input, dim)


def atan2_ext(input, other):
    r"""
    Returns arctangent of input/other element-wise.

    It returns :math:`\theta\ \in\ [-\pi, \pi]`
    such that :math:`input = r*\sin(\theta), other = r*\cos(\theta)`, where :math:`r = \sqrt{input^2 + other^2}`.

    Note:
        - Arg `input` and `other` comply with the implicit type conversion rules to make the data types consistent.
          If they have different data types, the lower precision data type will be converted to relatively the
          highest precision data type.

    Args:
        input (Tensor, Number.number): The input tensor or scalar.
        other (Tensor, Number.number): The input tensor or scalar. It has the same shape with `input` or
            its shape is able to broadcast with `input`.

    Returns:
        Tensor, the shape is the same as the one after broadcasting.
        The dtype of output is float32 when dtype of `input` is in
        [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as `input`.

    Raises:
        TypeError: If `input` or `other` is not a Tensor or scalar.
        RuntimeError: If the data type of `input` and `other` conversion of Parameter is required
                    when data type conversion of Parameter is not supported.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([0, 1]), mindspore.float32)
        >>> other = Tensor(np.array([1, 1]), mindspore.float32)
        >>> output = ops.auto_generate.atan2_ext(input, other)
        >>> print(output)
        [0.        0.7853982]
    """
    return atan2_ext_op(input, other)


def gcd(input, other):
    r"""
    Computes greatest common divisor of input tensors element-wise.

    Support broadcasting and type promotion. Data types should be one of: int16 (supported when using the Ascend
    backend, Graph mode is only supported when the graph compilation level is O0), int32, int64.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The first input tensor.
        other (Tensor): The second input tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([7, 8, 9])
        >>> other = mindspore.tensor([14, 6, 12])
        >>> mindspore.ops.gcd(input, other)
        Tensor(shape=[3], dtype=Int64, value= [7, 2, 3])
    """
    return gcd_op(input, other)


def masked_scatter(input, mask, source):
    r"""
    
    """
    return masked_scatter_op(input, mask, source)


def less_equal(input, other):
    r"""
    Compute the value of :math:`input <= other` element-wise.

    .. math::
        out_{i} =\begin{cases}
            & \text{True,    if } input_{i}<=other_{i} \\
            & \text{False,   if } input_{i}>other_{i}
            \end{cases}

    .. note::
        - Support implicit type conversion.
        - When the inputs are one tensor and one scalar, the scalar could only be a constant.

    Args:
        input (Union[Tensor, Number, bool]): The first input.
        other (Union[Tensor, Number, bool]): The second input.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> # case 1: The shape of two inputs are different
        >>> input = mindspore.tensor([1, 2, 3], mindspore.float32)
        >>> output = mindspore.ops.less_equal(input, 2.0)
        >>> print(output)
        [True  True False]
        >>> # case 2: The shape of two inputs are the same
        >>> input = mindspore.tensor([1, 2, 3], mindspore.int32)
        >>> other = mindspore.tensor([1, 2, 4], mindspore.int32)
        >>> output = mindspore.ops.less_equal(input, other)
        >>> print(output)
        [ True  True  True]
    """
    return less_equal_op(input, other)


def avg_pool1d_ext(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True):
    r"""
    Applies a 1D average pooling over an input Tensor which can be regarded as a composition of 1D input planes.

    Typically the input is of shape :math:`(N_{in}, C_{in}, L_{in})`, avg_pool1d outputs regional average in the
    :math:`(L_{in})`-dimension. Given kernel size as :math:`ks = l_{ker}` and `stride` as :math:`s = s_0`, the
    operation is as follows.

    .. math::
        \text{output}(N_i, C_j, l) = \frac{1}{l_{ker}} \sum_{n=0}^{l_{ker}-1}
        \text{input}(N_i, C_j, s_0 \times l + n)

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): Tensor of shape :math:`(N, C_{in}, L_{in})`.
        kernel_size (Union(int, tuple[int])): The size of kernel window used to take the average value.
        stride (Union(int, tuple[int]), optional): The distance of kernel moving. `stride` can either be an int
            number or a tuple of one int number. Default: ``None``, the same value as `kernel_size`.
        padding (Union(int, tuple[int]), optional): The pad length to be filled. `padding` can either be an integer
            or a tuple of one integer. Default: ``0`` .
        ceil_mode (bool, optional): If True, apply ceil instead of floor to compute the output shape. Default: ``False``.
        count_include_pad (bool, optional): If True, include the zero-padding in the averaging calculation. Default: ``True`` .

    Returns:
        Tensor of shape :math:`(N, C_{in}, L_{out})`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `kernel_size` or `stride` is not an int.
        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.
        ValueError: If `kernel_size` or `stride` is less than `1`.
        ValueError: If `kernel_size` or `stride` or `padding` is not int nor a tuple whose length is greater than `1`.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_x = Tensor(np.random.randint(0, 10, [1, 3, 6]), mindspore.float32)
        >>> output = ops.auto_generate.avg_pool1d_ext(input_x, kernel_size=6, stride=1)
        >>> print(output.shape)
        (1, 3, 1)
    """
    return avg_pool1d_op(input, kernel_size, stride, padding, ceil_mode, count_include_pad)


def max_unpool2d_ext(input, indices, kernel_size, stride=None, padding=0, output_size=None):
    r"""
    Computes the inverse of `max_pool2d`.

    `max_unpool2d` keeps the maximal value and set all position of non-maximal values to zero. Typically the input is of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`, and the output is of shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`. The operation is as follows.

    .. math::
        \begin{array}{ll} \\
        H_{out} = (H_{in} - 1) \times stride[0] - 2 \times padding[0] + kernel\_size[0] \\
        W_{out} = (W_{in} - 1) \times stride[1] - 2 \times padding[1] + kernel\_size[1] \\
        \end{array}

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input Tensor to invert. Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.
        indices (Tensor): Max values' index represented by the indices. Tensor of shape must be same with input 'input'. Values of indices must belong to :math:`[0, H_{in} \times W_{in} - 1]`. Data type must be in int32 or int64.
        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value, an int number that represents height and width of the kernel, or a tuple of two int numbers that represent height and width respectively.
        stride (Union[int, tuple[int]], optional): The distance of kernel moving, an int number that represents the height and width of movement are both stride, or a tuple of two int numbers that represent height and width of movement respectively. Default: ``None`` , which indicates the moving step is `kernel_size` .
        padding (Union[int, tuple[int]], optional): The pad value to be filled. Default: ``0`` . If `padding` is an integer, the paddings of height and width are the same, equal to padding. If `padding` is a tuple of two integers, the padding of height and width equal to padding[0] and padding[1] correspondingly.
        output_size (tuple[int], optional): The target output size. Default: ``None`` . If output_size == (), then the shape of output computed by `kernel_size`, `stride` and `padding`. If output_size != (), then output_size must be :math:`(N, C, H, W)` , :math:`(C, H, W)` or :math:`(H, W)` and output_size must belong to :math:`[(N, C, H_{out} - stride[0], W_{out} - stride[1]), (N, C, H_{out} + stride[0], W_{out} + stride[1])]`.

    Returns:
        Tensor, with shape :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, with the same data type with `input`.

    Raises:
        TypeError: If data type of `input` or `indices` is not supported.
        TypeError: If `kernel_size`, `stride` or `padding` is neither an int nor a tuple.
        ValueError: If numbers in `stride`, `padding` or `kernel_size` are not positive.
        ValueError: If the shapes of `input` and `indices` are different.
        ValueError: If the length of `input` is not 3 or 4.
        ValueError: If the type of `output_size` is not tuple.
        ValueError: If `output_size` is not close to output size computed by attr `kernel_size`, `stride`, `padding`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([[[[0, 1], [8, 9]]]]).astype(np.float32))
        >>> indices = Tensor(np.array([[[[0, 1], [2, 3]]]]).astype(np.int64))
        >>> output = ops.max_unpool2d_ext(input, indices, 1, stride=1, padding=0)
        >>> print(output.asnumpy())
        [[[[0. 1.]
           [8. 9.]]]]
    """
    return max_unpool2d_ext_op(input, indices, kernel_size, stride, padding, output_size)


def inplace_scatter_add(input, dim, index, src):
    r"""
    
    """
    return inplace_scatter_add_op(input, dim, index, src)


def format_cast(input, acl_format):
    r"""
    Change tensor format.

    .. warning::
        FormatCast will not work in the ge backend, origin input will be returned.

    Args:
        input (Tensor): The input tensor. 
        acl_format (int): enum value of acl format, the valid values are below: 
            - ``0`` NCHW
            - ``1`` NHWC
            - ``2`` ND
            - ``3`` NC1HWC0
            - ``4`` FRACTAL_Z
            - ``27`` NDHWC
            - ``29`` FRACTAL_NZ
            - ``30`` NCDHW
            - ``32`` NDC1HWC0
            - ``33`` FRACTAL_Z_3D

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> input = mindspore.ops.randn((2, 3, 4, 5))
        >>> output = mindspore.ops.format_cast(input, 2)
        >>> print(output.shape)
        (2, 3, 4, 5)
    """
    return format_cast_op(input, acl_format)


def unfold_ext(input, kernel_size, dilation=1, padding=0, stride=1):
    r"""
    Extracts sliding local blocks from a batched input tensor.

    Consider a batched input tensor of shape :math:`(N, C, *)`,
    where :math:`N` is the batch dimension, :math:`C` is the channel dimension,
    and :math:`*` represent arbitrary spatial dimensions. This operation flattens
    each sliding `Kernel_size`- sized block within the spatial dimensions
    of `input` into a column (i.e., last dimension) of a 3-D output
    tensor of shape :math:`(N, C \times \prod(\text{kernel_size}), L)`, where
    :math:`C \times \prod(\text{kernel_size})` is the total number of values
    within each block (a block has :math:`\prod(\text{kernel_size})` spatial
    locations each containing a `C`-channeled vector), and :math:`L` is
    the total number of such blocks:

    .. math::
        L = \prod_d \left\lfloor\frac{\text{spatial_size}[d] + 2 \times \text{padding}[d] %
            - \text{dilation}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

    where :math:`\text{spatial_size}` is formed by the spatial dimensions
    of `input` (:math:`*` above), and :math:`d` is over all spatial
    dimensions.

    Therefore, indexing `output` at the last dimension (column dimension)
    gives all values within a certain block.

    The `dilation`, `padding` and `stride` arguments specify
    how the sliding blocks are retrieved.

    .. warning::
        - Currently, batched(4D) image-like tensors are supported.
        - For Ascend, it is only supported on platforms above Atlas A2.

    Args:
        input (Tensor): 4-D Tensor.
        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int
            for height and width. If type is int, it means that height equal with width. Must be specified.
        dilation (Union[int, tuple[int], list[int]], optional): The dilation of the window, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``1`` .
        padding (Union[int, tuple[int], list[int]], optional): The pad of the window, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``0`` .
        stride (Union[int, tuple[int], list[int]], optional): The stride of the window, should be two int
            for height and width. If type is int, it means that height equal with width. Default: ``1`` .

    Returns:
        A Tensor, with same type as `input` .

    Shape:
        - Input: :math:`(N, C, *)`
        - Output: :math:`(N, C \times \prod(\text{kernel_size}), L)`

    Raises:
        TypeError: If any data type of `kernel_size`, `stride`, `dilation`, `padding` is not int, tuple or list.
        ValueError: If `kernel_size`, `dilation`, `stride` value is not
            greater than zero or elements number more than `2`.
        ValueError: If `padding` value is less than zero.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.random.rand(4, 4, 32, 32), mindspore.float32)
        >>> output = ops.auto_generate.unfold_ext(x, kernel_size=3, dilation=1, stride=1)
        >>> print(output.shape)
        (4, 36, 900)
    """
    return im2col_ext_op(input, kernel_size, dilation, padding, stride)


def dense(input, weight, bias=None):
    r"""
    Applies the dense connected operation to the `input`. The dense function is defined as:

    .. math::
        output = input * weight^{T} + bias

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - On the Ascend platform, if `bias` is not 1D, the `input` cannot be greater than 6D in PYNATIVE or KBK mode.

    Args:
        input (Tensor): Input Tensor of shape :math:`(*, in\_channels)`,
            where :math:`*` means any number of additional dimensions.
        weight (Tensor): The weight applied to the input.
            The shape is :math:`(out\_channels, in\_channels)` or :math:`(in\_channels)`.
        bias (Tensor, optional): Additive biases to the output.
            The shape is :math:`(out\_channels)` or :math:`()`. Defaults: ``None``, the `bias` is 0.

    Returns:
        Output whose shape is determined by the shape of the input and the weight.

    Raises:
        TypeError: If `input` is not Tensor.
        TypeError: If `weight` is not Tensor.
        TypeError: If `bias` is not Tensor.
        RuntimeError: On the Ascend platform, if `bias` is not 1D and `input` is greater than 6D in PYNATIVE or KBK mode.

    Supported Platforms:
        ``Ascend`` ``GPU``  ``CPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> input = Tensor([[-1., 1., 2.], [-3., -3., 1.]], mindspore.float32)
        >>> weight = Tensor([[-2., -2., -2.], [0., -1., 0.]], mindspore.float32)
        >>> bias = Tensor([0., 1.], mindspore.float32)
        >>> output = ops.dense(input, weight, bias)
        >>> print(output)
        [[-4.  0.]
         [10.  4.]]
    """
    return dense_op(input, weight, bias)


def add_layer_norm_grad(dy, x1, x2, rstd, mean, gamma, dsumOptional):
    r"""
    
    """
    return add_layer_norm_grad_op(dy, x1, x2, rstd, mean, gamma, dsumOptional)


def reshape_and_cache(key, value=None, key_cache=None, value_cache=None, slot_mapping=None):
    r"""
    The ReshapeAndCache is used for updating the block-wise KVCache of transformer network.

    Args:
        key (Tensor): The key tensor with data type of float16.
          :math:`(num\_tokens, num\_head, head\_dim)`.
        value (Tensor, optional): The value tensor with data type of float16.
          :math:`(num\_tokens, num\_head, head\_dim)`.
        key_cache (Tensor): The cache tensor with data type of float16.
          :math:`(num\_blocks, block\_size, num\_head, head\_dim)`.
        value_cache (Tensor, optional): The cache tensor with data type of float16.
          :math:`(num\_blocks, block\_size, num\_head, head\_dim)`.
        slot_mapping (Tensor): The slot mapping tensor with data type of int32.
          :math:`(num\_tokens,)`.
                    
    Outputs:
        With same data type and same shape as `key` tensor.

    Notes:
        No backend implementation in MindSpore, only use to export MindIr and run in MindSpore Lite.

    Examples:
        >>> from mindspore.ops.operations import _inner_ops
        >>> num_tokens =  = 4
        >>> num_head = 40
        >>> head_dim = 128
        >>> block_size = 16
        >>> num_blocks = 128
        >>> key = Tensor(np.random.randn(num_tokens, num_head, head_dim).astype(np.float16))
        >>> value = Tensor(np.random.randn(num_tokens, num_head, head_dim).astype(np.float16))
        >>> key_cache = Parameter(default_input=Tensor(np.random.randn(num_blocks, block_size, num_head, head_dim).astype(np.float16)))
        >>> value_cache = Parameter(default_input=Tensor(np.random.randn(num_blocks, block_size, num_head, head_dim).astype(np.float16)))
        >>> slot_mapping = Tensor(np.random.shuffle(np.arange(num_tokens, dtype=np.int32)))
        >>> reshape_and_cache = _inner_ops.ReshapAndCache()
        >>> output = reshape_and_cache(key, value, key_cache, value_cache, slot_mapping)
        >>> print(key_cache)
    """
    return reshape_and_cache_op(key, value, key_cache, value_cache, slot_mapping)


def randperm(n, seed=0, offset=0, dtype=mstype.int64):
    r"""
    Generates random permutation of integers from 0 to n-1.

    .. warning::
        - This is an experimental API that is subject to change or deletion.
        - The Ascend backend does not support the reproducibility of random numbers, so
          the `seed` parameter has no effect.


    Args:
        n (Union[Tensor, int]): The upper bound (exclusive).
        seed (int, optional): Random seed. Default ``0`` . When seed is -1, offset is 0,
            it's determined by time.
        offset (int, optional): Offset to generate random numbers. Priority is higher than random seed.
            Default ``0`` . It must be non-negative.
        dtype (mindspore.dtype, optional): The data type returned. Default ``mstype.int64``.

    Returns:
        Tensor. Its shape is specified by the required args `n`.

    Supported Platforms:
        ``CPU``

    Examples:
        >>> import mindspore
        >>> n, seed, offset = 4, 0, 0
        >>> output = mindspore.ops.randperm(n, seed, offset, dtype=mindspore.int64)
        >>> print(output)
        [0 2 1 3]
    """
    randperm_v2_op = _get_cache_prim(RandpermV2)(seed, offset, dtype)
    return randperm_v2_op(n)


def grouped_matmul_v4(x, weight, bias=None, scale=None, offset=None, antiquant_scale=None, antiquant_offset=None, pre_token_scale=None, group_list=None, activation_input=None, activation_quant_scale=None, activation_quant_offset=None, split_item=0, group_type=-1, group_list_type=0, act_type=0, output_dtype=None):
    r"""
    Group calculation matmul.

    ** Non-Quant: **

    .. math::
            y_i = x_i\times weight_i + bias_i

    ** Antiquant-Quant: **

    .. math::
            y_i = x_i\times (weight_i + antiquant\_offset_i) * antiquant\_scale_i + bias_i

    .. note::
        - Only when `bias` , `scale` , `offset` , `antiquant_scale` and `antiquant_offset` are all None, `group_type` is 0,
          and `split_item` is 3, the reverse derivative is supported.
        - When `x` type is int8 and `weight` type is int4, the `scale` should be of the uint64 data type,
          but its memory needs to be arranged in float32 format.
        
    ** Per-Token-Quant **

    .. math::
             y_i = (x_i\times weight_i + bias_i) * scale_i * per\_token\_scale_i

    Args:
        x (TensorList): The input tensor list, include 2D-6D Tensors. Supported dtypes: Float16, Float32, Int8.
                        The shape of the tensor in tensorlist is :math:`(M, K)` or :math:`(..., M, K)`.
        weight (TensorList): The weight tensor list, include 2D-3D Tensors. Supported dtypes: Float16, Float32, Int8.
                             The shape of the tensor in tensorlist is :math:`(K, N)` or :math:`(E, K, N)`.
        bias (TensorList, optional): The list of bias tensors in matrix multiplication, include 1D-2D Tensors.
                                     Supported dtypes: Float16, Float32, Int32. Default: ``None``.
                                     Length is the same as the weight length. The shape of the tensor is :math:`(N)` or :math:`(E, N)`.
        scale (TensorList, optional): Scale factors of quant(A8W8) parameters. Supported dtypes: BFloat16.
                                      Length is the same as the weight length. The shape of the tensor is :math:`(N)` or :math:`(E, N)`.
        offset (TensorList, optional): Offset factors of quant(A8W8) parameters. Supported dtypes: Float32.
                                       Length is the same as the weight length. Currently not supported. Default: ``None``.
        antiquant_scale (TensorList, optional): Scale factors of antiquant(A16W8) parameters. Supported dtypes: Float16.
                                                Length is the same as the weight length. The shape of the tensor is :math:`(N)` or
                                                :math:`(E, N)`. Only use in antiquant. Default: ``None``.
        antiquant_offset (TensorList, optional): Offset factors of antiquant(A16W8) parameters. Supported dtypes: Float16.
                                                 Length is the same as the weight length. The shape of the tensor is :math:`(N)` or
                                                 :math:`(E, N)`. Only use in antiquant. Default: ``None``.
        pre_token_scale (TensorList, optional): Per-token quant parameters, include 1D Tensors. Supported dtypes: Float32.
                                                Length is the same as the x length. The shape of the tensor is :math:`(M)`.
                                                Only use in per-token quant. Default: ``None``.
        group_list (Tensor, optional): Grouping positions for the M-axis of input x. Supported dtypes: Int64. Default: ``None``.
        activation_input (TensorList, optional): Inputs of actiation function. Currently not supported. Default: ``None``.
        activation_quant_scale (TensorList, optional): Scale factors of activation. Currently not supported. Default: ``None``.
        activation_quant_offset (TensorList, optional): Offset factors of activation. Currently not supported. Default: ``None``.
        split_item (int): Splitting input mode. Only support 0 and 3. 0 represents multiple Tensors, and 3 represents a single Tensor.
                          Default: ``0``.
        group_type (int): The axis to be split. Only support -1 and 0. If the matrix is multiplied by A[m,k]xB[k,n]=C[m,n].
                          -1: No grouping, 0: Group on the m-axis. Default: ``-1``.
        group_list_type (int): The format type of grouping positions. Only support 0 and 1. 0 represents the grouping positions
                               as the cumsum of grouping size in each group, and 1 represents the positions as the grouping size in
                               each group. Default: ``0``.
        act_type (int): Activation function type. Currently not supported. Default: ``0``.
        output_dtype (mindspore.dtype): Specifies the output data type, currently taking effect only when input x is int8 and weight is int4.
                                        If None is passed in, bfloat16 will be used by default. Default: ``None``.


        Parameter limitations 1
        =========== ============ =========== ====================================================================================================
        split_item  group_type   group_list  notes
        =========== ============ =========== ====================================================================================================
        0           -1           None        The length of x is n, tensor in x must be 2D-6D. The length of weight is n, tensor in weight must be 2D.
                                             The length of all TensorList inputs must be the same.
        3           0            1D Tensor   The length of x is 1, tensor in x must be 2D. The length of weight is 1, tensor in weight must be 3D.
                                             (group_list.shape)[0] must be equal to (weight.shape)[0], and its numbers must be positive
                                             (increasing when group_list_type is 0).
                                             The sum of group_list (last number when group_list_type is 0) can be less equal than length of x.
        =========== ============ =========== ====================================================================================================

        Parameter limitations 2
        Non-quant type table
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        x           weight     bias          scale      offset     antiquant_scale   antiquant_offset   pre_token_scale  y
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        Float16     Float16    Float16/None  None       None       None              None               None             Float16
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        BFloat16    BFloat16   Float32/None  None       None       None              None               None             BFloat16
        =========   =========  ============  =========  =========  ================  =================  ===============  =========

        Parameter limitations 3
        Only in split_item=3, group_type=0
        =========   =========  ============  =========  =========  ================  =================  ===============   =========
        x           weight     bias          scale      offset     antiquant_scale   antiquant_offset   pre_token_scale   y
        =========   =========  ============  =========  =========  ================  =================  ===============   =========
        Float32     Float32    Float32/None  None       None       None              None               None              Float32
        =========   =========  ============  =========  =========  ================  =================  ===============   =========
        
        Parameter limitations 4
        Anti-quant
        The antiquant_scale and antiquant_offset must be used in the same time.
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        x           weight     bias          scale      offset     antiquant_scale   antiquant_offset   pre_token_scale  y
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        Float16     Int8       Float16/None  None       None       Float16           Float16            None             Float16
        =========   =========  ============  =========  =========  ================  =================  ===============  ========
        BFloat16    Int8       Float32/None  None       None       BFloat16          BFloat16           None             BFloat16
        =========   =========  ============  =========  =========  ================  =================  ===============  =========
        
        Parameter limitations 4
        Per-token-quant
        The scale and pre_token_scale must be used in the same time.
        =========   =========  ==========  =========  =========  ================  =================  ===============  =========
        x           weight     bias        scale      offset     antiquant_scale   antiquant_offset   pre_token_scale  y
        =========   =========  ==========  =========  =========  ================  =================  ===============  =========
        Int8        Int8       int32/None  BFloat16   None       None              None               Float32          BFloat16
        =========   =========  ==========  =========  =========  ================  =================  ===============  =========

    Returns:
        TensorList, include 2D Tensors. The shape of the tensor is :math:`(M, N)`.

    Raises:
        TypeError: If `split_item` is not 0 or 3.
        TypeError: If `group_type` is not -1 or 0.
        TypeError: If `group_list_type` is not 0 or 1.
        TypeError: when `split_item` is 0, `group_type` is not -1.
        TypeError: when `split_item` is 3, `group_type` is not 0.
        TypeError: when `split_item` is 3, `group_list` is None.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> import numpy as np
        >>> from mindspore import nn, context
        >>> from mindspore.ops.auto_generate import grouped_matmul_v4
        >>> ms.set_device(device_target="Ascend")
        >>> context.set_context(mode=ms.GRAPH_MODE)
        >>> x = [ms.Tensor(np.array([[0, 0, 0, 0],
        ...                          [1, 1, 1, 1],
        ...                          [2, 2, 2, 2],
        ...                          [2, 2, 2, 2],
        ...                          [1, 1, 1, 1],
        ...                          [1, 1, 1, 1]]), ms.float16)]
        >>> weight = [ms.Tensor(np.arange(32).reshape((4, 4, 2)), ms.float16)]
        >>> group_list = ms.Tensor([1, 2, 1, 2], ms.int64)
        >>> output = grouped_matmul_v4(x, weight, group_list=group_list, split_item=3, group_type=0, group_list_type=1)
        >>> print(output[0])
        [[0   0  ]
         [44  48 ]
         [88  96 ]
         [152 160]
         [108 112]
         [108 112]]
    """
    return grouped_matmul_v4_op(x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, pre_token_scale, group_list, activation_input, activation_quant_scale, activation_quant_offset, split_item, group_type, group_list_type, act_type, output_dtype)


def kv_cache_scatter_update(var, indices, updates, axis, reduce='none'):
    r"""
    Update var with updates and indices along sequence axis.

    Args:
        var (Tensor): 4-D tensor, the target tensor.
        indices (Tensor): 1-D tensor, the index tensor.
        updates (Tensor): 4-D tensor, the tensor doing the update operation.
        axis (Int): Which axis to scatter, can be '-1' and '-2'.
        reduce (String): Scatter mode, default to string "none" and can be "update".

    Returns:
        Tensor, has the same data type and shape as original `var`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> from mindspore.ops.operations._infer_ops import KVCacheScatterUpdate
        >>> kv_cache_scatter_update_op = KVCacheScatterUpdate()
        >>> var_shape = [1, 5, 128, 4096]
        >>> var = np.random.uniform(low=1, high=10, size=var_shape).astype(np.float32)
        >>> indices_shape = [1]
        >>> indices = np.random.randint(low=1, high=10, size=indices_shape).astype(np.int64)
        >>> updates_shape = [1, 5, 128, 1]
        >>> updates = np.random.uniform(low=1, high=10, size=updates_shape).astype(np.float32)
        >>> output = kv_cache_scatter_update_op(Tensor(var), Tensor(indices), Tensor(updates), -1, 'update')
        >>> print(output.shape)
    """
    return kv_cache_scatter_update_op(var, indices, updates, axis, reduce)


def grouped_matmul(x, weight, bias=None, scale=None, offset=None, antiquant_scale=None, antiquant_offset=None, group_list=None, split_item=0, group_type=-1, transpose_a=False, transpose_b=False):
    r"""
    Group calculation matmul.

    ** Non-Quant: **

    .. math::
            y_i = x_i\times weight_i + bias_i

    ** Antiquant-Quant: **

    .. math::
            y_i = x_i\times (weight_i + antiquant\_offset_i) * antiquant\_scale_i + bias_i

    .. note::
        Only when `bias` , `scale` , `offset` , `antiquant_scale` and `antiquant_offset` are all None, `group_type` is 0,
        and `split_item` is 3, the reverse derivative is supported.

    Args:
        split_item (int): Splitting input mode. Only support 0 and 3. 0 represents multiple Tensors, and 3 represents a single Tensor.
        group_type (int): The axis to be split. Only support -1 and 0. If the matrix is multiplied by A[m,k]xB[k,n]=C[m,n].
                          -1: No grouping, 0: Group on the m-axis
        transpose_a (bool): If ``True`` , `x` is transposed before multiplication. Default: ``False`` .
        transpose_b (bool): If ``True`` , `weight` is transposed before multiplication. Default: ``False`` .

    Inputs:
        x (TensorList): TensorList, including 2D-6D Tensors. Supported dtypes: Float16, Float32.
                        The shape of the tensor in tensorlist is :math:`(M, N)` or :math:`(..., M, N)`.
        weight (TensorList): TensorList, include 2D-3D Tensors. Supported dtypes: Float16, Float32, int8.
                             The shape of the tensor in tensorlist is :math:`(N, K)` or :math:`(E, N, K)`.
        bias (TensorList, optional): TensorList, include 1D-2D Tensors. Supported dtypes: Float16, Float32. If not used, None.
                                     Length is the same as the weight length. The shape of the tensor is :math:`(N)` or :math:`(E, N)`.
        scale (TensorList, optional): TensorList, scale factor of quant(A8W8) parameters. Supported dtypes: Unit64.
                                      Length is the same as the weight length. Currently not supported, use None.
        offset (TensorList, optional): TensorList, offset of quant(A8W8) parameters. Supported dtypes: Float32.
                                       Length is the same as the weight length. Currently not supported, use None.
        antiquant_scale (TensorList, optional): TensorList, scale factor of antiquant(A16W8) parameters. Supported dtypes: Float16.
                                                Length is the same as the weight length. Only use in antiquant. If not used, None.
        antiquant_offset (TensorList, optional): TensorList, offset factor of antiquant(A16W8) parameters. Supported dtypes: Float16.
                                                 Length is the same as the weight length.  Only use in antiquant. If not used, None.
        group_list (Tensor, optional): Grouping positions for the M-axis of input x. Supported dtypes: Int64


        Parameter limitations 1
        =========== ============ =========== ====================================================================================================
        split_item  group_type   group_list  notes
        =========== ============ =========== ====================================================================================================
        0           -1           None        The length of x is n, tensor in x must be 2D-6D. The length of weight is n, tensor in weight must be 2D.
        3           0            1D Tensor   The length of x is 1, tensor in x must be 2D. The length of weight is 1, tensor in weight must be 3D.
                                             (group_list.shape)[0] must be equal to (weight.shape)[0]
                                             The last number in group_list needs to be equal to the 0th dimension of the shape with weight
        =========== ============ =========== ====================================================================================================

        Parameter limitations 2
        Non-quant tyep table
        =========   =========  =========  =========  =========  ================  =================   =========
        x           weight     bias       scale      offset     antiquant_scale   antiquant_offset    y
        =========   =========  =========  =========  =========  ================  =================   =========
        Float16     Float16    Float16    None       None       None              None                Float16
        =========   =========  =========  =========  =========  ================  =================   =========

        Parameter limitations 3
        Only in split_item=3, group_type=0
        =========   =========  =========  =========  =========  ================  =================   =========
        x           weight     bias       scale      offset     antiquant_scale   antiquant_offset    y
        =========   =========  =========  =========  =========  ================  =================   =========
        Float32     Float32    Float32    None       None       None              None                Float32
        =========   =========  =========  =========  =========  ================  =================   =========

    Outputs:
        y (TensorList): TensorList, include 2D Tensors. The shape of the tensor is :math:`(M, K)`.

    Raises:
        TypeError: If `split_item` is not 0 or 3.
        TypeError: If `group_type` is not -1 or 0.
        TypeError: when `split_item` is 0, `group_type` is not -1.
        TypeError: when `split_item` is 3, `group_type` is not 0.
        TypeError: when `split_item` is 3, `group_list` is None.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> import numpy as np
        >>> from mindspore import nn, context
        >>> from mindspore.ops.auto_generate import GroupedMatmul
        >>> class Net(nn.Cell):
        ...     def __init__(self, split_item=3, group_type=0):
        ...         super(Net, self).__init__()
        ...         self.gmm = GroupedMatmul(split_item, group_type)
        ...
        ...     def construct(self, x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, group_list):
        ...         result = self.gmm(x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, group_list)
        ...         return result
        ...
        >>> ms.set_device(device_target="Ascend")
        >>> context.set_context(mode=ms.GRAPH_MODE)
        >>> x = [ms.Tensor(np.array([[0, 0, 0, 0],
        ...                          [1, 1, 1, 1],
        ...                          [2, 2, 2, 2],
        ...                          [2, 2, 2, 2],
        ...                          [1, 1, 1, 1],
        ...                          [1, 1, 1, 1]]), ms.float16)]
        >>> weight = [ms.Tensor(np.arange(32).reshape((4, 4, 2)), ms.float16)]
        >>> bias = None
        >>> scale = None
        >>> offset = None
        >>> antiquant_scale = None
        >>> antiquant_offset = None
        >>> group_list = ms.Tensor([1, 3, 4, 6], ms.int64)
        >>> net = Net()
        >>> output = net(x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, group_list)
        >>> print(output[0])
        [[0   0  ]
         [44  48 ]
         [88  96 ]
         [152 160]
         [108 112]
         [108 112]]
    """
    return grouped_matmul_impl(x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, group_list, split_item, group_type, transpose_a, transpose_b)


def moe_finalize_routing(expanded_x, x1, x2=None, bias=None, scales=None, expanded_row_idx=None, expanded_expert_idx=None):
    r"""
    In MoE calculation, merge the results output by FFN and rearrange the output in time order by experts.

    Notes:
        - E: The number of experts, such as 8.
        - K: The number of experts selected by a token, such as 1 or 2.
        - N: The number of rows in x1, which is the number of original tokens.
        - H: The number of cols in x1, which is the hiddens of tokens.

    .. math::

        expertid = expanded_expert_idx[i,k]
        out(i,j) = x1_{i,j} + x2_{i,j} + \sum_{k=0}^{K}(scales_{i,k}*(expanded\_x_{expanded\_row\_idx_{i+k*N},j} + bias_{expertid,j}))

    Inputs:
        expanded_x (Tensor): The output of MoE FFN. The tensor must be 2D tensor. The shape of the tensor must be :math:`(K*N, H)`.
                             Supported dtypes: Float16, Float32.
        x1 (Tensor): The output of attention. The tensor must be 2D tensor. The shape of the tensor must be :math:`(N, H)`.
                     Data type requirements should be consistent with expanded_x.
                     If not used, the required values to be passed are all 0, The shape of the Tensor meets the requirements
        x2 (Tensor, optional): The output of attention. The tensor must be 2D tensor. The shape of the tensor must be :math:`(N, H)`. If not used, None.
                               Data type requirements should be consistent with expanded_x.
        bias (Tensor): The bias of the last matmul in MoE FFN. The tensor must be 2D tensor. The shape of the tensor must be :math:`(E, H)`.
                       Data type requirements should be consistent with expanded_x.
        scales (Tensor): Weighted expanded when each token corresponds to multiple experts. The tensor must be 2D tensor.
                         The shape of the tensor must be :math:`(N, K)`. Data type requirements should be consistent with expanded_x.
                         If not used, the required values to be passed are all 1. The shape of the Tensor meets the requirements
        expanded_row_idx (Tensor): The index in time order. The tensor must be 1D tensor. The shape of the tensor must be :math:`(K*N)`. Supported dtypes: Int32.
                                   The value in Tensor must be between 0 and K*N, and the value cannot be repeated.
        expanded_expert_idx (Tensor): The experts selected for each token are used to find the bias of which experts need to be accumulated.
                                      The tensor must be 2D tensor. The shape of the tensor must be :math:`(N, K)`. Supported dtypes: Int32.

    Outputs:
        Tensor, the merged and sorted results. The tensor is 2D tensor. The shape of the tensor is :math:`(N, H)`. Data type consistent with expanded_x.

    Raises:
        TypeError: If the data type of input Tensor does not match the description in args.
        ShapeError: If the shape of input Tensor does not match the description in args.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> import numpy as np
        >>> from mindspore import Tensor, nn, context
        >>> from mindspore.ops.auto_generate import MoeFinalizeRouting
        >>> class Net(nn.Cell):
        ...     def __init__(self):
        ...         super(Net, self).__init__()
        ...         self.moe_finalize_routing = MoeFinalizeRouting()
        ...
        ...     def construct(self, expanded_x, x1, x2, bias, scales, expanded_row_idx, expanded_expert_idx):
        ...         result = self.moe_finalize_routing(expanded_x, x1, x2, bias, scales, expanded_row_idx, expanded_expert_idx)
        ...         return result
        ...
        >>> ms.set_device(device_target="Ascend")
        >>> context.set_context(mode=ms.GRAPH_MODE)
        >>> # E = 4, K = 2, N = 3, H = 4
        >>> expanded_x = ms.Tensor(np.array([[0.1, 0.1, 0.1, 0.1],
        ...                                  [0.2, 0.2, 0.2, 0.2],
        ...                                  [0.3, 0.3, 0.3, 0.3],
        ...                                  [0.1, 0.1, 0.1, 0.1],
        ...                                  [0.2, 0.2, 0.2, 0.2],
        ...                                  [0.3, 0.3, 0.3, 0.3]]), ms.float16)
        >>> x1 = ms.Tensor(np.array([[1, 1, 1, 1],
        ...                          [0.2, 0.2, 0.2, 0.2],
        ...                          [0.3, 0.3, 0.3, 0.3]]), ms.float16)
        >>> x2 = None
        >>> bias = ms.Tensor(np.array([[0.1, 0.1, 0.1, 0.1],
        ...                            [0.2, 0.2, 0.2, 0.2],
        ...                            [0.3, 0.3, 0.3, 0.3],
        ...                            [0.4, 0.4, 0.4, 0.4]]), ms.float16)
        >>> scales = ms.Tensor(np.array([[0.7, 0.3],
        ...                              [0.8, 0.2],
        ...                              [0.8, 0.2]]), ms.float16)
        >>> expanded_row_idx = ms.Tensor(np.array([2, 3, 1, 0, 5, 4]), ms.int32)
        >>> expanded_expert_idx = ms.Tensor(np.array([[0, 1],
        ...                                           [0, 2],
        ...                                           [1, 3]]), ms.int32)
        >>> net = Net()
        >>> output = net(expanded_x, x1, x2, bias, scales, expanded_row_idx, expanded_expert_idx)
        >>> print(output)
        [[1.37 1.37 1.37 1.37]
         [0.48 0.48 0.48 0.48]
         [0.74 0.74 0.74 0.74]]
    """
    return moe_finalize_routing_op(expanded_x, x1, x2, bias, scales, expanded_row_idx, expanded_expert_idx)


def moe_init_routing(x, row_idx, expert_idx, active_num):
    r"""
    Performs routing on the computation result of MoeGatingTopKSoftmax.

    Inputs:
        - **x** (Tensor) - 2D tensor, which contains input feature tokens. The shape is (NUM_ROWS, H).
        - **row_idx** (Tensor) - Original row ID of each position. The shape must be the same as that of expertForSourceRow.
        - **expert_idx** (Tensor) - 2D tensor, indicating k experts corresponding to each row of features in the output of aclnnMoeGatingTopKSoftmax. The shape is (NUM_ROWS, K). 
        active_num (int64): maximum number of rows that can be processed, that is, the maximum number of rows that are valid in expandedXOut.

    Outputs:
        - **expanded_x** (Tensor) - 2D tensor, indicating features extended based on expertIdx. The shape is (min(NUM_ROWS, activeNum) * k, H).
        - **expanded_row_idx** (Tensor) - 1D tensor, indicating mapping between expandedX and x. The shape is (NUM_ROWS*K).
        - **expanded_expert_idx** (Tensor) - sorted result of expertIdx.

    Raises:
        ShapeError: If the shape of input Tensor does not match the description in args.

    Supported Platforms:
        ``Ascend``
    """
    return moe_init_routing_op(x, row_idx, expert_idx, active_num)


def grouped_matmul_v2(x, weight, bias=None, scale=None, offset=None, antiquant_scale=None, antiquant_offset=None, group_list=None, split_item=0, group_type=-1):
    r"""
    
    """
    return grouped_matmul_v2_op(x, weight, bias, scale, offset, antiquant_scale, antiquant_offset, group_list, split_item, group_type)


def weight_quant_batch_matmul(x, weight, antiquant_scale, antiquant_offset=None, quant_scale=None, quant_offset=None, bias=None, transpose_x=False, transpose_weight=False, antiquant_group_size=0):
    r"""
    
    """
    return weight_quant_batch_matmul_impl(x, weight, antiquant_scale, antiquant_offset, quant_scale, quant_offset, bias, transpose_x, transpose_weight, antiquant_group_size)


def quant_matmul(x1, x2, scale, offset=None, pertoken_scale=None, bias=None, output_dtype=None, x1_dtype=None, x2_dtype=None, pertoken_scale_dtype=None, scale_dtype=None, group_sizes=None):
    r"""
    
    """
    return quant_matmul_op(x1, x2, scale, offset, pertoken_scale, bias, output_dtype, x1_dtype, x2_dtype, pertoken_scale_dtype, scale_dtype, group_sizes)


def quant_batch_matmul(x1, x2, scale, offset=None, bias=None, pertokenScaleOptional=None, transpose_x1=False, transpose_x2=False, dtype=mstype.float16):
    r"""
    
    """
    return quant_batch_matmul_impl(x1, x2, scale, offset, bias, pertokenScaleOptional, transpose_x1, transpose_x2, dtype)


def moe_compute_expert_tokens(sorted_experts, num_expert):
    r"""
    In MoE calculation, Search for the last index processed by each expert through binary search.

    .. math::
        expert_tokens_{i} = BinarySearch(sorted_experts, num_expert)

    Inputs:
        - **sorted_experts** (Tensor) - A tensor which represent sorted experts, must be 1D tensor.
          Supported type: Int32.
        - **num_expert** (int) - The number of experts, must be greater than 0.

    Outputs:
        Tensor, have the same dtype with sorted_experts.

    Raises:
        TypeError: if `sorted_experts` is not a tensor.
        ValueError: if `num_expert` is less than 0.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore as ms
        >>> from mindspore.ops.auto_generate import MoeComputeExpertTokens
        >>> sorted_experts = ms.Tensor([0, 0, 1, 2, 2], dtype=ms.int32)
        >>> num_expert = 5
        >>> net = MoeComputeExpertTokens()
        >>> expert_tokens = net(sorted_experts, num_expert)
        >>> print(expert_tokens)
        [2, 3, 5]
    """
    return moe_compute_expert_tokens_op(sorted_experts, num_expert)


def moe_token_unpermute(permuted_tokens, sorted_indices, probs=None, padded_mode=False, restore_shape=None):
    r"""
    Unpermute a tensor of permuted tokens based on sorted indices, and optionally merge the tokens with their corresponding probabilities.

    .. warning::
        - It is only supported on Atlas A2 Training Series Products.
        - `sorted_indices` must not have duplicate values, otherwise the result is undefined.

    Args:
        permuted_tokens (Tensor): The tensor of permuted tokens to be unpermuted.
            The shape is :math:`[num\_tokens * topk, hidden\_size]` , where `num_tokens`, `topk` and `hidden_size` are positive integers.
        sorted_indices (Tensor): The tensor of sorted indices used to unpermute the tokens.
            The shape is :math:`[num\_tokens * topk,]`, where `num_tokens` and `topk` are positive integers.
            It only supports the int32 data type.
        probs (Tensor, optional): The tensor of probabilities corresponding to the permuted tokens.
            If provided, the unpermuted tokens will be merged with their respective probabilities.
            The shape is :math:`[num\_tokens, topk]`, where `num_tokens` and `topk` are positive integers. Default: ``None`` .
        padded_mode (bool, optional): If ``True``, indicating the indices are padded to denote selected tokens per expert. Default: ``False`` .
        restore_shape (Union[tuple[int], list[int]], optional): The input shape before permutation, only used in padding mode. Default: ``None`` .

    Returns:
        Tensor, with the same dtype as `permuted_tokens`. If `padded_mode` is ``False``, the shape will be [`num_tokens`, `hidden_size`].
        If `padded_mode` is ``True``, the shape will be specified by `restore_shape`.

    Raises:
        TypeError: If `permuted_tokens` is not a Tensor.
        ValueError: Only supported when `padded_mode` is ``False``.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, ops
        >>> permuted_token = Tensor([
        ...                          [1, 1, 1],
        ...                          [0, 0, 0],
        ...                          [0, 0, 0],
        ...                          [3, 3, 3],
        ...                          [2, 2, 2],
        ...                          [1, 1, 1],
        ...                          [2, 2, 2],
        ...                          [3, 3, 3]], dtype=mindspore.bfloat16)
        >>> sorted_indices = Tensor([0, 6, 7, 5, 3, 1, 2, 4], dtype=mindspore.int32)
        >>> out = ops.moe_token_unpermute(permuted_token, sorted_indices)
        >>> out.shape
        (8, 3)
        
        
    """
    return moe_token_unpermute_op(permuted_tokens, sorted_indices, probs, padded_mode, restore_shape)


def cosine_embedding_loss(input1, input2, target, margin=0.0, reduction='mean'):
    r"""
    Creates a criterion to measure the similarity between two tensors using cosine distance.

    Given two Tensors :math:`x1`, :math:`x2`, and a Tensor label :math:`y` (positive samples use 1 and negative samples use -1),
    the formula is as follows:

    .. math::
        loss(x_1, x_2, y) = \begin{cases}
        1-cos(x_1, x_2), & \text{if } y = 1\\
        \max(0, cos(x_1, x_2)-margin), & \text{if } y = -1\\
        \end{cases}


    Args:
        input1 (Tensor): Input Tensor of shape :math:`(N, D)` or :math:`(D)` , where :math:`N` is the batch size and :math:`D` is the embedding dimension.
        input2 (Tensor): Input Tensor of shape :math:`(N, D)` or :math:`(D)` , which has same dtype as `input1`, and its shape should be the same as `input1` or broadcastable to the shape of `input1`.
        target (Tensor): Target Tensor of shape :math:`(N)` or :math:`()` , contains value 1 or -1.
        margin (float, optional): A tuning factor used in the negative-sample branch, which should be in [-1.0, 1.0], values outside this range will not raise an error,
            but have no practical meaning. Default: ``0.0`` .
        reduction (str, optional): Apply specific reduction method to the output: ``'none'`` , ``'mean'`` ,
            ``'sum'`` . Default: ``'mean'`` .

            - ``'none'`` : no reduction will be applied.
            - ``'mean'`` : compute and return the mean of elements in the output.
            - ``'sum'`` : the output elements will be summed.  

    Returns:
        Tensor or Scalar, if `reduction` is ``"none"``, a Tensor with the same shape as `target` will be returned. Otherwise, a Scalar value will be returned.

    Raises:
        ValueError: If `reduction` is not ``"none"``, ``"mean"`` or ``"sum"``.
        ValueError: If the shapes of `input1` and `input2` do not match.
        ValueError: If the shape of `target` does not match the shapes of `input1` and `input2`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input1 = Tensor(np.array([[0.3, 0.8], [0.4, 0.3]]), mindspore.float32)
        >>> input2 = Tensor(np.array([[0.4, 1.2], [-0.4, -0.9]]), mindspore.float32)
        >>> target = Tensor(np.array([1, -1]), mindspore.int32)
        >>> output = ops.cosine_embedding_loss(input1, input2, target)
        >>> print(output)
        0.0003425479
    """
    return cosine_embedding_loss_op(input1, input2, target, margin, reduction)
