{
  "total_runs": 48,
  "total_runs_with_submissions": 48,
  "total_valid_submissions": 47,
  "total_medals": 40,
  "total_gold_medals": 17,
  "total_silver_medals": 17,
  "total_bronze_medals": 6,
  "total_above_median": 43,
  "competition_reports": [
    {
      "competition_id": "aerial-cactus-identification",
      "score": 1.0,
      "gold_threshold": 1.0,
      "silver_threshold": 1.0,
      "bronze_threshold": 1.0,
      "median_threshold": 0.9991,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:19.394545",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-25-GMT_run-group_piml_v1__aerial-cactus-identification_unknown_llm_0steps_24hrs_gpu_40cores/aerial-cactus-identification_49ab5615-8fb8-497e-9af1-0bfbabdbe8c6/submission/submission.csv"
    },
    {
      "competition_id": "aptos2019-blindness-detection",
      "score": 0.9388,
      "gold_threshold": 0.930508,
      "silver_threshold": 0.919654,
      "bronze_threshold": 0.914492,
      "median_threshold": 0.888912,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:19.464862",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-25-GMT_run-group_piml_v1__aptos2019-blindness-detection_unknown_llm_0steps_24hrs_gpu_40cores/aptos2019-blindness-detection_276ab6ef-93a7-4ae9-9264-fb882f0c1098/submission/submission.csv"
    },
    {
      "competition_id": "denoising-dirty-documents",
      "score": 0.0117,
      "gold_threshold": 0.01794,
      "silver_threshold": 0.02609,
      "bronze_threshold": 0.04517,
      "median_threshold": 0.07325,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:32.000836",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-26-GMT_run-group_piml_v1__denoising-dirty-documents_unknown_llm_0steps_24hrs_gpu_40cores/denoising-dirty-documents_eeaa5998-adac-4f48-bf90-0751d5356ac1/submission/submission.csv"
    },
    {
      "competition_id": "detecting-insults-in-social-commentary",
      "score": 0.95966,
      "gold_threshold": 0.83321,
      "silver_threshold": 0.82307,
      "bronze_threshold": 0.79111,
      "median_threshold": 0.77842,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:32.254299",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-31-GMT_run-group_piml_v1__detecting-insults-in-social-commentary_unknown_llm_0steps_24hrs_gpu_40cores/detecting-insults-in-social-commentary_5f63bda3-db5b-4021-a9fe-e95332afbacb/submission/submission.csv"
    },
    {
      "competition_id": "dogs-vs-cats-redux-kernels-edition",
      "score": 0.00691,
      "gold_threshold": 0.03882,
      "silver_threshold": 0.05038,
      "bronze_threshold": 0.06127,
      "median_threshold": 0.12216,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:32.322108",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-32-GMT_run-group_piml_v1__dogs-vs-cats-redux-kernels-edition_unknown_llm_0steps_24hrs_gpu_40cores/dogs-vs-cats-redux-kernels-edition_9ec63b8b-83b4-4b0d-8cba-72f25a4dece1/submission/submission.csv"
    },
    {
      "competition_id": "histopathologic-cancer-detection",
      "score": 0.99791,
      "gold_threshold": 0.9835,
      "silver_threshold": 0.9798,
      "bronze_threshold": 0.9738,
      "median_threshold": 0.9477,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:32.523385",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-33-GMT_run-group_piml_v1__histopathologic-cancer-detection_unknown_llm_0steps_24hrs_gpu_40cores/histopathologic-cancer-detection_a93b8e67-b838-49b5-9c84-5a2f1a704b18/submission/submission.csv"
    },
    {
      "competition_id": "jigsaw-toxic-comment-classification-challenge",
      "score": 0.9872,
      "gold_threshold": 0.9874,
      "silver_threshold": 0.98668,
      "bronze_threshold": 0.98639,
      "median_threshold": 0.98079,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:33.032487",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-38-GMT_run-group_piml_v1__jigsaw-toxic-comment-classification-challenge_unknown_llm_0steps_24hrs_gpu_40cores/jigsaw-toxic-comment-classification-challenge_683270c4-1189-4086-bcf1-077bc54d6a75/submission/submission.csv"
    },
    {
      "competition_id": "mlsp-2013-birds",
      "score": 0.96333,
      "gold_threshold": 0.93527,
      "silver_threshold": 0.90038,
      "bronze_threshold": 0.87372,
      "median_threshold": 0.86572,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:33.101661",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-44-GMT_run-group_piml_v1__mlsp-2013-birds_unknown_llm_0steps_24hrs_gpu_40cores/mlsp-2013-birds_00eeff1b-036e-40d5-a7bf-50c461219e4d/submission/submission.csv"
    },
    {
      "competition_id": "nomad2018-predict-transparent-conductors",
      "score": 0.05842,
      "gold_threshold": 0.05589,
      "silver_threshold": 0.06229,
      "bronze_threshold": 0.06582,
      "median_threshold": 0.06988,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:33.168898",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-46-GMT_run-group_piml_v1__nomad2018-predict-transparent-conductors_unknown_llm_0steps_24hrs_gpu_40cores/nomad2018-predict-transparent-conductors_5f6e05c4-0a1f-4522-ad83-e26af2033f11/submission/submission.csv"
    },
    {
      "competition_id": "plant-pathology-2020-fgvc7",
      "score": 0.9972,
      "gold_threshold": 0.97836,
      "silver_threshold": 0.97465,
      "bronze_threshold": 0.97361,
      "median_threshold": 0.94852,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:33.235528",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-50-GMT_run-group_piml_v1__plant-pathology-2020-fgvc7_unknown_llm_0steps_24hrs_gpu_40cores/plant-pathology-2020-fgvc7_a3a3eafb-0f3e-4149-9bbb-431a32e8c561/submission/submission.csv"
    },
    {
      "competition_id": "random-acts-of-pizza",
      "score": 0.70779,
      "gold_threshold": 0.97908,
      "silver_threshold": 0.76482,
      "bronze_threshold": 0.6921,
      "median_threshold": 0.5995950000000001,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:33.312283",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-52-GMT_run-group_piml_v1__random-acts-of-pizza_unknown_llm_0steps_24hrs_gpu_40cores/random-acts-of-pizza_8205e9c1-ff60-4d1e-87d8-dc4bd2fe4d89/submission/submission.csv"
    },
    {
      "competition_id": "spooky-author-identification",
      "score": 0.26205,
      "gold_threshold": 0.16506,
      "silver_threshold": 0.26996,
      "bronze_threshold": 0.29381,
      "median_threshold": 0.418785,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:33.388203",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-57-57-GMT_run-group_piml_v1__spooky-author-identification_unknown_llm_0steps_24hrs_gpu_40cores/spooky-author-identification_4672acd4-b2d4-4a8c-ae0d-f74b83641e50/submission/submission.csv"
    },
    {
      "competition_id": "tabular-playground-series-dec-2021",
      "score": 0.96344,
      "gold_threshold": 0.9566,
      "silver_threshold": 0.95658,
      "bronze_threshold": 0.95658,
      "median_threshold": 0.95342,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:34.252744",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-58-02-GMT_run-group_piml_v1__tabular-playground-series-dec-2021_unknown_llm_0steps_24hrs_gpu_40cores/tabular-playground-series-dec-2021_c7d22421-8688-495e-b1fb-33d546c27f2c/submission/submission.csv"
    },
    {
      "competition_id": "the-icml-2013-whale-challenge-right-whale-redux",
      "score": 0.88327,
      "gold_threshold": 0.98961,
      "silver_threshold": 0.95017,
      "bronze_threshold": 0.90521,
      "median_threshold": 0.86521,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:34.377220",
      "submission_path": "submission_12hrs/seed2/2025-12-25T07-58-10-GMT_run-group_piml_v1__the-icml-2013-whale-challenge-right-whale-redux_unknown_llm_0steps_24hrs_gpu_40cores/the-icml-2013-whale-challenge-right-whale-redux_14005bc8-bb0a-48f8-8df0-e43f804a5c41/submission/submission.csv"
    },
    {
      "competition_id": "iwildcam-2019-fgvc6",
      "score": null,
      "gold_threshold": 0.212,
      "silver_threshold": 0.129,
      "bronze_threshold": 0.114,
      "median_threshold": 0.108,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": false,
      "submission_exists": true,
      "valid_submission": false,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:34.490621",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-11-35-GMT_run-group_piml_v1__iwildcam-2019-fgvc6_unknown_llm_0steps_24hrs_gpu_40cores/iwildcam-2019-fgvc6_5af50307-5416-4214-af44-73edd3583243/submission/submission.csv"
    },
    {
      "competition_id": "nfl-player-contact-detection",
      "score": 0.63002,
      "gold_threshold": 0.78167,
      "silver_threshold": 0.72203,
      "bronze_threshold": 0.69652,
      "median_threshold": 0.69139,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": false,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:35.883675",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-11-40-GMT_run-group_piml_v1__nfl-player-contact-detection_unknown_llm_0steps_24hrs_gpu_40cores/nfl-player-contact-detection_ed469e5a-9139-4434-b82b-a601daa83076/submission/submission.csv"
    },
    {
      "competition_id": "predict-volcanic-eruptions-ingv-oe",
      "score": 1580459.0,
      "gold_threshold": 3971366.0,
      "silver_threshold": 4808057.0,
      "bronze_threshold": 4999330.0,
      "median_threshold": 6300677.0,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:35.962946",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-11-42-GMT_run-group_piml_v1__predict-volcanic-eruptions-ingv-oe_unknown_llm_0steps_24hrs_gpu_40cores/predict-volcanic-eruptions-ingv-oe_046aeac2-a9df-4258-afbc-35f0041db377/submission/submission.csv"
    },
    {
      "competition_id": "stanford-covid-vaccine",
      "score": 0.21573,
      "gold_threshold": 0.34728,
      "silver_threshold": 0.35175,
      "bronze_threshold": 0.3534,
      "median_threshold": 0.363095,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:36.183693",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-11-48-GMT_run-group_piml_v1__stanford-covid-vaccine_unknown_llm_0steps_24hrs_gpu_40cores/stanford-covid-vaccine_68d31912-a1a2-4b93-8b4c-6b3744969c65/submission/submission.csv"
    },
    {
      "competition_id": "vinbigdata-chest-xray-abnormalities-detection",
      "score": 0.4473,
      "gold_threshold": 0.289,
      "silver_threshold": 0.257,
      "bronze_threshold": 0.243,
      "median_threshold": 0.223,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:38.199645",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-12-46-GMT_run-group_piml_v1__vinbigdata-chest-xray-abnormalities-detection_unknown_llm_0steps_24hrs_gpu_40cores/vinbigdata-chest-xray-abnormalities-detection_ded3b27e-b2ba-4c0a-9191-afcedfb74235/submission/submission.csv"
    },
    {
      "competition_id": "billion-word-imputation",
      "score": 7.06417,
      "gold_threshold": 5.37017,
      "silver_threshold": 5.54372,
      "bronze_threshold": 5.55209,
      "median_threshold": 5.55211,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": false,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:39.754523",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-43-GMT_run-group_piml_v1__billion-word-imputation_unknown_llm_0steps_24hrs_gpu_40cores/billion-word-imputation_c5fe8cd7-9851-437e-9cda-f2c555ed1757/submission/submission.csv"
    },
    {
      "competition_id": "cassava-leaf-disease-classification",
      "score": 0.89761,
      "gold_threshold": 0.9013,
      "silver_threshold": 0.8992,
      "bronze_threshold": 0.8978,
      "median_threshold": 0.8909,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:39.847373",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-46-GMT_run-group_piml_v1__cassava-leaf-disease-classification_unknown_llm_0steps_24hrs_gpu_40cores/cassava-leaf-disease-classification_69e549f9-664e-4133-ad88-fd2c708ad713/submission/submission.csv"
    },
    {
      "competition_id": "cdiscount-image-classification-challenge",
      "score": 0.71022,
      "gold_threshold": 0.77604,
      "silver_threshold": 0.73526,
      "bronze_threshold": 0.70898,
      "median_threshold": 0.44055,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:40.205637",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-50-GMT_run-group_piml_v1__cdiscount-image-classification-challenge_unknown_llm_0steps_24hrs_gpu_40cores/cdiscount-image-classification-challenge_93c65004-1088-4fd1-b36d-0a082e85d625/submission/submission.csv"
    },
    {
      "competition_id": "chaii-hindi-and-tamil-question-answering",
      "score": 0.74539,
      "gold_threshold": 0.76305,
      "silver_threshold": 0.73984,
      "bronze_threshold": 0.73725,
      "median_threshold": 0.72756,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:40.270039",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-52-GMT_run-group_piml_v1__chaii-hindi-and-tamil-question-answering_unknown_llm_0steps_24hrs_gpu_40cores/chaii-hindi-and-tamil-question-answering_cca873d3-9838-45be-9b03-347768c7b8f4/submission/submission.csv"
    },
    {
      "competition_id": "freesound-audio-tagging-2019",
      "score": 0.74034,
      "gold_threshold": 0.74399,
      "silver_threshold": 0.7181,
      "bronze_threshold": 0.69233,
      "median_threshold": 0.54819,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:41.021833",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-56-GMT_run-group_piml_v1__freesound-audio-tagging-2019_unknown_llm_0steps_24hrs_gpu_40cores/freesound-audio-tagging-2019_a25dcad6-9a1e-4851-9cb6-8b95e47cd6cb/submission/submission.csv"
    },
    {
      "competition_id": "google-quest-challenge",
      "score": 0.39947,
      "gold_threshold": 0.42278,
      "silver_threshold": 0.39597,
      "bronze_threshold": 0.37496,
      "median_threshold": 0.357205,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:41.113003",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-16-58-GMT_run-group_piml_v1__google-quest-challenge_unknown_llm_0steps_24hrs_gpu_40cores/google-quest-challenge_5720844c-3791-4520-a331-3c39577237e0/submission/submission.csv"
    },
    {
      "competition_id": "herbarium-2020-fgvc7",
      "score": 0.53419,
      "gold_threshold": 0.63151,
      "silver_threshold": 0.2805,
      "bronze_threshold": 0.05334,
      "median_threshold": 0.05334,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:41.520717",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-17-04-GMT_run-group_piml_v1__herbarium-2020-fgvc7_unknown_llm_0steps_24hrs_gpu_40cores/herbarium-2020-fgvc7_34212684-dc02-4fda-aa2a-13898aaea117/submission/submission.csv"
    },
    {
      "competition_id": "herbarium-2021-fgvc8",
      "score": 0.3678,
      "gold_threshold": 0.54332,
      "silver_threshold": 0.41067,
      "bronze_threshold": 0.13026,
      "median_threshold": 0.05155,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:42.297640",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-17-08-GMT_run-group_piml_v1__herbarium-2021-fgvc8_unknown_llm_0steps_24hrs_gpu_40cores/herbarium-2021-fgvc8_fd5b2db2-bd5d-4aef-9480-88e66fb25dce/submission/submission.csv"
    },
    {
      "competition_id": "herbarium-2022-fgvc9",
      "score": 0.60796,
      "gold_threshold": 0.84602,
      "silver_threshold": 0.75373,
      "bronze_threshold": 0.5965,
      "median_threshold": 0.22549,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:42.630882",
      "submission_path": "submission_12hrs/seed2/2025-12-25T08-17-10-GMT_run-group_piml_v1__herbarium-2022-fgvc9_unknown_llm_0steps_24hrs_gpu_40cores/herbarium-2022-fgvc9_d8f23631-e40a-470a-a883-412c7c35661e/submission/submission.csv"
    },
    {
      "competition_id": "hotel-id-2021-fgvc8",
      "score": 0.61269,
      "gold_threshold": 0.7205,
      "silver_threshold": 0.39,
      "bronze_threshold": 0.0216,
      "median_threshold": 0.0006,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:42.746841",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-19-GMT_run-group_piml_v1__hotel-id-2021-fgvc8_unknown_llm_0steps_24hrs_gpu_40cores/hotel-id-2021-fgvc8_dd2598f4-c3b6-430a-b7c8-f3b9e544b986/submission/submission.csv"
    },
    {
      "competition_id": "inaturalist-2019-fgvc6",
      "score": 0.99814,
      "gold_threshold": 0.15955,
      "silver_threshold": 0.26058,
      "bronze_threshold": 0.39028,
      "median_threshold": 0.43478,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": false,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:42.851378",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-25-GMT_run-group_piml_v1__inaturalist-2019-fgvc6_unknown_llm_0steps_24hrs_gpu_40cores/inaturalist-2019-fgvc6_81176533-6a76-446d-9f63-fa021364bf24/submission/submission.csv"
    },
    {
      "competition_id": "iwildcam-2020-fgvc7",
      "score": 0.37038,
      "gold_threshold": 0.778,
      "silver_threshold": 0.688,
      "bronze_threshold": 0.594,
      "median_threshold": 0.524,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": false,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:43.061082",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-27-GMT_run-group_piml_v1__iwildcam-2020-fgvc7_unknown_llm_0steps_24hrs_gpu_40cores/iwildcam-2020-fgvc7_a653a8e8-a700-446e-b8fe-829b3146c7aa/submission/submission.csv"
    },
    {
      "competition_id": "kuzushiji-recognition",
      "score": 0.77778,
      "gold_threshold": 0.903,
      "silver_threshold": 0.8,
      "bronze_threshold": 0.658,
      "median_threshold": 0.5985,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:43.430715",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-31-GMT_run-group_piml_v1__kuzushiji-recognition_unknown_llm_0steps_24hrs_gpu_40cores/kuzushiji-recognition_c4882524-bc17-4073-afac-c2a55a3c9290/submission/submission.csv"
    },
    {
      "competition_id": "learning-agency-lab-automated-essay-scoring-2",
      "score": 0.8475,
      "gold_threshold": 0.83583,
      "silver_threshold": 0.83518,
      "bronze_threshold": 0.83471,
      "median_threshold": 0.828265,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:43.560179",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-35-GMT_run-group_piml_v1__learning-agency-lab-automated-essay-scoring-2_unknown_llm_0steps_24hrs_gpu_40cores/learning-agency-lab-automated-essay-scoring-2_7514d43c-2668-4108-861d-ed7195436d14/submission/submission.csv"
    },
    {
      "competition_id": "lmsys-chatbot-arena",
      "score": 0.91666,
      "gold_threshold": 0.98392,
      "silver_threshold": 0.99277,
      "bronze_threshold": 1.00283,
      "median_threshold": 1.05212,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:43.822404",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-38-GMT_run-group_piml_v1__lmsys-chatbot-arena_unknown_llm_0steps_24hrs_gpu_40cores/lmsys-chatbot-arena_02698f7f-fa02-4049-80be-aeb1ea7d523b/submission/submission.csv"
    },
    {
      "competition_id": "petfinder-pawpularity-score",
      "score": 17.22173,
      "gold_threshold": 16.95041,
      "silver_threshold": 17.06636,
      "bronze_threshold": 17.0971,
      "median_threshold": 17.702984999999998,
      "any_medal": false,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:43.898090",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-44-GMT_run-group_piml_v1__petfinder-pawpularity-score_unknown_llm_0steps_24hrs_gpu_40cores/petfinder-pawpularity-score_32e21716-19b9-4915-99cb-6831bc0f6768/submission/submission.csv"
    },
    {
      "competition_id": "plant-pathology-2021-fgvc8",
      "score": 0.93324,
      "gold_threshold": 0.8612,
      "silver_threshold": 0.85305,
      "bronze_threshold": 0.83508,
      "median_threshold": 0.754445,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:43.978024",
      "submission_path": "submission_12hrs/seed2/2025-12-26T11-29-46-GMT_run-group_piml_v1__plant-pathology-2021-fgvc8_unknown_llm_0steps_24hrs_gpu_40cores/plant-pathology-2021-fgvc8_c7886cda-e93b-485d-ac21-ccfe3987879b/submission/submission.csv"
    },
    {
      "competition_id": "tweet-sentiment-extraction",
      "score": 0.71825,
      "gold_threshold": 0.72689,
      "silver_threshold": 0.71752,
      "bronze_threshold": 0.71705,
      "median_threshold": 0.71378,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:44.060607",
      "submission_path": "submission_12hrs/seed2/2025-12-26T12-22-31-GMT_run-group_piml_v1__tweet-sentiment-extraction_unknown_llm_0steps_24hrs_gpu_40cores/tweet-sentiment-extraction_1c4f1990-50e8-4b25-9c6a-f9983863c761/submission/submission.csv"
    },
    {
      "competition_id": "text-normalization-challenge-russian-language",
      "score": 0.98455,
      "gold_threshold": 0.99012,
      "silver_threshold": 0.98232,
      "bronze_threshold": 0.97592,
      "median_threshold": 0.97591,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:48.051775",
      "submission_path": "submission_12hrs/seed2/2025-12-29T07-21-05-GMT_run-group_piml_v1_Low_text-normalization-challenge-russian-language_unknown_llm_0steps_24hrs_gpu_40cores/text-normalization-challenge-russian-language_01efff33-638c-4e98-aff0-ea82127cde65/submission/submission.csv"
    },
    {
      "competition_id": "text-normalization-challenge-english-language",
      "score": 0.99186,
      "gold_threshold": 0.99724,
      "silver_threshold": 0.99135,
      "bronze_threshold": 0.99038,
      "median_threshold": 0.99037,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:51.293886",
      "submission_path": "submission_12hrs/seed2/2025-12-29T10-49-58-GMT_run-group_piml_v1_Low_text-normalization-challenge-english-language_unknown_llm_0steps_24hrs_gpu_40cores/text-normalization-challenge-english-language_1e88c33f-42dd-46f9-b5a1-80dde781fdca/submission/submission.csv"
    },
    {
      "competition_id": "us-patent-phrase-to-phrase-matching",
      "score": 0.87088,
      "gold_threshold": 0.87,
      "silver_threshold": 0.863,
      "bronze_threshold": 0.8616,
      "median_threshold": 0.851,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:51.412223",
      "submission_path": "submission_12hrs/seed2/2025-12-29T10-55-58-GMT_run-group_piml_v1_Medium_us-patent-phrase-to-phrase-matching_unknown_llm_0steps_24hrs_gpu_40cores/us-patent-phrase-to-phrase-matching_97967257-7f67-4679-af50-112f7e9cbfc8/submission/submission.csv"
    },
    {
      "competition_id": "rsna-miccai-brain-tumor-radiogenomic-classification",
      "score": 0.59765,
      "gold_threshold": 0.60096,
      "silver_threshold": 0.5815,
      "bronze_threshold": 0.57449,
      "median_threshold": 0.52553,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:51.474392",
      "submission_path": "submission_12hrs/seed2/2025-12-29T11-00-02-GMT_run-group_piml_v1_High_rsna-miccai-brain-tumor-radiogenomic-classification_unknown_llm_0steps_24hrs_gpu_40cores/rsna-miccai-brain-tumor-radiogenomic-classification_0d6ad346-8e96-4a9d-8275-268e2fdfe98f/submission/submission.csv"
    },
    {
      "competition_id": "leaf-classification",
      "score": 0.00382,
      "gold_threshold": 0.0,
      "silver_threshold": 0.00791,
      "bronze_threshold": 0.01526,
      "median_threshold": 0.108345,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": true,
      "created_at": "2026-01-06T18:11:51.551801",
      "submission_path": "submission_12hrs/seed2/2025-12-29T13-17-32-GMT_run-group_piml_v1_Low_leaf-classification_unknown_llm_0steps_24hrs_gpu_40cores/leaf-classification_8b02cfc1-1f6d-4608-b34b-43729658a4bd/submission/submission.csv"
    },
    {
      "competition_id": "seti-breakthrough-listen",
      "score": 0.79119,
      "gold_threshold": 0.79806,
      "silver_threshold": 0.78095,
      "bronze_threshold": 0.77439,
      "median_threshold": 0.75889,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:51.626228",
      "submission_path": "submission_12hrs/seed2/2025-12-30T07-41-01-GMT_run-group_piml_v1_Medium_seti-breakthrough-listen_unknown_llm_0steps_24hrs_gpu_40cores/seti-breakthrough-listen_0ef5f5a9-2462-405d-bad6-8958d55044c9/submission/submission.csv"
    },
    {
      "competition_id": "whale-categorization-playground",
      "score": 0.51045,
      "gold_threshold": 0.56236,
      "silver_threshold": 0.44852,
      "bronze_threshold": 0.40515,
      "median_threshold": 0.32788,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:51.699035",
      "submission_path": "submission_12hrs/seed2/2025-12-30T07-41-01-GMT_run-group_piml_v1_Medium_whale-categorization-playground_unknown_llm_0steps_24hrs_gpu_40cores/whale-categorization-playground_616e3b46-9be6-4594-9002-d148963fc503/submission/submission.csv"
    },
    {
      "competition_id": "hubmap-kidney-segmentation",
      "score": 0.94585,
      "gold_threshold": 0.9484,
      "silver_threshold": 0.9455,
      "bronze_threshold": 0.9439,
      "median_threshold": 0.0,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:11:59.680653",
      "submission_path": "submission_12hrs/seed2/2025-12-30T16-23-59-GMT_run-group_piml_v1_Medium_hubmap-kidney-segmentation_unknown_llm_0steps_24hrs_gpu_40cores/hubmap-kidney-segmentation_b79c641c-1df8-45d5-a86b-7c575a306253/submission/submission.csv"
    },
    {
      "competition_id": "3d-object-detection-for-autonomous-vehicles",
      "score": 0.28105,
      "gold_threshold": 0.139,
      "silver_threshold": 0.053,
      "bronze_threshold": 0.046,
      "median_threshold": 0.042,
      "any_medal": true,
      "gold_medal": true,
      "silver_medal": false,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:17:52.727694",
      "submission_path": "submission_12hrs/seed2/2025-12-30T16-25-03-GMT_run-group_piml_v1_High_3d-object-detection-for-autonomous-vehicles_unknown_llm_0steps_24hrs_gpu_40cores/3d-object-detection-for-autonomous-vehicles_bc60275f-5972-4416-8c2e-1f47eeda0d39/submission/submission.csv"
    },
    {
      "competition_id": "h-and-m-personalized-fashion-recommendations",
      "score": 0.02759,
      "gold_threshold": 0.03354,
      "silver_threshold": 0.02517,
      "bronze_threshold": 0.02394,
      "median_threshold": 0.02177,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": true,
      "bronze_medal": false,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:17:55.774451",
      "submission_path": "submission_12hrs/seed2/2025-12-30T16-27-58-GMT_run-group_piml_v1_Medium_h-and-m-personalized-fashion-recommendations_unknown_llm_0steps_24hrs_gpu_40cores/h-and-m-personalized-fashion-recommendations_6e037f61-c2d1-4fe8-96d1-ffd6f2f4c84b/submission/submission.csv"
    },
    {
      "competition_id": "siim-isic-melanoma-classification",
      "score": 0.9372,
      "gold_threshold": 0.9455,
      "silver_threshold": 0.9401,
      "bronze_threshold": 0.937,
      "median_threshold": 0.9128,
      "any_medal": true,
      "gold_medal": false,
      "silver_medal": false,
      "bronze_medal": true,
      "above_median": true,
      "submission_exists": true,
      "valid_submission": true,
      "is_lower_better": false,
      "created_at": "2026-01-06T18:17:55.880806",
      "submission_path": "submission_12hrs/seed2/2026-01-02T08-58-11-GMT_run-group_piml_v1_Low_siim-isic-melanoma-classification_unknown_llm_0steps_24hrs_gpu_40cores/siim-isic-melanoma-classification_3d5bc185-a84e-45be-9cfb-70cc65b39e9d/submission/submission.csv"
    }
  ]
}