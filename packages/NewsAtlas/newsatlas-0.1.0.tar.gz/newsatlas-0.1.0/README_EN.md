# NewsAtlas ğŸ“°

**English** | [ç®€ä½“ä¸­æ–‡](README.md)

**Focusing on solving the pain point of news list page parsing.**

While there are many mature tools for parsing news **detail pages** in the market, automated parsing of **list page** data has always been a gap and a challenge in the industry. **NewsAtlas was born to solve this pain point**â€”it can intelligently identify and extract news list data of various forms, filling the last piece of the puzzle in the general crawler field.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

## âœ¨ Features

- ğŸ” **Intelligent List Parsing** - Based on DOM heatmap clustering algorithm, automatically identifying news lists
- ğŸ“„ **High-Quality Content Extraction** - Based on trafilatura, accurately extracting body content
- ğŸ”— **URL Similarity Matching** - Based on structural similarity, expanding to discover more news links
- ğŸŒ **Auto Encoding Detection** - Supports auto-identification of multiple encodings like GBK, UTF-8
- âš¡ **Easy to Use** - 5 core functions covering all usage scenarios

## ğŸ“¦ Installation

Install using uv (Recommended):

```bash
cd newsatlas
uv venv
uv pip install -e .
```

Or using pip:

```bash
pip install -e .
```

## ğŸš€ Quick Start

### Core Functions

```python
from newsatlas import NewsAtlas

atlas = NewsAtlas()

# Feature 1: Fetch and parse news list page
result = atlas.fetch_list("https://news.example.com")
for item in result.items:
    print(item.title, item.url)

# Feature 2: Parse list page HTML directly
items = atlas.parse_list(html_content, base_url="...")

# Feature 3: Fetch and parse detail page
article = atlas.fetch_article("https://news.example.com/article/123")
print(article.title, article.content)

# Feature 4: Parse detail page HTML directly
article = atlas.parse_article(html_content)

# Feature 5: Full collection flow (list + details)
result = atlas.collect("https://news.example.com")
print(f"Collected {len(result.articles)} articles")
```



### Convenience Functions

```python
from newsatlas import fetch_news_list, fetch_article_content, collect_news

# Fetch news list
items = fetch_news_list("https://news.example.com")

# Fetch article content
article = fetch_article_content("https://news.example.com/article/123")

# Complete harvesting
result = collect_news("https://news.example.com", max_articles=20)
```

## ğŸ“– API Documentation

### NewsAtlas

The main entry class providing all core functions.

```python
from newsatlas import NewsAtlas, AtlasConfig

# Custom configuration
config = AtlasConfig(
    timeout=15,                    # Request timeout
    retry_times=2,                 # Retry times
    request_delay=(1.0, 2.0),      # Request delay range
    min_title_length=8,            # Minimum title length
    max_articles_per_list=50,      # Max articles per list page
)

atlas = NewsAtlas(config)
```

### Data Models

#### NewsItem

News list item.

- `title`: Title
- `url`: Link
- `publish_time`: Publish time (formatted string)
- `timestamp`: Timestamp

#### ArticleContent

Detail page content.

- `title`: Title
- `content`: Body content
- `author`: Author
- `publish_time`: Publish time
- `raw_html`: Raw HTML (optional)
- `success`: Success status
- `error`: Error message

#### AtlasResult

Full collection result.

- `list_items`: News list (`List[NewsItem]`)
- `articles`: Detail page content list (`List[ArticleContent]`)
- `success_count`: Success count
- `failed_count`: Failed count

## ğŸ› ï¸ Convenience Functions

```python
from newsatlas import fetch_news_list, fetch_article_content, collect_news

# 1. Fetch list only
result = fetch_news_list("https://news.example.com")

# 2. Fetch article content only
article = fetch_article_content("https://news.example.com/article/123")

# 3. Full collection
result = collect_news("https://news.example.com", max_articles=20)
```

## ğŸ—ï¸ Project Structure

```
newsatlas/
â”œâ”€â”€ pyproject.toml          # Project configuration
â”œâ”€â”€ README.md               # Documentation (Chinese)
â”œâ”€â”€ README_EN.md            # Documentation (English)
â”œâ”€â”€ LICENSE                 # License
â”œâ”€â”€ src/
â”‚   â””â”€â”€ newsatlas/
â”‚       â”œâ”€â”€ __init__.py     # Public API
â”‚       â”œâ”€â”€ core.py         # Main entry class
â”‚       â”œâ”€â”€ models.py       # Data models
â”‚       â”œâ”€â”€ crawler.py      # Web crawler
â”‚       â”œâ”€â”€ detail_parser.py # Detail page parser
â”‚       â””â”€â”€ list_parser/    # List page parser module
â”‚           â”œâ”€â”€ extractor.py
â”‚           â”œâ”€â”€ matchers/   # URL matching algorithms
â”‚           â””â”€â”€ utils/      # Utility functions
â””â”€â”€ examples/
    â””â”€â”€ basic_usage.py      # Usage examples
```

## ğŸ”¬ Technical Principles

### List Page Parsing Principles

NewsAtlas employs a Hybrid Strategy Algorithm, combining rule matching, heuristic evaluation, and machine learning ideas to ensure high recognition rates across different types of web pages. The core logic includes the following 6 layers:

1.  **Multi-dimensional Scoring**
    The system scores each potential container (`<div>`, `<ul>`, etc.) on the page. Scoring dimensions include:
    *   **Link Density**: Body lists usually have a higher ratio of link text.
    *   **Title Features**: Controlled by `min_title_length` (default 8) and `max_title_length`, filtering out non-news links that are too short or too long.
    *   **Path Depth Consistency**: News links in the same list usually have similar URL path depths.
    *   **Time Elements**: List items containing time/dates get higher weights.

2.  **Pre-defined Rules**
    Built-in dozens of common **XPath** and **CSS Selector** patterns (e.g., `class="news-list"`, `id="post-list"`), prioritizing standard naming conventions for list containers.

3.  **DOM Heatmap Clustering**
    This is a density-based structure learning algorithm:
    *   The system first identifies all "seed links" in the page that match news URL characteristics.
    *   These seed links are lit up in the DOM tree, forming "hotspots".
    *   Heat propagates upwards, automatically defining "hotzone containers" based on hotspot clustering density. This allows the system to identify raw HTML lists without any semantic Class names.

4.  **Diffusion Boundary Algorithm**
    After determining the hotzone, the algorithm diffuses outwards from each anchor (looking up for parent nodes) until it meets the boundary of adjacent news items. This solves the complex combination extraction problem (e.g., an `<li>` containing image, title, intro, time), accurately slicing out complete NewsItems.

5.  **Structure Similarity Matching**
    The system analyzes the URL patterns of seed links (e.g., `/2024/01/15/...`), using **Levenshtein Distance** and **Path Feature Vectors** to find all structurally similar links on the page. Even if some links are not in the main list (e.g., in a scroll-loading area), they can be recalled.

6.  **Intelligent Time Parsing**
    Relies not on a single rule but a mix of:
    *   HTML `time` tags and `datetime` attributes.
    *   Regex matching optimized for contexts (supports "30 mins ago", "2024-01-01", etc.).
    *   Automatically extracting date information from URLs as a supplement.

### Detail Page Parsing

Based on the [trafilatura](https://github.com/adbar/trafilatura) library, using multiple strategies to extract body content:
- HTML semantic tag recognition
- Text density analysis
- Boilerplate text filtering
- Metadata extraction
- Metadata extraction

## ğŸ“¢ Test Sources & Feedback

The development and testing of this project are mainly based on Chinese news websites listed in [seed.txt](seed.txt).

Due to the ever-changing structure of web pages, parsing failures (especially for list pages) are inevitable in actual use.
**If you find a website that cannot be parsed correctly, please submit an Issue with the URL, and I will strictly optimize the iteration engine.**

## ğŸ™Œ Acknowledgements

This project references and draws inspiration from the design ideas of the following excellent open-source projects, paying tribute to:

*   [readability](https://github.com/mozilla/readability) - Mozilla's library for extracting article body, the forefather of content extraction.
*   [trafilatura](https://github.com/adbar/trafilatura) - Currently the most advanced/powerful content extraction library in Python, NewsAtlas relies on it for detail page parsing.
*   [GeneralNewsExtractor](https://github.com/GeneralNewsExtractor/GeneralNewsExtractor) - An excellent general news extractor.
*   [GerapyAutoExtractor](https://github.com/Gerapy/GerapyAutoExtractor) - An excellent project for automatic list page extraction.
*   [newspaper4k](https://github.com/AndyTheFactory/newspaper4k) - A modern branch of the classic Python article extraction library.

## ğŸ“„ License

Apache License 2.0 - See [LICENSE](LICENSE) file for details.

## ğŸ¤ Contribution

Issues and Pull Requests are welcome!

## ğŸ“® Contact

If you have any questions or suggestions, please submit an Issue.
