{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModSSC | Data loader\n",
    "\n",
    "Load a dataset end to end with the ModSSC data loader.\n",
    "\n",
    "## Objective\n",
    "- Show the minimal steps to run this component in a notebook setting.\n",
    "- Provide the exact objects to look at (outputs, shapes, metrics) to confirm it worked.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.11+.\n",
    "- `pip install modssc`.\n",
    "- Optional dependencies depend on datasets and backends. If an import fails, install the matching extra and rerun.\n",
    "\n",
    "## Outline\n",
    "1) Imports and configuration\n",
    "2) Core run (the part that does the work)\n",
    "3) Sanity checks and outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook notes\n",
    "\n",
    "This notebook demonstrates how to use ModSSC's data loading capabilities to access, download, and manage datasets.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To use the data loader, you need to install the package.\n",
    "\n",
    "Base install (lightweight, no heavy dependencies):\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "For full data loading capabilities (including pandas, etc.), install with the `data` extra:\n",
    "```bash\n",
    "pip install -e \".[data]\"\n",
    "```\n",
    "\n",
    "## List available datasets\n",
    "\n",
    "ModSSC comes with a curated catalog of datasets. We can list them and inspect their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modssc.data_loader import available_datasets, dataset_info\n",
    "\n",
    "# List all dataset keys in the catalog\n",
    "keys = available_datasets()\n",
    "print(\"Catalog:\", keys)\n",
    "\n",
    "# Inspect metadata for the 'toy' dataset\n",
    "# This returns a DatasetSpec object containing modality, size, etc.\n",
    "print(\"Toy spec:\", dataset_info(\"toy\").as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Download and load (toy)\n",
    "\n",
    "We can download and load a dataset using the `load_dataset` function. Here, we load a small toy dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modssc.data_loader import download_dataset, load_dataset\n",
    "\n",
    "# Download the 'toy' dataset.\n",
    "# force=True ensures we re-download/re-generate it even if it exists.\n",
    "ds = download_dataset(\"toy\", force=True)\n",
    "\n",
    "# The loaded dataset object contains splits (train, val, test) and metadata.\n",
    "print(\"train X:\", ds.train.X.shape)\n",
    "print(\"train y:\", ds.train.y.shape)\n",
    "print(\"test present:\", ds.test is not None)\n",
    "\n",
    "# We can also load without downloading if we know it's cached.\n",
    "ds_offline = load_dataset(\"toy\", download=False)\n",
    "print(\"offline ok:\", ds_offline.train.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Bulk download (best effort)\n",
    "\n",
    "We can bulk download all datasets using the `bulk_download_datasets` function. This function attempts to download all available datasets, skipping any that fail to download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modssc.data_loader import download_all_datasets\n",
    "\n",
    "# Attempt to download all datasets in the catalog.\n",
    "# ignore_missing_extras=True: Don't fail if we lack dependencies for some datasets (e.g. audio libs).\n",
    "# skip_cached=True: Don't re-download if already present.\n",
    "report = download_all_datasets(ignore_missing_extras=True, skip_cached=True)\n",
    "\n",
    "print(report.summary())\n",
    "print(\"missing_extras:\", report.missing_extras)\n",
    "print(\"failed:\", report.failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## Cache inspection (CLI)\n",
    "\n",
    "We can inspect the cache using the ModSSC CLI. The following command lists all cached datasets and their details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def run_cli(*args):\n",
    "    cmd = [sys.executable, \"-m\", \"modssc\", *args]\n",
    "    res = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    return res.returncode, res.stdout.strip(), res.stderr.strip()\n",
    "\n",
    "\n",
    "print(run_cli(\"datasets\", \"cache\", \"ls\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## Provider URIs (requires extras)\n",
    "\n",
    "Some datasets can be accessed via different providers. We can list the available URIs for a dataset using the `get_dataset_uris` function.\n",
    "\n",
    "We can list the available providers for a dataset and choose one to load the dataset from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "from modssc.data_loader import download_dataset, load_dataset\n",
    "from modssc.data_loader.errors import OptionalDependencyError\n",
    "\n",
    "# Increase default socket timeout to 10 minutes for large/slow downloads (e.g. OpenML)\n",
    "socket.setdefaulttimeout(600)\n",
    "\n",
    "# ModSSC supports generic providers to load datasets NOT in the curated catalog.\n",
    "# The URI format is \"provider:dataset_name\".\n",
    "#\n",
    "# Supported providers:\n",
    "# - openml:ID -> fetches by OpenML ID via sklearn\n",
    "# - hf:name/config -> fetches via HuggingFace datasets\n",
    "# - torchvision:ClassName -> instantiates torchvision.datasets.ClassName\n",
    "# - torchaudio:ClassName -> instantiates torchaudio.datasets.ClassName\n",
    "# - pyg:ClassName -> instantiates torch_geometric.datasets.ClassName\n",
    "\n",
    "uris = [\n",
    "    \"openml:31\",  # Credit-g (Tabular)\n",
    "    \"hf:rotten_tomatoes\",  # Rotten Tomatoes (Text)\n",
    "    \"torchvision:FashionMNIST\",  # FashionMNIST (Vision)\n",
    "    \"pyg:KarateClub\",  # KarateClub (Graph)\n",
    "    \"torchaudio:CMUARCTIC\",  # CMU ARCTIC (Audio)\n",
    "]\n",
    "\n",
    "for u in uris:\n",
    "    try:\n",
    "        print(f\"--- Processing {u} ---\")\n",
    "        # download_dataset will cache the processed data locally\n",
    "        download_dataset(u)\n",
    "\n",
    "        # load_dataset returns the standard LoadedDataset object\n",
    "        ds = load_dataset(u)\n",
    "        print(f\"[OK] Loaded {u}\")\n",
    "\n",
    "        # Inspect the data shape/length\n",
    "        # Note: Some providers return lists (Audio/Text) or Tensors/Arrays (Vision/Tabular)\n",
    "        train_len = ds.train.X.shape if hasattr(ds.train.X, \"shape\") else len(ds.train.X)\n",
    "        print(f\"  Train X shape/len: {train_len}\")\n",
    "\n",
    "        if ds.train.y is not None:\n",
    "            y_len = ds.train.y.shape if hasattr(ds.train.y, \"shape\") else len(ds.train.y)\n",
    "            print(f\"  Train y shape/len: {y_len}\")\n",
    "\n",
    "        print(f\"  Meta: {ds.meta}\")\n",
    "\n",
    "    except OptionalDependencyError as e:\n",
    "        print(f\"[SKIP] {u} missing extra: {e.extra}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {u} {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d85e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- The last cells should print key shapes and a minimal metric or artifact summary.\n",
    "- If something fails early, the error should point to a missing optional dependency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Explore the adjacent notebooks in this folder for the other pipeline components.\n",
    "- If you hit an optional dependency error, install the suggested extra and rerun.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
