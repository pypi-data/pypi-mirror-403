{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModSSC | Preprocessing\n",
    "\n",
    "Run preprocessing plans and inspect available steps and models.\n",
    "\n",
    "## Objective\n",
    "- Show the minimal steps to run this component in a notebook setting.\n",
    "- Provide the exact objects to look at (outputs, shapes, metrics) to confirm it worked.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.11+.\n",
    "- `pip install modssc`.\n",
    "- Optional dependencies depend on datasets and backends. If an import fails, install the matching extra and rerun.\n",
    "\n",
    "## Outline\n",
    "1) Imports and configuration\n",
    "2) Core run (the part that does the work)\n",
    "3) Sanity checks and outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook notes\n",
    "\n",
    "This notebook shows how to build a deterministic, cacheable preprocessing pipeline and (optionally) compute embeddings with pretrained models.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Base install (no heavy extras):\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "With embedding extras (optional):\n",
    "```bash\n",
    "pip install -e \".[preprocess]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader import load_dataset\n",
    "from modssc.preprocess import PreprocessPlan, StepConfig, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Load a dataset\n",
    "\n",
    "We use the built-in `toy` dataset (deterministic, small, offline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 4), (48,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"toy\")\n",
    "ds.train.X.shape, ds.train.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7aa00",
   "metadata": {},
   "source": [
    "## List available preprocessing steps\n",
    "\n",
    "Let's list the available preprocessing steps in ModSSC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd86bccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio.wav2vec2\n",
      "core.cast_dtype\n",
      "core.ensure_2d\n",
      "core.pca\n",
      "core.random_projection\n",
      "embeddings.auto\n",
      "graph.attach_edge_weight\n",
      "graph.edge_sparsify\n",
      "labels.ensure_onehot\n",
      "text.ensure_strings\n",
      "text.hash_tokenizer\n",
      "text.sentence_transformer\n",
      "text.tfidf\n",
      "vision.channels_order\n",
      "vision.ensure_num_channels\n",
      "vision.normalize\n",
      "vision.openclip\n",
      "vision.resize\n",
      "vision.zca_whitening\n"
     ]
    }
   ],
   "source": [
    "from modssc.preprocess.registry import available_steps\n",
    "\n",
    "print(\"\\n\".join(available_steps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d87ffe",
   "metadata": {},
   "source": [
    "Some code to list available preprocessing steps with their metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1425f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio.wav2vec2\n",
      "  kind: featurizer\n",
      "  modalities: ['audio']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: preprocess-audio\n",
      "core.cast_dtype\n",
      "  kind: transform\n",
      "  modalities: []\n",
      "  consumes: ['features.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: None\n",
      "core.ensure_2d\n",
      "  kind: transform\n",
      "  modalities: []\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: None\n",
      "core.pca\n",
      "  kind: fittable\n",
      "  modalities: []\n",
      "  consumes: ['features.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: None\n",
      "core.random_projection\n",
      "  kind: fittable\n",
      "  modalities: []\n",
      "  consumes: ['features.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: None\n",
      "embeddings.auto\n",
      "  kind: featurizer\n",
      "  modalities: []\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: None\n",
      "graph.attach_edge_weight\n",
      "  kind: transform\n",
      "  modalities: ['graph']\n",
      "  consumes: ['graph.edge_index']\n",
      "  produces: ['graph.edge_weight']\n",
      "  required_extra: None\n",
      "graph.edge_sparsify\n",
      "  kind: transform\n",
      "  modalities: ['graph']\n",
      "  consumes: ['graph.edge_index', 'graph.edge_weight']\n",
      "  produces: ['graph.edge_index', 'graph.edge_weight']\n",
      "  required_extra: None\n",
      "labels.ensure_onehot\n",
      "  kind: transform\n",
      "  modalities: []\n",
      "  consumes: ['raw.y']\n",
      "  produces: ['labels.y_onehot', 'labels.is_labeled']\n",
      "  required_extra: None\n",
      "text.ensure_strings\n",
      "  kind: transform\n",
      "  modalities: ['text']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n",
      "text.hash_tokenizer\n",
      "  kind: transform\n",
      "  modalities: ['text']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['tokens.input_ids', 'tokens.attention_mask']\n",
      "  required_extra: None\n",
      "text.sentence_transformer\n",
      "  kind: featurizer\n",
      "  modalities: ['text']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: preprocess-text\n",
      "text.tfidf\n",
      "  kind: fittable\n",
      "  modalities: ['text']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: preprocess-sklearn\n",
      "vision.channels_order\n",
      "  kind: transform\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n",
      "vision.ensure_num_channels\n",
      "  kind: transform\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n",
      "vision.normalize\n",
      "  kind: transform\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n",
      "vision.openclip\n",
      "  kind: featurizer\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['features.X']\n",
      "  required_extra: preprocess-vision\n",
      "vision.resize\n",
      "  kind: transform\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n",
      "vision.zca_whitening\n",
      "  kind: fittable\n",
      "  modalities: ['vision']\n",
      "  consumes: ['raw.X']\n",
      "  produces: ['raw.X']\n",
      "  required_extra: None\n"
     ]
    }
   ],
   "source": [
    "from modssc.preprocess.registry import available_steps, step_info\n",
    "\n",
    "for sid in available_steps():\n",
    "    info = step_info(sid)\n",
    "    print(sid)\n",
    "    print(\"  kind:\", info[\"kind\"])\n",
    "    print(\"  modalities:\", info[\"modalities\"])\n",
    "    print(\"  consumes:\", info[\"consumes\"])\n",
    "    print(\"  produces:\", info[\"produces\"])\n",
    "    print(\"  required_extra:\", info[\"required_extra\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824a543235b476cb70bfccb18831a90",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "You can access the same registries via the CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1522daeb616454f935ee04212aeaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def run_cli(*args):\n",
    "    cmd = [sys.executable, \"-m\", \"modssc\", *args]\n",
    "    res = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    return res.returncode, res.stdout.strip(), res.stderr.strip()\n",
    "\n",
    "\n",
    "print(run_cli(\"preprocess\", \"steps\", \"list\"))\n",
    "print(run_cli(\"preprocess\", \"models\", \"list\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## Build a plan\n",
    "\n",
    "For tabular data, we typically:\n",
    "- create a 2D numeric matrix (`features.X`)\n",
    "- cast dtype\n",
    "- optionally reduce dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessPlan(steps=(StepConfig(step_id='core.ensure_2d', params={}, modalities=(), requires_fields=(), enabled=True), StepConfig(step_id='core.cast_dtype', params={'dtype': 'float32'}, modalities=(), requires_fields=(), enabled=True), StepConfig(step_id='core.pca', params={'n_components': 3}, modalities=(), requires_fields=(), enabled=True)), output_key='features.X')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Preprocessing Plan\n",
    "# A plan consists of a sequence of steps.\n",
    "# Each step is defined by its ID (e.g., \"core.pca\") and optional parameters.\n",
    "# output_key defines which artifact is considered the \"main\" output (usually \"features.X\").\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"core.ensure_2d\"),\n",
    "        StepConfig(\"core.cast_dtype\", params={\"dtype\": \"float32\"}),\n",
    "        StepConfig(\"core.pca\", params={\"n_components\": 3}),\n",
    "    ),\n",
    "    output_key=\"features.X\",\n",
    ")\n",
    "plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## Run preprocessing\n",
    "\n",
    "Fittable steps need `fit_indices` (relative to the training split). Here we fit on the full train split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 3), (48,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the preprocessing pipeline\n",
    "# - fit_indices: Specifies which samples are used to FIT the steps (e.g., PCA).\n",
    "#   Usually, this is the training set indices to avoid data leakage.\n",
    "# - cache=True: Enables caching of intermediate and final results.\n",
    "\n",
    "fit_idx = np.arange(ds.train.y.shape[0], dtype=np.int64)\n",
    "res = preprocess(ds, plan, seed=0, fit_indices=fit_idx, cache=True)\n",
    "\n",
    "out = res.dataset\n",
    "print(\"Processed X shape:\", out.train.X.shape)\n",
    "print(\"Processed y shape:\", out.train.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## Cache\n",
    "\n",
    "When cache is enabled, each step stores its outputs under the dataset fingerprint directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/melvin/Library/Caches/modssc/preprocess/dataset:c132a331e6104ef264c05eb27e89e26bd5380f1bc447110f50f75fc1276a5408'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c669133",
   "metadata": {},
   "source": [
    "## Tabular Preprocessing\n",
    "\n",
    "Note: Requires `pip install -e \".[tabular]\"` and internet access to download datasets.\n",
    "\n",
    "For tabular data, we typically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03cb266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 7)\n",
      "onehot (64, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(size=(64, 10)).astype(np.float32)\n",
    "y = rng.integers(0, 3, size=(64,), dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"tabular\", \"dataset_fingerprint\": \"synthetic:tab64\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"core.ensure_2d\"),\n",
    "        StepConfig(\"core.cast_dtype\"),\n",
    "        StepConfig(\"core.pca\", params={\"n_components\": 5}),\n",
    "        StepConfig(\"core.random_projection\", params={\"n_components\": 7}),\n",
    "        StepConfig(\"labels.ensure_onehot\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 32, dtype=np.int64), cache=False)\n",
    "print(\"X\", res.dataset.train.X.shape)\n",
    "print(\"onehot\", res.train_artifacts.require(\"labels.y_onehot\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34704569",
   "metadata": {},
   "source": [
    "## Vision Preprocessing\n",
    "\n",
    "Note: Requires `pip install -e \".[vision]\"` and internet access to download datasets.\n",
    "\n",
    "For vision data, we typically:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4310ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw.X (32, 3, 8, 8)\n",
      "X (32, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.random(size=(32, 16, 16, 1)).astype(np.float32)\n",
    "y = rng.integers(0, 2, size=(32,), dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"vision\", \"dataset_fingerprint\": \"synthetic:vision32\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"vision.channels_order\", params={\"order\": \"NCHW\"}),\n",
    "        StepConfig(\"vision.ensure_num_channels\", params={\"num_channels\": 3}),\n",
    "        StepConfig(\"vision.resize\", params={\"height\": 8, \"width\": 8}),\n",
    "        StepConfig(\"vision.normalize\"),\n",
    "        StepConfig(\"vision.zca_whitening\", params={\"max_features\": 4096}),\n",
    "        StepConfig(\"embeddings.auto\"),\n",
    "        StepConfig(\"core.random_projection\", params={\"n_components\": 6}),\n",
    "    )\n",
    ")\n",
    "\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 16, dtype=np.int64), cache=False)\n",
    "print(\"raw.X\", np.asarray(res.train_artifacts.require(\"raw.X\")).shape)\n",
    "print(\"X\", res.dataset.train.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d48831",
   "metadata": {},
   "source": [
    "## Graph Preprocessing\n",
    "Note: Requires `pip install -e \".[graph]\"` and internet access to download datasets.\n",
    "\n",
    "For graph data, we typically:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "856a1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index (2, 64)\n",
      "edge_weight (64,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "N, F = 50, 8\n",
    "X = rng.normal(size=(N, F)).astype(np.float32)\n",
    "y = rng.integers(-1, 3, size=(N,), dtype=np.int64)\n",
    "E = 200\n",
    "edge_index = np.stack(\n",
    "    [rng.integers(0, N, size=(E,), dtype=np.int64), rng.integers(0, N, size=(E,), dtype=np.int64)],\n",
    "    axis=0,\n",
    ")\n",
    "masks = {\n",
    "    \"train\": (np.arange(N) < 25),\n",
    "    \"val\": ((np.arange(N) >= 25) & (np.arange(N) < 35)),\n",
    "    \"test\": (np.arange(N) >= 35),\n",
    "}\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y, edges=edge_index, masks=masks),\n",
    "    meta={\"modality\": \"graph\", \"dataset_fingerprint\": \"synthetic:graph50\"},\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"graph.attach_edge_weight\"),\n",
    "        StepConfig(\"graph.edge_sparsify\", params={\"keep_fraction\": 0.3}),\n",
    "    )\n",
    ")\n",
    "\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 25, dtype=np.int64), cache=False)\n",
    "edges = res.dataset.train.edges\n",
    "print(\n",
    "    \"edge_index\", edges[\"edge_index\"].shape if isinstance(edges, dict) else np.asarray(edges).shape\n",
    ")\n",
    "print(\"edge_weight\", edges[\"edge_weight\"].shape if isinstance(edges, dict) else \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c08b2",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "Note: Requires `pip install -e \".[audio]\"` and internet access to download datasets.\n",
    "\n",
    "For audio data, we typically:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1474c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (40, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X = [rng.standard_normal(800, dtype=np.float32) for _ in range(40)]\n",
    "y = rng.integers(0, 4, size=(40,), dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"audio\", \"dataset_fingerprint\": \"synthetic:audio40\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"embeddings.auto\"),\n",
    "        StepConfig(\"core.random_projection\", params={\"n_components\": 5}),\n",
    "    )\n",
    ")\n",
    "\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 20, dtype=np.int64), cache=False)\n",
    "print(\"X\", res.dataset.train.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2f5cc",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Note: Requires `pip install -e \".[text]\"` and internet access to download datasets.\n",
    "\n",
    "For text data, we typically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d27adc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 8)\n",
      "tokens (64, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "X = np.array([\"a b c\", \"hello world\", \"x y\", \"\"] * 16, dtype=object)\n",
    "y = np.array([0, 1, 0, 1] * 16, dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"text\", \"dataset_fingerprint\": \"synthetic:text64\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"text.ensure_strings\"),\n",
    "        StepConfig(\"text.hash_tokenizer\", params={\"max_length\": 16, \"vocab_size\": 2000}),\n",
    "        StepConfig(\"embeddings.auto\"),\n",
    "        StepConfig(\"core.random_projection\", params={\"n_components\": 8}),\n",
    "        StepConfig(\"labels.ensure_onehot\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 32, dtype=np.int64), cache=False)\n",
    "print(\"X\", res.dataset.train.X.shape)\n",
    "print(\"tokens\", res.train_artifacts.require(\"tokens.input_ids\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e046a0a",
   "metadata": {},
   "source": [
    "## Custom Steps\n",
    "\n",
    "You can define custom steps and register them dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec5884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " [[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "Processed:\n",
      " [[10. 20.]\n",
      " [30. 40.]\n",
      " [50. 60.]]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "from modssc.preprocess.registry import default_step_registry\n",
    "from modssc.preprocess.store import ArtifactStore\n",
    "from modssc.preprocess.types import StepSpec\n",
    "\n",
    "\n",
    "# --- 1. Define a custom step class ---\n",
    "# The class must implement a `transform` method.\n",
    "# It can optionally implement `fit` if it needs to learn parameters.\n",
    "@dataclass\n",
    "class MyCustomStep:\n",
    "    factor: float = 2.0\n",
    "\n",
    "    def transform(self, store: ArtifactStore, *, rng: np.random.Generator) -> dict[str, Any]:\n",
    "        # Access artifacts from the store\n",
    "        # We assume \"features.X\" is already available (e.g. from ensure_2d)\n",
    "        X = store.require(\"features.X\")\n",
    "        return {\"features.X\": X * self.factor}\n",
    "\n",
    "\n",
    "# --- 2. Register it dynamically ---\n",
    "# We need a StepSpec to tell the registry how to instantiate and use it.\n",
    "# import_path=\"__main__:MyCustomStep\" works because we defined it in the notebook (main module).\n",
    "my_spec = StepSpec(\n",
    "    step_id=\"custom.multiply\",\n",
    "    import_path=\"__main__:MyCustomStep\",\n",
    "    kind=\"transform\",\n",
    "    description=\"Multiply features by a factor.\",\n",
    "    required_extra=None,\n",
    "    modalities=(),\n",
    "    consumes=(\"features.X\",),\n",
    "    produces=(\"features.X\",),\n",
    ")\n",
    "\n",
    "# Get the default registry and add our spec\n",
    "reg = default_step_registry()\n",
    "reg.specs[\"custom.multiply\"] = my_spec\n",
    "\n",
    "# --- 3. Create a simple numeric dataset for demonstration ---\n",
    "# (Previous cells might have left 'ds' as text or audio, which ensure_2d can't handle directly)\n",
    "X_custom = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=np.float32)\n",
    "y_custom = np.array([0, 1, 0], dtype=np.int64)\n",
    "ds_custom = LoadedDataset(\n",
    "    train=Split(X=X_custom, y=y_custom),\n",
    "    meta={\"modality\": \"tabular\", \"dataset_fingerprint\": \"custom_demo\"},\n",
    ")\n",
    "\n",
    "# --- 4. Build and run a plan using the custom step ---\n",
    "plan_custom = PreprocessPlan(\n",
    "    steps=(\n",
    "        StepConfig(\"core.ensure_2d\"),\n",
    "        StepConfig(\"custom.multiply\", params={\"factor\": 10.0}),\n",
    "    ),\n",
    "    output_key=\"features.X\",\n",
    ")\n",
    "\n",
    "# Pass the registry explicitly so it finds \"custom.multiply\"\n",
    "res_custom = preprocess(ds_custom, plan_custom, seed=0, fit_indices=None, registry=reg)\n",
    "\n",
    "print(\"Original:\\n\", X_custom)\n",
    "print(\"Processed:\\n\", res_custom.dataset.train.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2885b",
   "metadata": {},
   "source": [
    "## Using Pretrained Embeddings\n",
    "\n",
    "This section demonstrates how to use pretrained models (like Sentence Transformers, OpenCLIP, Wav2Vec2) to generate embeddings.\n",
    "\n",
    "### Sentence-Transformers (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed094203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (128,)\n",
      "X_embedded_shape (128, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "X = np.array([\"bonjour le monde\", \"exemple court\"] * 64, dtype=object)\n",
    "y = np.array([0, 1] * 64, dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"text\", \"dataset_fingerprint\": \"synthetic:text_st\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(steps=(StepConfig(\"text.sentence_transformer\"),))\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 10, dtype=np.int64), cache=False)\n",
    "\n",
    "print(\"X_shape\", X.shape)\n",
    "print(\"X_embedded_shape\", res.dataset.train.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47c2c2",
   "metadata": {},
   "source": [
    "### Open-clip (vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melvin/Desktop/ModSSC/.venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape (8, 64, 64, 3)\n",
      "X_embedded_shape (8, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X = (rng.random(size=(8, 64, 64, 3)) * 255).astype(np.uint8)\n",
    "y = rng.integers(0, 2, size=(8,), dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y),\n",
    "    meta={\"modality\": \"vision\", \"dataset_fingerprint\": \"synthetic:vision_clip\"},\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(steps=(StepConfig(\"vision.openclip\"),))\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 4, dtype=np.int64), cache=False)\n",
    "print(\"X_shape\", X.shape)\n",
    "print(\"X_embedded_shape\", res.dataset.train.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b934d85",
   "metadata": {},
   "source": [
    "### Wav2Vec2 (audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bea54c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape 4 [(16000,), (16000,), (16000,), (16000,)]\n",
      "X_embedded_shape (4, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from modssc.data_loader.types import LoadedDataset, Split\n",
    "from modssc.preprocess.api import preprocess\n",
    "from modssc.preprocess.plan import PreprocessPlan, StepConfig\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X = [rng.standard_normal(16000, dtype=np.float32) for _ in range(4)]\n",
    "y = rng.integers(0, 2, size=(4,), dtype=np.int64)\n",
    "ds = LoadedDataset(\n",
    "    train=Split(X=X, y=y), meta={\"modality\": \"audio\", \"dataset_fingerprint\": \"synthetic:audio_w2v\"}\n",
    ")\n",
    "\n",
    "plan = PreprocessPlan(steps=(StepConfig(\"audio.wav2vec2\"),))\n",
    "res = preprocess(ds, plan, fit_indices=np.arange(0, 2, dtype=np.int64), cache=False)\n",
    "print(\"X_shape\", len(X), [x.shape for x in X])\n",
    "print(\"X_embedded_shape\", res.dataset.train.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274f32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- The last cells should print key shapes and a minimal metric or artifact summary.\n",
    "- If something fails early, the error should point to a missing optional dependency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Explore the adjacent notebooks in this folder for the other pipeline components.\n",
    "- If you hit an optional dependency error, install the suggested extra and rerun.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
