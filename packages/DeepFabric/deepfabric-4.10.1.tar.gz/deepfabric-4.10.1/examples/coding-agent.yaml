#####################################################################
# Spin VFS Tools Dataset Configuration
#####################################################################
# This configuration demonstrates using real tool execution via Spin.
# Prerequisites:
#   1. Build and run the Spin service: cd tools-sdk && spin build && spin up
#   2. Run deepfabric: deepfabric start examples/spin-vfs-tools.yaml
#####################################################################

# Topic generation - creates a tree of related topics
topics:
  prompt: "DevOps and Platform Engineering"
  mode: graph
  prompt_style: anchored
  system_prompt: ""
  depth: 3
  degree: 3
  save_as: "devops-topics.jsonl"

  llm:
    provider: "openai"
    model: "gpt-4.1-mini-2025-04-14"
    temperature: 0.8

# Data generation with real tool execution
generation:
  system_prompt: |
    Generate scenrios of common devops tasks and actions, involving
    software engineering and writing of code to perform infrastructure management

  instructions: |
    The following files are available for tool operations of reading, writing, and listing:
    - Dockerfile
    - main.py
    - config.json
    - terraform/main.tf
    - scripts/deploy.sh
    - README.md
    - requirements.txt
    - infrastructure.yml

  # Agent mode is implicit when tools are configured
  conversation:
    type: cot
    reasoning_style: agent

  tools:
    # Spin service endpoint for real tool execution
    spin_endpoint: "http://localhost:3000"

    # Filter to specific tools (empty/omitted = all tools from endpoint)
    components:
      builtin:  # Built-in VFS tools
        - read_file
        - write_file
        - list_files

    max_per_query: 3
    strict: false

    # ReAct loop configuration - agent thinks step-by-step
    max_agent_steps: 5  # Maximum reasoning steps before conclusion

    # Optional: Seed initial files into the VFS before generation
    scenario_seed:
      files:
        "Dockerfile": |
          FROM python:3.13
          WORKDIR /usr/local/app

          # Install the application dependencies
          COPY requirements.txt ./
          RUN pip install --no-cache-dir -r requirements.txt

          # Copy in the source code
          COPY src ./src
          EXPOSE 8080

          # Setup an app user so the container doesn't run as the root user
          RUN useradd app
          USER app

          CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
        "main.py": |
          def greet(name):
              return f"Hello, {name}!"

          if __name__ == "__main__":
              print(greet("World"))
        "config.json": |
          {
            "version": "1.0.0",
            "debug": true,
            "max_retries": 3
          }

        "terraform/main.tf": |
          provider "aws" {
            region = "us-west-2"
          }

          resource "aws_instance" "web" {
            ami           = "ami-0c55b159cbfafe1f0"
            instance_type = "t2.micro"

            tags = {
              Name = "HelloWorld"
            }
          }

        "scripts/deploy.sh": |
          #!/bin/bash
          echo "Starting deployment..."
          # Deployment commands go here
          echo "Deployment complete."

        "README.md": |
          # DevOps Project
          This project contains infrastructure as code and deployment scripts for the application.

        "requirements.txt": |
          fastapi
          uvicorn
          boto3

        "infrastructure.yml": |
          resources:
            - type: aws_instance
              properties:
                instance_type: t2.micro
                ami: ami-0c55b159cbfafe1f0

  max_retries: 3

  llm:
    provider: "openai"
    model: "gpt-4.1-mini-2025-04-14"
    temperature: 0.7

# Output configuration
output:
  system_prompt: |
    You are an AI assistant with access to file system tools.
    When given a task:
    1. Analyze what files need to be read or modified
    2. Execute file operations with proper paths
    3. Interpret results and provide clear answers

  include_system_message: true
  num_samples: 3
  batch_size: 3
  save_as: "coding-agent-dataset.jsonl"
