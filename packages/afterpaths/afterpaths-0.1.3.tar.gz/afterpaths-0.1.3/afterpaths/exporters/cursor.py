"""Cursor exporter - writes to .cursor/rules/ directory."""

import re
from datetime import datetime
from pathlib import Path

from .base import ExportResult, Rule, RuleExporter

# Category display names for Cursor format
CATEGORY_TITLES = {
    "dead_ends": "Dead Ends",
    "decisions": "Decisions",
    "gotchas": "Gotchas",
    "patterns": "Patterns",
}


class CursorExporter(RuleExporter):
    """Exports rules to .cursor/rules/ for Cursor IDE.

    Creates a single consolidated file that Cursor auto-loads.
    Uses .mdc extension (Cursor's markdown format).
    """

    name = "cursor"
    description = "Cursor IDE (.cursor/rules/)"

    def detect(self, project_root: Path) -> bool:
        """Detect if Cursor is being used (has .cursor directory)."""
        return (project_root / ".cursor").exists()

    def get_output_paths(self, project_root: Path) -> list[Path]:
        """Return path to the rules file."""
        return [project_root / ".cursor" / "rules" / "afterpaths-rules.mdc"]

    def export(self, rules: dict[str, list[Rule]], project_root: Path) -> ExportResult:
        """Write rules to .cursor/rules/afterpaths-rules.mdc."""
        rules_dir = project_root / ".cursor" / "rules"
        rules_dir.mkdir(parents=True, exist_ok=True)

        file_path = rules_dir / "afterpaths-rules.mdc"
        content = self._format_consolidated_file(rules)
        file_path.write_text(content)

        total_rules = sum(len(r) for r in rules.values())

        return ExportResult(
            target=self.name,
            files_written=[file_path],
            rules_count=total_rules,
        )

    def load_existing(self, project_root: Path) -> dict[str, list[Rule]]:
        """Load existing rules from .cursor/rules/afterpaths-rules.mdc."""
        file_path = project_root / ".cursor" / "rules" / "afterpaths-rules.mdc"
        if not file_path.exists():
            return {}

        return self._parse_consolidated_file(file_path)

    def _format_consolidated_file(self, rules: dict[str, list[Rule]]) -> str:
        """Format all rules into a single markdown file."""
        # Count unique source sessions across all rules
        all_sessions = set()
        total_rules = 0
        for category_rules in rules.values():
            for rule in category_rules:
                all_sessions.update(rule.source_sessions)
                total_rules += 1

        lines = [
            "# Afterpaths: Extracted Rules",
            "",
            f"> Auto-generated by `afterpaths distill` | Updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"> {total_rules} rules from {len(all_sessions)} session(s)",
            "",
        ]

        # Write each category that has rules
        for category, title in CATEGORY_TITLES.items():
            category_rules = rules.get(category, [])
            if not category_rules:
                continue

            lines.append(f"## {title}")
            lines.append("")

            for rule in category_rules:
                sources = rule.source_sessions or ["unknown"]
                source_str = ", ".join(s[:8] for s in sources[:3])
                if len(sources) > 3:
                    source_str += f" (+{len(sources) - 3} more)"

                lines.append(f"- **{rule.title}**: {rule.content}")
                lines.append(f"  _Source: {source_str}_")
                lines.append("")

        lines.extend([
            "---",
            "_Generated by afterpaths_",
        ])

        return "\n".join(lines)

    def _parse_consolidated_file(self, file_path: Path) -> dict[str, list[Rule]]:
        """Parse existing rules from the consolidated file."""
        content = file_path.read_text()
        rules = {}
        current_category = None

        # Map section headers back to category keys
        header_to_category = {v.lower(): k for k, v in CATEGORY_TITLES.items()}

        for line in content.split("\n"):
            # Detect category headers
            if line.startswith("## "):
                header = line[3:].strip().lower()
                current_category = header_to_category.get(header)
                if current_category and current_category not in rules:
                    rules[current_category] = []

        # Now parse rules with regex
        pattern = r'-\s+\*\*([^*]+)\*\*:\s*(.+?)\n\s+_Source:\s*([^_]+)_'

        # Find which category each rule belongs to by position
        for match in re.finditer(pattern, content, re.MULTILINE):
            title = match.group(1).strip()
            rule_content = match.group(2).strip()
            source_str = match.group(3).strip()

            # Determine category based on position in file
            pos = match.start()
            category = self._find_category_at_position(content, pos, header_to_category)

            if category:
                sources = []
                source_parts = source_str.replace("(+", "").replace("more)", "").split(",")
                for part in source_parts:
                    part = part.strip()
                    if part and not part.isdigit():
                        sources.append(part)

                if category not in rules:
                    rules[category] = []

                rules[category].append(Rule(
                    category=category,
                    title=title,
                    content=rule_content,
                    source_sessions=sources,
                ))

        return rules

    def _find_category_at_position(
        self, content: str, pos: int, header_map: dict[str, str]
    ) -> str | None:
        """Find which category a position in the file belongs to."""
        # Find the last ## header before this position
        last_category = None
        for match in re.finditer(r'^## (.+)$', content[:pos], re.MULTILINE):
            header = match.group(1).strip().lower()
            if header in header_map:
                last_category = header_map[header]
        return last_category
