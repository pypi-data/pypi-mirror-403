# jax2onnx/plugins/flax/nnx/layer_norm.py

from __future__ import annotations

from typing import Any, ClassVar, Final, List, Optional, Sequence, cast
import jax.numpy as jnp
from flax import nnx
from jax.core import ShapedArray
from jax.extend.core import Primitive

import onnx_ir as ir
from jax2onnx.plugins.plugin_system import (
    PrimitiveLeafPlugin,
    construct_and_call,
    register_primitive,
    with_requested_dtype,
    with_rng_seed,
)
from jax2onnx.plugins._post_check_onnx_graph import expect_graph as EG

# Cast helper that inserts CastLike only when needed; no-op if dtypes already match
from jax2onnx.plugins._utils import cast_param_like
from jax2onnx.plugins._patching import MonkeyPatchSpec
from jax2onnx.plugins._ir_shapes import (
    _dim_label_from_value_or_aval,
    _stamp_type_and_shape,
    _ensure_value_metadata,
)


def _attr_value(node: Any, name: str) -> Optional[Any]:
    attrs = (
        getattr(node, "attribute", None)
        or getattr(node, "attributes", None)
        or getattr(node, "_attributes", None)
    )
    if attrs is None:
        return None
    for attr in attrs:
        if getattr(attr, "name", "") != name:
            continue
        if hasattr(attr, "i"):
            try:
                has_field = getattr(attr, "HasField", None)
                if callable(has_field) and not has_field("i"):
                    pass
                else:
                    return getattr(attr, "i")
            except Exception:
                return getattr(attr, "i")
        if hasattr(attr, "f"):
            try:
                has_field = getattr(attr, "HasField", None)
                if callable(has_field) and not has_field("f"):
                    pass
                else:
                    return getattr(attr, "f")
            except Exception:
                return getattr(attr, "f")
        floats = getattr(attr, "floats", None)
        if floats:
            return list(floats)
        ints = getattr(attr, "ints", None)
        if ints:
            return list(ints)
        sval = getattr(attr, "s", None)
        if sval is not None:
            if isinstance(sval, bytes):
                try:
                    return sval.decode("utf-8")
                except Exception:
                    return sval
            return sval
        strings = getattr(attr, "strings", None)
        if strings:
            try:
                return [
                    s.decode("utf-8") if isinstance(s, bytes) else s for s in strings
                ]
            except Exception:
                return list(strings)
    return None


def _layer_norm_attr_check(
    model: Any,
    *,
    axis: Optional[int] = None,
    epsilon: Optional[float] = None,
) -> bool:
    graph = getattr(model, "graph", None)
    if graph is None:
        return False
    nodes: Sequence[Any] = getattr(graph, "node", [])
    ln_nodes = [n for n in nodes if getattr(n, "op_type", "") == "LayerNormalization"]
    if len(ln_nodes) != 1:
        return False
    ln = ln_nodes[0]
    if axis is not None:
        axis_attr = _attr_value(ln, "axis")
        try:
            if axis_attr is None:
                return False
            if isinstance(axis_attr, (list, tuple)):
                if not axis_attr:
                    return False
                axis_attr = axis_attr[0]
            if int(axis_attr) != int(axis):
                return False
        except Exception:
            return False
    if epsilon is not None:
        eps_attr = _attr_value(ln, "epsilon")
        try:
            if eps_attr is None:
                return False
            if isinstance(eps_attr, (list, tuple)):
                if not eps_attr:
                    return False
                eps_attr = eps_attr[0]
            if abs(float(eps_attr) - float(epsilon)) > 1e-9:
                return False
        except Exception:
            return False
    return True


LAYER_NORM_PRIM: Final[Primitive] = Primitive("nnx.layer_norm")
LAYER_NORM_PRIM.multiple_results = False


@register_primitive(
    jaxpr_primitive=LAYER_NORM_PRIM.name,
    jax_doc="https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm",
    onnx=[
        {
            "component": "LayerNormalization",
            "doc": "https://onnx.ai/onnx/operators/onnx__LayerNormalization.html",
        }
    ],
    since="0.1.0",
    context="primitives.nnx",
    component="layer_norm",
    testcases=[
        {
            "testcase": "layer_norm",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                epsilon=1e-5,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": EG(
                ["LayerNormalization:Bx20x32"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_no_bias_no_scale",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                use_bias=False,
                use_scale=False,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": EG(
                ["LayerNormalization:Bx20x32"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_bias_no_scale",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                use_bias=True,
                use_scale=False,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            # Small drift depending on epsilon and tensor contents; relax slightly.
            "rtol": 6e-5,
            "atol": 1e-6,
            "post_check_onnx_graph": EG(
                ["LayerNormalization:Bx20x32"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_no_bias_scale",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                use_bias=False,
                use_scale=True,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": EG(
                ["LayerNormalization:Bx20x32"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_bias_scale",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                use_bias=True,
                use_scale=True,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": EG(
                ["LayerNormalization:Bx20x32"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_multiaxis",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=3 * 3 * 64,
                reduction_axes=(1, 2, 3),
                feature_axes=(1, 2, 3),
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 3, 3, 64)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": EG(
                ["Reshape:Bx576 -> LayerNormalization:Bx576 -> Reshape:Bx3x3x64"],
                symbols={"B": None},
                no_unused_inputs=True,
            ),
        },
        {
            "testcase": "layer_norm_symbolic_batch",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=16,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 8, 16)],
            "run_only_f32_variant": True,
            # ORT LayerNormalization kernel vs JAX introduces tiny f32 drift.
            # Keep the graph clean (1 LN, no helpers) and relax tolerances slightly.
            "rtol": 6e-5,
            "atol": 1e-6,
            "post_check_onnx_graph": lambda m: all(
                n.op_type not in ("Unsqueeze", "Reshape") for n in m.graph.node
            ),
        },
        # ----------------------------------------------------------------------
        # Mirrors the SuperBlock shape used in examples: (B, 10, 3) with features on the last dim.
        # Ensures we produce a single LayerNormalization with no extra reshape helpers
        # when normalizing the last dimension (axis=-1/2 in this rank).
        {
            "testcase": "layer_norm_symbolic_batch_seq10_feat3",
            # Use epsilon=1e-5 to match ONNX LayerNormalization default (so we can omit the attr)
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=3,
                epsilon=1e-5,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 10, 3)],
            "run_only_f32_variant": True,
            # Small f32 differences; keep single LN node and bump tolerance a hair.
            "rtol": 1e-4,
            "atol": 1e-5,
            "post_check_onnx_graph": lambda m: (
                sum(1 for n in m.graph.node if n.op_type == "LayerNormalization") == 1
                and all(n.op_type not in ("Reshape", "Unsqueeze") for n in m.graph.node)
            ),
        },
        {
            "testcase": "layer_norm_symbolic_batch_seq10_feat3_2",
            # Exercise the JAX default epsilon (1e-6). Ensure the exported ONNX keeps
            # the explicit epsilon/axis attrs so inference matches JAX numerics.
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=3,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 10, 3)],
            "run_only_f32_variant": True,
            # Dynamic-batch variants + small eps (JAX default 1e-6) can drift up to ~1.5e-3 in f32
            # vs. ORT due to accumulation/rounding. Keep single-LN-node contract and relax rtol a bit.
            # (atol remains tight to catch gross errors.)
            "rtol": 1.2e-2,
            "atol": 1e-5,
            "post_check_onnx_graph": lambda m: (
                sum(1 for n in m.graph.node if n.op_type == "LayerNormalization") == 1
                and all(n.op_type not in ("Reshape", "Unsqueeze") for n in m.graph.node)
                and _layer_norm_attr_check(m, axis=2, epsilon=1e-6)
            ),
        },
        {
            "testcase": "layer_norm_negative_axis_no_div",
            "callable": construct_and_call(
                nnx.LayerNorm,
                num_features=32,
                epsilon=1e-5,
                reduction_axes=-1,
                feature_axes=-1,
                dtype=with_requested_dtype(),
                param_dtype=with_requested_dtype(),
                rngs=with_rng_seed(0),
            ),
            "input_shapes": [("B", 20, 32)],
            "run_only_f32_variant": True,
            "post_check_onnx_graph": lambda m: any(
                n.op_type == "LayerNormalization" for n in m.graph.node
            )
            and all(n.op_type != "Div" for n in m.graph.node),
        },
    ],
)
class LayerNormPlugin(PrimitiveLeafPlugin):
    _PRIM: ClassVar[Primitive] = LAYER_NORM_PRIM

    @staticmethod
    def abstract_eval(x, scale, bias, *, epsilon: float, axis: int):
        x_aval = x if isinstance(x, ShapedArray) else ShapedArray(x.shape, x.dtype)
        return ShapedArray(x_aval.shape, x_aval.dtype)

    def lower(self, ctx, eqn, params: dict[str, Any] | None = None):
        """
        Emit a single LayerNormalization node.
        scale/bias are already shaped to X.shape[axis:] by the monkey-patch,
        so we don't need any Reshape or attributes here.
        """
        x_v = ctx.get_value_for_var(eqn.invars[0])
        scale_v = ctx.get_value_for_var(eqn.invars[1])
        bias_v = ctx.get_value_for_var(eqn.invars[2])

        # --- IMPORTANT: align param dtypes with input to avoid FP32/FP64 drift ---
        # On symbolic shapes, JAX literals/params can surface with a wider dtype
        # (e.g., float64). ORT will then cast internally and accumulate slightly
        # different rounding than the JAX ground truth. Make it explicit and stable:
        scale_v = cast_param_like(ctx, scale_v, x_v, name_hint="ln_scale_cast")
        bias_v = cast_param_like(ctx, bias_v, x_v, name_hint="ln_bias_cast")

        builder = getattr(ctx, "builder", None)
        if builder is None:
            raise AttributeError("IR build context missing builder")

        # Read axis/epsilon from JAXPR params so the builder attaches them directly.
        p = params or getattr(eqn, "params", {}) or {}
        in_shape = tuple(getattr(eqn.invars[0].aval, "shape", ()))
        rank = len(in_shape)
        axis = int(p.get("axis", -1))
        if axis < 0:
            axis += rank
        eps = float(p.get("epsilon", 1e-5))

        y_val = builder.LayerNormalization(
            x_v,
            scale_v,
            bias_v,
            axis=int(axis),
            epsilon=float(eps),
            _outputs=[ctx.fresh_name("LayerNorm")],
        )

        x_dtype = getattr(getattr(x_v, "type", None), "dtype", None)
        if x_dtype is not None:
            y_val.type = ir.TensorType(x_dtype)

        out_aval_shape = tuple(getattr(eqn.outvars[0].aval, "shape", ()))
        if out_aval_shape:
            dims: list[Any] = []
            for idx in range(len(out_aval_shape)):
                label = _dim_label_from_value_or_aval(x_v, in_shape, idx)
                dims.append(label if label is not None else out_aval_shape[idx])
            _stamp_type_and_shape(y_val, tuple(dims))
        _ensure_value_metadata(ctx, y_val)

        bind_value = getattr(ctx, "bind_value_for_var", None)
        if not callable(bind_value):
            raise AttributeError("IR build context missing bind_value_for_var")
        bind_value(eqn.outvars[0], y_val)

    # ---- patch nnx.LayerNorm.__call__ to bind our primitive ----------------
    @classmethod
    def binding_specs(cls):
        return [MonkeyPatchSpec(nnx.LayerNorm, "__call__", cls._patch_call)]

    @staticmethod
    def _patch_call(orig):
        def wrapped(self: nnx.LayerNorm, x):
            # Prefer explicit reduction_axes, then feature_axes, else last dim
            if getattr(self, "reduction_axes", None) is not None:
                axes = self.reduction_axes
            elif getattr(self, "feature_axes", None) is not None:
                axes = self.feature_axes
            else:
                axes = -1

            if isinstance(axes, int):
                axes = (axes,)
            axes = tuple(a if a >= 0 else a + x.ndim for a in axes)
            axis0 = min(axes)

            param_dtype = getattr(self, "param_dtype", None) or x.dtype
            if x.dtype != param_dtype:
                x = x.astype(param_dtype)

            # Tail shape across all normalized axes (NOT including batch/seq)
            tail_shape = tuple(x.shape[a] for a in axes)
            # Build base scale/bias (matching tail dimensions)
            if (
                getattr(self, "use_scale", False)
                and getattr(self, "scale", None) is not None
            ):
                sv = self.scale.value
                base_scale = (
                    jnp.reshape(sv, tail_shape) if tuple(sv.shape) != tail_shape else sv
                )
            else:
                base_scale = jnp.ones(tail_shape, dtype=param_dtype)
            if (
                getattr(self, "use_bias", False)
                and getattr(self, "bias", None) is not None
            ):
                bv = self.bias.value
                base_bias = (
                    jnp.reshape(bv, tail_shape) if tuple(bv.shape) != tail_shape else bv
                )
            else:
                base_bias = jnp.zeros(tail_shape, dtype=param_dtype)

            eps = float(getattr(self, "epsilon", 1e-5))

            # If we normalize the last dim only, we can bind directly.
            # Otherwise, flatten the tail so ONNX LN default (last dim) is correct,
            # bind with vector scale/bias, then reshape back to the original shape.
            needs_flatten = (axis0 != x.ndim - 1) or (len(axes) > 1)
            if not needs_flatten:
                return LAYER_NORM_PRIM.bind(
                    x,
                    base_scale,  # shape: (last_dim,)
                    base_bias,  # shape: (last_dim,)
                    epsilon=eps,
                    axis=int(axis0),
                )

            # Flatten tail dims to a single last dimension
            orig_shape = x.shape
            x_flat = jnp.reshape(x, (*orig_shape[:axis0], -1))
            scale_vec = jnp.reshape(base_scale, (-1,))
            bias_vec = jnp.reshape(base_bias, (-1,))

            y_flat = LAYER_NORM_PRIM.bind(
                x_flat,
                scale_vec,
                bias_vec,
                epsilon=eps,
                axis=x_flat.ndim - 1,  # last dimension
            )
            return jnp.reshape(y_flat, orig_shape)

        return wrapped


# Bind abstract eval on the primitive
LAYER_NORM_PRIM.def_abstract_eval(LayerNormPlugin.abstract_eval)


# ---------------------------------------------------------------------------
# runtime Python impl for the primitive (eager JAX path for validation).
# This mirrors the ONNX LayerNormalization math & order-of-ops to minimize
# numeric drift vs. ORT, while our lowering still emits a single LN node.
# ---------------------------------------------------------------------------
def _ln_impl(x, scale, bias, *, epsilon: float, axis: int):
    """
    Compute LayerNorm like ORT:
      var = E[x^2] - (E[x])^2
      y   = (x - mean) * rsqrt(var + eps) * scale + bias
    Broadcast scale/bias across the normalized axes the same way ONNX does.
    """
    # Normalize axis to positive. Keep everything explicitly typed for mypy.
    axis_val: int = int(axis)
    ndim_val: int = int(cast(int, getattr(x, "ndim", 0)))
    normalized_axis: int
    if axis_val >= 0:
        normalized_axis = axis_val
    else:
        normalized_axis = int(ndim_val + axis_val)
    # mean over the last axis only (this primitive is bound so that
    # multi-axis cases are flattened before binding, matching our lowering)
    # Keep dims for broadcasting
    mean = jnp.mean(x, axis=normalized_axis, keepdims=True)
    mean2 = jnp.mean(jnp.square(x), axis=normalized_axis, keepdims=True)
    var = mean2 - jnp.square(mean)
    inv = jnp.reciprocal(jnp.sqrt(var + epsilon))

    # Broadcast scale/bias like ONNX: shape (..., C) on the last axis.
    # If scale/bias are already 1D of size C, reshape to match x for broadcast.
    # Otherwise (e.g., (C,) already broadcasts), this is a no-op.
    rank_i: int = int(cast(int, getattr(x, "ndim", 0)))
    bshape: List[int] = [1] * rank_i
    bshape_idx: int = int(normalized_axis)
    bshape[bshape_idx] = int(x.shape[bshape_idx])
    try:
        scale_b = jnp.reshape(scale, bshape)
    except Exception:
        scale_b = scale
    try:
        bias_b = jnp.reshape(bias, bshape)
    except Exception:
        bias_b = bias

    return (x - mean) * inv * scale_b + bias_b


# Attach impl so eager JAX uses this for the baseline in numeric checks.
LAYER_NORM_PRIM.def_impl(_ln_impl)
