id: silent-failure-detection
title: "Assess Risk of Silent AI Failure in Production Operations"
stack: aiops
difficulty: Hard
xp: 150

goal: "Assess operational risk when an AIOps system fails silently without triggering human intervention"

skills:
  - silent-failure-analysis
  - aiops-risk-assessment
  - operational-awareness
  - accountability-evaluation

hint:
  - Assume the AI produced no explicit error.
  - Focus on damage caused by absence of action.

validator: "validator.py"

mentor:
  why: >
    Silent failures are more dangerous than visible ones.
    When an AI system fails quietly, humans may assume
    stability while damage accumulates.
  intent: stretch
  guidance:
    before: |
      Consider how engineers detect problems when
      automation provides no warnings.

      Think about:
      - What humans assume when systems are quiet
      - How silence can reinforce false confidence
      - Why delayed awareness amplifies impact
    after: >
      Reflect on whether silence from AI systems
      should ever be interpreted as correctness.
  tags:
    - aiops
    - silent-failure
    - reliability
    - risk
    - safety

solution:
  overview: >
    This solution frames senior-level reasoning when
    an AI system fails without visible signals.
    The focus is on recognizing harm caused by inaction
    and reasserting human responsibility.

  professional_reasoning:
    - Treat silence as a risk signal, not reassurance
    - Assume inaction compounds damage over time
    - Reassert human ownership of system health
    - Evaluate whether silence masked operational risk

  real_world_context:
    - Do not introduce new detection or monitoring tools
    - Do not attempt to fix or redesign the AI system
    - Identify how absence of signals influenced decisions
    - Assess impact realistically
    - Favor restriction when silence undermines safety

execution:
  runtime: docker
  requires_execution: false
  required_tools:
    - none
