# Operational Judgment Doctrine â€” Post-Incident Trust

This document defines the judgment standards applied in this lab.
It does not describe correct actions or expected conclusions.

---

## Purpose of This Lab

This lab evaluates whether continued trust in an AI system
is defensible after it has caused verified operational harm.

The focus is on trust as a deliberate operational decision,
not on restoring confidence or improving the system.

---

## Authority and Trust Decisions

In this environment:

- AI systems may fail in production
- AI systems do not regain trust automatically
- Human decision-makers own trust decisions explicitly

Continuing to use a known-risk system
is itself a conscious operational choice.

Responsibility cannot be deferred.

---

## Trust Judgment Standard

A judgment failure exists when:

- An AI system causes verified harm
- Continued use exposes systems to repeat risk
- Trust is maintained without justification

Describing failure without addressing continued exposure
does not meet this standard.

---

## Accountability Expectation

A valid analysis must:

- Identify who owns the trust decision
- Explain why continued trust is or is not defensible
- Assign accountability to a role or authority holder

Implicit or inherited trust is insufficient.

---

## Post-Failure Control Expectation

After trust failure, acceptable responses focus on:

- Restricting or revoking AI use
- Reasserting human governance over trust decisions
- Preventing repeated exposure to known risk

Trust-rebuilding mechanisms or metrics
do not address the judgment failure evaluated here.

---

## Evaluation Boundary

This document defines how reasoning is evaluated.
It does not prescribe correct answers.

You are expected to apply these standards independently
to the incident presented in the lab.
