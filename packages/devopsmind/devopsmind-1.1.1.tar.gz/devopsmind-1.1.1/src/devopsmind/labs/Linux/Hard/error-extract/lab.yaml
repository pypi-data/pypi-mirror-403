id: error-extract
title: "Extract Unique ERROR Logs"
stack: linux
difficulty: Hard
xp: 150
goal: "Extract and analyze unique ERROR log entries from a log file using Linux command-line tools"

skills:
  - log-analysis
  - text-processing
  - linux-command-pipelines

hint: "Use a command pipeline to extract ERROR lines, remove duplicates, sort by timestamp, and write the result to errors.txt."

validator: "validator.py"

inputs:
  - app.log

mentor:
  why: "Effective log analysis relies on transforming raw log data into concise, meaningful signals using composable Linux text-processing tools."
  intent: stretch

  guidance:
    before: |
      Think about how each stage of a command pipeline transforms the data and why ordering matters when deduplicating and sorting log entries.

      Optional manual commands (not required for validation):
      These commands operate only on local files.

      - sort
      - uniq
    after: |
      Consider how similar pipelines are used in production to extract alerts or error summaries from very large log files.

  tags:
    - linux
    - logs
    - text-processing
    - pipelines

solution:
  overview: >
    At hard level, log analysis is about building precise and repeatable
    data transformation pipelines. This lab evaluates whether an
    engineer can reliably extract meaningful error information from noisy
    logs without manual inspection.
  professional_reasoning:
    - Log pipelines should be composable and deterministic
    - Deduplication must occur after correct field extraction
    - Sorting ensures stable and reviewable output
    - Clean output enables automation and alerting
  real_world_context:
    - Production systems generate large volumes of logs
    - Engineers routinely extract error summaries for incidents
    - Poor log parsing leads to missed or duplicated alerts

execution:
  runtime: docker
  requires_execution: true
  required_tools:
    - linux
