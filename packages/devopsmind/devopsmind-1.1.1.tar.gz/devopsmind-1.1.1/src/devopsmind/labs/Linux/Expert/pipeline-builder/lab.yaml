id: pipeline-builder
title: "Build Robust Data Processing Pipeline"
stack: linux
difficulty: Expert
xp: 300
goal: "Build a robust Bash data-processing pipeline that handles errors correctly"

skills:
  - bash-pipelines
  - error-handling
  - shell-debugging

hint: "Use a Bash pipeline that correctly propagates errors and prints only the final numeric result."

validator: "validator.py"

mentor:
  why: "Production-grade shell pipelines must handle errors explicitly and fail predictably, especially when used in automation or monitoring contexts."
  intent: stretch

  guidance:
    before: |
      Think about how Bash pipelines handle exit codes and how failures in one command affect the overall script.

      Optional manual commands (not required for validation):
      These commands operate only on local files.
      Results are for personal verification only.

      - cat numbers.txt
      - wc -l numbers.txt
    after: |
      Consider how strict shell modes or explicit error checks improve the reliability of automation scripts.

  tags:
    - linux
    - bash
    - pipelines
    - error-handling

solution:
  overview: >
    At expert level, shell scripting is about correctness under failure.
    This lab evaluates whether an engineer can build a data-processing
    pipeline that produces accurate results while also failing safely when
    required inputs are missing or invalid.
  professional_reasoning:
    - Pipelines must propagate failures, not hide them
    - Numeric output should be clean and machine-readable
    - Scripts should validate prerequisites before processing
    - Reliable automation favors explicit error handling
  real_world_context:
    - Data-processing pipelines often run unattended
    - Silent failures can corrupt reports or metrics
    - Expert engineers design scripts to fail loudly and predictably

execution:
  runtime: docker
  requires_execution: true
  required_tools:
    - linux
