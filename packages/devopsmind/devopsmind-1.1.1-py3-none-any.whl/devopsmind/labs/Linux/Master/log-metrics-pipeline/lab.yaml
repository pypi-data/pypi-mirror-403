id: log-metrics-pipeline
title: "Generate metrics from log file"
stack: linux
difficulty: Master
xp: 500
goal: "Build a command-line pipeline to generate meaningful metrics from log files"

skills:
  - log-analysis
  - text-processing
  - metrics-generation
  - advanced-linux-pipelines

hint: "Build a pipeline that extracts status codes, aggregates counts, and prints stable metrics output."

validator: "validator.py"

mentor:
  why: "Senior engineers are expected to transform raw access logs into reliable metrics that can be used for monitoring, alerting, and capacity planning."
  intent: stretch

  guidance:
    before: |
      Think about which fields in access logs are meaningful as metrics and how aggregation and sorting affect the reliability of reported numbers.

      Optional manual commands (not required for validation):
      These commands operate only on local files.
      Results are for personal verification only.

      - awk
      - sort
      - uniq -c
    after: |
      Consider how similar pipelines are used to generate metrics for dashboards and alerting systems.

  tags:
    - linux
    - logs
    - metrics
    - pipelines

solution:
  overview: >
    At master level, metric generation is evaluated on determinism,
    clarity, and correctness. This lab assesses whether an engineer
    can design a pipeline that produces stable, reviewable metrics from
    raw log data without relying on assumptions or hardcoded values.
  professional_reasoning:
    - Metrics pipelines must be deterministic and reproducible
    - Aggregation should be independent of input ordering
    - Output format must be stable for downstream consumers
    - Clean metrics reduce noise in monitoring systems
  real_world_context:
    - Access logs are primary data sources for traffic metrics
    - Incorrect aggregation leads to misleading dashboards
    - Senior engineers design metrics others can trust without revalidation

execution:
  runtime: docker
  requires_execution: true
  required_tools:
    - linux
