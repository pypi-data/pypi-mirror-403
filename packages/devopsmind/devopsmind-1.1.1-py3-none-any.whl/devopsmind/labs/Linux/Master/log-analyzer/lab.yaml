id: log-analyzer
title: "Analyze Logs and Generate Summary"
stack: linux
difficulty: Master
xp: 500
goal: "Analyze log files and generate a summarized report using Bash tooling"

skills:
  - log-analysis
  - text-processing
  - advanced-shell-scripting
  - data-aggregation

hint: "Use text-processing pipelines to extract log levels and aggregate counts deterministically."

validator: "validator.py"

mentor:
  why: "Senior engineers are expected to transform raw operational logs into concise, reliable summaries that support troubleshooting and decision-making."
  intent: stretch

  guidance:
    before: |
      Think about how log analysis pipelines extract fields, aggregate counts, and enforce stable output ordering.

      Optional manual commands (not required for validation):
      These commands operate only on local files.

      - awk
      - sort
      - uniq -c
    after: |

      Consider how log summaries can be extended to support alerting, dashboards, or structured outputs in production systems.

  tags:
    - linux
    - logs
    - analysis
    - shell-scripting

solution:
  overview: >
    At master level, log analysis is evaluated on clarity, determinism,
    and correctness rather than command knowledge alone. This lab
    assesses whether an engineer can build a pipeline that produces a
    stable, reviewable summary suitable for automation and reporting.
  professional_reasoning:
    - Aggregation must be deterministic and reproducible
    - Output ordering matters for machine and human consumers
    - Pipelines should avoid assumptions about input ordering
    - Clean summaries reduce cognitive load during incidents
  real_world_context:
    - Log summaries are widely used in incident response
    - Inconsistent output leads to alert fatigue or missed signals
    - Senior engineers design pipelines others can trust without rechecking

execution:
  runtime: docker
  requires_execution: true
  required_tools:
    - linux
