id: false-negative-risk
title: "Assess Risk of AI False Negatives During Production Incidents"
stack: aiops
difficulty: Hard
xp: 150

goal: "Assess operational damage when an AIOps system fails to detect a real incident condition"

skills:
  - false-negative-analysis
  - incident-risk-assessment
  - reliability-ownership
  - operational-accountability

hint:
  - Assume the AI missed a real issue.
  - Focus on consequences of non-detection, not detection accuracy.

validator: "validator.py"

mentor:
  why: >
    False negatives are often more dangerous than false positives.
    When an AI system fails to detect a real problem, humans may
    assume stability and delay action until damage becomes visible.
  intent: stretch
  guidance:
    before: |
      Consider how engineers react when no alerts are raised.

      Think about:
      - What assumptions are made when systems appear quiet
      - How trust in detection suppresses manual checks
      - Why late discovery increases blast radius
    after: >
      Reflect on whether non-detection should ever be treated
      as evidence of system health.
  tags:
    - aiops
    - false-negatives
    - incident-detection
    - reliability
    - safety

solution:
  overview: >
    This solution frames senior-level reasoning when an AI system
    fails to detect a genuine incident. The emphasis is on
    recognizing harm caused by assumed safety and restoring
    explicit human ownership.

  professional_reasoning:
    - Treat non-detection as a risk condition
    - Assume delayed awareness compounds damage
    - Reassert human responsibility for system health
    - Evaluate whether trust in detection was misplaced

  real_world_context:
    - Do not introduce new detection mechanisms
    - Do not attempt to improve AI accuracy
    - Identify how non-detection shaped human assumptions
    - Assess impact realistically
    - Favor restriction when detection failure undermines trust

execution:
  runtime: docker
  requires_execution: false
  required_tools:
    - none
