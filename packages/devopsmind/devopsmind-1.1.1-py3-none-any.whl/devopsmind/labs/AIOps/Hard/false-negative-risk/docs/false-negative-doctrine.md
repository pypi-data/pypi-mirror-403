# Operational Judgment Doctrine — False Negatives

This document defines the judgment standards applied in this lab.
It does not describe correct actions or expected conclusions.

---

## Purpose of This Lab

This lab evaluates how human responders interpret
the absence of signals from AI detection systems.

The focus is on judgment, authority, and accountability —
not on improving detection accuracy or alerting coverage.

---

## Authority and Responsibility

In this environment:

- AI systems may fail to detect real conditions
- AI systems do not own system health
- Human responders retain responsibility for awareness and action

Silence is not evidence of safety.
Treating non-detection as assurance is a decision.

Responsibility cannot be delegated to detection systems.

---

## False-Negative Judgment Standard

A judgment failure exists when:

- A real incident condition is present
- AI systems fail to raise signals
- Human responders delay action due to assumed stability

Describing technical non-detection without identifying
human assumptions does not meet this standard.

---

## Accountability Expectation

A valid analysis must:

- Identify who owned awareness and response
- Explain why non-detection influenced behavior
- Assign accountability to a role or decision owner

Diffuse or passive responsibility is insufficient.

---

## Post-Failure Control Expectation

After false-negative failure, acceptable responses focus on:

- Restricting AI influence as a sole indicator of health
- Reasserting human ownership of detection and awareness
- Preventing silence from being treated as assurance

Adding alerts or improving accuracy
does not address the judgment failure evaluated here.

---

## Evaluation Boundary

This document defines how reasoning is evaluated.
It does not prescribe correct answers.

You are expected to apply these standards independently
to the incident presented in the lab.
