# Operational Judgment Doctrine — AI Override Failure

This document defines the judgment standards applied in this lab.
It does not describe correct actions or expected conclusions.

---

## Purpose of This Lab

This lab evaluates how human responders exercise authority
when automated systems provide guidance during a critical incident.

The focus is on hesitation, authority, and accountability —
not on improving AI accuracy or tooling.

---

## Authority and Responsibility

In this environment:

- AI systems may recommend actions or interpretations
- AI systems do not hold responsibility
- Human responders retain full override authority

Failing to override is a human decision.
Delaying override is a human decision.

Responsibility cannot be delegated to automation.

---

## Override Hesitation Standard

A judgment failure exists when:

- Conflicting operational signals are visible
- Override authority is available
- Human action is delayed or withheld due to trust, fear, or ambiguity

Describing AI behavior without examining human hesitation
does not meet this standard.

---

## Accountability Expectation

A valid analysis must:

- Identify where override authority resided
- Explain why it was not exercised
- Assign accountability to a role or decision owner

Diffuse responsibility or passive framing is insufficient.

---

## Post-Failure Control Expectation

After an override failure, acceptable responses focus on:

- Restricting unsafe AI influence
- Enforcing clearer human authority boundaries
- Reducing hesitation during future incidents

Retraining, tuning, or tooling changes alone
do not address the judgment failure evaluated here.

---

## Evaluation Boundary

This document defines how reasoning is evaluated.
It does not prescribe correct answers.

You are expected to apply these standards independently
to the incident presented in the lab.
