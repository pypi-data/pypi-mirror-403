id: post-incident-trust
title: "Decide Post-Incident Trust in an AI System After Verified Failure"
stack: aiops
difficulty: Hard
xp: 150

goal: "Decide whether an AI system remains safe to trust after causing verified operational harm"

skills:
  - post-incident-judgment
  - trust-evaluation
  - aiops-risk-ownership
  - decision-accountability

hint:
  - Assume the AI failure is confirmed and understood.
  - Focus on trust decisions, not future fixes.

validator: "validator.py"

mentor:
  why: >
    After an AI system causes real damage, the most difficult
    decision is not analysis but trust. Engineers must decide
    whether the system remains acceptable in production.
  intent: stretch
  guidance:
    before: |
      Consider how trust is restored or revoked after incidents.

      Think about:
      - What evidence is sufficient to resume trust
      - Whether partial trust is meaningful
      - How repeated exposure to risk compounds damage
    after: >
      Reflect on whether continued use of a known-risk system
      is itself an operational decision.
  tags:
    - aiops
    - trust-management
    - post-incident
    - reliability
    - safety

solution:
  overview: >
    This solution frames senior-level reasoning when deciding
    whether an AI system should remain trusted after causing
    operational harm. The focus is on explicit trust decisions
    and accountability for continued exposure to risk.

  professional_reasoning:
    - Treat trust as a deliberate operational choice
    - Assume known risk compounds over time
    - Reassert ownership over continued system use
    - Evaluate whether trust is defensible after harm

  real_world_context:
    - Do not propose trust-rebuilding steps or fixes
    - Do not redesign the AI system
    - Identify why continued trust may be unjustified
    - Assess ongoing exposure realistically
    - Favor restriction when trust cannot be defended

execution:
  runtime: docker
  requires_execution: false
  required_tools:
    - none
