# Operational Judgment Doctrine — Automation Bias in Escalation

This document defines the judgment standards applied in this lab.
It does not describe correct actions or expected conclusions.

---

## Purpose of This Lab

This lab evaluates how human responders exercise judgment
when automated guidance influences escalation decisions
during a production incident.

The focus is on authority, hesitation, and accountability —
not on improving AI systems or escalation tooling.

---

## Authority and Responsibility

In this environment:

- AI systems may provide advisory input
- AI systems do not hold escalation authority
- Human responders retain full responsibility for escalation decisions

Accepting or deferring to AI guidance is a human decision.
Failing to escalate is also a decision.

Responsibility cannot be delegated to automation.

---

## Automation Bias Judgment Standard

A judgment failure exists when:

- Conflicting operational signals are present
- Escalation authority exists
- Human responders delay action due to reliance on automated guidance

Describing AI behavior without examining human suppression
does not meet this standard.

---

## Accountability Expectation

A valid analysis must:

- Identify where escalation authority resided
- Explain why it was not exercised
- Assign accountability to a role or decision owner

Diffuse or passive responsibility is insufficient.

---

## Post-Incident Control Expectation

After escalation failure, acceptable responses focus on:

- Restricting AI influence over escalation decisions
- Reinforcing human authority boundaries
- Reducing hesitation in future incidents

Tooling improvements or retraining alone
do not address the judgment failure evaluated here.

---

## Evaluation Boundary

This document defines how reasoning is evaluated.
It does not prescribe correct answers.

You are expected to apply these standards independently
to the incident presented in the lab.
