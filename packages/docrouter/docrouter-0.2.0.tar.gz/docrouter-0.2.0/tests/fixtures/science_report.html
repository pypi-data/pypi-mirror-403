<!DOCTYPE html>
<html>
<head><title>Scientific Report</title></head>
<body>
<h1>Transformer Architecture Analysis</h1>

<p>The attention mechanism computes:</p>
<p>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V</p>

<h2>Performance Results</h2>
<table border="1">
<tr><th>Dataset</th><th>BLEU Score</th><th>Parameters</th></tr>
<tr><td>WMT 2014 EN-DE</td><td>28.4</td><td>65M</td></tr>
<tr><td>WMT 2014 EN-FR</td><td>41.0</td><td>213M</td></tr>
</table>

<h2>Mathematical Details</h2>
<p>The scaled dot-product attention uses scaling factor 1/sqrt(d_k) where d_k is the dimension of keys.</p>
<p>Multi-head attention: MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O</p>

<script>console.log('this should be stripped');</script>
<style>.hidden { display: none; }</style>
</body>
</html>