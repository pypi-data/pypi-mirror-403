EXPERIMENT LOG - Run #2847
Date: 2024-01-15
Author: Research Team

CONFIGURATION:
  Model: transformer-xl
  Hidden size: 1024
  Layers: 24
  Attention heads: 16

RESULTS:
  Epoch 1: loss=4.523, perplexity=92.1
  Epoch 2: loss=3.891, perplexity=48.9
  Epoch 3: loss=3.412, perplexity=30.3
  Epoch 4: loss=3.102, perplexity=22.2
  Epoch 5: loss=2.891, perplexity=18.0

NOTES:
Learning rate warmup over first 4000 steps proved effective.
Memory usage peaked at 24GB during training.
