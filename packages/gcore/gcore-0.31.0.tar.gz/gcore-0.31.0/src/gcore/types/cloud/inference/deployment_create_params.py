# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import Dict, Iterable, Optional
from typing_extensions import Required, Annotated, TypedDict

from ...._types import SequenceNotStr
from ...._utils import PropertyInfo
from ..laas_index_retention_policy_param import LaasIndexRetentionPolicyParam

__all__ = [
    "DeploymentCreateParams",
    "Container",
    "ContainerScale",
    "ContainerScaleTriggers",
    "ContainerScaleTriggersCPU",
    "ContainerScaleTriggersGPUMemory",
    "ContainerScaleTriggersGPUUtilization",
    "ContainerScaleTriggersHTTP",
    "ContainerScaleTriggersMemory",
    "ContainerScaleTriggersSqs",
    "IngressOpts",
    "Logging",
    "Probes",
    "ProbesLivenessProbe",
    "ProbesLivenessProbeProbe",
    "ProbesLivenessProbeProbeExec",
    "ProbesLivenessProbeProbeHTTPGet",
    "ProbesLivenessProbeProbeTcpSocket",
    "ProbesReadinessProbe",
    "ProbesReadinessProbeProbe",
    "ProbesReadinessProbeProbeExec",
    "ProbesReadinessProbeProbeHTTPGet",
    "ProbesReadinessProbeProbeTcpSocket",
    "ProbesStartupProbe",
    "ProbesStartupProbeProbe",
    "ProbesStartupProbeProbeExec",
    "ProbesStartupProbeProbeHTTPGet",
    "ProbesStartupProbeProbeTcpSocket",
]


class DeploymentCreateParams(TypedDict, total=False):
    project_id: int
    """Project ID"""

    containers: Required[Iterable[Container]]
    """List of containers for the inference instance."""

    flavor_name: Required[str]
    """Flavor name for the inference instance."""

    image: Required[str]
    """Docker image for the inference instance.

    This field should contain the image name and tag in the format 'name:tag', e.g.,
    'nginx:latest'. It defaults to Docker Hub as the image registry, but any
    accessible Docker image URL can be specified.
    """

    listening_port: Required[int]
    """Listening port for the inference instance."""

    name: Required[str]
    """Inference instance name."""

    api_keys: SequenceNotStr[str]
    """List of API keys for the inference instance.

    Multiple keys can be attached to one deployment.If `auth_enabled` and `api_keys`
    are both specified, a ValidationError will be raised.
    """

    auth_enabled: bool
    """Set to `true` to enable API key authentication for the inference instance.

    `"Authorization": "Bearer *****"` or `"X-Api-Key": "*****"` header is required
    for the requests to the instance if enabled. This field is deprecated and will
    be removed in the future. Use `api_keys` field instead.If `auth_enabled` and
    `api_keys` are both specified, a ValidationError will be raised.
    """

    command: Optional[SequenceNotStr[str]]
    """Command to be executed when running a container from an image."""

    credentials_name: Optional[str]
    """Registry credentials name"""

    description: Optional[str]
    """Inference instance description."""

    envs: Dict[str, str]
    """Environment variables for the inference instance."""

    ingress_opts: Optional[IngressOpts]
    """Ingress options for the inference instance"""

    logging: Optional[Logging]
    """Logging configuration for the inference instance"""

    probes: Optional[Probes]
    """Probes configured for all containers of the inference instance.

    If probes are not provided, and the `image_name` is from a the Model Catalog
    registry, the default probes will be used.
    """

    api_timeout: Annotated[Optional[int], PropertyInfo(alias="timeout")]
    """
    Specifies the duration in seconds without any requests after which the
    containers will be downscaled to their minimum scale value as defined by
    `scale.min`. If set, this helps in optimizing resource usage by reducing the
    number of container instances during periods of inactivity. The default value
    when the parameter is not set is 120.
    """


class ContainerScaleTriggersCPU(TypedDict, total=False):
    """CPU trigger configuration"""

    threshold: Required[int]
    """Threshold value for the trigger in percentage"""


class ContainerScaleTriggersGPUMemory(TypedDict, total=False):
    """GPU memory trigger configuration.

    Calculated by `DCGM_FI_DEV_MEM_COPY_UTIL` metric
    """

    threshold: Required[int]
    """Threshold value for the trigger in percentage"""


class ContainerScaleTriggersGPUUtilization(TypedDict, total=False):
    """GPU utilization trigger configuration.

    Calculated by `DCGM_FI_DEV_GPU_UTIL` metric
    """

    threshold: Required[int]
    """Threshold value for the trigger in percentage"""


class ContainerScaleTriggersHTTP(TypedDict, total=False):
    """HTTP trigger configuration"""

    rate: Required[int]
    """Request count per 'window' seconds for the http trigger"""

    window: Required[int]
    """Time window for rate calculation in seconds"""


class ContainerScaleTriggersMemory(TypedDict, total=False):
    """Memory trigger configuration"""

    threshold: Required[int]
    """Threshold value for the trigger in percentage"""


class ContainerScaleTriggersSqs(TypedDict, total=False):
    """SQS trigger configuration"""

    activation_queue_length: Required[int]
    """Number of messages for activation"""

    aws_region: Required[str]
    """AWS region"""

    queue_length: Required[int]
    """Number of messages for one replica"""

    queue_url: Required[str]
    """SQS queue URL"""

    secret_name: Required[str]
    """Auth secret name"""

    aws_endpoint: Optional[str]
    """Custom AWS endpoint"""

    scale_on_delayed: bool
    """Scale on delayed messages"""

    scale_on_flight: bool
    """Scale on in-flight messages"""


class ContainerScaleTriggers(TypedDict, total=False):
    """Triggers for scaling actions"""

    cpu: Optional[ContainerScaleTriggersCPU]
    """CPU trigger configuration"""

    gpu_memory: Optional[ContainerScaleTriggersGPUMemory]
    """GPU memory trigger configuration.

    Calculated by `DCGM_FI_DEV_MEM_COPY_UTIL` metric
    """

    gpu_utilization: Optional[ContainerScaleTriggersGPUUtilization]
    """GPU utilization trigger configuration.

    Calculated by `DCGM_FI_DEV_GPU_UTIL` metric
    """

    http: Optional[ContainerScaleTriggersHTTP]
    """HTTP trigger configuration"""

    memory: Optional[ContainerScaleTriggersMemory]
    """Memory trigger configuration"""

    sqs: Optional[ContainerScaleTriggersSqs]
    """SQS trigger configuration"""


class ContainerScale(TypedDict, total=False):
    """Scale for the container"""

    max: Required[int]
    """Maximum scale for the container"""

    min: Required[int]
    """Minimum scale for the container"""

    cooldown_period: Optional[int]
    """Cooldown period between scaling actions in seconds"""

    polling_interval: Optional[int]
    """Polling interval for scaling triggers in seconds"""

    triggers: ContainerScaleTriggers
    """Triggers for scaling actions"""


class Container(TypedDict, total=False):
    region_id: Required[int]
    """Region id for the container"""

    scale: Required[ContainerScale]
    """Scale for the container"""


class IngressOpts(TypedDict, total=False):
    """Ingress options for the inference instance"""

    disable_response_buffering: bool
    """Disable response buffering if true.

    A client usually has a much slower connection and can not consume the response
    data as fast as it is produced by an upstream application. Ingress tries to
    buffer the whole response in order to release the upstream application as soon
    as possible.By default, the response buffering is enabled.
    """


class Logging(TypedDict, total=False):
    """Logging configuration for the inference instance"""

    destination_region_id: Optional[int]
    """ID of the region in which the logs will be stored"""

    enabled: bool
    """Enable or disable log streaming"""

    retention_policy: Optional[LaasIndexRetentionPolicyParam]
    """Logs retention policy"""

    topic_name: Optional[str]
    """The topic name to stream logs to"""


class ProbesLivenessProbeProbeExec(TypedDict, total=False):
    """Exec probe configuration"""

    command: Required[SequenceNotStr[str]]
    """Command to be executed inside the running container."""


class ProbesLivenessProbeProbeHTTPGet(TypedDict, total=False):
    """HTTP GET probe configuration"""

    port: Required[int]
    """Port number the probe should connect to."""

    headers: Dict[str, str]
    """HTTP headers to be sent with the request."""

    host: Optional[str]
    """Host name to send HTTP request to."""

    path: str
    """The endpoint to send the HTTP request to."""

    schema: str
    """Schema to use for the HTTP request."""


class ProbesLivenessProbeProbeTcpSocket(TypedDict, total=False):
    """TCP socket probe configuration"""

    port: Required[int]
    """Port number to check if it's open."""


class ProbesLivenessProbeProbe(TypedDict, total=False):
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""

    exec: Optional[ProbesLivenessProbeProbeExec]
    """Exec probe configuration"""

    failure_threshold: int
    """The number of consecutive probe failures that mark the container as unhealthy."""

    http_get: Optional[ProbesLivenessProbeProbeHTTPGet]
    """HTTP GET probe configuration"""

    initial_delay_seconds: int
    """The initial delay before starting the first probe."""

    period_seconds: int
    """How often (in seconds) to perform the probe."""

    success_threshold: int
    """The number of consecutive successful probes that mark the container as healthy."""

    tcp_socket: Optional[ProbesLivenessProbeProbeTcpSocket]
    """TCP socket probe configuration"""

    timeout_seconds: int
    """The timeout for each probe."""


class ProbesLivenessProbe(TypedDict, total=False):
    """Liveness probe configuration"""

    enabled: Required[bool]
    """Whether the probe is enabled or not."""

    probe: ProbesLivenessProbeProbe
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""


class ProbesReadinessProbeProbeExec(TypedDict, total=False):
    """Exec probe configuration"""

    command: Required[SequenceNotStr[str]]
    """Command to be executed inside the running container."""


class ProbesReadinessProbeProbeHTTPGet(TypedDict, total=False):
    """HTTP GET probe configuration"""

    port: Required[int]
    """Port number the probe should connect to."""

    headers: Dict[str, str]
    """HTTP headers to be sent with the request."""

    host: Optional[str]
    """Host name to send HTTP request to."""

    path: str
    """The endpoint to send the HTTP request to."""

    schema: str
    """Schema to use for the HTTP request."""


class ProbesReadinessProbeProbeTcpSocket(TypedDict, total=False):
    """TCP socket probe configuration"""

    port: Required[int]
    """Port number to check if it's open."""


class ProbesReadinessProbeProbe(TypedDict, total=False):
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""

    exec: Optional[ProbesReadinessProbeProbeExec]
    """Exec probe configuration"""

    failure_threshold: int
    """The number of consecutive probe failures that mark the container as unhealthy."""

    http_get: Optional[ProbesReadinessProbeProbeHTTPGet]
    """HTTP GET probe configuration"""

    initial_delay_seconds: int
    """The initial delay before starting the first probe."""

    period_seconds: int
    """How often (in seconds) to perform the probe."""

    success_threshold: int
    """The number of consecutive successful probes that mark the container as healthy."""

    tcp_socket: Optional[ProbesReadinessProbeProbeTcpSocket]
    """TCP socket probe configuration"""

    timeout_seconds: int
    """The timeout for each probe."""


class ProbesReadinessProbe(TypedDict, total=False):
    """Readiness probe configuration"""

    enabled: Required[bool]
    """Whether the probe is enabled or not."""

    probe: ProbesReadinessProbeProbe
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""


class ProbesStartupProbeProbeExec(TypedDict, total=False):
    """Exec probe configuration"""

    command: Required[SequenceNotStr[str]]
    """Command to be executed inside the running container."""


class ProbesStartupProbeProbeHTTPGet(TypedDict, total=False):
    """HTTP GET probe configuration"""

    port: Required[int]
    """Port number the probe should connect to."""

    headers: Dict[str, str]
    """HTTP headers to be sent with the request."""

    host: Optional[str]
    """Host name to send HTTP request to."""

    path: str
    """The endpoint to send the HTTP request to."""

    schema: str
    """Schema to use for the HTTP request."""


class ProbesStartupProbeProbeTcpSocket(TypedDict, total=False):
    """TCP socket probe configuration"""

    port: Required[int]
    """Port number to check if it's open."""


class ProbesStartupProbeProbe(TypedDict, total=False):
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""

    exec: Optional[ProbesStartupProbeProbeExec]
    """Exec probe configuration"""

    failure_threshold: int
    """The number of consecutive probe failures that mark the container as unhealthy."""

    http_get: Optional[ProbesStartupProbeProbeHTTPGet]
    """HTTP GET probe configuration"""

    initial_delay_seconds: int
    """The initial delay before starting the first probe."""

    period_seconds: int
    """How often (in seconds) to perform the probe."""

    success_threshold: int
    """The number of consecutive successful probes that mark the container as healthy."""

    tcp_socket: Optional[ProbesStartupProbeProbeTcpSocket]
    """TCP socket probe configuration"""

    timeout_seconds: int
    """The timeout for each probe."""


class ProbesStartupProbe(TypedDict, total=False):
    """Startup probe configuration"""

    enabled: Required[bool]
    """Whether the probe is enabled or not."""

    probe: ProbesStartupProbeProbe
    """Probe configuration (exec, `http_get` or `tcp_socket`)"""


class Probes(TypedDict, total=False):
    """Probes configured for all containers of the inference instance.

    If probes are not provided, and the `image_name` is from a the Model Catalog registry, the default probes will be used.
    """

    liveness_probe: Optional[ProbesLivenessProbe]
    """Liveness probe configuration"""

    readiness_probe: Optional[ProbesReadinessProbe]
    """Readiness probe configuration"""

    startup_probe: Optional[ProbesStartupProbe]
    """Startup probe configuration"""
