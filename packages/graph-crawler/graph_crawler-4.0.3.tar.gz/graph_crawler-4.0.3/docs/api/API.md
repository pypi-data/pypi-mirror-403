# API Reference (–ü—É–±–ª—ñ—á–Ω–µ API)

> **–¶—ñ–ª—å–æ–≤–∞ –∞—É–¥–∏—Ç–æ—Ä—ñ—è:** –í—Å—ñ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∏, –≤—ñ–¥ Junior –¥–æ Senior

## –û–≥–ª—è–¥

```
graph_crawler module
‚îú‚îÄ‚îÄ crawl()           - –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥ (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
‚îú‚îÄ‚îÄ async_crawl()     - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥
‚îú‚îÄ‚îÄ crawl_sitemap()   - –ö—Ä–∞—É–ª—ñ–Ω–≥ —á–µ—Ä–µ–∑ sitemap.xml
‚îú‚îÄ‚îÄ Crawler           - Reusable —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª–µ—Ä
‚îî‚îÄ‚îÄ AsyncCrawler      - Reusable –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª–µ—Ä
```

---

## crawl()

```python
def crawl(
    url: Optional[str] = None,
    *,
    seed_urls: Optional[list[str]] = None,
    base_graph: Optional[Graph] = None,

    # –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    max_depth: int = 3,
    max_pages: Optional[int] = 100,
    same_domain: bool = True,
    timeout: Optional[int] = None,
    request_delay: float = 0.5,

    # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
    driver: Optional[str | IDriver] = None,
    driver_config: Optional[dict] = None,
    storage: Optional[str | IStorage] = None,
    storage_config: Optional[dict] = None,
    plugins: Optional[list[BaseNodePlugin]] = None,

    # –ö–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—è
    node_class: Optional[type[Node]] = None,
    edge_class: Optional[type[Edge]] = None,
    url_rules: Optional[list[URLRule]] = None,
    edge_strategy: str = "all",

    # Callbacks
    on_progress: Optional[Callable] = None,
    on_node_scanned: Optional[Callable] = None,
    on_error: Optional[Callable] = None,
    on_completed: Optional[Callable] = None,

    # Distributed mode
    wrapper: Optional[dict] = None,
) -> Graph
```

–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥ –≤–µ–±-—Å–∞–π—Ç—É - **–ø—Ä–æ—Å—Ç–∏–π —è–∫ requests**.

üÜï **NEW in v3.2.0:** –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö —Ç–æ—á–æ–∫ –≤—Ö–æ–¥—É (`seed_urls`) —Ç–∞ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∫—Ä–∞—É–ª—ñ–Ω–≥—É (`base_graph`)!

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä          | –¢–∏–ø          | Default  | –û–ø–∏—Å                                                    |
| ----------------- | ------------ | -------- | ------------------------------------------------------- |
| `url`             | str          | None     | URL –¥–ª—è –ø–æ—á–∞—Ç–∫—É –∫—Ä–∞—É–ª—ñ–Ω–≥—É (—è–∫—â–æ seed_urls –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ)  |
| `seed_urls` üÜï    | list[str]    | None     | –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–æ—á–∞—Ç–∫—É –∫—Ä–∞—É–ª—ñ–Ω–≥—É (–º–Ω–æ–∂–∏–Ω–Ω—ñ —Ç–æ—á–∫–∏ –≤—Ö–æ–¥—É) |
| `base_graph` üÜï   | Graph        | None     | –Ü—Å–Ω—É—é—á–∏–π –≥—Ä–∞—Ñ –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –∫—Ä–∞—É–ª—ñ–Ω–≥—É (incremental)   |
| `max_depth`       | int          | 3        | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ –æ–±—Ö–æ–¥—É                              |
| `max_pages`       | int          | 100      | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫                          |
| `same_domain`     | bool         | True     | –°–∫–∞–Ω—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç–æ—á–Ω–∏–π –¥–æ–º–µ–Ω                         |
| `timeout`         | int          | None     | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —á–∞—Å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö                             |
| `request_delay`   | float        | 0.5      | –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏                                   |
| `driver`          | str/IDriver  | "http"   | "http", "async", "playwright", "stealth"                |
| `driver_config`   | dict         | None     | –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥—Ä–∞–π–≤–µ—Ä–∞                                   |
| `storage`         | str/IStorage | "memory" | "memory", "json", "sqlite", "postgresql"                |
| `storage_config`  | dict         | None     | –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è storage                                    |
| `plugins`         | list         | default  | –°–ø–∏—Å–æ–∫ –ø–ª–∞–≥—ñ–Ω—ñ–≤                                         |
| `node_class`      | type         | Node     | –ö–∞—Å—Ç–æ–º–Ω–∏–π –∫–ª–∞—Å Node                                     |
| `edge_class`      | type         | Edge     | –ö–∞—Å—Ç–æ–º–Ω–∏–π –∫–ª–∞—Å Edge                                     |
| `url_rules`       | list         | None     | –°–ø–∏—Å–æ–∫ URLRule                                          |
| `edge_strategy`   | str          | "all"    | "all", "new_only", "max_in_degree", "deeper_only"       |
| `on_progress`     | Callable     | None     | Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É                                   |
| `on_node_scanned` | Callable     | None     | Callback –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –Ω–æ–¥–∏                          |
| `on_error`        | Callable     | None     | Callback –¥–ª—è –ø–æ–º–∏–ª–æ–∫                                    |
| `on_completed`    | Callable     | None     | Callback –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è                               |
| `wrapper`         | dict         | None     | –ö–æ–Ω—Ñ—ñ–≥ distributed crawling                             |

**–ü–æ–≤–µ—Ä—Ç–∞—î:** `Graph`

**–ü—Ä–∏–∫–ª–∞–¥–∏:**

```python
import graph_crawler as gc

# –ë–∞–∑–æ–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
graph = gc.crawl("https://example.com")

# üÜï NEW: –ú–Ω–æ–∂–∏–Ω–Ω—ñ —Ç–æ—á–∫–∏ –≤—Ö–æ–¥—É
graph = gc.crawl(
    seed_urls=[
        "https://www.work.ua/jobs/by-category/it/",
        "https://www.work.ua/jobs/by-category/marketing/",
        "https://www.work.ua/jobs/by-category/sales/",
    ],
    max_depth=3
)

# üÜï NEW: –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∫—Ä–∞—É–ª—ñ–Ω–≥—É
graph1 = gc.crawl("https://example.com", max_pages=50)
# ... –∑–±–µ—Ä–µ–≥—Ç–∏ –≥—Ä–∞—Ñ ...
# –ü—ñ–∑–Ω—ñ—à–µ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏
graph2 = gc.crawl(base_graph=graph1, max_pages=100)

# üÜï NEW: –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –æ–±–æ—Ö
sitemap_graph = gc.crawl_sitemap("https://example.com")
# –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –Ω–æ–¥–∏...
result = gc.crawl(
    base_graph=filtered_graph,
    seed_urls=["https://example.com/new-section"],
    max_pages=100
)

# –ó –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
graph = gc.crawl(
    "https://example.com",
    max_depth=5,
    max_pages=200,
    driver="playwright"
)

# Distributed —Ä–µ–∂–∏–º
config = {
    "broker": {"type": "redis", "host": "server.com", "port": 6379},
    "database": {"type": "mongodb", "host": "server.com", "port": 27017}
}
graph = gc.crawl("https://example.com", wrapper=config)
```

---

## async_crawl()

```python
async def async_crawl(
    url: str,
    *,
    # ... —Ç—ñ —Å–∞–º—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —â–æ —ñ crawl() (–±–µ–∑ wrapper)
) -> Graph
```

Async –≤–µ—Ä—Å—ñ—è crawl() –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ.

**–ü—Ä–∏–∫–ª–∞–¥–∏:**

```python
import asyncio
import graph_crawler as gc

# –ë–∞–∑–æ–≤–µ
graph = await gc.async_crawl("https://example.com")

# –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥
graphs = await asyncio.gather(
    gc.async_crawl("https://site1.com"),
    gc.async_crawl("https://site2.com"),
)
```

---

## crawl_sitemap()

```python
def crawl_sitemap(
    url: str,
    *,
    max_urls: Optional[int] = None,
    include_urls: bool = True,
    timeout: Optional[int] = None,
    driver: Optional[str | IDriver] = None,
    driver_config: Optional[dict] = None,
    storage: Optional[str | IStorage] = None,
    storage_config: Optional[dict] = None,
    wrapper: Optional[dict] = None,
    on_progress: Optional[Callable] = None,
    on_error: Optional[Callable] = None,
    on_completed: Optional[Callable] = None,
) -> Graph
```

–ö—Ä–∞—É–ª—ñ–Ω–≥ —á–µ—Ä–µ–∑ sitemap.xml - –ø–∞—Ä—Å–∏—Ç—å robots.txt ‚Üí –∑–Ω–∞—Ö–æ–¥–∏—Ç—å sitemap ‚Üí –æ–±—Ä–æ–±–ª—è—î –≤—Å—ñ URL.

**–ü—Ä–∏–∫–ª–∞–¥–∏:**

```python
# –ë–∞–∑–æ–≤–µ
graph = gc.crawl_sitemap("https://example.com")

# –¢—ñ–ª—å–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–±–µ–∑ –∫—ñ–Ω—Ü–µ–≤–∏—Ö URL)
graph = gc.crawl_sitemap("https://example.com", include_urls=False)

# –ó –ª—ñ–º—ñ—Ç–æ–º
graph = gc.crawl_sitemap("https://example.com", max_urls=1000)
```

---

## Crawler

```python
class Crawler:
    def __init__(
        self,
        *,
        max_depth: int = 3,
        max_pages: Optional[int] = 100,
        same_domain: bool = True,
        request_delay: float = 0.5,
        driver: Optional[str | IDriver] = None,
        driver_config: Optional[dict] = None,
        storage: Optional[str | IStorage] = None,
        storage_config: Optional[dict] = None,
        plugins: Optional[list[BaseNodePlugin]] = None,
        node_class: Optional[type[Node]] = None,
        edge_strategy: str = "all",
    ): ...

    def crawl(self, url: str, **kwargs) -> Graph: ...
    def close(self) -> None: ...
    def __enter__(self) -> Crawler: ...
    def __exit__(self, ...): ...
```

Reusable —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫—Ä–∞—É–ª–µ—Ä.

**–ü—Ä–∏–∫–ª–∞–¥:**

```python
with gc.Crawler(max_depth=5) as crawler:
    graph1 = crawler.crawl("https://site1.com")
    graph2 = crawler.crawl("https://site2.com")
```

---

## AsyncCrawler

```python
class AsyncCrawler:
    async def crawl(self, url: str, **kwargs) -> Graph: ...
    async def close(self) -> None: ...
    async def __aenter__(self) -> AsyncCrawler: ...
    async def __aexit__(self, ...): ...
```

Async –≤–µ—Ä—Å—ñ—è Crawler.

**–ü—Ä–∏–∫–ª–∞–¥:**

```python
async with gc.AsyncCrawler() as crawler:
    graphs = await asyncio.gather(
        crawler.crawl("https://site1.com"),
        crawler.crawl("https://site2.com"),
    )
```

---

## Graph

```python
class Graph:
    nodes: Dict[str, Node]       # {node_id: Node}
    edges: List[Edge]            # –°–ø–∏—Å–æ–∫ —Ä–µ–±–µ—Ä

    def add_node(self, node: Node, overwrite: bool = False) -> Node: ...
    def add_edge(self, edge: Edge) -> Edge: ...
    def get_node_by_url(self, url: str) -> Optional[Node]: ...
    def get_node_by_id(self, node_id: str) -> Optional[Node]: ...
    def has_edge(self, source_id: str, target_id: str) -> bool: ...
    def remove_node(self, node_id: str) -> bool: ...
    def get_stats(self) -> Dict[str, int]: ...
    def copy(self) -> Graph: ...
    def clear(self) -> None: ...

    # Edge Analysis
    def get_popular_nodes(self, top_n: int = 10, by: str = 'in_degree') -> List[Node]: ...
    def get_edge_statistics(self) -> Dict[str, Any]: ...
    def find_cycles(self, max_cycles: Optional[int] = None) -> List[List[str]]: ...
    def export_edges(self, filepath: str, format: str = 'json') -> Any: ...

    # –û–ø–µ—Ä–∞—Ü—ñ—ó
    def __add__(self, other: Graph) -> Graph: ...  # union
    def __sub__(self, other: Graph) -> Graph: ...  # difference
    def __and__(self, other: Graph) -> Graph: ...  # intersection
    def __or__(self, other: Graph) -> Graph: ...   # union
    def __xor__(self, other: Graph) -> Graph: ...  # symmetric_difference

    # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    def __eq__(self, other: Graph) -> bool: ...
    def __lt__(self, other: Graph) -> bool: ...    # is_subgraph (strict)
    def __le__(self, other: Graph) -> bool: ...    # is_subgraph
    def __gt__(self, other: Graph) -> bool: ...    # is_supergraph (strict)
    def __ge__(self, other: Graph) -> bool: ...    # is_supergraph

    # –ö–æ–ª–µ–∫—Ü—ñ–π–Ω—ñ
    def __len__(self) -> int: ...
    def __iter__(self) -> Iterator[Node]: ...
    def __contains__(self, item: str | Node) -> bool: ...
    def __getitem__(self, key: str | int) -> Node: ...
```

---

## Node

```python
class Node(BaseModel):
    url: str
    node_id: str = Field(default_factory=uuid4)  # UUID (auto-generated)
    depth: int = 0
    scanned: bool = False
    should_scan: bool = True
    can_create_edges: bool = True
    priority: Optional[int] = None    # 1-10, None = default
    metadata: Dict[str, Any] = {}
    user_data: Dict[str, Any] = {}
    content_hash: Optional[str] = None
    response_status: Optional[int] = None
    created_at: datetime = Field(default_factory=datetime.now)
    lifecycle_stage: NodeLifecycle = NodeLifecycle.URL_STAGE

    async def process_html(self, html: str) -> List[str]: ...
    def mark_as_scanned(self) -> None: ...
    def get_content_hash(self) -> str: ...

    # Metadata helpers (Law of Demeter)
    def get_title(self) -> Optional[str]: ...
    def get_description(self) -> Optional[str]: ...
    def get_h1(self) -> Optional[str]: ...
    def get_keywords(self) -> Optional[str]: ...
    def get_meta_value(self, key: str, default: Any = None) -> Any: ...
```

---

## Edge

```python
class Edge(BaseModel):
    source_node_id: str
    target_node_id: str
    edge_id: str              # UUID (auto-generated)
    metadata: Dict[str, Any] = {}

    def add_metadata(self, key: str, value: Any) -> None: ...
    def get_meta_value(self, key: str, default: Any = None) -> Any: ...
```

---

## URLRule

```python
class URLRule(BaseModel):
    pattern: str                              # Regex –ø–∞—Ç–µ—Ä–Ω
    priority: int = 5                         # 1-10 (default: 5)
    should_scan: Optional[bool] = None        # –ü–µ—Ä–µ–±–∏–≤–∞—î —Ñ—ñ–ª—å—Ç—Ä–∏
    should_follow_links: Optional[bool] = None
    create_edge: Optional[bool] = None
```

---

## BaseNodePlugin

```python
class BaseNodePlugin(ABC):
    @property
    @abstractmethod
    def plugin_type(self) -> NodePluginType: ...

    @property
    @abstractmethod
    def name(self) -> str: ...

    @abstractmethod
    def execute(self, context: NodePluginContext) -> NodePluginContext: ...

    def setup(self) -> None: ...
    def teardown(self) -> None: ...
```

**NodePluginType:**

- `ON_NODE_CREATED` - –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è Node (–ï–¢–ê–ü 1: URL)
- `ON_BEFORE_SCAN` - –ø–µ—Ä–µ–¥ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è–º (–ï–¢–ê–ü 2: HTML)
- `ON_HTML_PARSED` - –ø—ñ—Å–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É HTML (–ï–¢–ê–ü 2: HTML)
- `ON_AFTER_SCAN` - –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è (–ï–¢–ê–ü 2: HTML)
- `BEFORE_CRAWL` - –ø–µ—Ä–µ–¥ –ø–æ—á–∞—Ç–∫–æ–º –∫—Ä–∞—É–ª—ñ–Ω–≥—É (–ï–¢–ê–ü 3: CRAWL)
- `AFTER_CRAWL` - –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –∫—Ä–∞—É–ª—ñ–Ω–≥—É (–ï–¢–ê–ü 3: CRAWL)

**NodePluginContext:**

```python
@dataclass
class NodePluginContext:
    node: Any                           # Node –æ–±'—î–∫—Ç
    url: str
    depth: int
    should_scan: bool
    can_create_edges: bool
    html: Optional[str] = None          # –¢—ñ–ª—å–∫–∏ –ï–¢–ê–ü 2
    html_tree: Optional[Any] = None     # –¢—ñ–ª—å–∫–∏ –ï–¢–ê–ü 2
    parser: Optional[Any] = None        # –¢—ñ–ª—å–∫–∏ –ï–¢–ê–ü 2
    metadata: Dict[str, Any] = {}
    extracted_links: List[str] = []
    user_data: Dict[str, Any] = {}
    skip_link_extraction: bool = False
    skip_metadata_extraction: bool = False
```
