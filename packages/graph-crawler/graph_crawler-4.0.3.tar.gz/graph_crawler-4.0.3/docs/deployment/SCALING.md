# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è GraphCrawler

> **–§—ñ–ª–æ—Å–æ—Ñ—ñ—è:** –û–¥–∏–Ω –∫–æ–Ω—Ñ—ñ–≥, –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —á–µ—Ä–µ–∑ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É  
> **–í–µ—Ä—Å—ñ—è:** 3.2.0

---

## –ü—Ä–∏–Ω—Ü–∏–ø–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

### ‚úÖ –©–æ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è

1. **–ö—ñ–ª—å–∫—ñ—Å—Ç—å workers** (1 ‚Üí 10 ‚Üí 100 ‚Üí 1000)
2. **–ü–æ—Ç—É–∂–Ω—ñ—Å—Ç—å –ë–î** (memory ‚Üí MongoDB ‚Üí MongoDB Cluster)
3. **–ü–æ—Ç—É–∂–Ω—ñ—Å—Ç—å Broker** (Redis ‚Üí Redis Cluster)
4. **RAM —Ç–∞ CPU** –Ω–∞ worker
5. **–ú–µ—Ä–µ–∂–µ–≤–∞ –ø—Ä–æ–ø—É—Å–∫–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å**

### ‚ùå –©–æ –ù–ï –∑–º—ñ–Ω—é—î—Ç—å—Å—è

1. **–í–∞—à Python –∫–æ–¥** - –∑–∞–≤–∂–¥–∏ –æ–¥–Ω–∞–∫–æ–≤–∏–π!
2. **API –≤–∏–∫–ª–∏–∫–∏** - `gc.crawl(..., wrapper=config)`
3. **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ñ—ñ–≥—É** - —Ç–æ–π —Å–∞–º–∏–π dict
4. **–õ–æ–≥—ñ–∫–∞ –∫—Ä–∞—É–ª—ñ–Ω–≥—É** - –Ω–µ–∑–º—ñ–Ω–Ω–∞

---

## –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–µ vs –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

### –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–µ (–±—ñ–ª—å—à–∏–π —Å–µ—Ä–≤–µ—Ä)

```yaml
# –û–¥–∏–Ω –ø–æ—Ç—É–∂–Ω–∏–π worker
worker:
  replicas: 1
  deploy:
    resources:
      limits:
        memory: 32G   # –ë–∞–≥–∞—Ç–æ RAM
        cpus: '16'    # –ë–∞–≥–∞—Ç–æ CPU
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ü—Ä–æ—Å—Ç—ñ—à–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- ‚úÖ –ú–µ–Ω—à–µ overhead
- ‚úÖ –®–≤–∏–¥—à–∞ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è –≤ –º–µ–∂–∞—Ö –ø—Ä–æ—Ü–µ—Å—É

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –û–±–º–µ–∂–µ–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Å–µ—Ä–≤–µ—Ä–∞
- ‚ùå Single point of failure
- ‚ùå –î–æ—Ä–æ–∂—á–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–ª—ñ —Ç–∞ —Å–µ—Ä–µ–¥–Ω—ñ –ø—Ä–æ–µ–∫—Ç–∏
- –û–±–º–µ–∂–µ–Ω–∏–π –±—é–¥–∂–µ—Ç –Ω–∞ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
- –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å

---

### –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ (–±—ñ–ª—å—à–µ —Å–µ—Ä–≤–µ—Ä—ñ–≤)

```yaml
# –ë–∞–≥–∞—Ç–æ –º–∞–ª–∏—Ö workers
worker:
  replicas: 100   # 100 workers
  deploy:
    resources:
      limits:
        memory: 2G    # –ú–µ–Ω—à–µ RAM
        cpus: '2'     # –ú–µ–Ω—à–µ CPU
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ù–µ–æ–±–º–µ–∂–µ–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
- ‚úÖ Fault tolerance (–æ–¥–∏–Ω –ø–∞–¥–∞—î - —ñ–Ω—à—ñ –ø—Ä–∞—Ü—é—é—Ç—å)
- ‚úÖ –î–µ—à–µ–≤—à–µ (commodity hardware)
- ‚úÖ Cloud-friendly

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –°–∫–ª–∞–¥–Ω—ñ—à–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- ‚ùå –ë—ñ–ª—å—à–µ overhead (–º–µ—Ä–µ–∂–∞, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è)
- ‚ùå –ü–æ—Ç—Ä—ñ–±–Ω–∏–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä (Kubernetes)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –í–µ–ª–∏–∫—ñ –ø—Ä–æ–µ–∫—Ç–∏ (100+ —Å–∞–π—Ç—ñ–≤)
- –ö—Ä–∏—Ç–∏—á–Ω–∞ –≤—ñ–¥–º–æ–≤–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å
- Cloud deployment
- Enterprise –º–∞—Å—à—Ç–∞–±

---

## –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Ä–µ—Å—É—Ä—Å—ñ–≤

### –§–æ—Ä–º—É–ª–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É

```python
# –ë–∞–∑–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
pages_per_site = 1000
num_sites = 100
total_pages = pages_per_site * num_sites  # 100,000

# Workers
pages_per_hour_per_worker = 500  # HTTP driver
workers_needed = total_pages / pages_per_hour_per_worker / hours_available

# RAM –Ω–∞ worker
ram_per_worker_http = 1 * 1024  # 1 GB –¥–ª—è HTTP
ram_per_worker_playwright = 4 * 1024  # 4 GB –¥–ª—è Playwright

# Redis
avg_url_size = 200  # bytes
redis_memory = (total_pages * avg_url_size) / (1024**3)  # GB

# MongoDB
avg_page_size = 50 * 1024  # 50 KB –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É
mongodb_storage = (total_pages * avg_page_size) / (1024**3)  # GB
```

### –ü—Ä–∏–∫–ª–∞–¥–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—ñ–≤

#### –°—Ü–µ–Ω–∞—Ä—ñ–π 1: 10 —Å–∞–π—Ç—ñ–≤ √ó 1,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫ = 10,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫

```python
# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
total_pages = 10_000
hours_available = 2  # –•–æ—á—É –∑–∞–≤–µ—Ä—à–∏—Ç–∏ –∑–∞ 2 –≥–æ–¥–∏–Ω–∏

# Workers
workers = 10_000 / 500 / 2 = 10 workers

# RAM
total_ram_workers = 10 √ó 1 GB = 10 GB

# Redis
redis_memory = 10_000 √ó 200 / (1024^3) ‚âà 0.002 GB ‚Üí 1 GB –¥–æ—Å—Ç–∞—Ç–Ω—å–æ

# MongoDB
mongodb_storage = 10_000 √ó 50 KB / (1024^3) ‚âà 0.5 GB ‚Üí 1 GB –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
```

**–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:**
```yaml
redis:
  command: redis-server --maxmemory 1gb

worker:
  replicas: 10
  resources:
    limits:
      memory: 1G

mongodb:
  resources:
    limits:
      memory: 2G
      storage: 5G
```

---

#### –°—Ü–µ–Ω–∞—Ä—ñ–π 2: 100 —Å–∞–π—Ç—ñ–≤ √ó 10,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫ = 1,000,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫

```python
# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
total_pages = 1_000_000
hours_available = 10

# Workers
workers = 1_000_000 / 500 / 10 = 200 workers

# RAM
total_ram_workers = 200 √ó 2 GB = 400 GB

# Redis
redis_memory = 1_000_000 √ó 200 / (1024^3) ‚âà 0.2 GB ‚Üí 2 GB

# MongoDB
mongodb_storage = 1_000_000 √ó 50 KB / (1024^3) ‚âà 50 GB
```

**–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:**
```yaml
redis:
  command: redis-server --maxmemory 4gb

worker:
  replicas: 200
  resources:
    limits:
      memory: 2G

mongodb:
  replicas: 3  # Replica Set
  resources:
    limits:
      memory: 32G
      storage: 200G  # –ó –∑–∞–ø–∞—Å–æ–º
```

---

#### –°—Ü–µ–Ω–∞—Ä—ñ–π 3: 10,000 —Å–∞–π—Ç—ñ–≤ √ó 10,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫ = 100,000,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫

```python
# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
total_pages = 100_000_000
hours_available = 24 * 7  # –¢–∏–∂–¥–µ–Ω—å

# Workers
workers = 100_000_000 / 500 / 168 = 1,190 workers ‚Üí 1,200 workers

# RAM
total_ram_workers = 1,200 √ó 2 GB = 2,400 GB = 2.4 TB

# Redis (Cluster –ø–æ—Ç—Ä—ñ–±–µ–Ω!)
redis_memory = 100_000_000 √ó 200 / (1024^3) ‚âà 20 GB ‚Üí Redis Cluster

# MongoDB (Sharded Cluster)
mongodb_storage = 100_000_000 √ó 50 KB / (1024^3) ‚âà 5,000 GB = 5 TB
```

**–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:**
```yaml
redis:
  type: RedisCluster
  shards: 10
  replicas: 3
  resources:
    limits:
      memory: 8Gi

worker:
  replicas: 1200
  autoscaling:
    enabled: true
    minReplicas: 500
    maxReplicas: 2000
  resources:
    limits:
      memory: 2Gi

mongodb:
  type: ShardedCluster
  shards: 20
  replicas: 3
  resources:
    limits:
      memory: 64Gi
      storage: 1Ti
```

---

## Bloom Filter —Ç–∞ –ø–∞–º'—è—Ç—å

### –ö–æ–¥ –∑ scheduler.py

```python
# package_crawler/package_crawler/scheduler.py
bloom_capacity = 10_000_000  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 10M URLs
bloom_error_rate = 0.001     # 0.1% false positive

# –ï–∫–æ–Ω–æ–º—ñ—è –ø–∞–º'—è—Ç—ñ:
# 10M URLs –≤ Python set:  ~800 MB
# 10M URLs –≤ Bloom Filter: ~12 MB
# –ï–∫–æ–Ω–æ–º—ñ—è: 67x
```

### –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è Bloom Filter

```python
# –î–ª—è 100M URLs
bloom_capacity = 100_000_000  # 100M
# –ü–∞–º'—è—Ç—å: ~120 MB (–∑–∞–º—ñ—Å—Ç—å ~8 GB –¥–ª—è set!)

# –î–ª—è 1B URLs
bloom_capacity = 1_000_000_000  # 1B
# –ü–∞–º'—è—Ç—å: ~1.2 GB (–∑–∞–º—ñ—Å—Ç—å ~80 GB!)
```

**Bloom Filter –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è!** –ü—Ä–æ—Å—Ç–æ –ø—Ä–∞—Ü—é—î! üéâ

---

## Celery –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

### Worker Prefetch Multiplier

```python
# celery_unified.py
worker_prefetch_multiplier = 4  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
```

**–©–æ —Ü–µ –æ–∑–Ω–∞—á–∞—î:**
- –ö–æ–∂–µ–Ω worker –±–µ—Ä–µ 4 –∑–∞–¥–∞—á—ñ –æ–¥–Ω–æ—á–∞—Å–Ω–æ
- 100 workers √ó 4 = 400 –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ

**–ö–æ–ª–∏ –∑–º—ñ–Ω—é–≤–∞—Ç–∏:**

```python
# –ë–∞–≥–∞—Ç–æ workers (100+) - –∑–º–µ–Ω—à–∏—Ç–∏
worker_prefetch_multiplier = 2
# 100 workers √ó 2 = 200 –∑–∞–¥–∞—á
# –ö—Ä–∞—â–µ —Ä–æ–∑–ø–æ–¥—ñ–ª –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

# –ú–∞–ª–æ workers (1-10) - –∑–±—ñ–ª—å—à–∏—Ç–∏
worker_prefetch_multiplier = 8
# 10 workers √ó 8 = 80 –∑–∞–¥–∞—á
# –ú–µ–Ω—à–µ idle —á–∞—Å—É
```

**–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:**
```bash
celery -A package_crawler.infrastructure.messaging.celery_unified worker \
  --prefetch-multiplier=2
```

---

### Worker Max Tasks Per Child

```python
# celery_unified.py
worker_max_tasks_per_child = 100
```

**–©–æ —Ü–µ –æ–∑–Ω–∞—á–∞—î:**
- Worker –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è 100 –∑–∞–¥–∞—á
- –ó–≤—ñ–ª—å–Ω—è—î –ø–∞–º'—è—Ç—å –≤—ñ–¥ leaks
- –ó–∞–ø–æ–±—ñ–≥–∞—î memory bloat

**–ö–æ–ª–∏ –∑–º—ñ–Ω—é–≤–∞—Ç–∏:**

```python
# Memory leaks - –∑–º–µ–Ω—à–∏—Ç–∏
worker_max_tasks_per_child = 50
# –ß–∞—Å—Ç—ñ—à–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫

# –°—Ç–∞–±—ñ–ª—å–Ω–∞ –ø–∞–º'—è—Ç—å - –∑–±—ñ–ª—å—à–∏—Ç–∏
worker_max_tasks_per_child = 500
# –†—ñ–¥—à–∏–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫, –º–µ–Ω—à–µ overhead
```

---

### Task Time Limit

```python
# celery_unified.py
task_time_limit = 600       # 10 —Ö–≤–∏–ª–∏–Ω hard limit
task_soft_time_limit = 540  # 9 —Ö–≤–∏–ª–∏–Ω soft limit
```

**–ö–æ–ª–∏ –∑–º—ñ–Ω—é–≤–∞—Ç–∏:**

```python
# –ü–æ–≤—ñ–ª—å–Ω—ñ —Å–∞–π—Ç–∏ - –∑–±—ñ–ª—å—à–∏—Ç–∏
task_time_limit = 1200      # 20 —Ö–≤–∏–ª–∏–Ω
task_soft_time_limit = 1080

# –®–≤–∏–¥–∫—ñ —Å–∞–π—Ç–∏ - –∑–º–µ–Ω—à–∏—Ç–∏
task_time_limit = 300       # 5 —Ö–≤–∏–ª–∏–Ω
task_soft_time_limit = 270
```

---

## Auto-scaling (Kubernetes)

### Horizontal Pod Autoscaler (HPA)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: package_crawler-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: package_crawler-worker
  minReplicas: 10
  maxReplicas: 500
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

**–©–æ —Ü–µ —Ä–æ–±–∏—Ç—å:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–æ–¥–∞—î workers –ø—Ä–∏ CPU > 70%
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–¥–∞–ª—è—î workers –ø—Ä–∏ CPU < 70%
- Scale up: —à–≤–∏–¥–∫–æ (50% –∑–∞ 60 —Å–µ–∫)
- Scale down: –ø–æ–≤—ñ–ª—å–Ω–æ (10% –∑–∞ 60 —Å–µ–∫)

---

### Custom Metrics (Celery Queue)

```yaml
apiVersion: v2
kind: HorizontalPodAutoscaler
metadata:
  name: package_crawler-worker-queue-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: package_crawler-worker
  minReplicas: 10
  maxReplicas: 1000
  metrics:
  - type: Pods
    pods:
      metric:
        name: celery_queue_length
      target:
        type: AverageValue
        averageValue: "100"  # 100 –∑–∞–¥–∞—á –Ω–∞ worker
```

**–©–æ —Ü–µ —Ä–æ–±–∏—Ç—å:**
- –ú–∞—Å—à—Ç–∞–±—É—î –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–≤–∂–∏–Ω–∏ —á–µ—Ä–≥–∏ Celery
- 1000 –∑–∞–¥–∞—á ‚Üí 10 workers
- 10,000 –∑–∞–¥–∞—á ‚Üí 100 workers
- 100,000 –∑–∞–¥–∞—á ‚Üí 1000 workers

---

## Redis –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

### Standalone ‚Üí Sentinel ‚Üí Cluster

#### 1. Standalone (< 100 workers)

```yaml
redis:
  image: redis:7-alpine
  command: redis-server --maxmemory 4gb
```

**–û–±–º–µ–∂–µ–Ω–Ω—è:**
- Single point of failure
- Max ~16 GB RAM
- Max ~100,000 ops/sec

---

#### 2. Sentinel (100-500 workers)

```yaml
# Master
redis-master:
  image: redis:7-alpine
  command: redis-server --maxmemory 8gb

# Replica 1
redis-replica-1:
  image: redis:7-alpine
  command: redis-server --replicaof redis-master 6379

# Replica 2
redis-replica-2:
  image: redis:7-alpine
  command: redis-server --replicaof redis-master 6379

# Sentinel
redis-sentinel:
  image: redis:7-alpine
  command: redis-sentinel /etc/sentinel.conf
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ High availability (auto failover)
- ‚úÖ Read scaling (replicas)
- ‚ùå –ù–ï –º–∞—Å—à—Ç–∞–±—É—î writes

---

#### 3. Cluster (500+ workers)

```yaml
redis:
  type: RedisCluster
  shards: 10       # 10 shards
  replicas: 2      # 2 replicas per shard
  resources:
    limits:
      memory: 16Gi
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ Write scaling (sharding)
- ‚úÖ –ù–µ–æ–±–º–µ–∂–µ–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
- ‚úÖ Automatic sharding
- ‚úÖ High availability

**–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:**
```bash
# –ö–æ–∂–µ–Ω shard –æ—Ç—Ä–∏–º—É—î —á–∞—Å—Ç–∏–Ω—É –∫–ª—é—á—ñ–≤
# 10 shards = 10x write throughput
# 20 nodes (10 masters + 10 replicas)
```

---

## MongoDB –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

### Standalone ‚Üí Replica Set ‚Üí Sharded Cluster

#### 1. Standalone (< 1M –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)

```yaml
mongodb:
  image: mongo:7
  command: mongod --wiredTigerCacheSizeGB 4
```

**–û–±–º–µ–∂–µ–Ω–Ω—è:**
- Single point of failure
- Max ~16 GB WiredTiger cache
- –ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è

---

#### 2. Replica Set (1M-10M –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)

```yaml
mongodb:
  replicas: 3
  command: |
    mongod --replSet rs0 --wiredTigerCacheSizeGB 16
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ High availability
- ‚úÖ Read scaling (secondaries)
- ‚ùå –ù–ï –º–∞—Å—à—Ç–∞–±—É—î writes

**–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è:**
```javascript
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo-1:27017" },
    { _id: 1, host: "mongo-2:27017" },
    { _id: 2, host: "mongo-3:27017" }
  ]
})
```

---

#### 3. Sharded Cluster (10M+ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤)

```yaml
# Config Servers
mongocfg:
  replicas: 3

# Shards
mongod:
  shards: 10
  replicas: 3  # Per shard

# Router (mongos)
mongos:
  replicas: 3
```

**–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:**
```
Client ‚Üí mongos ‚Üí Config Servers
              ‚Üì
         Shard 1 (RS)
         Shard 2 (RS)
         ...
         Shard N (RS)
```

**Shard Key:**
```javascript
// Sharding –ø–æ URL hash
sh.shardCollection("crawler_db.nodes", { url: "hashed" })
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ Write scaling (sharding)
- ‚úÖ –ù–µ–æ–±–º–µ–∂–µ–Ω–µ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
- ‚úÖ Automatic balancing

---

## –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç–∞ –∞–ª–µ—Ä—Ç–∏

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É

```python
# Worker metrics
- Active workers
- Tasks per second
- Average task duration
- Memory usage per worker
- CPU usage per worker

# Redis metrics
- Queue length
- Memory usage
- Operations per second
- Connected clients

# MongoDB metrics
- Documents count
- Storage size
- Query latency
- Connections count
```

### Prometheus + Grafana

```yaml
# docker-compose.yml
prometheus:
  image: prom/prometheus
  volumes:
    - ./prometheus.yml:/etc/prometheus/prometheus.yml
  ports:
    - "9090:9090"

grafana:
  image: grafana/grafana
  ports:
    - "3000:3000"
  environment:
    - GF_AUTH_ANONYMOUS_ENABLED=true
```

**prometheus.yml:**
```yaml
scrape_configs:
  - job_name: 'celery'
    static_configs:
      - targets: ['flower:5555']
  
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
  
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb-exporter:9216']
```

---

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∏–π —Å–ø–∏—Å–æ–∫ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è

- ‚úÖ –í–∏–∑–Ω–∞—á–∏–≤ —Ü—ñ–ª—å–æ–≤–∏–π –º–∞—Å—à—Ç–∞–± (–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∞–π—Ç—ñ–≤, —Å—Ç–æ—Ä—ñ–Ω–æ–∫)
- ‚úÖ –†–æ–∑—Ä–∞—Ö—É–≤–∞–≤ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏ (workers, RAM, storage)
- ‚úÖ –û–±—Ä–∞–≤ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É (standalone, cluster)
- ‚úÖ –ù–∞–ª–∞—à—Ç—É–≤–∞–≤ auto-scaling (–¥–ª—è Kubernetes)
- ‚úÖ –ù–∞–ª–∞—à—Ç—É–≤–∞–≤ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
- ‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞–≤ –Ω–∞ –º–∞–ª–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ
- ‚úÖ –ü–æ—Å—Ç—É–ø–æ–≤–æ –∑–±—ñ–ª—å—à—É—é –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
- ‚úÖ –ú–æ–Ω—ñ—Ç–æ—Ä—é –º–µ—Ç—Ä–∏–∫–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—é

---

## –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏

- [KUBERNETES.md](./KUBERNETES.md) - Kubernetes deployment
- [PRODUCTION.md](./PRODUCTION.md) - Production best practices
- [MONITORING.md](./MONITORING.md) - –î–µ—Ç–∞–ª—å–Ω–∏–π –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥

---

**–ü–∞–º'—è—Ç–∞–π—Ç–µ:** –í–∞—à –∫–æ–¥ –Ω–µ –º—ñ–Ω—è—î—Ç—å—Å—è! –¢—ñ–ª—å–∫–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å workers —Ç–∞ –ø–æ—Ç—É–∂–Ω—ñ—Å—Ç—å —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏! üöÄ
