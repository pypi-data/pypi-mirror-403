# 3. Component Catalog (–ö–∞—Ç–∞–ª–æ–≥ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤)

## üìã –ó–º—ñ—Å—Ç

1. [Domain Entities](#domain-entities)
2. [Application Use Cases](#application-use-cases)
3. [Infrastructure Components](#infrastructure-components)
4. [Extensions Components](#extensions-components)

---

## Domain Entities

### Node (–í—É–∑–æ–ª –≥—Ä–∞—Ñ—É)

**–§–∞–π–ª:** `domain/entities/node.py`

**–†–æ–ª—å:** –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫—É –≤ –≥—Ä–∞—Ñ—ñ.

**–ñ–∏—Ç—Ç—î–≤–∏–π —Ü–∏–∫–ª:**
```
URL_STAGE (—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è) ‚Üí HTML_STAGE (–ø—ñ—Å–ª—è process_html)
```

**–ö–ª—é—á–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:**

| –ê—Ç—Ä–∏–±—É—Ç | –¢–∏–ø | –û–ø–∏—Å |
|---------|-----|------|
| `url` | str | URL —Å—Ç–æ—Ä—ñ–Ω–∫–∏ |
| `node_id` | str | UUID –≤—É–∑–ª–∞ |
| `depth` | int | –ì–ª–∏–±–∏–Ω–∞ –≤—ñ–¥ root |
| `should_scan` | bool | –ß–∏ —Å–∫–∞–Ω—É–≤–∞—Ç–∏ |
| `can_create_edges` | bool | –ß–∏ —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —Ä–µ–±—Ä–∞ |
| `metadata` | Dict | title, h1, description |
| `user_data` | Dict | –î–∞–Ω—ñ –≤—ñ–¥ –ø–ª–∞–≥—ñ–Ω—ñ–≤ |
| `content_hash` | str | SHA256 –¥–ª—è incremental |
| `priority` | int | –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 1-10 (–¥–ª—è Scheduler) |
| `lifecycle_stage` | NodeLifecycle | URL_STAGE –∞–±–æ HTML_STAGE |
| `content_type` | ContentType | –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É (HTML, JSON, IMAGE, EMPTY, ERROR —Ç–æ—â–æ) |

**–ú–µ—Ç–æ–¥–∏:**

```python
# –û–±—Ä–æ–±–∫–∞ HTML (async)
await node.process_html(html)  # ‚Üí List[str] extracted_links

# Metadata accessors (Law of Demeter)
node.get_title()        # ‚Üí Optional[str]
node.get_description()  # ‚Üí Optional[str]
node.get_h1()           # ‚Üí Optional[str]
node.get_keywords()     # ‚Üí Optional[str]

# Incremental crawling
node.get_content_hash() # ‚Üí str (SHA256)

# Serialization (Pydantic)
node.model_dump()       # ‚Üí Dict
Node.model_validate(data, context={...})  # ‚Üí Node

# –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –ø—ñ—Å–ª—è –¥–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
node.restore_dependencies(
    plugin_manager=pm,
    tree_parser=parser,
    hash_strategy=strategy
)
```

**–¢–æ—á–∫–∏ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è:**

```python
# 1. –ö–∞—Å—Ç–æ–º–Ω–∏–π Node –∫–ª–∞—Å
class MLNode(Node):
    ml_score: Optional[float] = None
    ml_priority: Optional[int] = None
    embedding: Optional[List[float]] = None

graph = crawl(url, node_class=MLNode)

# 2. –ö–∞—Å—Ç–æ–º–Ω–∞ hash strategy
class H1HashStrategy:
    def compute_hash(self, node: Node) -> str:
        return hashlib.sha256(node.metadata['h1'].encode()).hexdigest()

node.hash_strategy = H1HashStrategy()
```

---

### ContentType (–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É)

**–§–∞–π–ª:** `domain/value_objects/models.py`

**–†–æ–ª—å:** Value Object –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏.

**–ó–Ω–∞—á–µ–Ω–Ω—è enum:**

| –ó–Ω–∞—á–µ–Ω–Ω—è | –û–ø–∏—Å | HTTP Content-Type |
|----------|------|-------------------|
| `UNKNOWN` | –ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø | - |
| `HTML` | HTML —Å—Ç–æ—Ä—ñ–Ω–∫–∞ | text/html |
| `JSON` | JSON endpoint | application/json |
| `XML` | XML/Sitemap/RSS | text/xml, application/xml |
| `TEXT` | Plain text | text/plain |
| `CSS` | CSS —Ñ–∞–π–ª | text/css |
| `JAVASCRIPT` | JavaScript | application/javascript |
| `IMAGE` | –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è | image/* |
| `VIDEO` | –í—ñ–¥–µ–æ | video/* |
| `AUDIO` | –ê—É–¥—ñ–æ | audio/* |
| `PDF` | PDF –¥–æ–∫—É–º–µ–Ω—Ç | application/pdf |
| `DOC` | Word/Excel | application/msword |
| `BINARY` | –ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª | application/octet-stream |
| `ARCHIVE` | –ê—Ä—Ö—ñ–≤ | application/zip |
| `EMPTY` | HTTP 200, –ø—É—Å—Ç–∏–π body | - |
| `ERROR` | –ü–æ–º–∏–ª–∫–∞ (4xx, 5xx) | - |
| `REDIRECT` | HTTP 3xx | - |

**–ú–µ—Ç–æ–¥–∏ –¥–µ—Ç–µ–∫—Ü—ñ—ó:**

```python
# –ó HTTP Content-Type header (primary)
ct = ContentType.from_content_type_header("text/html; charset=utf-8")
# ‚Üí ContentType.HTML

# –ó URL extension (fallback)
ct = ContentType.from_url("https://example.com/data.json")
# ‚Üí ContentType.JSON
```

**Helper –º–µ—Ç–æ–¥–∏:**

```python
# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—É
ct.is_text_based()  # True –¥–ª—è HTML, JSON, XML, TEXT, CSS, JS
ct.is_media()       # True –¥–ª—è IMAGE, VIDEO, AUDIO
ct.is_scannable()   # True –¥–ª—è HTML, XML (–º—ñ—Å—Ç—è—Ç—å –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
```

**–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞ –¥–µ—Ç–µ–∫—Ü—ñ—è (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ):**

```python
# –ú–µ—Ç–æ–¥ detect() –≤–∫–ª—é—á–∞—î –≤—Å—é –ª–æ–≥—ñ–∫—É –¥–µ—Ç–µ–∫—Ü—ñ—ó –≤ Domain Layer
ct = ContentType.detect(
    content_type_header="text/html; charset=utf-8",  # HTTP header (primary)
    url="https://example.com/page.html",             # URL fallback
    content="<!DOCTYPE html>...",                     # Content heuristic
    status_code=200,                                  # HTTP status
    has_error=False                                   # Fetch error flag
)

# –ü—Ä–∏–∫–ª–∞–¥–∏
ContentType.detect(status_code=404)  # ‚Üí ERROR
ContentType.detect(content="   ")    # ‚Üí EMPTY  
ContentType.detect(content='{"key": "value"}')  # ‚Üí JSON (heuristic)
```

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**

```python
# –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
html_nodes = [n for n in graph if n.content_type == ContentType.HTML]
empty_nodes = [n for n in graph if n.content_type == ContentType.EMPTY]
media_nodes = [n for n in graph if n.content_type.is_media()]

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ–±—Ä–æ–±–∫–æ—é
if node.content_type.is_scannable():
    links = await node.process_html(html)
```

---

### Edge (–†–µ–±—Ä–æ –≥—Ä–∞—Ñ—É)

**–§–∞–π–ª:** `domain/entities/edge.py`

**–†–æ–ª—å:** –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –º—ñ–∂ —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º–∏.

**–ö–ª—é—á–æ–≤—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:**

| –ê—Ç—Ä–∏–±—É—Ç | –¢–∏–ø | –û–ø–∏—Å |
|---------|-----|------|
| `edge_id` | str | UUID —Ä–µ–±—Ä–∞ |
| `source_node_id` | str | ID –≤—É–∑–ª–∞-–¥–∂–µ—Ä–µ–ª–∞ |
| `target_node_id` | str | ID —Ü—ñ–ª—å–æ–≤–æ–≥–æ –≤—É–∑–ª–∞ |
| `anchor_text` | str | –¢–µ–∫—Å—Ç –ø–æ—Å–∏–ª–∞–Ω–Ω—è |
| `link_type` | List[str] | –¢–∏–ø–∏: internal, external, deeper |
| `metadata` | Dict | Redirect info, custom data |

**–¢–æ—á–∫–∏ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è:**

```python
# –ö–∞—Å—Ç–æ–º–Ω–∏–π Edge –∫–ª–∞—Å
class SEOEdge(Edge):
    follow: bool = True
    sponsored: bool = False
    ugc: bool = False

graph = crawl(url, edge_class=SEOEdge)
```

---

### Graph (–ú–µ–Ω–µ–¥–∂–µ—Ä –≥—Ä–∞—Ñ—É)

**–§–∞–π–ª:** `domain/entities/graph.py`

**–†–æ–ª—å:** –£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –≤—É–∑–ª–∞–º–∏ —Ç–∞ —Ä–µ–±—Ä–∞–º–∏, –æ–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥ –≥—Ä–∞—Ñ–∞–º–∏.

**CRUD –æ–ø–µ—Ä–∞—Ü—ñ—ó:**

```python
# –î–æ–¥–∞–≤–∞–Ω–Ω—è
graph.add_node(node, overwrite=False)  # ‚Üí Node
graph.add_edge(edge)                    # ‚Üí Edge

# –û—Ç—Ä–∏–º–∞–Ω–Ω—è
graph.get_node_by_url(url)              # ‚Üí Optional[Node]
graph.get_node_by_id(node_id)           # ‚Üí Optional[Node]
graph.has_edge(source_id, target_id)    # ‚Üí bool (O(1))

# –í–∏–¥–∞–ª–µ–Ω–Ω—è
graph.remove_node(node_id)              # ‚Üí bool

# –†–µ–¥—ñ—Ä–µ–∫—Ç–∏
graph.handle_redirect(original_node, final_url, redirect_chain)
```

**Python API (–æ–ø–µ—Ä–∞—Ü—ñ—ó):**

```python
# –ö–æ–ª–µ–∫—Ü—ñ–π–Ω—ñ
len(graph)              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤—É–∑–ª—ñ–≤
for node in graph:      # –Ü—Ç–µ—Ä–∞—Ü—ñ—è
'url' in graph          # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ
graph['url']            # –î–æ—Å—Ç—É–ø –∑–∞ URL
graph[0]                # –î–æ—Å—Ç—É–ø –∑–∞ —ñ–Ω–¥–µ–∫—Å–æ–º

# –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–Ω—ñ
g3 = g1 + g2            # Union (–æ–±'—î–¥–Ω–∞–Ω–Ω—è)
g3 = g2 - g1            # Difference (—Ä—ñ–∑–Ω–∏—Ü—è)
g3 = g1 & g2            # Intersection (–ø–µ—Ä–µ—Ç–∏–Ω)
g3 = g1 | g2            # Union (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
g3 = g1 ^ g2            # Symmetric difference

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
g1 == g2                # –†—ñ–≤–Ω—ñ—Å—Ç—å
g1 < g2                 # g1 —î –ø—ñ–¥–≥—Ä–∞—Ñ–æ–º g2
g1 <= g2                # –ü—ñ–¥–≥—Ä–∞—Ñ –∞–±–æ —Ä—ñ–≤–Ω–∏–π
g1 > g2                 # g1 —î –Ω–∞–¥–≥—Ä–∞—Ñ–æ–º g2
```

**Merge Strategies (–ø—Ä–∏ union):**

```python
# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è default —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
graph = Graph(default_merge_strategy='merge')

# –ê–±–æ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç
from graph_crawler.application.context import with_merge_strategy

with with_merge_strategy('newest'):
    g3 = g1 + g2  # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î 'newest'
```

| –°—Ç—Ä–∞—Ç–µ–≥—ñ—è | –û–ø–∏—Å |
|-----------|------|
| `first` | –ó–∞–ª–∏—à–∏—Ç–∏ node –∑ –ø–µ—Ä—à–æ–≥–æ –≥—Ä–∞—Ñ–∞ |
| `last` | –í–∑—è—Ç–∏ node –∑ –¥—Ä—É–≥–æ–≥–æ –≥—Ä–∞—Ñ–∞ (default) |
| `merge` | –Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–µ –æ–±'—î–¥–Ω–∞–Ω–Ω—è metadata |
| `newest` | –í–∏–±—Ä–∞—Ç–∏ node –∑ –Ω–∞–π–Ω–æ–≤—ñ—à–∏–º created_at |
| `oldest` | –í–∏–±—Ä–∞—Ç–∏ node –∑ –Ω–∞–π—Å—Ç–∞—Ä—ñ—à–∏–º created_at |
| `custom` | –ö–æ—Ä–∏—Å—Ç—É–≤–∞—Ü—å–∫–∞ —Ñ—É–Ω–∫—Ü—ñ—è |

```python
# –ö–∞—Å—Ç–æ–º–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è merge
def my_merge(n1, n2):
    return n1 if n1.scanned else n2

from graph_crawler.domain.entities.merge_strategies import NodeMerger
merger = NodeMerger(strategy='custom', custom_merge_fn=my_merge)
```

---

### EventBus (–®–∏–Ω–∞ –ø–æ–¥—ñ–π)

**–§–∞–π–ª:** `domain/events/event_bus.py`

**–†–æ–ª—å:** Observer Pattern –¥–ª—è loose coupling –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤.

```python
from graph_crawler.domain.events import EventBus, EventType, CrawlerEvent

bus = EventBus()

# –ü—ñ–¥–ø–∏—Å–∫–∞
def on_node_scanned(event: CrawlerEvent):
    print(f"Scanned: {event.data['url']}")

bus.subscribe(EventType.NODE_SCANNED, on_node_scanned)

# Async –ø—ñ–¥–ø–∏—Å–∫–∞
async def async_handler(event: CrawlerEvent):
    await save_to_db(event.data)

bus.subscribe(EventType.NODE_SCANNED, async_handler)

# –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è
bus.publish(CrawlerEvent.create(EventType.NODE_SCANNED, data={'url': '...'}))
await bus.publish_async(event)  # –î–ª—è async handlers

# –Ü—Å—Ç–æ—Ä—ñ—è –ø–æ–¥—ñ–π
bus.enable_history(max_size=1000)
history = bus.get_history(EventType.NODE_SCANNED)
```

**50+ —Ç–∏–ø—ñ–≤ –ø–æ–¥—ñ–π:**

| –ö–∞—Ç–µ–≥–æ—Ä—ñ—è | –ü–æ–¥—ñ—ó |
|-----------|-------|
| Node | NODE_CREATED, NODE_SCANNED, NODE_FAILED, NODE_SKIPPED_UNCHANGED |
| Crawler | CRAWL_STARTED, CRAWL_COMPLETED, CRAWL_PAUSED, BATCH_COMPLETED |
| Scheduler | URL_ADDED_TO_QUEUE, URL_EXCLUDED, URL_PRIORITIZED |
| Middleware | RATE_LIMIT_WAIT, PROXY_SELECTED, RETRY_STARTED |
| Storage | GRAPH_SAVED, GRAPH_LOADED, STORAGE_UPGRADED |
| Plugin | PLUGIN_STARTED, PLUGIN_COMPLETED, PLUGIN_FAILED |
| Fetch | FETCH_STARTED, FETCH_SUCCESS, FETCH_ERROR |

---

## Application Use Cases

### Spider (–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä)

**–§–∞–π–ª:** `application/use_cases/crawling/spider.py`

**–†–æ–ª—å:** –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ—è –≤—Å—å–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É –∫—Ä–∞—É–ª—ñ–Ω–≥—É.

**–í–∞—Ä—ñ–∞–Ω—Ç–∏ Spider:**

| –ö–ª–∞—Å | –†–µ–∂–∏–º | –û–ø–∏—Å |
|------|-------|------|
| `GraphSpider` | sequential | –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π async spider |
| `MultiprocessingSpider` | multiprocessing | –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π (–¥–æ 32 workers) |
| `CeleryBatchSpider` | celery | –†–æ–∑–ø–æ–¥—ñ–ª–µ–Ω–∏–π (–¥–æ 1000 workers) |
| `SitemapSpider` | sitemap | –î–ª—è sitemap.xml |

```python
# –í–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É —á–µ—Ä–µ–∑ config
config = CrawlerConfig(
    url="https://example.com",
    mode="multiprocessing",  # sequential, multiprocessing, celery
    workers=8
)
```

**–†–µ—î—Å—Ç—Ä–∞—Ü—ñ—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ Spider:**

```python
from graph_crawler.domain.entities.registries import register_crawl_mode

class MyDistributedSpider(BaseSpider):
    async def crawl(self):
        ...

register_crawl_mode("distributed", MyDistributedSpider)

# –¢–µ–ø–µ—Ä –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
config = CrawlerConfig(url="...", mode="distributed")
```

---

### Scheduler (–ü–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫)

**–§–∞–π–ª:** `application/use_cases/crawling/scheduler.py`

**–†–æ–ª—å:** –ß–µ—Ä–≥–∞ URL –∑ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ —Ç–∞ –ø—Ä–∞–≤–∏–ª–∞–º–∏.

**–ú–µ—Ö–∞–Ω—ñ–∑–º–∏ –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑–∞—Ü—ñ—ó:**

```python
# 1. URLRule (—Å—Ç–∞—Ç–∏—á–Ω–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
rules = [
    URLRule(pattern=r"/products/", priority=10),  # –í–∏—Å–æ–∫–∏–π
    URLRule(pattern=r"/blog/", priority=3),       # –ù–∏–∑—å–∫–∏–π
]

# 2. Dynamic Priority (—á–µ—Ä–µ–∑ Node.priority)
class MLNode(Node):
    priority: Optional[int] = None

# ML –ø–ª–∞–≥—ñ–Ω –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –¥–∏–Ω–∞–º—ñ—á–Ω–æ
context.user_data['child_priorities'] = {url: 10}

# 3. Scheduler —á–∏—Ç–∞—î Node.priority –ü–ï–†–ï–î URLRule
# –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç: node.priority > URLRule.priority > default(5)
```

---

### LinkProcessor (–û–±—Ä–æ–±–Ω–∏–∫ –ø–æ—Å–∏–ª–∞–Ω—å)

**–§–∞–π–ª:** `application/use_cases/crawling/link_processor.py`

**–†–æ–ª—å:** –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å.

**Edge Creation Strategies:**

```python
from graph_crawler.domain.value_objects.models import EdgeCreationStrategy

graph = crawl(
    url,
    edge_strategy="new_only",        # –¢—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—ñ
    # edge_strategy="max_in_degree",  # –õ—ñ–º—ñ—Ç incoming edges
    # edge_strategy="deeper_only",    # –¢—ñ–ª—å–∫–∏ –≤–≥–ª–∏–±
    max_in_degree_threshold=100       # –î–ª—è max_in_degree
)
```

| –°—Ç—Ä–∞—Ç–µ–≥—ñ—è | –û–ø–∏—Å | –†–µ–∑—É–ª—å—Ç–∞—Ç |
|-----------|------|-----------|
| `all` | –í—Å—ñ edges (default) | –ü–æ–≤–Ω–∏–π –≥—Ä–∞—Ñ |
| `new_only` | Edge —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ node | –î–µ—Ä–µ–≤–æ |
| `max_in_degree` | –õ—ñ–º—ñ—Ç incoming edges | –ë–µ–∑ hub-—ñ–≤ |
| `deeper_only` | –¢—ñ–ª—å–∫–∏ –≤–≥–ª–∏–± | –ë–µ–∑ backlinks |
| `same_depth_only` | –¢—ñ–ª—å–∫–∏ –Ω–∞ —Ç–æ–º—É –∂ —Ä—ñ–≤–Ω—ñ | Siblings |
| `first_encounter_only` | –ü–µ—Ä—à–∏–π edge –Ω–∞ URL | –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –≥—Ä–∞—Ñ |

**Explicit Filter Override (ML):**

```python
# ML –ø–ª–∞–≥—ñ–Ω –º–æ–∂–µ –ø–µ—Ä–µ–±–∏–≤–∞—Ç–∏ —Ñ—ñ–ª—å—Ç—Ä–∏!
context.user_data['explicit_scan_decisions'] = {
    'https://external-job-site.com/vacancy': True,  # –î–æ–∑–≤–æ–ª–∏—Ç–∏ (–ø–µ—Ä–µ–±–∏–≤–∞—î domain filter)
    'https://spam-site.com': False                   # –ó–∞–±–æ—Ä–æ–Ω–∏—Ç–∏
}
```

---

## Infrastructure Components

### Drivers (–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç)

**–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å:** `infrastructure/transport/base.py`

| Driver | –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—è | Use Case |
|--------|------------|----------|
| `HTTPDriver` | aiohttp | –°—Ç–∞—Ç–∏—á–Ω—ñ —Å–∞–π—Ç–∏ (default) |
| `AsyncDriver` | aiohttp + semaphore | High concurrency |
| `PlaywrightDriver` | Playwright | JavaScript SPA |
| `StealthDriver` | Playwright + stealth | Anti-bot bypass |

**–†–µ—î—Å—Ç—Ä–∞—Ü—ñ—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞:**

```python
from graph_crawler.application.services import register_driver

class SeleniumDriver(BaseDriver):
    async def fetch(self, url: str) -> FetchResponse:
        ...

register_driver("selenium", lambda cfg: SeleniumDriver(**cfg))

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
graph = crawl(url, driver="selenium")
```

---

### Storage (–°—Ö–æ–≤–∏—â–∞)

**–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å:** `infrastructure/persistence/base.py`

| Storage | –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—è | –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è |
|---------|------------|-------------------|
| `MemoryStorage` | In-memory | < 1,000 nodes |
| `JSONStorage` | aiofiles | 1,000 - 10,000 nodes |
| `SQLiteStorage` | aiosqlite | 10,000 - 100,000 nodes |
| `PostgreSQLStorage` | asyncpg | 100,000+ nodes |
| `MongoDBStorage` | motor | 100,000+ nodes |
| `AutoStorage` | Auto-scale | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä |

**–†–µ—î—Å—Ç—Ä–∞—Ü—ñ—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ storage:**

```python
from graph_crawler.application.services import register_storage

class RedisStorage(BaseStorage):
    async def save_graph(self, graph):
        ...

register_storage("redis", lambda cfg: RedisStorage(**cfg))

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
graph = crawl(url, storage="redis", storage_config={"host": "localhost"})
```

---

### Adapters (HTML –ø–∞—Ä—Å–µ—Ä–∏)

**–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å:** `infrastructure/adapters/base.py`

| Adapter | –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ | –®–≤–∏–¥–∫—ñ—Å—Ç—å |
|---------|------------|----------|
| `BeautifulSoupAdapter` | BeautifulSoup4 | –°–µ—Ä–µ–¥–Ω—è (default) |
| `lxmlAdapter` | lxml | –í–∏—Å–æ–∫–∞ |
| `ScrapyAdapter` | Scrapy | –í–∏—Å–æ–∫–∞ |

```python
# –ö–∞—Å—Ç–æ–º–Ω–∏–π tree parser
from graph_crawler.infrastructure.adapters import get_default_parser

# –ê–±–æ —Å–≤—ñ–π
class MyAdapter:
    def parse(self, html: str):
        return my_tree

node.tree_parser = MyAdapter()
```

---

## Extensions Components

### Node Plugins

**–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å:** `extensions/plugins/node/base.py`

**–¢–∏–ø–∏ –ø–ª–∞–≥—ñ–Ω—ñ–≤ (–µ—Ç–∞–ø–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è):**

```
ON_NODE_CREATED ‚Üí ON_BEFORE_SCAN ‚Üí ON_HTML_PARSED ‚Üí ON_AFTER_SCAN
     ‚Üë                                                    ‚Üë
  URL_STAGE                                          HTML_STAGE
```

| –¢–∏–ø | –ö–æ–ª–∏ | –©–æ –¥–æ—Å—Ç—É–ø–Ω–æ |
|-----|------|-------------|
| ON_NODE_CREATED | –ü—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ Node | url, depth |
| ON_BEFORE_SCAN | –ü–µ—Ä–µ–¥ fetch | url, depth, should_scan |
| ON_HTML_PARSED | –ü—ñ—Å–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É | html, html_tree, metadata |
| ON_AFTER_SCAN | –ü—ñ—Å–ª—è –ø–ª–∞–≥—ñ–Ω—ñ–≤ | metadata, user_data, links |

**–í–±—É–¥–æ–≤–∞–Ω—ñ –ø–ª–∞–≥—ñ–Ω–∏:**

| –ü–ª–∞–≥—ñ–Ω | –¢–∏–ø | –†–µ–∑—É–ª—å—Ç–∞—Ç |
|--------|-----|-----------|
| MetadataExtractorPlugin | ON_HTML_PARSED | title, h1, description |
| LinkExtractorPlugin | ON_HTML_PARSED | extracted_links |
| TextExtractorPlugin | ON_HTML_PARSED | text_content |
| PhoneExtractorPlugin | ON_HTML_PARSED | phones[] |
| EmailExtractorPlugin | ON_HTML_PARSED | emails[] |
| PriceExtractorPlugin | ON_HTML_PARSED | prices[] |
| RealTimeVectorizerPlugin | ON_AFTER_SCAN | embedding[] |

**–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø–ª–∞–≥—ñ–Ω–∞:**

```python
from graph_crawler.extensions.plugins.node import BaseNodePlugin, NodePluginType

class MLDecisionPlugin(BaseNodePlugin):
    @property
    def name(self):
        return "ml_decision"
    
    @property
    def plugin_type(self):
        return NodePluginType.ON_AFTER_SCAN
    
    def execute(self, context):
        # –ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        score = self.analyze(context.html_tree)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        context.user_data['ml_score'] = score
        
        # Dynamic priority –¥–ª—è child nodes
        context.user_data['child_priorities'] = {
            url: 10 for url in context.extracted_links
            if self.is_important(url)
        }
        
        # Explicit filter override
        context.user_data['explicit_scan_decisions'] = {
            'https://external.com/job': True  # –ü–µ—Ä–µ–±–∏–≤–∞—î domain filter!
        }
        
        return context

graph = crawl(url, plugins=[MLDecisionPlugin()])
```

---

### Middleware

**–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å:** `extensions/middleware/base.py`

**–¢–∏–ø–∏ middleware:**

```
PRE_REQUEST ‚Üí [Driver fetch] ‚Üí POST_REQUEST ‚Üí POST_RESPONSE
```

| Middleware | –¢–∏–ø | –û–ø–∏—Å |
|------------|-----|------|
| RateLimitMiddleware | PRE_REQUEST | Token bucket |
| ProxyMiddleware | PRE_REQUEST | Proxy rotation |
| UserAgentMiddleware | PRE_REQUEST | UA rotation |
| RetryMiddleware | POST_REQUEST | Retry –∑ backoff |
| CacheMiddleware | PRE_REQUEST | HTTP –∫–µ—à—É–≤–∞–Ω–Ω—è |
| RobotsMiddleware | PRE_REQUEST | robots.txt compliance |
| ErrorRecoveryMiddleware | POST_REQUEST | Error handling |

**–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ middleware:**

```python
from graph_crawler.extensions.middleware import BaseMiddleware, MiddlewareType

class AuthMiddleware(BaseMiddleware):
    @property
    def name(self):
        return "auth"
    
    @property
    def middleware_type(self):
        return MiddlewareType.PRE_REQUEST
    
    def process(self, context):
        context.headers['Authorization'] = f'Bearer {self.token}'
        return context
```

**MiddlewareChain:**

```python
from graph_crawler.extensions.middleware import MiddlewareChain

chain = MiddlewareChain()
chain.add(RateLimitMiddleware(requests_per_second=10))
chain.add(ProxyMiddleware(proxies=[...]))
chain.add(AuthMiddleware(token='...'))

context = await chain.execute(MiddlewareType.PRE_REQUEST, context)
```
