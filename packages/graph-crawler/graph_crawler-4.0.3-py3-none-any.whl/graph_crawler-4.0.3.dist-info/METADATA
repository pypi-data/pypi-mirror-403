Metadata-Version: 2.4
Name: graph-crawler
Version: 4.0.3
Summary: Sync-First –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—É –≤–µ–±-—Å–∞–π—Ç—ñ–≤ - –ø—Ä–æ—Å—Ç–æ —è–∫ requests!
Home-page: https://gitlab.com/demoprogrammer/web_graf
Author: 0-EternalJunior-0
Author-email: 
Maintainer: 0-EternalJunior-0
License: MIT
Project-URL: Homepage, https://gitlab.com/demoprogrammer/web_graf
Project-URL: Documentation, https://gitlab.com/demoprogrammer/web_graf/-/blob/main/README.md
Project-URL: Repository, https://gitlab.com/demoprogrammer/web_graf
Project-URL: Bug Tracker, https://gitlab.com/demoprogrammer/web_graf/-/issues
Keywords: web,crawler,scraper,graph,spider,scrapy,vectorization,free-threading
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Internet :: WWW/HTTP :: Indexing/Search
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: Operating System :: OS Independent
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests>=2.31.0
Requires-Dist: beautifulsoup4>=4.12.0
Requires-Dist: lxml>=4.9.0
Requires-Dist: lxml_html_clean
Requires-Dist: pydantic>=2.5.0
Requires-Dist: pydantic-settings>=2.0.0
Requires-Dist: python-dateutil>=2.8.2
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: aiofiles>=23.2.0
Requires-Dist: aiosqlite>=0.19.0
Requires-Dist: selectolax>=0.3.0
Requires-Dist: aiodns>=3.1.0
Requires-Dist: orjson>=3.9.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: pybloom-live
Requires-Dist: fake-useragent
Requires-Dist: dependency-injector
Requires-Dist: fastapi
Requires-Dist: mmh3>=4.0.0
Provides-Extra: playwright
Requires-Dist: playwright>=1.40.0; extra == "playwright"
Provides-Extra: mongodb
Requires-Dist: motor>=3.3.0; extra == "mongodb"
Provides-Extra: postgresql
Requires-Dist: asyncpg>=0.29.0; extra == "postgresql"
Provides-Extra: embeddings
Requires-Dist: sentence-transformers>=2.2.0; extra == "embeddings"
Requires-Dist: numpy>=1.24.0; extra == "embeddings"
Provides-Extra: newspaper
Requires-Dist: newspaper3k>=0.2.8; extra == "newspaper"
Provides-Extra: goose
Requires-Dist: goose3>=3.1.0; extra == "goose"
Provides-Extra: readability
Requires-Dist: readability-lxml>=0.8.0; extra == "readability"
Provides-Extra: articles
Requires-Dist: newspaper3k>=0.2.8; extra == "articles"
Requires-Dist: goose3>=3.1.0; extra == "articles"
Requires-Dist: readability-lxml>=0.8.0; extra == "articles"
Provides-Extra: viz
Requires-Dist: pyvis>=0.3.0; extra == "viz"
Requires-Dist: networkx>=3.6; extra == "viz"
Requires-Dist: plotly>=5.18.0; extra == "viz"
Provides-Extra: celery
Requires-Dist: celery>=5.3.0; extra == "celery"
Requires-Dist: redis>=5.0.0; extra == "celery"
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Provides-Extra: all
Requires-Dist: playwright>=1.40.0; extra == "all"
Requires-Dist: motor>=3.3.0; extra == "all"
Requires-Dist: asyncpg>=0.29.0; extra == "all"
Requires-Dist: sentence-transformers>=2.2.0; extra == "all"
Requires-Dist: numpy>=1.24.0; extra == "all"
Requires-Dist: newspaper3k>=0.2.8; extra == "all"
Requires-Dist: goose3>=3.1.0; extra == "all"
Requires-Dist: readability-lxml>=0.8.0; extra == "all"
Requires-Dist: pyvis>=0.3.0; extra == "all"
Requires-Dist: networkx>=3.0; extra == "all"
Requires-Dist: plotly>=5.18.0; extra == "all"
Requires-Dist: celery>=5.3.0; extra == "all"
Requires-Dist: redis>=5.0.0; extra == "all"
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# GraphCrawler

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-4.0.0-green.svg)](CHANGELOG.md)
[![Performance](https://img.shields.io/badge/speedup-3.2x-brightgreen.svg)]()

Python –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –≤–µ–±-—Å–∞–π—Ç—ñ–≤ —Ç–∞ –ø–æ–±—É–¥–æ–≤–∏ –≥—Ä–∞—Ñ—É —ó—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏.

## üöÄ Python 3.14 Optimizations

GraphCrawler 4.0 –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è Python 3.14 –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é **free-threading**:

- ‚ö° **2-4x —à–≤–∏–¥—à–µ** HTML –ø–∞—Ä—Å–∏–Ω–≥ (free-threading)
- üöÄ **3.2x —à–≤–∏–¥—à–µ** end-to-end crawling
- üìâ **16% –º–µ–Ω—à–µ** memory usage
- ‚è±Ô∏è **30% —à–≤–∏–¥—à–∏–π** startup

### Free-threading Mode (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)

```bash
# –£–≤—ñ–º–∫–Ω—É—Ç–∏ free-threading –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ
export PYTHON_GIL=0
python your_script.py
```

## –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è

```bash
pip install -e .
```

### Optional dependencies

```bash
# Playwright driver (–¥–ª—è JavaScript —Å–∞–π—Ç—ñ–≤)
pip install -e ".[playwright]"

# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (–ø–ª–∞–≥—ñ–Ω)
pip install -e ".[embeddings]"

# Content extractors (–ø–ª–∞–≥—ñ–Ω–∏)
pip install -e ".[articles]"

# MongoDB/PostgreSQL storage
pip install -e ".[mongodb,postgresql]"

# –í—Å–µ —Ä–∞–∑–æ–º
pip install -e ".[all]"
```

## –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç

```python
import graph_crawler as gc

# –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π API (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
graph = gc.crawl("https://example.com")

print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(graph.nodes)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫")
print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(graph.edges)} –ø–æ—Å–∏–ª–∞–Ω—å")
```

## API

### Sync API

```python
import graph_crawler as gc

# –§—É–Ω–∫—Ü—ñ—è crawl()
graph = gc.crawl(
    "https://example.com",
    max_depth=3,        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ (default: 3)
    max_pages=100,      # –ú–∞–∫—Å–∏–º—É–º —Å—Ç–æ—Ä—ñ–Ω–æ–∫ (default: 100)
    same_domain=True,   # –¢—ñ–ª—å–∫–∏ –ø–æ—Ç–æ—á–Ω–∏–π –¥–æ–º–µ–Ω (default: True)
    timeout=300,        # –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    request_delay=0.5,  # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (default: 0.5)
    driver="http",      # "http", "async", "playwright"
)

# –ö–ª–∞—Å Crawler (reusable)
with gc.Crawler(max_depth=3) as crawler:
    graph1 = crawler.crawl("https://site1.com")
    graph2 = crawler.crawl("https://site2.com")
```

### Async API

```python
import asyncio
import graph_crawler as gc

async def main():
    # –§—É–Ω–∫—Ü—ñ—è async_crawl()
    graph = await gc.async_crawl("https://example.com")
    
    # –ö–ª–∞—Å AsyncCrawler (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∫—Ä–∞—É–ª—ñ–Ω–≥)
    async with gc.AsyncCrawler() as crawler:
        graphs = await asyncio.gather(
            crawler.crawl("https://site1.com"),
            crawler.crawl("https://site2.com"),
        )
    return graphs

graphs = asyncio.run(main())
```

### –û–ø–µ—Ä–∞—Ü—ñ—ó –∑ –≥—Ä–∞—Ñ–æ–º

```python
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = graph.get_stats()
# {'total_nodes': 47, 'scanned_nodes': 45, 'total_edges': 156, ...}

# –ü–æ—à—É–∫ –≤—É–∑–ª–∞
node = graph.get_node_by_url("https://example.com/page")

# –û–ø–µ—Ä–∞—Ü—ñ—ó –Ω–∞–¥ –≥—Ä–∞—Ñ–∞–º–∏
merged = graph1 + graph2      # –û–±'—î–¥–Ω–∞–Ω–Ω—è
diff = graph2 - graph1        # –†—ñ–∑–Ω–∏—Ü—è
common = graph1 & graph2      # –ü–µ—Ä–µ—Ç–∏–Ω

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
if graph1 < graph2:
    print("graph1 —î –ø—ñ–¥–≥—Ä–∞—Ñ–æ–º graph2")

# –ï–∫—Å–ø–æ—Ä—Ç
graph.export_edges("edges.json", format="json")
graph.export_edges("edges.csv", format="csv")
graph.export_edges("graph.dot", format="dot")
```

### URL Rules

```python
from graph_crawler import crawl, URLRule

rules = [
    URLRule(pattern=r".*\.pdf$", should_scan=False),     # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ PDF
    URLRule(pattern=r"/products/", priority=10),         # –í–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
    URLRule(pattern=r"/admin/", should_scan=False),      # –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ admin
]

graph = crawl("https://example.com", url_rules=rules)
```

### –ü–ª–∞–≥—ñ–Ω–∏

```python
from graph_crawler import crawl, BaseNodePlugin, NodePluginType

class CustomPlugin(BaseNodePlugin):
    @property
    def name(self):
        return "custom_plugin"
    
    @property
    def plugin_type(self):
        return NodePluginType.ON_HTML_PARSED
    
    def execute(self, context):
        # context.html_tree - BeautifulSoup –æ–±'—î–∫—Ç
        # context.extracted_links - —Å–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
        # context.user_data - —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –¥–∞–Ω–∏—Ö
        images = context.html_tree.find_all('img')
        context.user_data['image_count'] = len(images)
        return context

graph = crawl("https://example.com", plugins=[CustomPlugin()])
```

## –î—Ä–∞–π–≤–µ—Ä–∏

| –î—Ä–∞–π–≤–µ—Ä | –û–ø–∏—Å | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|---------|------|-------------|
| `http` | Async HTTP (aiohttp) | –°—Ç–∞—Ç–∏—á–Ω—ñ —Å–∞–π—Ç–∏ (default) |
| `async` | Alias –¥–ª—è http | –ó–≤–æ—Ä–æ—Ç–Ω—è —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å |
| `playwright` | –ë—Ä–∞—É–∑–µ—Ä –∑ JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º | JavaScript —Å–∞–π—Ç–∏ |

```python
# HTTP –¥—Ä–∞–π–≤–µ—Ä (default)
graph = gc.crawl("https://example.com", driver="http")

# Playwright –¥–ª—è JavaScript —Å–∞–π—Ç—ñ–≤
graph = gc.crawl("https://spa-example.com", driver="playwright")
```

## Storage

| Storage | –û–ø–∏—Å | –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è |
|---------|------|------------------|
| `memory` | –í –ø–∞–º'—è—Ç—ñ | < 1,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫ |
| `json` | JSON —Ñ–∞–π–ª | 1,000 - 20,000 —Å—Ç–æ—Ä—ñ–Ω–æ–∫ |
| `sqlite` | SQLite –±–∞–∑–∞ | 20,000+ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ |
| `postgresql` | PostgreSQL | –í–µ–ª–∏–∫—ñ –ø—Ä–æ–µ–∫—Ç–∏ |
| `mongodb` | MongoDB | –í–µ–ª–∏–∫—ñ –ø—Ä–æ–µ–∫—Ç–∏ |

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
graph_crawler/
‚îú‚îÄ‚îÄ api/              # Simple API (crawl, Crawler, async_crawl)
‚îú‚îÄ‚îÄ client/           # GraphCrawlerClient
‚îú‚îÄ‚îÄ core/             # Node, Edge, Graph, Events, Models
‚îú‚îÄ‚îÄ crawler/          # Spider, Scheduler, LinkProcessor, Filters
‚îú‚îÄ‚îÄ drivers/          # HTTP, Playwright –¥—Ä–∞–π–≤–µ—Ä–∏
‚îú‚îÄ‚îÄ storage/          # Memory, JSON, SQLite, PostgreSQL, MongoDB
‚îú‚îÄ‚îÄ plugins/          # Node –ø–ª–∞–≥—ñ–Ω–∏ (vectorization, content_extractors)
‚îú‚îÄ‚îÄ middleware/       # Rate limiting, Retry, Robots.txt, Proxy
‚îú‚îÄ‚îÄ factories/        # Driver, Storage factories
‚îú‚îÄ‚îÄ containers/       # Dependency Injection containers
‚îú‚îÄ‚îÄ adapters/         # BeautifulSoup adapter
‚îú‚îÄ‚îÄ exporters/        # JSON, CSV, DOT exporters
‚îî‚îÄ‚îÄ utils/            # URL utils, DNS cache, Bloom filter
```

## –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è

```bash
pytest
pytest --cov=package_crawler
```

## –í–∏–º–æ–≥–∏

- **Python 3.11+** (–º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–µ—Ä—Å—ñ—è)
- –ó–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ: –¥–∏–≤. [requirements.txt](requirements.txt)

### –Ø–∫—É –≤–µ—Ä—Å—ñ—é Python –æ–±—Ä–∞—Ç–∏?

| –í–µ—Ä—Å—ñ—è | –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è | –ü—Ä–∏–º—ñ—Ç–∫–∏ |
|--------|-------------------|----------|
| **3.14** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å | Free-threading (GIL=0), ~3.2x —à–≤–∏–¥—à–µ |
| **3.12-3.13** | –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∫–æ—Ä–æ–±–∫–∏ | –°—Ç–∞–±—ñ–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ (pyvis, networkx) |
| **3.11** | –°—É–º—ñ—Å–Ω—ñ—Å—Ç—å | –í—Å—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –ø—Ä–∞—Ü—é—é—Ç—å |

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è Python 3.14

```bash
# Free-threading –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
export PYTHON_GIL=0

# JIT compiler (—É–≤—ñ–º–∫–Ω–µ–Ω–æ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
export PYTHON_JIT=1

# –ó–∞–ø—É—Å–∫
python your_crawler.py
```

**–û—á—ñ–∫—É–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:**
- GIL enabled: ~115 pages/sec
- GIL disabled (free-threading): **~320 pages/sec** (3.2x —à–≤–∏–¥—à–µ!)

> ‚ö†Ô∏è **–ü—Ä–∏–º—ñ—Ç–∫–∞**: –î–µ—è–∫—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó (pyvis, plotly) –º–æ–∂—É—Ç—å –±—É—Ç–∏ –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ –Ω–∞ Python 3.14. –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∫–æ—Ä–æ–±–∫–∏ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Python 3.12 –∞–±–æ 3.13.

## –õ—ñ—Ü–µ–Ω–∑—ñ—è

[MIT](LICENSE)

## –ê–≤—Ç–æ—Ä

0-EternalJunior-0
