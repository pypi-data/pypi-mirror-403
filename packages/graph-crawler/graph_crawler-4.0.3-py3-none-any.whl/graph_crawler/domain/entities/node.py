"""–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—É–∑–ª–∞ –≥—Ä–∞—Ñ—É (–≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏) - Pydantic –º–æ–¥–µ–ª—å.

Python 3.14 Optimizations:
- Free-threading support –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ–≥–æ HTML –ø–∞—Ä—Å–∏–Ω–≥—É (2-4x speedup!)
- Adaptive thread pool sizing based on GIL status
- Thread-local parser instances –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è contention
"""

import asyncio
import logging
import os
import sys
import threading
import uuid
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Protocol, Tuple

from pydantic import BaseModel, ConfigDict, Field, PrivateAttr, field_validator

# –¶–µ –≤–∏—Ä—ñ—à—É—î circular imports —á–µ—Ä–µ–∑ Dependency Inversion Principle
from graph_crawler.domain.interfaces.node_interfaces import (
    INodePluginContext,
    INodePluginType,
    IPluginManager,
)
from graph_crawler.domain.value_objects.lifecycle import (
    NodeLifecycle,
    NodeLifecycleError,
)
from graph_crawler.domain.value_objects.models import ContentType
from graph_crawler.infrastructure.adapters.base import BaseTreeAdapter

if TYPE_CHECKING:
    from graph_crawler.extensions.plugins.node import NodePluginContext, NodePluginType

logger = logging.getLogger(__name__)

# ============ PYTHON 3.14 FREE-THREADING DETECTION ============

def _detect_free_threading() -> bool:
    """
    –í–∏–∑–Ω–∞—á–∞—î —á–∏ Python 3.14 free-threading enabled.
    
    Returns:
        True —è–∫—â–æ GIL disabled (free-threading mode)
        False —è–∫—â–æ GIL enabled –∞–±–æ Python < 3.14
    """
    if not hasattr(sys, '_is_gil_enabled'):
        return False
    return not sys._is_gil_enabled()


_is_free_threaded = _detect_free_threading()

# ============ OPTIMAL THREAD POOL SIZE (PYTHON 3.14 OPTIMIZED) ============

if _is_free_threaded:
    # üöÄ Free-threading: CPU-bound –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø—Ä–∞–≤–¥—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π!
    # –ú–æ–∂–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à–µ workers - 2x speedup –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–∏–π
    _max_html_workers = (os.cpu_count() or 4) * 2
    logger.info(
        f"üöÄ Python 3.14 Free-threading detected! "
        f"HTML parser optimized with {_max_html_workers} workers (2x CPU cores)"
    )
else:
    # –ó GIL: –æ–±–º–µ–∂—É—î–º–æ –¥–æ cpu_count (–±—ñ–ª—å—à–µ –Ω–µ –¥–∞—Å—Ç—å –µ—Ñ–µ–∫—Ç—É)
    _max_html_workers = os.cpu_count() or 4
    logger.info(
        f"GIL enabled. HTML parser threads: {_max_html_workers} "
        f"(Python {sys.version_info.major}.{sys.version_info.minor})"
    )


def _init_parser_thread():
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î parser resources –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ thread (Python 3.14 optimization).
    
    –í free-threaded —Ä–µ–∂–∏–º—ñ –∫–æ–∂–µ–Ω thread –º–∞—î —Å–≤—ñ–π:
    - lxml parser instance (—É–Ω–∏–∫–∞—î–º–æ contention)
    - BeautifulSoup cache
    - Thread-local buffer
    
    –¶–µ –¥–∞—î 2-4x speedup –¥–ª—è batch HTML parsing!
    """
    thread_id = threading.get_ident()
    
    if not hasattr(_init_parser_thread, 'parsers'):
        _init_parser_thread.parsers = {}
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ parser –¥–ª—è —Ü—å–æ–≥–æ thread
    try:
        from lxml import etree
        parser = etree.HTMLParser(
            remove_blank_text=True,
            remove_comments=True,
            encoding='utf-8',
            huge_tree=False,  # Security
        )
        _init_parser_thread.parsers[thread_id] = parser
        logger.debug(
            f"Parser initialized for thread {thread_id} "
            f"(free_threaded={_is_free_threaded})"
        )
    except ImportError:
        logger.warning("lxml not available, falling back to html.parser")


# ============ THREAD POOL –¥–ª—è HTML PARSING (PYTHON 3.14 OPTIMIZED) ============
_html_executor = ThreadPoolExecutor(
    max_workers=_max_html_workers,
    thread_name_prefix="html_parser_",
    initializer=_init_parser_thread,
)

logger.info(
    f"HTML executor initialized: "
    f"workers={_max_html_workers}, "
    f"free_threaded={_is_free_threaded}, "
    f"python={sys.version_info.major}.{sys.version_info.minor}"
)


# ============ –ê–ë–°–¢–†–ê–ö–¶–Ü–á ============
# Dependency Inversion Principle: Node –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ–π, –Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π


class ITreeAdapter(Protocol):
    """
    –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Tree Adapter (Protocol –¥–ª—è DIP).

    Node –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ü—å–æ–≥–æ —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É, –∞ –Ω–µ –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ BaseTreeAdapter.

    Example:
        >>> class CustomAdapter:
        ...     def parse(self, html: str):
        ...         return custom_tree
        >>> node = Node(url="...", tree_parser=CustomAdapter())
    """

    def parse(self, html: str) -> Any:
        """–ü–∞—Ä—Å–∏—Ç—å HTML –≤ –¥–µ—Ä–µ–≤–æ."""
        ...


class IContentHashStrategy(Protocol):
    """
    –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è content hash (Protocol).

        –ó–∞–±–µ–∑–ø–µ—á—É—î –¥–æ—Ç—Ä–∏–º–∞–Ω–Ω—è Liskov Substitution Principle:
        - –ß—ñ—Ç–∫–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç: –º–µ—Ç–æ–¥ –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä–æ–∫—É (SHA256 hex digest)
        - –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –º–æ–∂–µ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤–ª–∞—Å–Ω—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é, –Ω–µ –ø–æ—Ä—É—à—É—é—á–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç
        - –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≥–∞—Ä–∞–Ω—Ç—É—î —â–æ —Ü–µ –≤–∞–ª—ñ–¥–Ω–∏–π —Ö–µ—à

        Example:
            >>> class CustomHashStrategy:
            ...     def compute_hash(self, node: 'Node') -> str:
            ...         # –ü–æ–≤–µ—Ä—Ç–∞—î SHA256 hex digest (64 —Å–∏–º–≤–æ–ª–∏)
            ...         return hashlib.sha256(node.metadata['h1'].encode()).hexdigest()
            >>>
            >>> node.hash_strategy = CustomHashStrategy()
            >>> hash_value = node.get_content_hash()  # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î –∫–∞—Å—Ç–æ–º–Ω—É —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é
    """

    def compute_hash(self, node: "Node") -> str:
        """
        –û–±—á–∏—Å–ª—é—î content hash –¥–ª—è –Ω–æ–¥–∏.

        –ö–æ–Ω—Ç—Ä–∞–∫—Ç:
        - MUST –ø–æ–≤–µ—Ä—Ç–∞—Ç–∏ SHA256 hex digest (64 —Å–∏–º–≤–æ–ª–∏, lowercase)
        - MUST –±—É—Ç–∏ –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–º (–æ–¥–Ω–∞–∫–æ–≤—ñ –¥–∞–Ω—ñ ‚Üí –æ–¥–Ω–∞–∫–æ–≤–∏–π —Ö–µ—à)
        - MUST –≤–∏–∫–ª–∏–∫–∞—Ç–∏—Å—è —Ç—ñ–ª—å–∫–∏ –ø—ñ—Å–ª—è process_html() (HTML_STAGE)

        Args:
            node: Node –¥–ª—è —è–∫–æ—ó –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è —Ö–µ—à

        Returns:
            SHA256 hex digest string (64 —Å–∏–º–≤–æ–ª–∏)
        """
        ...


class Node(BaseModel):
    """
    –ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—É–∑–ª–∞ –≥—Ä–∞—Ñ—É (–≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∞) - Pydantic –º–æ–¥–µ–ª—å.

    –ö–æ–∂–µ–Ω –≤—É–∑–æ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –æ–¥–Ω—É –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏.
    –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –º–æ–∂—É—Ç—å —É—Å–ø–∞–¥–∫–æ–≤—É–≤–∞—Ç–∏ —Ü–µ–π –∫–ª–∞—Å –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –≤–ª–∞—Å–Ω–æ—ó –ª–æ–≥—ñ–∫–∏.

    –í–ê–ñ–õ–ò–í–û: HTML –Ω–µ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è —É –ø–∞–º'—è—Ç—ñ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó RAM.
    HTML –æ–±—Ä–æ–±–ª—è—î—Ç—å—Å—è –æ–¥—Ä–∞–∑—É —Ç–∞ –≤–∏–¥–∞–ª—è—î—Ç—å—Å—è.

     –ñ–ò–¢–¢–Ñ–í–ò–ô –¶–ò–ö–õ NODE (2 –ß–Ü–¢–ö–Ü –ï–¢–ê–ü–ò):

    –ï–¢–ê–ü 1: –°–¢–í–û–†–ï–ù–ù–Ø - URL_STAGE (__init__)
        –î–æ—Å—Ç—É–ø–Ω–æ: url, depth, should_scan, can_create_edges
        –©–æ –º–æ–∂–Ω–∞:
           * –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ URL –Ω–∞ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
           * –í–∏–∑–Ω–∞—á–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø–æ –¥–æ–º–µ–Ω—É
           * –í—Å—Ç–∞–Ω–æ–≤–ª—é–≤–∞—Ç–∏ should_scan, can_create_edges
           * –í–∏–∫–ª–∏–∫–∞—Ç–∏ on_node_created —Ö—É–∫–∏
         –©–æ –ù–ï–ú–û–ñ–ù–ê:
           * –ü—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ HTML (–π–æ–≥–æ —â–µ –Ω–µ–º–∞—î!)
           * –í–∏—Ç—è–≥—É–≤–∞—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ (—ó—Ö —â–µ –Ω–µ–º–∞—î!)
           * –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏

    –ï–¢–ê–ü 2: –û–ë–†–û–ë–ö–ê HTML - HTML_STAGE (process_html)
         INPUT (–Ω–∞ –ø–æ—á–∞—Ç–∫—É process_html):
           * html - HTML –∫–æ–Ω—Ç–µ–Ω—Ç (string)
           * html_tree - DOM –¥–µ—Ä–µ–≤–æ (–ø—ñ—Å–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É)
           * parser - Tree adapter –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–µ—Ä–µ–≤–æ–º

         –û–ë–†–û–ë–ö–ê (—á–µ—Ä–µ–∑ –ø–ª–∞–≥—ñ–Ω–∏):
           * MetadataExtractorPlugin –≤–∏—Ç—è–≥—É—î title, h1, description, keywords
           * LinkExtractorPlugin –≤–∏—Ç—è–≥—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è <a href>
           * CustomPlugins - –≤–∞—à–∞ –≤–ª–∞—Å–Ω–∞ –ª–æ–≥—ñ–∫–∞

         OUTPUT (–ø—ñ—Å–ª—è process_html):
           * metadata - –∑–∞–ø–æ–≤–Ω–µ–Ω—ñ –º–µ—Ç–∞–¥–∞–Ω—ñ (dict)
           * user_data - –¥–∞–Ω—ñ –≤—ñ–¥ –ø–ª–∞–≥—ñ–Ω—ñ–≤ (dict)
           * extracted_links - —Å–ø–∏—Å–æ–∫ URL (list)
           * HTML —Ç–∞ html_tree –í–ò–î–ê–õ–ï–ù–Ü –∑ –ø–∞–º'—è—Ç—ñ!

        –©–æ –º–æ–∂–Ω–∞:
           * –í–∏—Ç—è–≥—É–≤–∞—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ —á–µ—Ä–µ–∑ –ø–ª–∞–≥—ñ–Ω–∏
           * –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏
           * –®—É–∫–∞—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç—ñ
           * –í–∏—Ç—è–≥—É–≤–∞—Ç–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
           * –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –ø–ª–∞–≥—ñ–Ω–∏
         –©–æ –ù–ï–ú–û–ñ–ù–ê:
           * –ó–º—ñ–Ω—é–≤–∞—Ç–∏ –±–∞–∑–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (url, depth)

    –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –µ—Ç–∞–ø—ñ–≤ –∑–∞–ø–æ–±—ñ–≥–∞—î –ø–æ–º–∏–ª–∫–∞–º:
     –ü–æ—à—É–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –≤ HTML –¥–æ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
     –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è metadata –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –Ω–æ–¥–∏
     –í–∏–∫–ª–∏–∫ –º–µ—Ç–æ–¥—ñ–≤ –Ω–µ –Ω–∞ —Å–≤–æ—î–º—É –µ—Ç–∞–ø—ñ

    –ê—Ç—Ä–∏–±—É—Ç–∏:
        url: URL —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        node_id: –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –≤—É–∑–ª–∞
        metadata: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç–∞–¥–∞–Ω—ñ (–∑–∞–ø–æ–≤–Ω—é—é—Ç—å—Å—è –ø–ª–∞–≥—ñ–Ω–∞–º–∏ –ø—ñ—Å–ª—è process_html)
        scanned: –ß–∏ –±—É–ª–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø—Ä–æ—Å–∫–∞–Ω–æ–≤–∞–Ω–∞
        should_scan: –ß–∏ —Ç—Ä–µ–±–∞ —Å–∫–∞–Ω—É–≤–∞—Ç–∏ —Ü–µ–π –≤—É–∑–æ–ª (False –¥–ª—è –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö –ø–æ—Å–∏–ª–∞–Ω—å)
        can_create_edges: –ß–∏ –º–æ–∂–µ –≤—É–∑–æ–ª —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –Ω–æ–≤—ñ –∑–≤'—è–∑–∫–∏ (edges)
        depth: –ì–ª–∏–±–∏–Ω–∞ –≤—É–∑–ª–∞ –≤—ñ–¥ –∫–æ—Ä–µ–Ω–µ–≤–æ–≥–æ
        created_at: –ß–∞—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—É–∑–ª–∞
        response_status: HTTP —Å—Ç–∞—Ç—É—Å –∫–æ–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        lifecycle_stage: –ü–æ—Ç–æ—á–Ω–∏–π –µ—Ç–∞–ø –∂–∏—Ç—Ç—î–≤–æ–≥–æ —Ü–∏–∫–ª—É
        user_data: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥–∞–Ω—ñ –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–∫–∞—Å—Ç–æ–º–Ω—ñ –ø–æ–ª—è, –∑–∞–ø–æ–≤–Ω—é—é—Ç—å—Å—è –ø–ª–∞–≥—ñ–Ω–∞–º–∏)
    """

    # ============ PYDANTIC FIELDS ============
    # –ë–∞–∑–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–ï–¢–ê–ü 1: URL_STAGE)
    url: str
    node_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    depth: int = Field(default=0, ge=0)
    should_scan: bool = True
    can_create_edges: bool = True
    created_at: datetime = Field(default_factory=datetime.now)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ï–¢–ê–ü 2: HTML_STAGE
    metadata: Dict[str, Any] = Field(default_factory=dict)
    user_data: Dict[str, Any] = Field(default_factory=dict)
    scanned: bool = False
    response_status: Optional[int] = None

    # HTTP Redirect —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è (–∑–∞–ø–æ–≤–Ω—é—î—Ç—å—Å—è –≤ NodeScanner –ø—ñ—Å–ª—è fetch)
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ä–µ–¥—ñ—Ä–µ–∫—Ç—ñ–≤ –≤ –∫–∞—Å—Ç–æ–º–Ω–∏—Ö Node –∫–ª–∞—Å–∞—Ö
    # PrivateAttr - –Ω–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è, –∞–ª–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π —è–∫ –∞—Ç—Ä–∏–±—É—Ç
    _response_final_url: Optional[str] = PrivateAttr(default=None)
    _response_original_url: Optional[str] = PrivateAttr(default=None)
    _response_is_redirect: bool = PrivateAttr(default=False)

    # Content Type - –≤–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (HTML, JSON, image, empty —Ç–æ—â–æ)
    # –ó–∞–ø–æ–≤–Ω—é—î—Ç—å—Å—è –≤ NodeScanner –ø—ñ—Å–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è HTTP response
    content_type: ContentType = ContentType.UNKNOWN

    # Incremental Crawling - content hash (–æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è –ø—ñ—Å–ª—è process_html)
    content_hash: Optional[str] = None

    # Scheduler –ø–µ—Ä–µ–≤—ñ—Ä—è—î —Ü–µ –ø–æ–ª–µ –ü–ï–†–ï–î URLRule (–¥–∏–≤. scheduler.py)
    priority: Optional[int] = Field(default=None, ge=1, le=10)

    # Lifecycle
    lifecycle_stage: NodeLifecycle = NodeLifecycle.URL_STAGE

    # Plugin Manager (–Ω–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è)
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Any –∑–∞–º—ñ—Å—Ç—å Protocol –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ Pydantic
    plugin_manager: Optional[Any] = Field(default=None, exclude=True)

    # Tree Parser/Adapter (–Ω–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è)
    tree_parser: Optional[Any] = Field(default=None, exclude=True)

    hash_strategy: Optional[Any] = Field(default=None, exclude=True)

    # Pydantic configuration
    model_config = ConfigDict(
        arbitrary_types_allowed=True,  # –î–ª—è NodePluginManager, BeautifulSoup
        validate_assignment=True,  # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –ø—Ä–∏ –ø—Ä–∏—Å–≤–æ—î–Ω–Ω—ñ
        use_enum_values=False,  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ enum –æ–±'—î–∫—Ç–∏
    )

    @field_validator("url")
    @classmethod
    def validate_url(cls, v: str) -> str:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è URL."""
        from urllib.parse import urlparse

        from graph_crawler.shared.exceptions import InvalidURLError

        if not v:
            raise InvalidURLError("URL cannot be empty")

        if not v.startswith(("http://", "https://")):
            raise InvalidURLError(f"URL must start with http:// or https://, got: {v}")

        parsed = urlparse(v)
        if not parsed.netloc:
            raise InvalidURLError(f"URL must have a valid domain: {v}")

        return v

    def model_post_init(self, __context: Any) -> None:
        """–í–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ Pydantic."""
        # –í–∏–∫–ª–∏–∫–∞—î–º–æ —Ö—É–∫ ON_NODE_CREATED (–ï–¢–ê–ü 1)
        self._trigger_node_created_hook()

    def _trigger_node_created_hook(self):
        """
        –í–∏–∫–ª–∏–∫–∞—î —Ö—É–∫ ON_NODE_CREATED –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–¥–∏.

        –¶–µ –ï–¢–ê–ü 1 - –¥–æ—Å—Ç—É–ø–Ω–∏–π –¢–Ü–õ–¨–ö–ò URL.
        –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –º–æ–∂–µ –¥–æ–¥–∞—Ç–∏ —Å–≤–æ—é –ª–æ–≥—ñ–∫—É —á–µ—Ä–µ–∑ –ø–ª–∞–≥—ñ–Ω–∏.
        """
        if not self.plugin_manager:
            return

        # Node –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ Protocol interfaces (INodePluginContext, IPluginManager),
        # –∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó (NodePluginContext) —ñ–º–ø–æ—Ä—Ç—É—é—Ç—å—Å—è lazy –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è circular deps.
        from graph_crawler.extensions.plugins.node import (
            NodePluginContext,
            NodePluginType,
        )

        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–ª–∞–≥—ñ–Ω—ñ–≤
        context = NodePluginContext(
            node=self,
            url=self.url,
            depth=self.depth,
            should_scan=self.should_scan,
            can_create_edges=self.can_create_edges,
        )

        # –í–∏–∫–æ–Ω—É—î–º–æ –ø–ª–∞–≥—ñ–Ω–∏ ON_NODE_CREATED (sync)
        context = self.plugin_manager.execute_sync(
            NodePluginType.ON_NODE_CREATED, context
        )

        # –û–Ω–æ–≤–ª—é—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –º—ñ–≥ —ó—Ö –∑–º—ñ–Ω–∏—Ç–∏)
        self.should_scan = context.should_scan
        self.can_create_edges = context.can_create_edges
        self.user_data.update(context.user_data)

    async def process_html(self, html: str) -> List[str]:
        """
        ============ –ï–¢–ê–ü 2: HTML_STAGE ============

        Async –æ–±—Ä–æ–±–ª—è—î HTML —á–µ—Ä–µ–∑ –ß–ò–°–¢–£ –ü–õ–ê–ì–Ü–ù–ù–£ –°–ò–°–¢–ï–ú–£. –¢–µ–ø–µ—Ä async –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ async –ø–ª–∞–≥—ñ–Ω—ñ–≤ (ML, LLM, API).

        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö URL –ø–æ—Å–∏–ª–∞–Ω—å

        Raises:
            NodeLifecycleError: –Ø–∫—â–æ –Ω–æ–¥–∞ –≤–∂–µ –ø—Ä–æ—Å–∫–∞–Ω–æ–≤–∞–Ω–∞
        """
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ lifecycle
        if self.lifecycle_stage == NodeLifecycle.HTML_STAGE:
            logger.warning(f"Node already processed: {self.url}")
            return []

        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –Ω–∞ –ï–¢–ê–ü 2
        self.lifecycle_stage = NodeLifecycle.HTML_STAGE

        # –ö—Ä–æ–∫ 1: –ü–∞—Ä—Å–∏–Ω–≥ HTML (ASYNC —á–µ—Ä–µ–∑ ThreadPoolExecutor –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
        parser, html_tree = await self._parse_html_async(html)

        # –ö—Ä–æ–∫ 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ç–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–ª–∞–≥—ñ–Ω—ñ–≤ (async)
        # –ü–†–ò–ú–Ü–¢–ö–ê: _update_from_context —Ç–µ–ø–µ—Ä –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ _execute_plugins
        # –ø–µ—Ä–µ–¥ ON_AFTER_SCAN –ø–ª–∞–≥—ñ–Ω–∞–º–∏, —â–æ–± –∫–∞—Å—Ç–æ–º–Ω—ñ Node –∫–ª–∞—Å–∏ –º–æ–≥–ª–∏ –∑–∞–ø–æ–≤–Ω–∏—Ç–∏ –ø–æ–ª—è
        context = await self._execute_plugins(html, html_tree, parser)

        # –ö—Ä–æ–∫ 3: –§—ñ–Ω–∞–ª—å–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è user_data –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–¥–ª—è –∑–º—ñ–Ω –≤—ñ–¥ ON_AFTER_SCAN –ø–ª–∞–≥—ñ–Ω—ñ–≤)
        # metadata –≤–∂–µ –æ–Ω–æ–≤–ª–µ–Ω–æ –≤ _execute_plugins
        self.user_data.update(context.user_data)

        # –ö—Ä–æ–∫ 4: –û–±—á–∏—Å–ª–µ–Ω–Ω—è hash
        self._compute_content_hash()

        # –ö—Ä–æ–∫ 5: –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
        self._cleanup_memory(html, html_tree, context)

        logger.debug(
            f"Processed HTML for {self.url}: {len(context.extracted_links)} links, metadata keys: {list(self.metadata.keys())}"
        )

        return context.extracted_links

    def _parse_html_sync(self, html: str) -> Tuple[Any, Any]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ HTML –≤ –¥–µ—Ä–µ–≤–æ —á–µ—Ä–µ–∑ adapter.
        
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —á–µ—Ä–µ–∑ ThreadPoolExecutor –¥–ª—è –Ω–µ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è event loop.
        
        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            
        Returns:
            Tuple (parser, html_tree)
        """
        if self.tree_parser is None:
            from graph_crawler.infrastructure.adapters import get_default_parser

            parser = get_default_parser()
        else:
            parser = self.tree_parser

        html_tree = parser.parse(html)
        return parser, html_tree
    
    async def _parse_html_async(self, html: str) -> Tuple[Any, Any]:
        """
        Async –ø–∞—Ä—Å–∏–Ω–≥ HTML —á–µ—Ä–µ–∑ ThreadPoolExecutor.
        
        –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–Ø: BeautifulSoup –ø–∞—Ä—Å–∏–Ω–≥ —î CPU-bound –æ–ø–µ—Ä–∞—Ü—ñ—î—é —ñ –±–ª–æ–∫—É—î event loop.
        –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ —ó—ó –≤ ThreadPoolExecutor –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏.
        
        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            
        Returns:
            Tuple (parser, html_tree)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            _html_executor,
            self._parse_html_sync,
            html
        )

    async def _execute_plugins(self, html: str, html_tree: Any, parser: Any) -> Any:
        """
        Async –≤–∏–∫–æ–Ω—É—î –ø–ª–∞–≥—ñ–Ω–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ HTML. –¢–µ–ø–µ—Ä async –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ async –ø–ª–∞–≥—ñ–Ω—ñ–≤ (ML, LLM, API).

        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            html_tree: –ü–∞—Ä—Å–æ–≤–∞–Ω–µ –¥–µ—Ä–µ–≤–æ
            parser: Tree adapter

        Returns:
            NodePluginContext –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # Node –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ Protocol interfaces –¥–ª—è type hints, –∞–ª–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î
        # –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó —á–µ—Ä–µ–∑ lazy import –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è circular deps.
        from graph_crawler.extensions.plugins.node import (
            NodePluginContext,
            NodePluginType,
        )

        context = NodePluginContext(
            node=self,
            url=self.url,
            depth=self.depth,
            should_scan=self.should_scan,
            can_create_edges=self.can_create_edges,
            html=html,
            html_tree=html_tree,
            parser=parser,
            metadata=self.metadata.copy(),
            user_data=self.user_data.copy(),
        )

        if self.plugin_manager:
            context = await self.plugin_manager.execute(
                NodePluginType.ON_BEFORE_SCAN, context
            )
            context = await self.plugin_manager.execute(
                NodePluginType.ON_HTML_PARSED, context
            )

            # –û–Ω–æ–≤–ª—é—î–º–æ –Ω–æ–¥—É –ü–ï–†–ï–î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º ON_AFTER_SCAN –ø–ª–∞–≥—ñ–Ω—ñ–≤
            # –¶–µ –¥–æ–∑–≤–æ–ª—è—î –∫–∞—Å—Ç–æ–º–Ω–∏–º Node –∫–ª–∞—Å–∞–º –∑–∞–ø–æ–≤–Ω–∏—Ç–∏ –ø–æ–ª—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, text)
            # —è–∫—ñ –ø–æ—Ç—ñ–º –±—É–¥—É—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –ø–ª–∞–≥—ñ–Ω–∞–º–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, RealTimeVectorizerPlugin)
            self._update_from_context(context)

            context = await self.plugin_manager.execute(
                NodePluginType.ON_AFTER_SCAN, context
            )

        return context

    def _update_from_context(self, context: Any):
        """
        –û–Ω–æ–≤–ª—é—î –Ω–æ–¥—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

        Args:
            context: NodePluginContext –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–ª–∞–≥—ñ–Ω—ñ–≤
        """
        self.metadata = context.metadata
        self.user_data.update(context.user_data)

    def _compute_content_hash(self):
        """
        –û–±—á–∏—Å–ª—é—î content hash –¥–ª—è Incremental Crawling.
        """
        try:
            self.content_hash = self.get_content_hash()
            logger.debug(
                f"Content hash computed for {self.url}: {self.content_hash[:16]}..."
            )
        except Exception as e:
            logger.warning(f"Failed to compute content_hash for {self.url}: {e}")
            self.content_hash = None

    def _cleanup_memory(self, html: str, html_tree: Any, context: Any):
        """
        –í–∏–¥–∞–ª—è—î HTML —Ç–∞ –¥–µ—Ä–µ–≤–æ –∑ –ø–∞–º'—è—Ç—ñ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è 20k+ —Å—Ç–æ—Ä—ñ–Ω–æ–∫).

        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            html_tree: –ü–∞—Ä—Å–æ–≤–∞–Ω–µ –¥–µ—Ä–µ–≤–æ
            context: NodePluginContext
        """
        del html
        del html_tree
        context.html = None
        context.html_tree = None

    def get_content_hash(self) -> str:
        """
                –û–±—á–∏—Å–ª—é—î hash –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –¥–µ—Ç–µ–∫—Ü—ñ—ó –∑–º—ñ–Ω (Incremental Crawling).

        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î IContentHashStrategy Protocol –¥–ª—è –≥–∞—Ä–∞–Ω—Ç—ñ—ó –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É.

                –î–ï–§–û–õ–¢–ù–ê –†–ï–ê–õ–Ü–ó–ê–¶–Ü–Ø: SHA256 –≤—ñ–¥ —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏.

                –ö–û–†–ò–°–¢–£–í–ê–ß –ú–û–ñ–ï –ó–ê–î–ê–¢–ò –ö–ê–°–¢–û–ú–ù–£ –°–¢–†–ê–¢–ï–ì–Ü–Æ —á–µ—Ä–µ–∑ hash_strategy:

                Example 1: –ö–∞—Å—Ç–æ–º–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
                    >>> class H1HashStrategy:
                    ...     def compute_hash(self, node):
                    ...         return hashlib.sha256(node.metadata['h1'].encode()).hexdigest()
                    >>>
                    >>> node.hash_strategy = H1HashStrategy()
                    >>> hash_value = node.get_content_hash()

                Example 2: –ù–∞—Å–ª—ñ–¥—É–≤–∞–Ω–Ω—è (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
                    >>> class MyNode(Node):
                    ...     def get_content_hash(self):
                    ...         if self.hash_strategy:
                    ...             return self.hash_strategy.compute_hash(self)
                    ...         return hashlib.sha256(self.metadata['h1'].encode()).hexdigest()

                –í–ê–ñ–õ–ò–í–û: –ú–æ–∂–Ω–∞ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –¢–Ü–õ–¨–ö–ò –ø—ñ—Å–ª—è process_html() (–ï–¢–ê–ü 2: HTML_STAGE).

                Returns:
                    SHA256 hex digest string (64 —Å–∏–º–≤–æ–ª–∏, lowercase)

                Raises:
                    NodeLifecycleError: –Ø–∫—â–æ –≤–∏–∫–ª–∏–∫–∞–Ω–æ –¥–æ process_html()
                    ValueError: –Ø–∫—â–æ hash_strategy –ø–æ–≤–µ—Ä—Ç–∞—î –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π —Ö–µ—à
        """
        import hashlib
        import re

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ lifecycle - –º–æ–∂–Ω–∞ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø—ñ—Å–ª—è process_html
        if self.lifecycle_stage != NodeLifecycle.HTML_STAGE:
            raise NodeLifecycleError(
                f"Cannot compute content_hash at {self.lifecycle_stage.value}. "
                f"Call process_html() first (must be at HTML_STAGE)."
            )

        # –Ø–∫—â–æ –∑–∞–¥–∞–Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ó—ó
        if self.hash_strategy:
            hash_value = self.hash_strategy.compute_hash(self)

            # LSP: –í–∞–ª—ñ–¥–∞—Ü—ñ—è —â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Ü–µ –≤–∞–ª—ñ–¥–Ω–∏–π SHA256 —Ö–µ—à
            if not isinstance(hash_value, str):
                raise ValueError(
                    f"Hash strategy must return string, got {type(hash_value).__name__}. "
                    f"Strategy: {type(self.hash_strategy).__name__}"
                )

            from graph_crawler.shared.constants import (
                SHA256_HASH_LENGTH,
                SHA256_HASH_PATTERN,
            )

            if not re.match(SHA256_HASH_PATTERN, hash_value):
                raise ValueError(
                    f"Hash strategy must return valid SHA256 hex digest ({SHA256_HASH_LENGTH} chars, lowercase), "
                    f"got: '{hash_value[:20]}...' (len={len(hash_value)}). "
                    f"Strategy: {type(self.hash_strategy).__name__}"
                )

            # LSP: –í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–æ—Å—Ç—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –≤–∏–∫–ª–∏–∫—É
            if not hasattr(self, "_hash_determinism_validated"):
                self._validate_hash_strategy_deterministic(hash_value)
                self._hash_determinism_validated = True

            return hash_value

        # –î–µ—Ñ–æ–ª—Ç–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è - hash –≤—ñ–¥ —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        from graph_crawler.shared.constants import DEFAULT_HASH_ENCODING

        text = self.user_data.get("text_content", "")
        return hashlib.sha256(text.encode(DEFAULT_HASH_ENCODING)).hexdigest()

    def _validate_hash_strategy_deterministic(self, first_hash: str) -> None:
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ hash_strategy –¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∞ (LSP Principle).

        –í–∏–∫–ª–∏–∫–∞—î —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –¥–≤—ñ—á—ñ –∑ —Ç–∏–º–∏ —Å–∞–º–∏–º–∏ –¥–∞–Ω–∏–º–∏ —ñ –ø–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —Ö–µ—à—ñ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ.
        –¶–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è incremental crawling, –±–æ –Ω–µ–¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è
        –ø—Ä–∏–∑–≤–µ–¥–µ –¥–æ —Ö–∏–±–Ω–∏—Ö —Å–ø—Ä–∞—Ü—é–≤–∞–Ω—å change detection.

        Args:
            first_hash: –ü–µ—Ä—à–∏–π –æ–±—á–∏—Å–ª–µ–Ω–∏–π —Ö–µ—à –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

        Raises:
            ValueError: –Ø–∫—â–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –Ω–µ–¥–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∞ (–ø–æ–≤–µ—Ä—Ç–∞—î —Ä—ñ–∑–Ω—ñ —Ö–µ—à—ñ)

        Warning:
            –¶–µ–π –º–µ—Ç–æ–¥ –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ —Ö–µ—à—É.
            –Ø–∫—â–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–æ–≤–Ω—ñ—à–Ω—ñ –∑–º—ñ–Ω–Ω—ñ (—á–∞—Å, –≤–∏–ø–∞–¥–∫–æ–≤—ñ —á–∏—Å–ª–∞),
            —Ç–µ—Å—Ç –º–æ–∂–µ –¥–∞–≤–∞—Ç–∏ false positive.
        """
        if not self.hash_strategy:
            return

        # –í–∏–∫–ª–∏–∫–∞—î–º–æ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é –¥—Ä—É–≥–∏–π —Ä–∞–∑ –∑ —Ç–∏–º–∏ —Å–∞–º–∏–º–∏ –¥–∞–Ω–∏–º–∏
        second_hash = self.hash_strategy.compute_hash(self)

        if first_hash != second_hash:
            raise ValueError(
                f"Hash strategy is NOT DETERMINISTIC! "
                f"Got different hashes for same data:\n"
                f"  1st call: {first_hash[:32]}...\n"
                f"  2nd call: {second_hash[:32]}...\n"
                f"Strategy: {type(self.hash_strategy).__name__}\n\n"
                f"This will break incremental crawling! "
                f"Hash strategy MUST return same hash for same input data."
            )

    def mark_as_scanned(self):
        """–ü–æ–∑–Ω–∞—á–∞—î –≤—É–∑–æ–ª —è–∫ –ø—Ä–æ—Å–∫–∞–Ω–æ–≤–∞–Ω–∏–π."""
        self.scanned = True

    def model_dump(self, **kwargs) -> Dict[str, Any]:
        """
        –°–µ—Ä—ñ–∞–ª—ñ–∑—É—î –≤—É–∑–æ–ª —É —Å–ª–æ–≤–Ω–∏–∫.

        Pydantic –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
        - –°–µ—Ä—ñ–∞–ª—ñ–∑—É—î –≤—Å—ñ –ø–æ–ª—è (–≤–∫–ª—é—á–∞—é—á–∏ –∫–∞—Å—Ç–æ–º–Ω—ñ –≤ –ø—ñ–¥–∫–ª–∞—Å–∞—Ö)
        - –í–∏–∫–ª—é—á–∞—î plugin_manager (exclude=True)
        - –ö–æ–Ω–≤–µ—Ä—Ç—É—î datetime, enum –≤ JSON-compatible —Ñ–æ—Ä–º–∞—Ç–∏
        """
        data = super().model_dump(**kwargs)

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ lifecycle_stage –≤ string –¥–ª—è JSON
        if "lifecycle_stage" in data and isinstance(
            data["lifecycle_stage"], NodeLifecycle
        ):
            data["lifecycle_stage"] = data["lifecycle_stage"].value

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ content_type –≤ string –¥–ª—è JSON
        if "content_type" in data and isinstance(data["content_type"], ContentType):
            data["content_type"] = data["content_type"].value

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ datetime –≤ ISO format
        if "created_at" in data and isinstance(data["created_at"], datetime):
            data["created_at"] = data["created_at"].isoformat()

        return data

    @classmethod
    def model_validate(
        cls, obj: Any, context: Optional[Dict] = None, **kwargs
    ) -> "Node":
        """
        –î–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑—É—î –≤—É–∑–æ–ª –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞ –∞–±–æ JSON.

        Pydantic –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
        - –í–∞–ª—ñ–¥—É—î –≤—Å—ñ –ø–æ–ª—è
        - –ö–æ–Ω–≤–µ—Ä—Ç—É—î —Ç–∏–ø–∏ (str -> datetime, str -> enum)
        - –ü—ñ–¥—Ç—Ä–∏–º—É—î –∫–∞—Å—Ç–æ–º–Ω—ñ –ø–æ–ª—è –≤ –ø—ñ–¥–∫–ª–∞—Å–∞—Ö

        Args:
            obj: –û–±'—î–∫—Ç –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
            context: –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—è–º–∏ (plugin_manager, tree_parser)
            **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è Pydantic

        Returns:
            –í–∞–ª—ñ–¥–æ–≤–∞–Ω–∏–π Node –æ–±'—î–∫—Ç –∑ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–º–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—è–º–∏

        Example:
            >>> from graph_crawler.infrastructure.adapters.beautifulsoup_adapter import BeautifulSoupAdapter
            >>> context = {
            ...     'plugin_manager': NodePluginManager(),
            ...     'tree_parser': BeautifulSoupAdapter()
            ... }
            >>> node = Node.model_validate(node_dict, context=context)
        """
        # –Ø–∫—â–æ —Ü–µ —Å–ª–æ–≤–Ω–∏–∫, –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ lifecycle_stage, content_type —Ç–∞ created_at
        if isinstance(obj, dict):
            if "lifecycle_stage" in obj and isinstance(obj["lifecycle_stage"], str):
                obj["lifecycle_stage"] = NodeLifecycle(obj["lifecycle_stage"])

            if "content_type" in obj and isinstance(obj["content_type"], str):
                obj["content_type"] = ContentType(obj["content_type"])

            if "created_at" in obj and isinstance(obj["created_at"], str):
                obj["created_at"] = datetime.fromisoformat(obj["created_at"])

        node = super().model_validate(obj, **kwargs)

        # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ
        #  –í–ê–ñ–õ–ò–í–û: plugin_manager —Ç–∞ tree_parser –Ω–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è
        # –í–æ–Ω–∏ –º–∞—é—Ç—å –±—É—Ç–∏ –ø–µ—Ä–µ–¥–∞–Ω—ñ —á–µ—Ä–µ–∑ context –ø—Ä–∏ –¥–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        if context:
            if "plugin_manager" in context:
                node.plugin_manager = context["plugin_manager"]
            if "tree_parser" in context:
                node.tree_parser = context["tree_parser"]

        return node

    def restore_dependencies(
        self,
        plugin_manager: Optional[IPluginManager] = None,
        tree_parser: Optional[ITreeAdapter] = None,
        hash_strategy: Optional[IContentHashStrategy] = None,
    ):
        """
                –í—ñ–¥–Ω–æ–≤–ª—é—î –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –ø—ñ—Å–ª—è –¥–µ—Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó.

                 –í–ê–ñ–õ–ò–í–û: plugin_manager, tree_parser —Ç–∞ hash_strategy –Ω–µ —Å–µ—Ä—ñ–∞–ª—ñ–∑—É—é—Ç—å—Å—è.
                –ü—ñ—Å–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Node –∑ JSON/SQLite, —Ü—ñ –ø–æ–ª—è –±—É–¥—É—Ç—å None.
                –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ü–µ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π.

        –ü—Ä–∏–π–º–∞—î –±—É–¥—å-—è–∫–∏–π –æ–±'—î–∫—Ç —â–æ —Ä–µ–∞–ª—ñ–∑—É—î Protocol (–Ω–µ —Ç—ñ–ª—å–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∫–ª–∞—Å–∏)
        –î–æ–¥–∞–Ω–æ hash_strategy –¥–ª—è –∫–∞—Å—Ç–æ–º—ñ–∑–∞—Ü—ñ—ó –æ–±—á–∏—Å–ª–µ–Ω–Ω—è hash

                Args:
                    plugin_manager: –ë—É–¥—å-—è–∫–∏–π –æ–±'—î–∫—Ç –∑ –º–µ—Ç–æ–¥–æ–º execute() (IPluginManager Protocol)
                    tree_parser: –ë—É–¥—å-—è–∫–∏–π –æ–±'—î–∫—Ç –∑ –º–µ—Ç–æ–¥–æ–º parse() (ITreeAdapter Protocol)
                    hash_strategy: –ë—É–¥—å-—è–∫–∏–π –æ–±'—î–∫—Ç –∑ –º–µ—Ç–æ–¥–æ–º compute_hash() (IContentHashStrategy Protocol)

                Example:
                    >>> from graph_crawler.extensions.CustomPlugins.node import NodePluginManager
                    >>> from graph_crawler.infrastructure.adapters.beautifulsoup_adapter import BeautifulSoupAdapter
                    >>>
                    >>> node = Node.model_validate(node_dict)
                    >>> node.restore_dependencies(
                    ...     plugin_manager=NodePluginManager(),
                    ...     tree_parser=BeautifulSoupAdapter(),
                    ...     hash_strategy=CustomHashStrategy()
                    ... )
        """
        if plugin_manager is not None:
            self.plugin_manager = plugin_manager
        if tree_parser is not None:
            self.tree_parser = tree_parser
        if hash_strategy is not None:
            self.hash_strategy = hash_strategy

    # LAW OF DEMETER: –ú–µ—Ç–æ–¥–∏-–æ–±–≥–æ—Ä—Ç–∫–∏ –¥–ª—è metadata
    # –ó–∞–º—ñ—Å—Ç—å node.metadata.get("title") –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ node.get_title()

    def _get_metadata_field(self, field: str, default: Any = None) -> Any:
        """
                –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π helper –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–ª—ñ–≤ –∑ metadata.

        –£—Å—É–≤–∞—î –¥—É–±–ª—é–≤–∞–Ω–Ω—è –∫–æ–¥—É –≤ 6+ getter –º–µ—Ç–æ–¥–∞—Ö.
                –¶–µ–Ω—Ç—Ä–∞–ª—ñ–∑—É—î –ª–æ–≥—ñ–∫—É –¥–æ—Å—Ç—É–ø—É –¥–æ metadata –¥–ª—è –¥–æ—Ç—Ä–∏–º–∞–Ω–Ω—è DRY –ø—Ä–∏–Ω—Ü–∏–ø—É.

                Args:
                    field: –ù–∞–∑–≤–∞ –ø–æ–ª—è –≤ metadata
                    default: –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —è–∫—â–æ –ø–æ–ª–µ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ

                Returns:
                    –ó–Ω–∞—á–µ–Ω–Ω—è –ø–æ–ª—è –∞–±–æ default
        """
        return self.metadata.get(field, default) if self.metadata else default

    def get_title(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ title —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("title")

    def get_description(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ description —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("description")

    def get_h1(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("h1")

    def get_keywords(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ keywords —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("keywords")

    def get_canonical_url(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ canonical URL —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("canonical_url")

    def get_language(self) -> Optional[str]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –º–æ–≤—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (Law of Demeter wrapper)."""
        return self._get_metadata_field("language")

    def get_meta_value(self, key: str, default: Any = None) -> Any:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ metadata –∑–∞ –∫–ª—é—á–µ–º (Law of Demeter wrapper)."""
        return self._get_metadata_field(key, default)

    def __repr__(self):
        return (
            f"Node(url={self.url}, lifecycle={self.lifecycle_stage.value}, "
            f"content_type={self.content_type.value}, scanned={self.scanned}, "
            f"should_scan={self.should_scan}, can_create_edges={self.can_create_edges}, "
            f"depth={self.depth})"
        )
