"""SmartPageFinderPlugin - ML –ø–ª–∞–≥—ñ–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫.

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î g4f (GPT4Free) –¥–ª—è —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–æ–∫.
–ü–ª–∞–≥—ñ–Ω –ø—Ä–∏–π–º–∞—î –ø—Ä–æ–º–ø—Ç –∑ –æ–ø–∏—Å–æ–º —Ç–æ–≥–æ, —â–æ —à—É–∫–∞—î–º–æ, —ñ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
1. –ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
2. –í–∏–∑–Ω–∞—á–∞—î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (is_target_page)
3. –í–∏—Å—Ç–∞–≤–ª—è—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ –¥–ª—è –ø–æ—Å–∏–ª–∞–Ω—å (child_priorities)
4. –ú–æ–∂–µ –¥–æ–∑–≤–æ–ª—è—Ç–∏/–±–ª–æ–∫—É–≤–∞—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ URL (explicit_scan_decisions)

–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
    >>> from graph_crawler.extensions.plugins.node.smart_page_finder import SmartPageFinderPlugin
    >>>
    >>> # –ü–æ—à—É–∫ –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ–≤ –ø–µ–≤–Ω–æ—ó –º–∞—Ä–∫–∏
    >>> plugin = SmartPageFinderPlugin(
    ...     search_prompt="–®—É–∫–∞—é —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –∞–≤—Ç–æ–º–æ–±—ñ–ª—è–º–∏ BMW X5 2024 —Ä–æ–∫—É",
    ...     config={'min_relevance_score': 0.7}
    ... )
    >>>
    >>> # –ü–æ—à—É–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Ç–µ–º—É
    >>> plugin = SmartPageFinderPlugin(
    ...     search_prompt="–°—Ç–∞—Ç—Ç—ñ –ø—Ä–æ –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ",
    ... )
    >>>
    >>> # –ü–æ—à—É–∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É 18+
    >>> plugin = SmartPageFinderPlugin(
    ...     search_prompt="–°—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º –¥–ª—è –¥–æ—Ä–æ—Å–ª–∏—Ö (18+)",
    ...     config={'strict_mode': True}
    ... )
    >>>
    >>> # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ GraphCrawler
    >>> graph = gc.crawl(
    ...     "https://example.com",
    ...     plugins=[plugin],
    ...     node_class=SmartFinderNode  # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ
    ... )
    >>>
    >>> # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
    >>> target_pages = [n for n in graph if n.user_data.get('is_target_page')]
"""

import asyncio
import logging
import re
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum

from pydantic import Field

from graph_crawler.extensions.plugins.node.base import (
    BaseNodePlugin,
    NodePluginContext,
    NodePluginType,
)

logger = logging.getLogger(__name__)


class RelevanceLevel(str, Enum):
    """–†—ñ–≤–Ω—ñ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏."""
    HIGH = "high"           # 0.8-1.0 - —Ç–æ—á–Ω–æ —Ç–µ, —â–æ —à—É–∫–∞—î–º–æ
    MEDIUM = "medium"       # 0.5-0.8 - –º–æ–∂–ª–∏–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
    LOW = "low"             # 0.2-0.5 - –º–∞–ª–æ–π–º–æ–≤—ñ—Ä–Ω–æ
    IRRELEVANT = "irrelevant"  # 0.0-0.2 - —Ç–æ—á–Ω–æ –Ω–µ —Ç–µ


class SmartPageFinderPlugin(BaseNodePlugin):
    """
    ML –ø–ª–∞–≥—ñ–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤—ñ g4f.
    
    –ü—Ä–∞—Ü—é—î –Ω–∞ –µ—Ç–∞–ø—ñ ON_AFTER_SCAN - –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è HTML.
    –ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–∞ –≤–∏–∑–Ω–∞—á–∞—î —á–∏ —Ü–µ —Ç–µ, —â–æ —à—É–∫–∞—î–º–æ.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó:
        enabled (bool): –ß–∏ —É–≤—ñ–º–∫–Ω–µ–Ω–æ –ø–ª–∞–≥—ñ–Ω (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º True)
        min_relevance_score (float): –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π score –¥–ª—è –ø–æ–∑–Ω–∞—á–µ–Ω–Ω—è —è–∫ target (0.7)
        priority_boost (int): –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å (10)
        analyze_links (bool): –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å (True)
        analyze_content (bool): –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (True)
        max_text_length (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É (4000)
        model (str): –ú–æ–¥–µ–ª—å g4f –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è ('gpt-4o-mini')
        provider (str): –ü—Ä–æ–≤–∞–π–¥–µ—Ä g4f (None = –∞–≤—Ç–æ–≤–∏–±—ñ—Ä)
        cache_results (bool): –ö–µ—à—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É (True)
        strict_mode (bool): –°—É–≤–æ—Ä–∏–π —Ä–µ–∂–∏–º - —Ç—ñ–ª—å–∫–∏ –≤–∏—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ (False)
        
    –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ user_data:
        is_target_page (bool): –ß–∏ —Ü–µ —à—É–∫–∞–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞
        relevance_score (float): Score —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ 0.0-1.0
        relevance_level (str): –†—ñ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ (high/medium/low/irrelevant)
        relevance_reason (str): –ü–æ—è—Å–Ω–µ–Ω–Ω—è —á–æ–º—É —Ç–∞–∫–∞ –æ—Ü—ñ–Ω–∫–∞
        child_priorities (dict): –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ –¥–ª—è –¥–æ—á—ñ—Ä–Ω—ñ—Ö –ø–æ—Å–∏–ª–∞–Ω—å
        explicit_scan_decisions (dict): –†—ñ—à–µ–Ω–Ω—è –ø—Ä–æ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö URL
        
    –ü—Ä–∏–∫–ª–∞–¥ –∑ –∫–∞—Å—Ç–æ–º–Ω–æ—é Node:
        >>> class SmartFinderNode(gc.Node):
        ...     is_target: bool = Field(default=False)
        ...     relevance_score: float = Field(default=0.0)
        ...     
        ...     def _update_from_context(self, context):
        ...         super()._update_from_context(context)
        ...         self.is_target = context.user_data.get('is_target_page', False)
        ...         self.relevance_score = context.user_data.get('relevance_score', 0.0)
    """
    
    def __init__(self, search_prompt: str, config: Dict[str, Any] = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î SmartPageFinderPlugin.
        
        Args:
            search_prompt: –û–ø–∏—Å —Ç–æ–≥–æ, —â–æ —à—É–∫–∞—î–º–æ (–æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π)
            config: –°–ª–æ–≤–Ω–∏–∫ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        """
        super().__init__(config)
        
        if not search_prompt or not search_prompt.strip():
            raise ValueError("search_prompt –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º")
        
        self.search_prompt = search_prompt.strip()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∞–Ω–∞–ª—ñ–∑—É
        self.min_relevance_score = self.config.get("min_relevance_score", 0.7)
        self.priority_boost = self.config.get("priority_boost", 10)
        self.analyze_links = self.config.get("analyze_links", True)
        self.analyze_content = self.config.get("analyze_content", True)
        self.max_text_length = self.config.get("max_text_length", 4000)
        self.strict_mode = self.config.get("strict_mode", False)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ g4f
        self.model = self.config.get("model", "gpt-4o-mini")
        self.provider_name = self.config.get("provider", None)
        
        # –ö–µ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        self.cache_results = self.config.get("cache_results", True)
        self._cache: Dict[str, Dict[str, Any]] = {}
        
        # g4f –∫–ª—ñ—î–Ω—Ç (–ª—ñ–Ω–∏–≤–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è)
        self._g4f_client = None
        self._g4f_available = None
        
        logger.info(
            f"SmartPageFinderPlugin initialized: "
            f"prompt='{self.search_prompt[:50]}...', "
            f"min_score={self.min_relevance_score}, "
            f"model={self.model}"
        )
    
    @property
    def plugin_type(self) -> NodePluginType:
        """–¢–∏–ø –ø–ª–∞–≥—ñ–Ω—É - –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è."""
        return NodePluginType.ON_AFTER_SCAN
    
    @property
    def name(self) -> str:
        """–ù–∞–∑–≤–∞ –ø–ª–∞–≥—ñ–Ω—É."""
        return "SmartPageFinderPlugin"
    
    def _init_g4f(self) -> bool:
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î g4f –∫–ª—ñ—î–Ω—Ç.
        
        Returns:
            True —è–∫—â–æ g4f –¥–æ—Å—Ç—É–ø–Ω–∏–π, False —ñ–Ω–∞–∫—à–µ
        """
        if self._g4f_available is not None:
            return self._g4f_available
        
        try:
            import g4f
            from g4f.client import Client
            
            self._g4f_client = Client()
            self._g4f_available = True
            logger.info("g4f successfully initialized")
            return True
            
        except ImportError:
            logger.warning(
                "g4f not installed. Install with: pip install g4f\n"
                "Plugin will use fallback keyword-based analysis."
            )
            self._g4f_available = False
            return False
            
        except Exception as e:
            logger.error(f"Error initializing g4f: {e}")
            self._g4f_available = False
            return False
    
    def _get_provider(self):
        """–û—Ç—Ä–∏–º—É—î –ø—Ä–æ–≤–∞–π–¥–µ—Ä g4f."""
        if not self.provider_name:
            return None
        
        try:
            import g4f.Provider as Provider
            return getattr(Provider, self.provider_name, None)
        except Exception:
            return None
    
    def execute(self, context: NodePluginContext) -> NodePluginContext:
        """
        –í–∏–∫–æ–Ω—É—î –∞–Ω–∞–ª—ñ–∑ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å.
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑ –¥–∞–Ω–∏–º–∏ –Ω–æ–¥–∏
            
        Returns:
            –û–Ω–æ–≤–ª–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª—ñ–∑—É
        """
        try:
            url = context.url
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
            if self.cache_results and url in self._cache:
                cached = self._cache[url]
                context.user_data.update(cached)
                logger.debug(f"Using cached result for {url}")
                return context
            
            # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            page_data = self._extract_page_data(context)
            
            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            if self._init_g4f() and self.analyze_content:
                result = self._analyze_with_llm(page_data)
            else:
                result = self._analyze_with_keywords(page_data)
            
            context.user_data['is_target_page'] = result['is_target']
            context.user_data['relevance_score'] = result['score']
            context.user_data['relevance_level'] = result['level']
            context.user_data['relevance_reason'] = result['reason']
            
            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
            if self.analyze_links and context.extracted_links:
                priorities, decisions = self._analyze_links(
                    context.extracted_links, 
                    page_data,
                    result['score']
                )
                context.user_data['child_priorities'] = priorities
                context.user_data['explicit_scan_decisions'] = decisions
            
            # –ö–µ—à—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if self.cache_results:
                self._cache[url] = {
                    'is_target_page': result['is_target'],
                    'relevance_score': result['score'],
                    'relevance_level': result['level'],
                    'relevance_reason': result['reason'],
                }
            
            # –õ–æ–≥—É–≤–∞–Ω–Ω—è
            level_emoji = {
                RelevanceLevel.HIGH.value: "üéØ",
                RelevanceLevel.MEDIUM.value: "üî∂",
                RelevanceLevel.LOW.value: "üîπ",
                RelevanceLevel.IRRELEVANT.value: "‚ö™"
            }
            emoji = level_emoji.get(result['level'], "‚ùì")
            
            logger.info(
                f"{emoji} {url}: score={result['score']:.2f}, "
                f"level={result['level']}, target={result['is_target']}"
            )
            
        except Exception as e:
            logger.error(
                f"Error in SmartPageFinderPlugin for {context.url}: {e}",
                exc_info=True
            )
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
            context.user_data['is_target_page'] = False
            context.user_data['relevance_score'] = 0.0
            context.user_data['relevance_level'] = RelevanceLevel.IRRELEVANT.value
            context.user_data['relevance_reason'] = f"Analysis error: {str(e)}"
        
        return context
    
    def _extract_page_data(self, context: NodePluginContext) -> Dict[str, Any]:
        """
        –í–∏—Ç—è–≥—É—î –¥–∞–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–æ–¥–∏
            
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –¥–∞–Ω–∏–º–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        """
        data = {
            'url': context.url,
            'title': '',
            'h1': '',
            'description': '',
            'text': '',
            'links_count': len(context.extracted_links),
        }
        
        # –ú–µ—Ç–∞–¥–∞–Ω—ñ
        if context.metadata:
            data['title'] = context.metadata.get('title', '') or ''
            data['h1'] = context.metadata.get('h1', '') or ''
            data['description'] = context.metadata.get('description', '') or ''
        
        # –¢–µ–∫—Å—Ç –∑ HTML
        if context.html_tree:
            try:
                raw_text = getattr(context.html_tree, 'text', '') or ''
                text = ' '.join(raw_text.split())
                data['text'] = text[:self.max_text_length]
            except Exception:
                pass
        
        # –¢–µ–∫—Å—Ç –∑ –Ω–æ–¥–∏ (—è–∫—â–æ —î –∫–∞—Å—Ç–æ–º–Ω–µ –ø–æ–ª–µ)
        node = context.node
        if hasattr(node, 'text') and node.text:
            data['text'] = str(node.text)[:self.max_text_length]
        
        return data
    
    def _analyze_with_llm(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª—ñ–∑—É—î —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é LLM (g4f).
        
        Args:
            page_data: –î–∞–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
        """
        try:
            # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            system_prompt = """–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –ø–æ—à—É–∫–æ–≤–æ–º—É –∑–∞–ø–∏—Ç—É.

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON:
{
    "score": 0.0-1.0,
    "level": "high|medium|low|irrelevant",
    "reason": "–∫–æ—Ä–æ—Ç–∫–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é"
}

–†—ñ–≤–Ω—ñ:
- high (0.8-1.0): —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —Ç–æ—á–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞–ø–∏—Ç—É
- medium (0.5-0.8): —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —á–∞—Å—Ç–∫–æ–≤–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
- low (0.2-0.5): —î —Å–ª–∞–±–∫–∏–π –∑–≤'—è–∑–æ–∫ –∑ –∑–∞–ø–∏—Ç–æ–º
- irrelevant (0.0-0.2): —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –Ω–µ –º–∞—î –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –¥–æ –∑–∞–ø–∏—Ç—É"""

            user_prompt = f"""–ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç: {self.search_prompt}

–î–∞–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏:
URL: {page_data['url']}
Title: {page_data['title']}
H1: {page_data['h1']}
Description: {page_data['description']}
–¢–µ–∫—Å—Ç (–ø–µ—Ä—à—ñ 2000 —Å–∏–º–≤–æ–ª—ñ–≤): {page_data['text'][:2000]}

–û—Ü—ñ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å —Ü—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–æ –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É."""

            response = self._g4f_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                provider=self._get_provider(),
            )
            
            # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            content = response.choices[0].message.content
            result = self._parse_llm_response(content)
            
            return result
            
        except Exception as e:
            logger.warning(f"LLM analysis failed: {e}, falling back to keywords")
            return self._analyze_with_keywords(page_data)
    
    def _parse_llm_response(self, content: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å LLM.
        
        Args:
            content: –¢–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
        """
        import json
        
        try:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ JSON —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_match = re.search(r'\{[^{}]*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                
                score = float(data.get('score', 0.0))
                score = max(0.0, min(1.0, score))  # Clamp 0-1
                
                level = data.get('level', 'irrelevant')
                if level not in [e.value for e in RelevanceLevel]:
                    level = self._score_to_level(score)
                
                reason = data.get('reason', 'No reason provided')
                
                return {
                    'score': score,
                    'level': level,
                    'reason': reason,
                    'is_target': score >= self.min_relevance_score
                }
        except (json.JSONDecodeError, ValueError) as e:
            logger.debug(f"Failed to parse LLM response: {e}")
        
        # Fallback - –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        content_lower = content.lower()
        
        if any(w in content_lower for w in ['high', '–≤–∏—Å–æ–∫', '—Ç–æ—á–Ω–æ', '–≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î']):
            return {'score': 0.85, 'level': 'high', 'reason': content[:100], 'is_target': True}
        elif any(w in content_lower for w in ['medium', '—Å–µ—Ä–µ–¥–Ω', '—á–∞—Å—Ç–∫–æ–≤–æ', '–º–æ–∂–ª–∏–≤–æ']):
            return {'score': 0.6, 'level': 'medium', 'reason': content[:100], 'is_target': False}
        elif any(w in content_lower for w in ['low', '–Ω–∏–∑—å–∫', '–º–∞–ª–æ–π–º–æ–≤—ñ—Ä–Ω–æ']):
            return {'score': 0.3, 'level': 'low', 'reason': content[:100], 'is_target': False}
        else:
            return {'score': 0.1, 'level': 'irrelevant', 'reason': content[:100], 'is_target': False}
    
    def _analyze_with_keywords(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fallback –∞–Ω–∞–ª—ñ–∑ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ (–±–µ–∑ LLM).
        
        –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –∑ —Ç–æ—á–Ω—ñ—à–∏–º —Å–∫–æ—Ä–∏–Ω–≥–æ–º –¥–ª—è –ø–æ—à—É–∫—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É.
        
        Args:
            page_data: –î–∞–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª—ñ–∑—É
        """
        # –í–∏—Ç—è–≥—É—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ –ø—Ä–æ–º–ø—Ç—É
        keywords = self._extract_keywords(self.search_prompt)
        
        if not keywords:
            return {
                'score': 0.0,
                'level': RelevanceLevel.IRRELEVANT.value,
                'reason': 'No keywords extracted from search prompt',
                'is_target': False
            }
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ä—ñ–∑–Ω—ñ —á–∞—Å—Ç–∏–Ω–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ —Ä—ñ–∑–Ω–æ—é –≤–∞–≥–æ—é
        url_text = page_data['url'].lower()
        title_text = page_data['title'].lower()
        h1_text = page_data['h1'].lower()
        description_text = page_data['description'].lower()
        body_text = page_data['text'].lower()
        
        # –†–∞—Ö—É—î–º–æ –∑–±—ñ–≥–∏ –∑ –≤–∞–≥–∞–º–∏
        url_matches = sum(1 for kw in keywords if kw.lower() in url_text)
        title_matches = sum(1 for kw in keywords if kw.lower() in title_text)
        h1_matches = sum(1 for kw in keywords if kw.lower() in h1_text)
        desc_matches = sum(1 for kw in keywords if kw.lower() in description_text)
        body_matches = sum(1 for kw in keywords if kw.lower() in body_text)
        
        # –†–∞—Ö—É—î–º–æ —Ç–æ—á–Ω—ñ —Ñ—Ä–∞–∑–∏ (—è–∫—â–æ –ø—Ä–æ–º–ø—Ç –º—ñ—Å—Ç–∏—Ç—å —Ñ—Ä–∞–∑—É –≤ –ª–∞–ø–∫–∞—Ö –∞–±–æ —Ü–µ –Ω–∞–∑–≤–∞)
        exact_phrase = ' '.join(keywords[:4]).lower()  # –ü–µ—Ä—à—ñ 4 —Å–ª–æ–≤–∞ —è–∫ —Ñ—Ä–∞–∑–∞
        exact_phrase_alt = '-'.join(keywords[:4]).lower()  # Slug –≤–µ—Ä—Å—ñ—è
        
        exact_match_url = exact_phrase in url_text or exact_phrase_alt in url_text
        exact_match_title = exact_phrase in title_text
        exact_match_h1 = exact_phrase in h1_text
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ score –∑ –≤–∞–≥–∞–º–∏
        total_keywords = len(keywords)
        
        # –í–∞–≥–∏: URL —ñ title –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –¥–ª—è —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
        weighted_score = 0.0
        
        # URL –º—ñ—Å—Ç–∏—Ç—å keywords - –¥—É–∂–µ –≤–∞–∂–ª–∏–≤–æ (–≤–∞–≥–∞ 0.35)
        if total_keywords > 0:
            url_score = min(1.0, url_matches / min(total_keywords, 3)) * 0.35
            weighted_score += url_score
        
        # Title –º—ñ—Å—Ç–∏—Ç—å keywords (–≤–∞–≥–∞ 0.30)
        if total_keywords > 0:
            title_score = min(1.0, title_matches / min(total_keywords, 4)) * 0.30
            weighted_score += title_score
        
        # H1 –º—ñ—Å—Ç–∏—Ç—å keywords (–≤–∞–≥–∞ 0.15)
        if total_keywords > 0:
            h1_score = min(1.0, h1_matches / min(total_keywords, 4)) * 0.15
            weighted_score += h1_score
        
        # Body –º—ñ—Å—Ç–∏—Ç—å keywords (–≤–∞–≥–∞ 0.20)
        if total_keywords > 0:
            body_score = min(1.0, body_matches / total_keywords) * 0.20
            weighted_score += body_score
        
        # –ë–û–ù–£–°–ò –∑–∞ —Ç–æ—á–Ω—ñ –∑–±—ñ–≥–∏
        if exact_match_url:
            weighted_score = min(1.0, weighted_score + 0.3)  # –í–µ–ª–∏–∫–∏–π –±–æ–Ω—É—Å!
        if exact_match_title:
            weighted_score = min(1.0, weighted_score + 0.2)
        if exact_match_h1:
            weighted_score = min(1.0, weighted_score + 0.15)
        
        # –§—ñ–Ω–∞–ª—å–Ω–∏–π score
        score = min(1.0, weighted_score)
        
        level = self._score_to_level(score)
        is_target = score >= self.min_relevance_score
        
        if self.strict_mode and level != RelevanceLevel.HIGH.value:
            is_target = False
        
        # –§–æ—Ä–º—É—î–º–æ reason
        matched_in = []
        if url_matches > 0:
            matched_in.append(f"URL({url_matches})")
        if title_matches > 0:
            matched_in.append(f"title({title_matches})")
        if h1_matches > 0:
            matched_in.append(f"h1({h1_matches})")
        if body_matches > 0:
            matched_in.append(f"body({body_matches})")
        
        reason = f"Keywords found in: {', '.join(matched_in) if matched_in else 'none'}"
        if exact_match_url or exact_match_title:
            reason += " [EXACT MATCH!]"
        
        return {
            'score': score,
            'level': level,
            'reason': reason,
            'is_target': is_target
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        –í–∏—Ç—è–≥—É—î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ —Ç–µ–∫—Å—Ç—É.
        
        Args:
            text: –í—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        """
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Ç–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ)
        stop_words = {
            '—ñ', '—Ç–∞', '–∞–±–æ', '–∞', '–∞–ª–µ', '—â–æ', '—è–∫', '—Ü–µ', '–Ω–∞', '–≤', '—É', '–∑', '—ñ–∑',
            '–¥–æ', '–≤—ñ–¥', '–ø—Ä–æ', '–¥–ª—è', '–ø–æ', '–∑–∞', '–Ω–∞–¥', '–ø—ñ–¥', '–º—ñ–∂', '—á–µ—Ä–µ–∑',
            '—à—É–∫–∞—é', '–∑–Ω–∞–π—Ç–∏', '–ø–æ—Ç—Ä—ñ–±–Ω–æ', '—Ö–æ—á—É', '—Ç—Ä–µ–±–∞', '—Å—Ç–æ—Ä—ñ–Ω–∫–∏', '—Å—Ç–æ—Ä—ñ–Ω–∫–∞',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
            'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
            'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'about',
            'find', 'search', 'looking', 'want', 'need', 'pages', 'page', 'content'
        }
        
        # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
        words = re.findall(r'\b\w+\b', text.lower())
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è
        keywords = []
        for word in words:
            if len(word) >= 3 and word not in stop_words:
                keywords.append(word)
        
        return list(set(keywords))  # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ
    
    def _score_to_level(self, score: float) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î score —É —Ä—ñ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ."""
        if score >= 0.8:
            return RelevanceLevel.HIGH.value
        elif score >= 0.5:
            return RelevanceLevel.MEDIUM.value
        elif score >= 0.2:
            return RelevanceLevel.LOW.value
        else:
            return RelevanceLevel.IRRELEVANT.value
    
    def _analyze_links(
        self, 
        links: List[str], 
        page_data: Dict[str, Any],
        page_score: float
    ) -> Tuple[Dict[str, int], Dict[str, bool]]:
        """
        –ê–Ω–∞–ª—ñ–∑—É—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–∏ –¥–ª—è ML-–∫–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫—Ä–∞—É–ª—ñ–Ω–≥—É.
        
        –¶–µ –ö–õ–Æ–ß–û–í–ê —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø—Ä—ñ–æ—Ä–∏—Ç–∏–∑–∞—Ü—ñ—ó - –≤–∏–∑–Ω–∞—á–∞—î –∫—É–¥–∏ –∫—Ä–∞—É–ª–µ—Ä –ø—ñ–¥–µ –ø–µ—Ä—à–∏–º!
        
        Args:
            links: –°–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
            page_data: –î–∞–Ω—ñ –ø–æ—Ç–æ—á–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            page_score: Score —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ –ø–æ—Ç–æ—á–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            
        Returns:
            Tuple (priorities dict, explicit_decisions dict)
        """
        priorities = {}
        decisions = {}
        
        keywords = self._extract_keywords(self.search_prompt)
        
        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ (–≤–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        content_patterns = [
            r'/fiction/\d+',           # –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∫–Ω–∏–≥–∏/fiction
            r'/book/\d+',              # –°—Ç–æ—Ä—ñ–Ω–∫–∞ –∫–Ω–∏–≥–∏
            r'/novel/\d+',             # –°—Ç–æ—Ä—ñ–Ω–∫–∞ novel
            r'/story/\d+',             # –°—Ç–æ—Ä—ñ–Ω–∫–∞ story
            r'/article/',              # –°—Ç–∞—Ç—Ç—ñ
            r'/post/',                 # –ü–æ—Å—Ç–∏
            r'/product/',              # –ü—Ä–æ–¥—É–∫—Ç–∏
            r'/item/',                 # Items
            r'/details/',              # –î–µ—Ç–∞–ª—ñ
            r'/view/',                 # View pages
        ]
        
        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è –Ω–∞–≤—ñ–≥–∞—Ü—ñ–π–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ (—Å–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç - –º–æ–∂—É—Ç—å –≤–µ—Å—Ç–∏ –¥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É)
        navigation_patterns = [
            r'\?page=\d+',             # –ü–∞–≥—ñ–Ω–∞—Ü—ñ—è - –í–ê–ñ–õ–ò–í–û –¥–ª—è –ø–æ—à—É–∫—É!
            r'/page/\d+',              # –ü–∞–≥—ñ–Ω–∞—Ü—ñ—è
            r'/category/',             # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó
            r'/tag/',                  # –¢–µ–≥–∏
            r'/genre/',                # –ñ–∞–Ω—Ä–∏
            r'/list',                  # –°–ø–∏—Å–∫–∏
            r'/browse',                # Browse pages
            r'/search',                # Search —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            r'best-rated',             # Best rated
            r'popular',                # Popular
            r'trending',               # Trending
        ]
        
        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è —ñ–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è (–Ω–∏–∑—å–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        ignore_patterns = [
            r'/login', r'/register', r'/signup', r'/signin',
            r'/cart', r'/checkout', r'/payment',
            r'/privacy', r'/terms', r'/cookie', r'/legal', r'/tos',
            r'/contact', r'/about', r'/faq', r'/help',
            r'/profile/\d+$',          # –ü—Ä–æ—Ñ—ñ–ª—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
            r'/user/',                 # User pages
            r'/account',               # Account
            r'/settings',              # Settings
            r'/notifications',         # Notifications
            r'/messages',              # Messages
            r'/forums?/',              # Forums
            r'/comment',               # Comments
            r'/review',                # Reviews –±–µ–∑ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            r'\.(pdf|doc|docx|xls|xlsx|zip|rar|exe|dmg)$',
            r'/wp-admin', r'/admin', r'/dashboard',
            r'/api/', r'/ajax/',       # API endpoints
            r'/cdn/', r'/static/',     # Static resources
            r'#',                      # Anchors
            r'javascript:',            # JS links
            r'mailto:',                # Email links
        ]
        
        for link in links:
            link_lower = link.lower()
            
            # –ë–∞–∑–æ–≤–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            priority = 5
            
            # 1. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏
            should_ignore = False
            for pattern in ignore_patterns:
                if re.search(pattern, link_lower):
                    priority = 1  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
                    should_ignore = True
                    break
            
            if should_ignore:
                priorities[link] = priority
                continue
            
            # 2. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –≤ URL - –ù–ê–ô–í–ê–ñ–õ–ò–í–Ü–®–ï!
            keyword_matches = sum(1 for kw in keywords if kw.lower() in link_lower)
            
            if keyword_matches >= 3:
                priority = 15  # –î—É–∂–µ –≤–∏—Å–æ–∫–∏–π - –±–∞–≥–∞—Ç–æ –∑–±—ñ–≥—ñ–≤!
            elif keyword_matches >= 2:
                priority = 12  # –í–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            elif keyword_matches == 1:
                priority = 9   # –°–µ—Ä–µ–¥–Ω—å–æ-–≤–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            
            # 3. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏
            for pattern in content_patterns:
                if re.search(pattern, link_lower):
                    priority = max(priority, 10)  # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤–∞–∂–ª–∏–≤—ñ
                    # –Ø–∫—â–æ —î —ñ keywords - —â–µ –≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
                    if keyword_matches > 0:
                        priority = min(15, priority + keyword_matches * 2)
                    break
            
            # 4. –ù–∞–≤—ñ–≥–∞—Ü—ñ–π–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ - –º–æ–∂—É—Ç—å –≤–µ—Å—Ç–∏ –¥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            for pattern in navigation_patterns:
                if re.search(pattern, link_lower):
                    priority = max(priority, 7)  # –ù–∞–≤—ñ–≥–∞—Ü—ñ—è –≤–∞–∂–ª–∏–≤–∞ –¥–ª—è –ø–æ—à—É–∫—É
                    # –ü–∞–≥—ñ–Ω–∞—Ü—ñ—è –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ - –≤–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
                    if keyword_matches > 0 and ('page=' in link_lower or '/page/' in link_lower):
                        priority = min(13, priority + 3)
                    break
            
            # 5. –ë–æ–Ω—É—Å —è–∫—â–æ –ø–æ—Ç–æ—á–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
            if page_score >= 0.7:
                priority = min(15, priority + 2)  # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –≤–∞–∂–ª–∏–≤—ñ—à—ñ
            elif page_score >= 0.5:
                priority = min(13, priority + 1)
            
            # 6. –°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ URL —Ç–µ–∫—Å—Ç—É
            # –Ø–∫—â–æ –≤ URL —î —á–∞—Å—Ç–∏–Ω–∞ –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É (slug)
            search_slug = '-'.join(self.search_prompt.lower().split()[:3])
            if search_slug in link_lower or search_slug.replace('-', '') in link_lower.replace('-', ''):
                priority = 15  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç - –º–æ–∂–ª–∏–≤–æ —Ü–µ —Ç–µ —â–æ —à—É–∫–∞—î–º–æ!
            
            priorities[link] = priority
        
        # –õ–æ–≥—É—î–º–æ —Ç–æ–ø –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
        if priorities:
            top_links = sorted(priorities.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.debug(f"Top priority links: {[(url.split('/')[-1][:30], p) for url, p in top_links]}")
        
        return priorities, decisions
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –ø–ª–∞–≥—ñ–Ω—É.
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
        """
        if not self._cache:
            return {'total_analyzed': 0, 'target_pages': 0, 'avg_score': 0.0}
        
        scores = [v['relevance_score'] for v in self._cache.values()]
        targets = sum(1 for v in self._cache.values() if v['is_target_page'])
        
        return {
            'total_analyzed': len(self._cache),
            'target_pages': targets,
            'avg_score': sum(scores) / len(scores) if scores else 0.0,
            'score_distribution': {
                'high': sum(1 for v in self._cache.values() if v['relevance_level'] == 'high'),
                'medium': sum(1 for v in self._cache.values() if v['relevance_level'] == 'medium'),
                'low': sum(1 for v in self._cache.values() if v['relevance_level'] == 'low'),
                'irrelevant': sum(1 for v in self._cache.values() if v['relevance_level'] == 'irrelevant'),
            }
        }
    
    def clear_cache(self):
        """–û—á–∏—â—É—î –∫–µ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤."""
        self._cache.clear()
        logger.info("SmartPageFinderPlugin cache cleared")
    
    def __repr__(self):
        return (
            f"SmartPageFinderPlugin("
            f"prompt='{self.search_prompt[:30]}...', "
            f"min_score={self.min_relevance_score}, "
            f"model={self.model}, "
            f"enabled={self.enabled})"
        )


# –ö–ê–°–¢–û–ú–ù–ê –ù–û–î–ê –î–õ–Ø –ó–†–£–ß–ù–û–á –†–û–ë–û–¢–ò –ó –ü–õ–ê–ì–Ü–ù–û–ú

def create_smart_finder_node_class():
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ Node –∫–ª–∞—Å—É –¥–ª—è SmartPageFinderPlugin.
    
    –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
        >>> SmartFinderNode = create_smart_finder_node_class()
        >>> graph = gc.crawl(url, plugins=[plugin], node_class=SmartFinderNode)
        >>> for node in graph:
        ...     if node.is_target:
        ...         print(f"Found: {node.url} (score: {node.relevance_score})")
    """
    try:
        import graph_crawler as gc
        from pydantic import Field
        from typing import Optional
        
        class SmartFinderNode(gc.Node):
            """Node –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é SmartPageFinderPlugin."""
            
            is_target: bool = Field(default=False, description="–ß–∏ —Ü–µ —à—É–∫–∞–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞")
            relevance_score: float = Field(default=0.0, description="Score —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ 0.0-1.0")
            relevance_level: str = Field(default="irrelevant", description="–†—ñ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ")
            relevance_reason: str = Field(default="", description="–ü–æ—è—Å–Ω–µ–Ω–Ω—è –æ—Ü—ñ–Ω–∫–∏")
            text: Optional[str] = Field(default=None, description="–¢–µ–∫—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
            
            def _update_from_context(self, context):
                """–û–Ω–æ–≤–ª—é—î –¥–∞–Ω—ñ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è."""
                super()._update_from_context(context)
                
                # –í–∏—Ç—è–≥—É—î–º–æ —Ç–µ–∫—Å—Ç
                if context.html_tree:
                    try:
                        raw_text = getattr(context.html_tree, 'text', '') or ''
                        self.text = ' '.join(raw_text.split())[:5000]
                    except Exception:
                        pass
                
                # –ö–æ–ø—ñ—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–ª–∞–≥—ñ–Ω—É
                self.is_target = context.user_data.get('is_target_page', False)
                self.relevance_score = context.user_data.get('relevance_score', 0.0)
                self.relevance_level = context.user_data.get('relevance_level', 'irrelevant')
                self.relevance_reason = context.user_data.get('relevance_reason', '')
        
        return SmartFinderNode
        
    except ImportError:
        logger.warning("graph_crawler not available, returning None")
        return None


# –ï–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
SmartFinderNode = create_smart_finder_node_class()
