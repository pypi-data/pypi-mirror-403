#!/usr/bin/python
# -*- coding: UTF-8 -*-
import sys
import os
sys.path.insert(0, "/Users/oscar/projects/Crawlo")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«Redis Keyæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯åˆ†å¸ƒå¼çˆ¬è™«æ˜¯å¦ç¬¦åˆæ–°çš„Redis keyå‘½åè§„èŒƒ
"""
import sys
import os
import asyncio
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# å¯¼å…¥ç›¸å…³æ¨¡å—
from crawlo.queue.queue_manager import QueueManager, QueueConfig, QueueType
from crawlo.filters.aioredis_filter import AioRedisFilter
from crawlo.pipelines.redis_dedup_pipeline import RedisDedupPipeline


class MockSettings:
    """æ¨¡æ‹Ÿè®¾ç½®ç±»"""
    def __init__(self, project_name="telecom_licenses_distributed"):
        self.project_name = project_name
        self.REDIS_HOST = '127.0.0.1'
        self.REDIS_PORT = 6379
        self.REDIS_PASSWORD = ''
        self.REDIS_DB = 2
        self.REDIS_URL = f'redis://127.0.0.1:6379/{self.REDIS_DB}'
        self.REDIS_TTL = 0
        self.CLEANUP_FP = 0
        self.FILTER_DEBUG = True
        self.LOG_LEVEL = "INFO"
        self.DECODE_RESPONSES = True
        self.SCHEDULER_QUEUE_NAME = f'crawlo:{project_name}:queue:requests'
    
    def get(self, key, default=None):
        if key == 'PROJECT_NAME':
            return self.project_name
        elif key == 'REDIS_HOST':
            return self.REDIS_HOST
        elif key == 'REDIS_PASSWORD':
            return self.REDIS_PASSWORD
        elif key == 'REDIS_URL':
            return self.REDIS_URL
        elif key == 'FILTER_DEBUG':
            return self.FILTER_DEBUG
        elif key == 'LOG_LEVEL':
            return self.LOG_LEVEL
        elif key == 'DECODE_RESPONSES':
            return self.DECODE_RESPONSES
        elif key == 'SCHEDULER_QUEUE_NAME':
            return self.SCHEDULER_QUEUE_NAME
        return default
    
    def get_bool(self, key, default=False):
        if key == 'FILTER_DEBUG':
            return self.FILTER_DEBUG
        elif key == 'DECODE_RESPONSES':
            return self.DECODE_RESPONSES
        elif key == 'CLEANUP_FP':
            return self.CLEANUP_FP
        return default
    
    def get_int(self, key, default=0):  # ä¿®å¤æ–¹æ³•å
        if key == 'REDIS_TTL':
            return self.REDIS_TTL
        elif key == 'REDIS_PORT':
            return self.REDIS_PORT
        elif key == 'REDIS_DB':
            return self.REDIS_DB
        elif key == 'SCHEDULER_MAX_QUEUE_SIZE':
            return 1000
        elif key == 'QUEUE_MAX_RETRIES':
            return 3
        elif key == 'QUEUE_TIMEOUT':
            return 300
        return default


class MockCrawler:
    """æ¨¡æ‹Ÿçˆ¬è™«ç±»"""
    def __init__(self, project_name="telecom_licenses_distributed"):
        self.settings = MockSettings(project_name)
        self.stats = {}


async def test_telecom_spider_redis_key():
    """æµ‹è¯•ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«Redis keyå‘½åè§„èŒƒ"""
    print("ğŸ” æµ‹è¯•ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«Redis keyå‘½åè§„èŒƒ...")
    
    project_name = "telecom_licenses_distributed"
    expected_prefix = f"crawlo:{project_name}"
    
    try:
        # 1. æµ‹è¯•QueueManagerå’ŒRedisPriorityQueue
        print("   1. æµ‹è¯•é˜Ÿåˆ—ç®¡ç†å™¨...")
        queue_config = QueueConfig(
            queue_type=QueueType.REDIS,
            redis_url="redis://127.0.0.1:6379/2",
            queue_name=f"crawlo:{project_name}:queue:requests",  # ä½¿ç”¨ç»Ÿä¸€å‘½åè§„èŒƒ
            max_queue_size=1000,
            max_retries=3,
            timeout=300
        )
        
        queue_manager = QueueManager(queue_config)
        queue = await queue_manager._create_queue(QueueType.REDIS)
        
        # éªŒè¯é˜Ÿåˆ—åç§°æ˜¯å¦ç¬¦åˆè§„èŒƒ
        expected_queue_name = f"{expected_prefix}:queue:requests"
        expected_processing_queue = f"{expected_prefix}:queue:processing"
        expected_failed_queue = f"{expected_prefix}:queue:failed"
        
        assert queue.queue_name == expected_queue_name, f"é˜Ÿåˆ—åç§°ä¸åŒ¹é…: {queue.queue_name} != {expected_queue_name}"
        assert queue.processing_queue == expected_processing_queue, f"å¤„ç†ä¸­é˜Ÿåˆ—åç§°ä¸åŒ¹é…: {queue.processing_queue} != {expected_processing_queue}"
        assert queue.failed_queue == expected_failed_queue, f"å¤±è´¥é˜Ÿåˆ—åç§°ä¸åŒ¹é…: {queue.failed_queue} != {expected_failed_queue}"
        
        print(f"      è¯·æ±‚é˜Ÿåˆ—: {queue.queue_name}")
        print(f"      å¤„ç†ä¸­é˜Ÿåˆ—: {queue.processing_queue}")
        print(f"      å¤±è´¥é˜Ÿåˆ—: {queue.failed_queue}")
        
        # 2. æµ‹è¯•AioRedisFilter
        print("   2. æµ‹è¯•è¯·æ±‚å»é‡è¿‡æ»¤å™¨...")
        mock_crawler = MockCrawler(project_name)
        filter_instance = AioRedisFilter.create_instance(mock_crawler)
        
        expected_filter_key = f"{expected_prefix}:filter:fingerprint"
        assert filter_instance.redis_key == expected_filter_key, f"è¿‡æ»¤å™¨keyä¸åŒ¹é…: {filter_instance.redis_key} != {expected_filter_key}"
        
        print(f"      è¯·æ±‚å»é‡key: {filter_instance.redis_key}")
        
        # 3. æµ‹è¯•RedisDedupPipeline
        print("   3. æµ‹è¯•æ•°æ®é¡¹å»é‡ç®¡é“...")
        dedup_pipeline = RedisDedupPipeline.from_crawler(mock_crawler)
        
        expected_item_key = f"{expected_prefix}:item:fingerprint"
        assert dedup_pipeline.redis_key == expected_item_key, f"æ•°æ®é¡¹å»é‡keyä¸åŒ¹é…: {dedup_pipeline.redis_key} != {expected_item_key}"
        
        print(f"      æ•°æ®é¡¹å»é‡key: {dedup_pipeline.redis_key}")
        
        # 4. éªŒè¯æ‰€æœ‰keyéƒ½ä½¿ç”¨ç»Ÿä¸€å‰ç¼€
        print("   4. éªŒè¯ç»Ÿä¸€å‰ç¼€...")
        all_keys = [
            queue.queue_name,
            queue.processing_queue,
            queue.failed_queue,
            filter_instance.redis_key,
            dedup_pipeline.redis_key
        ]
        
        for key in all_keys:
            assert key.startswith(expected_prefix), f"Keyæœªä½¿ç”¨ç»Ÿä¸€å‰ç¼€: {key}"
            print(f"      {key}")
        
        print("ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«Redis keyå‘½åè§„èŒƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¸…ç†èµ„æº
        try:
            if 'queue' in locals():
                await queue.close()
            if 'filter_instance' in locals() and hasattr(filter_instance, 'redis'):
                await filter_instance.redis.close()
            if 'dedup_pipeline' in locals() and hasattr(dedup_pipeline, 'redis_client'):
                dedup_pipeline.redis_client.close()
        except:
            pass


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«Redis keyå‘½åè§„èŒƒæµ‹è¯•...")
    print("=" * 60)
    
    try:
        success = await test_telecom_spider_redis_key()
        
        print("=" * 60)
        if success:
            print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç”µä¿¡è®¾å¤‡è®¸å¯è¯çˆ¬è™«ç¬¦åˆæ–°çš„Redis keyå‘½åè§„èŒƒ")
        else:
            print("æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
            return 1
            
    except Exception as e:
        print("=" * 60)
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)