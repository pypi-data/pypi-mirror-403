#!/usr/bin/python
# -*- coding: UTF-8 -*-
import sys
import os
sys.path.insert(0, "/Users/oscar/projects/Crawlo")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Crawloæ¡†æ¶ä»£ç†é›†æˆæµ‹è¯•
æµ‹è¯•ä»£ç†ä¸­é—´ä»¶ä¸æ¡†æ¶çš„é›†æˆ
"""

import asyncio
import sys
import os
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from crawlo.config import CrawloConfig
from crawlo.crawler import CrawlerProcess
from crawlo import Spider, Request, Item
from crawlo.middleware.proxy import ProxyMiddleware


class TestItem(Item):
    """æµ‹è¯•ç»“æœé¡¹"""
    url = ''
    status = 0
    proxy = ''


class ProxyTestSpider(Spider):
    """ä»£ç†æµ‹è¯•çˆ¬è™«"""
    name = 'proxy_test_spider'
    
    async def start_requests(self):
        """å‘èµ·æµ‹è¯•è¯·æ±‚"""
        yield Request('https://httpbin.org/ip', callback=self.parse)
    
    async def parse(self, response):
        """è§£æå“åº”"""
        import json
        try:
            data = json.loads(response.text)
            ip_info = data.get('origin', 'Unknown')
        except:
            ip_info = 'Parse error'
        
        item = TestItem(
            url=response.url,
            status=response.status_code,
            proxy=str(response.meta.get('proxy', 'No proxy'))
        )
        
        self.logger.info(f"Proxy test result: IP={ip_info}, Proxy={item.proxy}")
        yield item


async def test_proxy_integration():
    """æµ‹è¯•ä»£ç†é›†æˆ"""
    print("ğŸ” æµ‹è¯•ä»£ç†é›†æˆ...")
    
    # åˆ›å»ºé…ç½®
    config = CrawloConfig.standalone(
        concurrency=1,
        download_delay=0.1,
        # ä»£ç†é…ç½®
        # é«˜çº§ä»£ç†é…ç½®ï¼ˆé€‚ç”¨äºProxyMiddlewareï¼‰
        # åªè¦é…ç½®äº†ä»£ç†API URLï¼Œä¸­é—´ä»¶å°±ä¼šè‡ªåŠ¨å¯ç”¨
        PROXY_API_URL="https://proxy-api.example.com/get",  # æ¨¡æ‹Ÿä»£ç†API
        
        # ä»£ç†é…ç½®ï¼ˆé€‚ç”¨äºProxyMiddlewareï¼‰
        # åªè¦é…ç½®äº†ä»£ç†åˆ—è¡¨ï¼Œä¸­é—´ä»¶å°±ä¼šè‡ªåŠ¨å¯ç”¨
        # PROXY_LIST=["http://proxy1:8080", "http://proxy2:8080"],
        LOG_LEVEL='WARNING'  # å‡å°‘æ—¥å¿—è¾“å‡º
    )
    
    # æ·»åŠ ä»£ç†ä¸­é—´ä»¶
    config.set('CUSTOM_MIDDLEWARES', [
        'crawlo.middleware.proxy.ProxyMiddleware',
    ])
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings=config.to_dict())
    
    # æ·»åŠ çˆ¬è™«
    process.crawl(ProxyTestSpider)
    
    # è¿è¡Œæµ‹è¯•
    await process.start()
    
    print("   ä»£ç†é›†æˆæµ‹è¯•å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹Crawloæ¡†æ¶ä»£ç†é›†æˆæµ‹è¯•...")
    print("=" * 50)
    
    try:
        await test_proxy_integration()
        
        print("=" * 50)
        print("æ‰€æœ‰ä»£ç†é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print("=" * 50)
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)