#!/usr/bin/python
# -*- coding: UTF-8 -*-
import sys
import os
sys.path.insert(0, "/Users/oscar/projects/Crawlo")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„ Scheduler åˆ†å¸ƒå¼é˜Ÿåˆ—åŠŸèƒ½
"""
import asyncio
import sys
from unittest.mock import Mock
from crawlo.core.scheduler import Scheduler
from crawlo.network.request import Request
from crawlo.logging import get_logger


class MockCrawler:
    """æ¨¡æ‹Ÿ Crawler å¯¹è±¡"""
    def __init__(self, use_redis=True):
        self.settings = MockSettings(use_redis)
        self.stats = Mock()


class MockSettings:
    """æ¨¡æ‹Ÿ Settings å¯¹è±¡"""
    def __init__(self, use_redis=True):
        self.use_redis = use_redis
        
    def get(self, key, default=None):
        config = {
            'FILTER_CLASS': 'crawlo.filters.memory_filter.MemoryFilter',
            'LOG_LEVEL': 'INFO',
            'DEPTH_PRIORITY': 1,
            'SCHEDULER_MAX_QUEUE_SIZE': 100,
            'SCHEDULER_QUEUE_NAME': 'test:crawlo:requests',
            'FILTER_DEBUG': False,
            'PROJECT_NAME': 'test',
            'QUEUE_TYPE': 'memory',
            'DEFAULT_DEDUP_PIPELINE': 'crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline',
        }
        if self.use_redis:
            config.update({
                'REDIS_URL': 'redis://localhost:6379/0',
                'QUEUE_TYPE': 'redis',
                'FILTER_CLASS': 'crawlo.filters.aioredis_filter.AioRedisFilter',
                'DEFAULT_DEDUP_PIPELINE': 'crawlo.pipelines.redis_dedup_pipeline.RedisDedupPipeline',
            })
        
        return config.get(key, default)
    
    def get_int(self, key, default=0):
        value = self.get(key, default)
        return int(value) if value is not None else default
        
    def get_bool(self, key, default=False):
        value = self.get(key, default)
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ('true', '1', 'yes')
        return bool(value) if value is not None else default

    def get_float(self, key, default=0.0):
        value = self.get(key, default)
        return float(value) if value is not None else default


class MockFilter:
    """æ¨¡æ‹Ÿå»é‡è¿‡æ»¤å™¨"""
    def __init__(self):
        self.seen = set()
        
    @classmethod
    def create_instance(cls, crawler):
        return cls()
    
    async def requested(self, request):
        if request.url in self.seen:
            return True
        self.seen.add(request.url)
        return False
    
    def log_stats(self, request):
        pass


async def test_memory_scheduler():
    """æµ‹è¯•å†…å­˜è°ƒåº¦å™¨"""
    print("ğŸ” æµ‹è¯•å†…å­˜è°ƒåº¦å™¨...")
    
    crawler = MockCrawler(use_redis=False)
    scheduler = Scheduler.create_instance(crawler)
    
    # æ¨¡æ‹Ÿå»é‡è¿‡æ»¤å™¨
    scheduler.dupe_filter = MockFilter()
    
    await scheduler.open()
    
    # æµ‹è¯•å…¥é˜Ÿ
    request1 = Request(url="https://example1.com")
    request2 = Request(url="https://example2.com")
    
    success1 = await scheduler.enqueue_request(request1)
    success2 = await scheduler.enqueue_request(request2)
    
    print(f"   ğŸ“¤ å…¥é˜Ÿç»“æœ: {success1}, {success2}")
    print(f"   é˜Ÿåˆ—å¤§å°: {len(scheduler)}")
    
    # æµ‹è¯•å‡ºé˜Ÿ
    req1 = await scheduler.next_request()
    req2 = await scheduler.next_request()
    
    print(f"   ğŸ“¥ å‡ºé˜Ÿç»“æœ: {req1.url if req1 else None}, {req2.url if req2 else None}")
    print(f"   å‰©ä½™å¤§å°: {len(scheduler)}")
    
    await scheduler.close()
    print("   å†…å­˜è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ")


async def test_redis_scheduler():
    """æµ‹è¯• Redis è°ƒåº¦å™¨"""
    print("ğŸ” æµ‹è¯• Redis è°ƒåº¦å™¨...")
    
    try:
        crawler = MockCrawler(use_redis=True)
        scheduler = Scheduler.create_instance(crawler)
        
        # æ¨¡æ‹Ÿå»é‡è¿‡æ»¤å™¨
        scheduler.dupe_filter = MockFilter()
        
        await scheduler.open()
        
        # æµ‹è¯•å…¥é˜Ÿ
        request1 = Request(url="https://redis-test1.com", priority=5)
        request2 = Request(url="https://redis-test2.com", priority=3)
        request3 = Request(url="https://redis-test3.com", priority=8)
        
        success1 = await scheduler.enqueue_request(request1)
        success2 = await scheduler.enqueue_request(request2)
        success3 = await scheduler.enqueue_request(request3)
        
        print(f"   å…¥é˜Ÿç»“æœ: {success1}, {success2}, {success3}")
        print(f"   é˜Ÿåˆ—å¤§å°: {len(scheduler)}")
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®© Redis æ“ä½œå®Œæˆ
        await asyncio.sleep(0.5)
        
        # æµ‹è¯•å‡ºé˜Ÿï¼ˆåº”è¯¥æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        req1 = await scheduler.next_request()
        req2 = await scheduler.next_request()
        req3 = await scheduler.next_request()
        
        print("   ğŸ“¥ å‡ºé˜Ÿç»“æœï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:")
        if req1:
            print(f"      {req1.url} (ä¼˜å…ˆçº§: {getattr(req1, 'priority', 0)})")
        if req2:
            print(f"      {req2.url} (ä¼˜å…ˆçº§: {getattr(req2, 'priority', 0)})")
        if req3:
            print(f"      {req3.url} (ä¼˜å…ˆçº§: {getattr(req3, 'priority', 0)})")
            
        print(f"   å‰©ä½™å¤§å°: {len(scheduler)}")
        
        await scheduler.close()
        print("   Redis è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"   Redis è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def test_concurrent_redis():
    """æµ‹è¯•å¹¶å‘ Redis æ“ä½œ"""
    print("ğŸ” æµ‹è¯•å¹¶å‘ Redis æ“ä½œ...")
    
    async def producer(scheduler, name, count):
        """ç”Ÿäº§è€…"""
        for i in range(count):
            request = Request(url=f"https://{name}-{i}.com", priority=i % 10)
            await scheduler.enqueue_request(request)
            await asyncio.sleep(0.01)
        print(f"   ç”Ÿäº§è€… {name} å®Œæˆ ({count} ä¸ªè¯·æ±‚)")
    
    async def consumer(scheduler, name, count):
        """æ¶ˆè´¹è€…"""
        consumed = 0
        for _ in range(count):
            request = await scheduler.next_request()
            if request:
                consumed += 1
                await asyncio.sleep(0.005)
            else:
                break
        print(f"   æ¶ˆè´¹è€… {name} å¤„ç†äº† {consumed} ä¸ªè¯·æ±‚")
    
    try:
        crawler = MockCrawler(use_redis=True)
        scheduler = Scheduler.create_instance(crawler)
        scheduler.dupe_filter = MockFilter()
        await scheduler.open()
        
        # å¹¶å‘è¿è¡Œç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
        tasks = [
            producer(scheduler, "producer-1", 5),
            producer(scheduler, "producer-2", 5),
            consumer(scheduler, "consumer-1", 3),
            consumer(scheduler, "consumer-2", 3),
            consumer(scheduler, "consumer-3", 4),
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"   æœ€ç»ˆé˜Ÿåˆ—å¤§å°: {len(scheduler)}")
        
        await scheduler.close()
        print("   å¹¶å‘æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"   å¹¶å‘æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„ Scheduler...")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•å†…å­˜è°ƒåº¦å™¨
        await test_memory_scheduler()
        print()
        
        # 2. æµ‹è¯• Redis è°ƒåº¦å™¨
        await test_redis_scheduler()
        print()
        
        # 3. æµ‹è¯•å¹¶å‘æ“ä½œ
        await test_concurrent_redis()
        
        print("=" * 50)
        print("æ‰€æœ‰ Scheduler æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print("=" * 50)
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«é¿å…è¿‡å¤šè¾“å‡º
    import logging
    logging.getLogger('crawlo').setLevel(logging.WARNING)
    
    asyncio.run(main())