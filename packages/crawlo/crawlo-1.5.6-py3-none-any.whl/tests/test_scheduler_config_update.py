#!/usr/bin/python
# -*- coding: UTF-8 -*-
import sys
import os
sys.path.insert(0, "/Users/oscar/projects/Crawlo")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è°ƒåº¦å™¨é…ç½®æ›´æ–°æ—¥å¿—ä¼˜åŒ–
"""
import asyncio
from unittest.mock import Mock
from crawlo.core.scheduler import Scheduler
from crawlo.network.request import Request
from crawlo.utils.log import get_logger


class MockCrawler:
    """æ¨¡æ‹Ÿ Crawler å¯¹è±¡"""
    def __init__(self, use_redis=True, filter_class=None, dedup_pipeline=None):
        self.settings = MockSettings(use_redis, filter_class, dedup_pipeline)
        self.stats = Mock()


class MockSettings:
    """æ¨¡æ‹Ÿ Settings å¯¹è±¡"""
    def __init__(self, use_redis=True, filter_class=None, dedup_pipeline=None):
        self.use_redis = use_redis
        self._settings = {
            'LOG_LEVEL': 'INFO',
            'DEPTH_PRIORITY': 1,
            'SCHEDULER_MAX_QUEUE_SIZE': 100,
            'SCHEDULER_QUEUE_NAME': 'test:crawlo:requests',
            'FILTER_DEBUG': False,
            'PROJECT_NAME': 'test',
        }
        
        # æ ¹æ®å‚æ•°è®¾ç½®ä¸åŒçš„é…ç½®
        if use_redis:
            self._settings.update({
                'REDIS_URL': 'redis://localhost:6379/0',
                'QUEUE_TYPE': 'redis',
                'FILTER_CLASS': filter_class or 'crawlo.filters.memory_filter.MemoryFilter',
                'DEFAULT_DEDUP_PIPELINE': dedup_pipeline or 'crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline',
            })
        else:
            self._settings.update({
                'QUEUE_TYPE': 'memory',
                'FILTER_CLASS': filter_class or 'crawlo.filters.memory_filter.MemoryFilter',
                'DEFAULT_DEDUP_PIPELINE': dedup_pipeline or 'crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline',
            })
        
    def get(self, key, default=None):
        return self._settings.get(key, default)
    
    def get_int(self, key, default=0):
        value = self.get(key, default)
        return int(value) if value is not None else default
        
    def get_bool(self, key, default=False):
        value = self.get(key, default)
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ('true', '1', 'yes')
        return bool(value) if value is not None else default

    def get_float(self, key, default=0.0):
        value = self.get(key, default)
        return float(value) if value is not None else default
        
    def set(self, key, value):
        self._settings[key] = value


class MockFilter:
    """æ¨¡æ‹Ÿå»é‡è¿‡æ»¤å™¨"""
    def __init__(self):
        self.seen = set()
        
    @classmethod
    def create_instance(cls, crawler):
        return cls()
    
    async def requested(self, request):
        if request.url in self.seen:
            return True
        self.seen.add(request.url)
        return False
    
    def log_stats(self, request):
        pass


async def test_config_update_logs():
    """æµ‹è¯•é…ç½®æ›´æ–°æ—¥å¿—ä¼˜åŒ–"""
    print("ğŸ” æµ‹è¯•é…ç½®æ›´æ–°æ—¥å¿—ä¼˜åŒ–...")
    
    # æ¨¡æ‹Ÿä»å†…å­˜æ¨¡å¼åˆ‡æ¢åˆ°Redisæ¨¡å¼çš„æƒ…å†µ
    crawler = MockCrawler(
        use_redis=True, 
        filter_class='crawlo.filters.memory_filter.MemoryFilter',
        dedup_pipeline='crawlo.pipelines.memory_dedup_pipeline.MemoryDedupPipeline'
    )
    
    scheduler = Scheduler.create_instance(crawler)
    scheduler.dupe_filter = MockFilter()
    
    # è¿™ä¼šè§¦å‘é…ç½®æ›´æ–°
    await scheduler.open()
    
    await scheduler.close()
    print("   é…ç½®æ›´æ–°æ—¥å¿—æµ‹è¯•å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•è°ƒåº¦å™¨é…ç½®æ›´æ–°æ—¥å¿—ä¼˜åŒ–...")
    print("=" * 50)
    
    try:
        await test_config_update_logs()
        
        print("=" * 50)
        print("è°ƒåº¦å™¨é…ç½®æ›´æ–°æ—¥å¿—ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print("=" * 50)
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«é¿å…è¿‡å¤šè¾“å‡º
    import logging
    logging.getLogger('crawlo').setLevel(logging.INFO)
    
    asyncio.run(main())