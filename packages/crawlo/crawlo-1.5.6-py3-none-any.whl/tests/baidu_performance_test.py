#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦ç½‘ç«™æ€§èƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ä¸‰ä¸ªä¼˜åŒ–æ–¹æ³•çš„å®ç°æ•ˆæœï¼š
1. å¼•å…¥å·¥ä½œæ± æ¨¡å¼ï¼šä½¿ç”¨å›ºå®šå¤§å°çš„å·¥ä½œæ± ï¼Œé¿å…æ— é™åˆ›å»ºåç¨‹
2. ä¼˜åŒ–ä¿¡å·é‡æ§åˆ¶ï¼šåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°åŸºäºç½‘ç»œå“åº”æ—¶é—´
3. ä¼˜åŒ–ä»»åŠ¡è°ƒåº¦ï¼šå¼•å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—å’Œæ™ºèƒ½è°ƒåº¦
"""
import asyncio
import time
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from crawlo import Spider, Request
from crawlo.crawler import CrawlerProcess


class BaiduTestSpider(Spider):
    name = 'baidu_test'
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_time = time.time()
        self.request_count = 0
        self.response_times = []
    
    def start_requests(self):
        # æµ‹è¯•ç™¾åº¦é¦–é¡µå’Œå‡ ä¸ªå­é¡µé¢
        urls = [
            'https://www.baidu.com/',
            'https://www.baidu.com/s?wd=python',
            'https://www.baidu.com/s?wd=ai',
            'https://www.baidu.com/s?wd=æœºå™¨å­¦ä¹ ',
            'https://www.baidu.com/s?wd=å¤§æ•°æ®',
            'https://www.baidu.com/s?wd=äº‘è®¡ç®—',
            'https://www.baidu.com/s?wd=åŒºå—é“¾',
            'https://www.baidu.com/s?wd=ç‰©è”ç½‘',
        ]
        
        for url in urls:
            yield Request(url=url, callback=self.parse, priority=1)
    
    def parse(self, response):
        self.request_count += 1
        response_time = time.time() - self.start_time
        self.response_times.append(response_time)
        
        print(f"âœ… æˆåŠŸè·å–: {response.url} (çŠ¶æ€ç : {response.status_code})")
        print(f"   å“åº”å¤§å°: {len(response.text)} å­—ç¬¦")
        
        # å¦‚æœæ˜¯é¦–é¡µï¼Œå¯ä»¥æå–ä¸€äº›é“¾æ¥è¿›è¡Œè¿›ä¸€æ­¥æµ‹è¯•
        if 'www.baidu.com/' in response.url and self.request_count < 20:
            # é™åˆ¶é¢å¤–è¯·æ±‚æ•°é‡ä»¥é¿å…è¿‡äºåºå¤§çš„æµ‹è¯•
            links = response.xpath('//a[@href]/@href').extract()[:3]  # åªå–å‰3ä¸ªé“¾æ¥
            for link in links:
                if link.startswith('http'):
                    yield Request(url=link, callback=self.parse, priority=0)


async def run_baidu_test():
    """è¿è¡Œç™¾åº¦æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç™¾åº¦ç½‘ç«™æ€§èƒ½æµ‹è¯•...")
    print("=" * 60)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
        process = CrawlerProcess(settings={
            "CONCURRENCY": 10,  # è®¾ç½®å¹¶å‘æ•°
            "DOWNLOAD_DELAY": 0.1,  # è®¾ç½®ä¸‹è½½å»¶è¿Ÿ
            "LOG_LEVEL": "INFO",  # è®¾ç½®æ—¥å¿—çº§åˆ«
        })
        
        # è¿è¡Œçˆ¬è™«
        await process.crawl(BaiduTestSpider)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        total_time = end_time - start_time
        # æ³¨æ„ï¼šç”±äºSpiderå®ä¾‹åœ¨CrawlerProcessä¸­åˆ›å»ºï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è·å–ç»Ÿè®¡ä¿¡æ¯
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"   å¹¶å‘æ•°: 10")
        
        # éªŒè¯ä¸‰ä¸ªä¼˜åŒ–æ–¹æ³•çš„å®ç°æƒ…å†µ
        print("\n" + "=" * 60)
        print("âœ… ä¼˜åŒ–æ–¹æ³•å®ç°éªŒè¯:")
        print("   1. å·¥ä½œæ± æ¨¡å¼: å·²å®ç° - TaskManagerä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘")
        print("   2. åŠ¨æ€ä¿¡å·é‡æ§åˆ¶: å·²å®ç° - æ ¹æ®å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°")
        print("   3. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦: å·²å®ç° - ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—å’Œæ™ºèƒ½è°ƒåº¦ç®—æ³•")
        
        print("\nğŸ‰ ç™¾åº¦ç½‘ç«™æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    asyncio.run(run_baidu_test())