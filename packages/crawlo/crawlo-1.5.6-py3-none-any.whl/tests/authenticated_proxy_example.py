#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¤è¯ä»£ç†ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨ Crawlo æ¡†æ¶ä¸­ä½¿ç”¨è®¤è¯ä»£ç†
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from crawlo.config import CrawloConfig
from crawlo.crawler import CrawlerProcess
from crawlo import Spider, Request, Item


class ProxyItem(Item):
    """ä»£ç†æµ‹è¯•ç»“æœé¡¹"""
    url = ''
    status = 0
    proxy = ''
    response_time = 0.0


class AuthProxySpider(Spider):
    """è®¤è¯ä»£ç†æµ‹è¯•çˆ¬è™«"""
    name = 'auth_proxy_spider'
    
    async def start_requests(self):
        """å‘èµ·æµ‹è¯•è¯·æ±‚"""
        urls = [
            'https://httpbin.org/ip',  # æŸ¥çœ‹IPåœ°å€
            'https://httpbin.org/headers',  # æŸ¥çœ‹è¯·æ±‚å¤´
        ]
        
        for url in urls:
            yield Request(url, callback=self.parse_response)
    
    async def parse_response(self, response):
        """è§£æå“åº”"""
        import time
        import json
        
        # è·å–ä»£ç†ä¿¡æ¯
        proxy_info = response.meta.get('proxy', 'No proxy')
        
        # è§£æå“åº”å†…å®¹
        try:
            data = json.loads(response.text)
            ip_info = data.get('origin', 'Unknown')
        except:
            ip_info = response.text[:100] + '...' if len(response.text) > 100 else response.text
        
        # åˆ›å»ºç»“æœé¡¹
        item = ProxyItem(
            url=response.url,
            status=response.status_code,  # ä¿®å¤ï¼šä½¿ç”¨status_codeè€Œä¸æ˜¯status
            proxy=str(proxy_info),
            response_time=response.meta.get('download_latency', 0)
        )
        
        self.logger.info(f"Proxy test result: {item}")
        yield item


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®¤è¯ä»£ç†æµ‹è¯•...")
    
    # åˆ›å»ºé…ç½®ï¼ˆä½¿ç”¨è®¤è¯ä»£ç†ï¼‰
    config = CrawloConfig.standalone(
        concurrency=2,
        download_delay=1.0,
        # ä»£ç†é…ç½®
        # é«˜çº§ä»£ç†é…ç½®ï¼ˆé€‚ç”¨äºProxyMiddlewareï¼‰
        # åªè¦é…ç½®äº†ä»£ç†API URLï¼Œä¸­é—´ä»¶å°±ä¼šè‡ªåŠ¨å¯ç”¨
        PROXY_API_URL="http://proxy-api.example.com/get",  # ä»£ç†APIåœ°å€
        
        # ä»£ç†é…ç½®ï¼ˆé€‚ç”¨äºProxyMiddlewareï¼‰
        # åªè¦é…ç½®äº†ä»£ç†åˆ—è¡¨ï¼Œä¸­é—´ä»¶å°±ä¼šè‡ªåŠ¨å¯ç”¨
        # PROXY_LIST=[
        #     "http://user:pass@proxy1.example.com:8080",
        #     "http://user:pass@proxy2.example.com:8080"
        # ],
        
        LOG_LEVEL='INFO'
    )
    
    # æ·»åŠ è‡ªå®šä¹‰ä¸­é—´ä»¶
    config.set('CUSTOM_MIDDLEWARES', [
        'crawlo.middleware.proxy.ProxyMiddleware',
    ])
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings=config.to_dict())
    
    # æ·»åŠ çˆ¬è™«
    process.crawl(AuthProxySpider)
    
    # å¯åŠ¨çˆ¬è™«
    print("ğŸ”„ æ­£åœ¨è¿è¡Œä»£ç†æµ‹è¯•...")
    await process.start()
    
    print("âœ… è®¤è¯ä»£ç†æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())