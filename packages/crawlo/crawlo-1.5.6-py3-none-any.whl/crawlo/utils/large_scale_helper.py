#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
å¤§è§„æ¨¡çˆ¬è™«ä¼˜åŒ–è¾…åŠ©å·¥å…·
"""
import asyncio
import json
import time
from typing import Generator, List, Dict, Any

from crawlo.logging import get_logger


class LargeScaleHelper:
    """å¤§è§„æ¨¡çˆ¬è™«è¾…åŠ©ç±»"""
    
    def __init__(self, batch_size: int = 1000, checkpoint_interval: int = 5000):
        self.batch_size = batch_size
        self.checkpoint_interval = checkpoint_interval
        self.logger = get_logger(self.__class__.__name__)
        
    def batch_iterator(self, data_source, start_offset: int = 0) -> Generator[List[Any], None, None]:
        """
        æ‰¹é‡è¿­ä»£å™¨ï¼Œé€‚ç”¨äºå¤§é‡æ•°æ®çš„åˆ†æ‰¹å¤„ç†
        
        Args:
            data_source: æ•°æ®æºï¼ˆæ”¯æŒå¤šç§ç±»å‹ï¼‰
            start_offset: èµ·å§‹åç§»é‡
            
        Yields:
            æ¯æ‰¹æ•°æ®çš„åˆ—è¡¨
        """
        if hasattr(data_source, '__iter__') and not isinstance(data_source, (str, bytes)):
            # å¯è¿­ä»£å¯¹è±¡
            yield from self._iterate_batches(data_source, start_offset)
        elif hasattr(data_source, 'get_batch'):
            # æ”¯æŒåˆ†æ‰¹è·å–çš„æ•°æ®æº
            yield from self._get_batches_from_source(data_source, start_offset)
        elif callable(data_source):
            # å‡½æ•°å½¢å¼çš„æ•°æ®æº
            yield from self._get_batches_from_function(data_source, start_offset)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {type(data_source)}")
    
    def _iterate_batches(self, iterable, start_offset: int) -> Generator[List[Any], None, None]:
        """ä»å¯è¿­ä»£å¯¹è±¡åˆ†æ‰¹è·å–æ•°æ®"""
        iterator = iter(iterable)
        
        # è·³è¿‡å·²å¤„ç†çš„æ•°æ®
        for _ in range(start_offset):
            try:
                next(iterator)
            except StopIteration:
                return
        
        while True:
            batch = []
            for _ in range(self.batch_size):
                try:
                    batch.append(next(iterator))
                except StopIteration:
                    if batch:
                        yield batch
                    return
            
            if batch:
                yield batch
    
    def _get_batches_from_source(self, data_source, start_offset: int) -> Generator[List[Any], None, None]:
        """ä»æ”¯æŒåˆ†æ‰¹è·å–çš„æ•°æ®æºè·å–æ•°æ®"""
        offset = start_offset
        
        while True:
            try:
                batch = data_source.get_batch(offset, self.batch_size)
                if not batch:
                    break
                
                yield batch
                offset += len(batch)
                
                if len(batch) < self.batch_size:
                    break  # å·²åˆ°è¾¾æ•°æ®æœ«å°¾
                    
            except Exception as e:
                self.logger.error(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥: {e}")
                break
    
    def _get_batches_from_function(self, func, start_offset: int) -> Generator[List[Any], None, None]:
        """ä»å‡½æ•°è·å–æ‰¹æ¬¡æ•°æ®"""
        offset = start_offset
        
        while True:
            try:
                batch = func(offset, self.batch_size)
                if not batch:
                    break
                
                yield batch
                offset += len(batch)
                
                if len(batch) < self.batch_size:
                    break
                    
            except Exception as e:
                self.logger.error(f"å‡½æ•°è·å–æ•°æ®å¤±è´¥: {e}")
                break


class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨"""
    
    def __init__(self, progress_file: str = "spider_progress.json"):
        self.progress_file = progress_file
        self.logger = get_logger(self.__class__.__name__)
        
    def load_progress(self) -> Dict[str, Any]:
        """åŠ è½½è¿›åº¦"""
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                progress = json.load(f)
                self.logger.info(f"åŠ è½½è¿›åº¦: {progress}")
                return progress
        except FileNotFoundError:
            self.logger.info("ğŸ“„ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹")
            return self._get_default_progress()
        except Exception as e:
            self.logger.error(f"åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            return self._get_default_progress()
    
    def save_progress(self, **kwargs):
        """ä¿å­˜è¿›åº¦"""
        try:
            progress = {
                **kwargs,
                'timestamp': time.time(),
                'formatted_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, indent=2, ensure_ascii=False)
                
            self.logger.debug(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦: {progress}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def _get_default_progress(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤è¿›åº¦"""
        return {
            'batch_num': 0,
            'processed_count': 0,
            'skipped_count': 0,
            'timestamp': time.time()
        }


class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_memory_mb: int = 500):
        self.max_memory_mb = max_memory_mb
        self.logger = get_logger(self.__class__.__name__)
        
    def check_memory_usage(self) -> Dict[str, float]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            
            memory_mb = memory_info.rss / 1024 / 1024
            memory_percent = process.memory_percent()
            
            return {
                'memory_mb': memory_mb,
                'memory_percent': memory_percent,
                'threshold_mb': self.max_memory_mb
            }
        except ImportError:
            self.logger.warning("psutil æœªå®‰è£…ï¼Œæ— æ³•ç›‘æ§å†…å­˜")
            return {}
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥å†…å­˜å¤±è´¥: {e}")
            return {}
    
    def should_pause_for_memory(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å› å†…å­˜ä¸è¶³è€Œæš‚åœ"""
        memory_info = self.check_memory_usage()
        
        if not memory_info:
            return False
            
        memory_mb = memory_info.get('memory_mb', 0)
        
        if memory_mb > self.max_memory_mb:
            self.logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_mb:.1f}MB > {self.max_memory_mb}MB")
            return True
            
        return False
    
    def force_garbage_collection(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        try:
            import gc
            collected = gc.collect()
            self.logger.debug(f"åƒåœ¾å›æ”¶: æ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
        except Exception as e:
            self.logger.error(f"åƒåœ¾å›æ”¶å¤±è´¥: {e}")


class DataSourceAdapter:
    """æ•°æ®æºé€‚é…å™¨"""
    
    @staticmethod
    def from_redis_queue(queue, batch_size: int = 1000):
        """ä»Redisé˜Ÿåˆ—åˆ›å»ºæ‰¹é‡æ•°æ®æº"""
        def get_batch(offset: int, limit: int) -> List[Dict]:
            try:
                # å¦‚æœé˜Ÿåˆ—æ”¯æŒèŒƒå›´æŸ¥è¯¢
                if hasattr(queue, 'get_range'):
                    return queue.get_range(offset, offset + limit - 1)
                
                # å¦‚æœé˜Ÿåˆ—æ”¯æŒæ‰¹é‡è·å–
                if hasattr(queue, 'get_batch'):
                    return queue.get_batch(offset, limit)
                
                # æ¨¡æ‹Ÿæ‰¹é‡è·å–
                results = []
                for _ in range(limit):
                    item = queue.get_nowait() if hasattr(queue, 'get_nowait') else None
                    if item:
                        results.append(item)
                    else:
                        break
                
                return results
                
            except Exception as e:
                print(f"è·å–æ‰¹æ¬¡å¤±è´¥: {e}")
                return []
        
        return get_batch
    
    @staticmethod
    def from_database(db_helper, query: str, batch_size: int = 1000):
        """ä»æ•°æ®åº“åˆ›å»ºæ‰¹é‡æ•°æ®æº"""
        def get_batch(offset: int, limit: int) -> List[Dict]:
            try:
                # æ·»åŠ åˆ†é¡µæŸ¥è¯¢
                paginated_query = f"{query} LIMIT {limit} OFFSET {offset}"
                return db_helper.execute_query(paginated_query)
            except Exception as e:
                print(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
                return []
        
        return get_batch
    
    @staticmethod
    def from_file(file_path: str, batch_size: int = 1000):
        """ä»æ–‡ä»¶åˆ›å»ºæ‰¹é‡æ•°æ®æº"""
        def get_batch(offset: int, limit: int) -> List[str]:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    # è·³è¿‡å·²å¤„ç†çš„è¡Œ
                    for _ in range(offset):
                        f.readline()
                    
                    # è¯»å–å½“å‰æ‰¹æ¬¡
                    batch = []
                    for _ in range(limit):
                        line = f.readline()
                        if not line:
                            break
                        batch.append(line.strip())
                    
                    return batch
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                return []
        
        return get_batch


class LargeScaleSpiderMixin:
    """å¤§è§„æ¨¡çˆ¬è™«æ··å…¥ç±»"""
    
    def __init__(self):
        super().__init__()
        self.large_scale_helper = LargeScaleHelper(
            batch_size=getattr(self, 'batch_size', 1000),
            checkpoint_interval=getattr(self, 'checkpoint_interval', 5000)
        )
        self.progress_manager = ProgressManager(
            progress_file=getattr(self, 'progress_file', f"{self.name}_progress.json")
        )
        self.memory_optimizer = MemoryOptimizer(
            max_memory_mb=getattr(self, 'max_memory_mb', 500)
        )
        
    def create_streaming_start_requests(self, data_source):
        """åˆ›å»ºæµå¼start_requestsç”Ÿæˆå™¨"""
        progress = self.progress_manager.load_progress()
        start_offset = progress.get('processed_count', 0)
        
        processed_count = start_offset
        skipped_count = progress.get('skipped_count', 0)
        
        for batch in self.large_scale_helper.batch_iterator(data_source, start_offset):
            
            # å†…å­˜æ£€æŸ¥
            if self.memory_optimizer.should_pause_for_memory():
                self.memory_optimizer.force_garbage_collection()
                # å¯ä»¥æ·»åŠ å»¶è¿Ÿæˆ–å…¶ä»–å¤„ç†
                asyncio.sleep(1)
            
            for item in batch:
                processed_count += 1
                
                # æ£€æŸ¥è¿›åº¦ä¿å­˜
                if processed_count % self.large_scale_helper.checkpoint_interval == 0:
                    self.progress_manager.save_progress(
                        processed_count=processed_count,
                        skipped_count=skipped_count
                    )
                
                # ç”Ÿæˆè¯·æ±‚
                request = self.create_request_from_item(item)
                if request:
                    yield request
                else:
                    skipped_count += 1
        
        # æœ€ç»ˆä¿å­˜è¿›åº¦
        self.progress_manager.save_progress(
            processed_count=processed_count,
            skipped_count=skipped_count,
            completed=True
        )
        
        self.logger.info(f"å¤„ç†å®Œæˆï¼æ€»è®¡: {processed_count}, è·³è¿‡: {skipped_count}")
    
    def create_request_from_item(self, item):
        """ä»æ•°æ®é¡¹åˆ›å»ºè¯·æ±‚ï¼ˆéœ€è¦å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° create_request_from_item æ–¹æ³•")