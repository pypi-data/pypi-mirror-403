import asyncio
import pickle
import time
import traceback
from typing import Optional, TYPE_CHECKING, List, Any

# å°è¯•å¯¼å…¥Redisé›†ç¾¤æ”¯æŒ
try:
    from redis.asyncio.cluster import RedisCluster
    REDIS_CLUSTER_AVAILABLE = True
except ImportError:
    RedisCluster = None
    REDIS_CLUSTER_AVAILABLE = False

# ä½¿ç”¨ TYPE_CHECKING é¿å…è¿è¡Œæ—¶å¾ªç¯å¯¼å…¥
if TYPE_CHECKING:
    from crawlo import Request

from crawlo.logging import get_logger
from crawlo.utils.request_serializer import RequestSerializer
try:
    import msgpack
    MSGPACK_AVAILABLE = True
except ImportError:
    MSGPACK_AVAILABLE = False
from crawlo.utils.error_handler import ErrorHandler, ErrorContext
from crawlo.utils.redis_manager import get_redis_pool, RedisConnectionPool, RedisKeyManager

# åˆ›å»ºloggerå®ä¾‹
logger = get_logger(__name__)

# å»¶è¿Ÿåˆå§‹åŒ–é¿å…å¾ªç¯ä¾èµ–
_error_handler = None


def get_module_error_handler():
    global _error_handler
    if _error_handler is None:
        _error_handler = ErrorHandler(__name__)
    return _error_handler


class RedisPriorityQueue:
    """
    åŸºäº Redis çš„åˆ†å¸ƒå¼å¼‚æ­¥ä¼˜å…ˆçº§é˜Ÿåˆ—
    """

    def __init__(
            self,
            redis_url: Optional[str] = None,
            queue_name: Optional[str] = None,
            failed_queue: Optional[str] = None,
            max_retries: int = 3,
            timeout: int = 300,
            max_connections: int = 10,
            project_name: str = "default",
            spider_name: Optional[str] = None,
            is_cluster: bool = False,
            cluster_nodes: Optional[List[str]] = None,
            serialization_format: str = 'pickle',  # æ–°å¢ï¼šåºåˆ—åŒ–æ ¼å¼
    ) -> None:
        """
        åˆå§‹åŒ– Redis ä¼˜å…ˆçº§é˜Ÿåˆ—
        
        Args:
            redis_url: Redisè¿æ¥URL
            queue_name: é˜Ÿåˆ—åç§°ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
            failed_queue: å¤±è´¥é˜Ÿåˆ—åç§°ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_connections: æœ€å¤§è¿æ¥æ•°
            project_name: é¡¹ç›®åç§°
            spider_name: çˆ¬è™«åç§°ï¼ˆå¯é€‰ï¼‰
            is_cluster: æ˜¯å¦ä¸ºé›†ç¾¤æ¨¡å¼
            cluster_nodes: é›†ç¾¤èŠ‚ç‚¹åˆ—è¡¨
        """
        # ç§»é™¤ç›´æ¥ä½¿ç”¨ os.getenv()ï¼Œè¦æ±‚é€šè¿‡å‚æ•°ä¼ é€’ redis_url
        if redis_url is None:
            # å¦‚æœæ²¡æœ‰æä¾› redis_urlï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ï¼Œè¦æ±‚åœ¨ settings ä¸­é…ç½®
            raise ValueError("redis_url must be provided. Configure it in settings instead of using os.getenv()")

        self.redis_url: str = redis_url
        self.is_cluster: bool = is_cluster
        self.cluster_nodes: Optional[List[str]] = cluster_nodes

        # åˆ›å»º Redis Key ç®¡ç†å™¨
        self.key_manager = RedisKeyManager(project_name, spider_name)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.debug(f"RedisPriorityQueue initialized with project_name: {project_name}, spider_name: {spider_name}")

        # å¦‚æœæœªæä¾› queue_nameï¼Œåˆ™æ ¹æ® key_manager è‡ªåŠ¨ç”Ÿæˆ
        self.queue_name = queue_name or self.key_manager.get_requests_queue_key()

        # å¦‚æœæœªæä¾› failed_queueï¼Œåˆ™æ ¹æ® key_manager è‡ªåŠ¨ç”Ÿæˆ
        self.failed_queue = failed_queue or self.key_manager.get_failed_queue_key()

        self.max_retries: int = max_retries
        self.timeout: int = timeout
        self.max_connections: int = max_connections
        self._redis_pool: Optional[RedisConnectionPool] = None
        self._redis: Optional[Any] = None
        self._lock: asyncio.Lock = asyncio.Lock()
        self.request_serializer: RequestSerializer = RequestSerializer(serialization_format=serialization_format)
        self.serialization_format: str = serialization_format  # æ–°å¢ï¼šå­˜å‚¨åºåˆ—åŒ–æ ¼å¼

    async def connect(self, max_retries: int = 3, delay: int = 1) -> Optional[Any]:
        """
        å¼‚æ­¥è¿æ¥ Redisï¼Œæ”¯æŒé‡è¯•
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            Optional[Any]: Redis å®¢æˆ·ç«¯å®ä¾‹
        """
        async with self._lock:
            if self._redis is not None:
                # å¦‚æœå·²ç»è¿æ¥ï¼Œæµ‹è¯•è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                try:
                    await self._redis.ping()
                    return self._redis
                except Exception:
                    # è¿æ¥å¤±æ•ˆï¼Œé‡æ–°è¿æ¥
                    self._redis = None

            for attempt in range(max_retries):
                try:
                    # ä½¿ç”¨ä¼˜åŒ–çš„è¿æ¥æ± ï¼Œç¡®ä¿ decode_responses=False ä»¥é¿å…ç¼–ç é—®é¢˜
                    self._redis_pool = get_redis_pool(
                        self.redis_url,
                        is_cluster=self.is_cluster,
                        cluster_nodes=self.cluster_nodes,
                        max_connections=self.max_connections,
                        socket_connect_timeout=5,
                        socket_timeout=30,
                        health_check_interval=30,
                        retry_on_timeout=True,
                        decode_responses=False,  # ç¡®ä¿ä¸è‡ªåŠ¨è§£ç å“åº”
                        encoding='utf-8'
                    )

                    self._redis = await self._redis_pool.get_connection()

                    # æµ‹è¯•è¿æ¥
                    if self._redis:
                        await self._redis.ping()
                    return self._redis
                except Exception as e:
                    error_msg = f"Redis è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}, Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {e}"
                    logger.warning(error_msg)
                    logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                    else:
                        raise ConnectionError(f"æ— æ³•è¿æ¥ Redis (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {e}")

    async def _ensure_connection(self) -> None:
        """ç¡®ä¿è¿æ¥æœ‰æ•ˆ"""
        if self._redis is None:
            await self.connect()
        try:
            if self._redis:
                await self._redis.ping()
        except Exception as e:
            logger.warning(f"Redis è¿æ¥å¤±æ•ˆ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})ï¼Œå°è¯•é‡è¿...: {e}")
            self._redis = None
            await self.connect()

    def _is_cluster_mode(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé›†ç¾¤æ¨¡å¼
        
        Returns:
            bool: æ˜¯å¦ä¸ºé›†ç¾¤æ¨¡å¼
        """
        if REDIS_CLUSTER_AVAILABLE and RedisCluster is not None:
            # æ£€æŸ¥ _redis æ˜¯å¦ä¸º RedisCluster å®ä¾‹
            if self._redis is not None and isinstance(self._redis, RedisCluster):
                return True
        return False

    def _execute_with_cluster_support(self, operation_func, *args, **kwargs):
        """
        æ‰§è¡Œæ”¯æŒé›†ç¾¤æ¨¡å¼çš„æ“ä½œ
        
        Args:
            operation_func: è¦æ‰§è¡Œçš„æ“ä½œå‡½æ•°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            æ“ä½œå‡½æ•°çš„è¿”å›ç»“æœ
        """
        # ç¡®ä¿è¿æ¥æœ‰æ•ˆ
        if self._redis is None:
            raise RuntimeError("Redisè¿æ¥æœªåˆå§‹åŒ–")
            
        # æ ¹æ®æ˜¯å¦ä¸ºé›†ç¾¤æ¨¡å¼æ‰§è¡Œæ“ä½œ
        if self._is_cluster_mode():
            return operation_func(cluster_mode=True, *args, **kwargs)
        else:
            return operation_func(cluster_mode=False, *args, **kwargs)

    async def put(self, request: 'Request', priority: int = 0) -> bool:
        """
        æ”¾å…¥è¯·æ±‚åˆ°é˜Ÿåˆ—
        
        Args:
            request: è¯·æ±‚å¯¹è±¡
            priority: ä¼˜å…ˆçº§
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ”¾å…¥é˜Ÿåˆ—
        """
        try:
            await self._ensure_connection()
            if not self._redis:
                return False
                
            # ä¿®å¤ä¼˜å…ˆçº§è¡Œä¸ºä¸€è‡´æ€§é—®é¢˜
            # åŸæ¥: score = -priority ï¼ˆå¯¼è‡´priorityå¤§çš„å…ˆå‡ºé˜Ÿï¼‰
            # ç°åœ¨: score = priority ï¼ˆç¡®ä¿priorityå°çš„å…ˆå‡ºé˜Ÿï¼Œä¸å†…å­˜é˜Ÿåˆ—ä¸€è‡´ï¼‰
            score = priority
            key = self._get_request_key(request)

            # ğŸ”¥ ä½¿ç”¨ä¸“ç”¨çš„åºåˆ—åŒ–å·¥å…·æ¸…ç† Request
            clean_request = self.request_serializer.prepare_for_serialization(request)

            # æ ¹æ®é…ç½®çš„åºåˆ—åŒ–æ ¼å¼è¿›è¡Œåºåˆ—åŒ–
            try:
                if self.serialization_format == 'msgpack' and MSGPACK_AVAILABLE:
                    # ä½¿ç”¨msgpackåºåˆ—åŒ–
                    serialized = msgpack.packb(clean_request, default=str)
                    # éªŒè¯åºåˆ—åŒ–æ•°æ®å¯ä»¥è¢«ååºåˆ—åŒ–
                    msgpack.unpackb(serialized, raw=False)
                else:
                    # ä½¿ç”¨pickleåºåˆ—åŒ–
                    serialized = pickle.dumps(clean_request)
                    # éªŒè¯åºåˆ—åŒ–æ•°æ®å¯ä»¥è¢«ååºåˆ—åŒ–
                    pickle.loads(serialized)
            except Exception as serialize_error:
                logger.error(f"è¯·æ±‚åºåˆ—åŒ–éªŒè¯å¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {serialize_error}")
                return False

            # å¤„ç†é›†ç¾¤æ¨¡å¼ä¸‹çš„æ“ä½œ
            try:
                if self._is_cluster_mode():
                    # åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œç¡®ä¿æ‰€æœ‰é”®éƒ½åœ¨åŒä¸€ä¸ªslotä¸­
                    # å¯ä»¥é€šè¿‡åœ¨é”®åä¸­æ·»åŠ ç›¸åŒçš„å“ˆå¸Œæ ‡ç­¾æ¥å®ç°
                    hash_tag = "{queue}"  # ä½¿ç”¨å“ˆå¸Œæ ‡ç­¾ç¡®ä¿é”®åœ¨åŒä¸€ä¸ªslot
                    queue_name_with_tag = f"{self.queue_name}{hash_tag}"
                    data_key_with_tag = self.key_manager.get_requests_data_key() + hash_tag
                    
                    pipe = self._redis.pipeline()
                    pipe.zadd(queue_name_with_tag, {key: score})
                    pipe.hset(data_key_with_tag, key, serialized)
                    result = await pipe.execute()
                    
                    # è®°å½•åºåˆ—åŒ–æ ¼å¼ä¿¡æ¯
                    logger.debug(f"Request enqueued with {self.serialization_format} serialization (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
                else:
                    pipe = self._redis.pipeline()
                    pipe.zadd(self.queue_name, {key: score})
                    pipe.hset(self.key_manager.get_requests_data_key(), key, serialized)
                    result = await pipe.execute()
                    
                    # è®°å½•åºåˆ—åŒ–æ ¼å¼ä¿¡æ¯
                    logger.debug(f"Request enqueued with {self.serialization_format} serialization (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
            except Exception as e:
                logger.error(f"Redisé˜Ÿåˆ—æ“ä½œå¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {e}")
                logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                return False

            if result and result[0] > 0:
                logger.debug(f"æˆåŠŸå…¥é˜Ÿ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
                # è®°å½•æˆåŠŸç»Ÿè®¡
                if hasattr(self, '_stats'):
                    self._stats['successful_puts'] = self._stats.get('successful_puts', 0) + 1
                else:
                    self._stats = {'successful_puts': 1}
            else:
                logger.warning(f"å…¥é˜Ÿå¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
                # è®°å½•å¤±è´¥ç»Ÿè®¡
                if hasattr(self, '_stats'):
                    self._stats['failed_puts'] = self._stats.get('failed_puts', 0) + 1
                else:
                    self._stats = {'failed_puts': 1}
            
            success = result and result[0] > 0 if result else False
            return success
        except Exception as e:
            error_context = ErrorContext(
                context=f"æ”¾å…¥é˜Ÿåˆ—å¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})"
            )
            ErrorHandler(__name__).handle_error(
                e,
                context=error_context,
                raise_error=False
            )
            return False

    async def get(self, timeout: float = 5.0) -> Optional['Request']:
        """
        è·å–è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
        
        Args:
            timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…æ— é™è½®è¯¢
            
        Returns:
            Optional[Request]: è¯·æ±‚å¯¹è±¡æˆ– None
        """
        try:
            await self._ensure_connection()
            if not self._redis:
                return None
                
            start_time = asyncio.get_event_loop().time()

            while True:
                # å°è¯•è·å–ä»»åŠ¡
                if self._is_cluster_mode():
                    # é›†ç¾¤æ¨¡å¼å¤„ç†
                    hash_tag = "{queue}"
                    queue_name_with_tag = f"{self.queue_name}{hash_tag}"
                    result = await self._redis.zpopmin(queue_name_with_tag, count=1)
                else:
                    result = await self._redis.zpopmin(self.queue_name, count=1)
                    
                if result:
                    key, score = result[0]
                    data_key = self.key_manager.get_requests_data_key()
                    if self._is_cluster_mode():
                        hash_tag = "{queue}"
                        data_key = self.key_manager.get_requests_data_key() + hash_tag
                        
                    serialized = await self._redis.hget(data_key, key)
                    if not serialized:
                        continue

                    # æ ¹æ®åºåˆ—åŒ–æ ¼å¼è¿›è¡Œååºåˆ—åŒ–
                    try:
                        if self.serialization_format == 'msgpack' and MSGPACK_AVAILABLE:
                            # ä½¿ç”¨msgpackååºåˆ—åŒ–
                            request = msgpack.unpackb(serialized, raw=False)
                        else:
                            # ä½¿ç”¨pickleååºåˆ—åŒ–
                            try:
                                # é¦–å…ˆå°è¯•æ ‡å‡†çš„ pickle ååºåˆ—åŒ–
                                request = pickle.loads(serialized)
                            except UnicodeDecodeError:
                                # å¦‚æœå‡ºç°ç¼–ç é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ latin1 è§£ç 
                                request = pickle.loads(serialized, encoding='latin1')
                        return request
                    except Exception as deserialize_error:
                        # å¦‚æœååºåˆ—åŒ–å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡è¿™ä¸ªä»»åŠ¡
                        logger.error(f"æ— æ³•ååºåˆ—åŒ–è¯·æ±‚æ•°æ® (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {deserialize_error}")
                        # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªä»»åŠ¡
                        continue

                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if asyncio.get_event_loop().time() - start_time > timeout:
                    return None

                # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç©ºè½®è¯¢ï¼Œä½†å‡å°‘ç­‰å¾…æ—¶é—´ä»¥æé«˜å“åº”é€Ÿåº¦
                await asyncio.sleep(0.001)  # ä»0.01å‡å°‘åˆ°0.001

        except Exception as e:
            error_context = ErrorContext(
                context=f"è·å–é˜Ÿåˆ—ä»»åŠ¡å¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})"
            )
            ErrorHandler(__name__).handle_error(
                e,
                context=error_context,
                raise_error=False
            )
            return None

    async def ack(self, request: 'Request') -> None:
        """
        ç¡®è®¤ä»»åŠ¡å®Œæˆ
        
        Args:
            request: è¯·æ±‚å¯¹è±¡
        """
        # ç”±äºæˆ‘ä»¬ä¸å†ä½¿ç”¨å¤„ç†é˜Ÿåˆ—ï¼Œackæ–¹æ³•ç°åœ¨æ˜¯ä¸€ä¸ªç©ºæ“ä½œ
        # ä»»åŠ¡åœ¨ä»ä¸»é˜Ÿåˆ—å–å‡ºæ—¶å°±å·²ç»è¢«è®¤ä¸ºæ˜¯å®Œæˆçš„
        logger.debug(f"ä»»åŠ¡ç¡®è®¤å®Œæˆ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")

    async def fail(self, request: 'Request', reason: str = "") -> None:
        """
        æ ‡è®°ä»»åŠ¡å¤±è´¥
        
        Args:
            request: è¯·æ±‚å¯¹è±¡
            reason: å¤±è´¥åŸå› 
        """
        try:
            await self._ensure_connection()
            if not self._redis:
                return
                
            key = self._get_request_key(request)
            await self.ack(request)

            retry_key = f"{self.failed_queue}:retries:{key}"
            failed_queue = self.failed_queue
            
            if self._is_cluster_mode():
                hash_tag = "{queue}"
                retry_key = f"{self.failed_queue}:retries:{key}{hash_tag}"
                failed_queue = f"{self.failed_queue}{hash_tag}"

            retries = await self._redis.incr(retry_key)
            await self._redis.expire(retry_key, 86400)

            if retries <= self.max_retries:
                await self.put(request, priority=request.priority + 1)
                logger.info(
                    f"ä»»åŠ¡é‡è¯• [{retries}/{self.max_retries}] (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
            else:
                failed_data = {
                    "url": request.url,
                    "reason": reason,
                    "retries": retries,
                    "failed_at": time.time(),
                    "request_pickle": pickle.dumps(request).hex(),  # å¯é€‰ï¼šä¿å­˜å®Œæ•´è¯·æ±‚
                }
                await self._redis.lpush(failed_queue, pickle.dumps(failed_data))
                logger.error(f"ä»»åŠ¡å½»åº•å¤±è´¥ [{retries}æ¬¡] (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {request.url}")
        except Exception as e:
            error_context = ErrorContext(
                context=f"æ ‡è®°ä»»åŠ¡å¤±è´¥å¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})"
            )
            ErrorHandler(__name__).handle_error(
                e,
                context=error_context,
                raise_error=False
            )

    def _get_request_key(self, request: 'Request') -> str:
        """
        ç”Ÿæˆè¯·æ±‚å”¯ä¸€é”®
        
        Args:
            request: è¯·æ±‚å¯¹è±¡
            
        Returns:
            str: è¯·æ±‚å”¯ä¸€é”®
        """
        # ä½¿ç”¨key_managerçš„namespaceæ¥ç¡®ä¿ä¸€è‡´æ€§
        return f"{self.key_manager.namespace}:url:{hash(request.url) & 0x7FFFFFFF}"  # ç¡®ä¿æ­£æ•°

    async def qsize(self) -> int:
        """
        Get queue size (åªæ£€æŸ¥ä¸»é˜Ÿåˆ—)

        Returns:
            int: é˜Ÿåˆ—å¤§å°ï¼ˆåªæ£€æŸ¥ä¸»é˜Ÿåˆ—ï¼‰
        """
        try:
            await self._ensure_connection()
            if not self._redis:
                return 0

            # åªæ£€æŸ¥ä¸»é˜Ÿåˆ—å¤§å°ï¼Œä¸å†æ£€æŸ¥å¤„ç†ä¸­é˜Ÿåˆ—
            main_queue_size = 0

            if self._is_cluster_mode():
                hash_tag = "{queue}"
                queue_name_with_tag = f"{self.queue_name}{hash_tag}"
                main_queue_size = await self._redis.zcard(queue_name_with_tag)
            else:
                main_queue_size = await self._redis.zcard(self.queue_name)

            logger.debug(f"é˜Ÿåˆ—å¤§å°æ£€æŸ¥ - ä¸»é˜Ÿåˆ—: {main_queue_size} (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})")

            return main_queue_size
        except Exception as e:
            error_context = ErrorContext(
                context=f"Failed to get queue size (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})"
            )
            get_module_error_handler().handle_error(
                e,
                context=error_context,
                raise_error=False
            )
            return 0

    async def close(self) -> None:
        """å…³é—­è¿æ¥"""
        try:
            # æ˜¾å¼å…³é—­Redisè¿æ¥
            if self._redis is not None:
                try:
                    # ä¸å†è‡ªåŠ¨æ¸…ç†Redisæ•°æ®ï¼Œä¿ç•™æ•°æ®ä»¥æ”¯æŒæ–­ç‚¹ç»­çˆ¬
                    logger.debug(f"ä¿ç•™Redisæ•°æ®ä»¥æ”¯æŒæ–­ç‚¹ç»­çˆ¬ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})")
                    
                    # å°è¯•å…³é—­è¿æ¥
                    if hasattr(self._redis, 'close'):
                        close_result = self._redis.close()
                        if asyncio.iscoroutine(close_result):
                            await close_result
                    
                    # ç­‰å¾…è¿æ¥å…³é—­å®Œæˆ
                    if hasattr(self._redis, 'wait_closed'):
                        wait_result = self._redis.wait_closed()
                        if asyncio.iscoroutine(wait_result):
                            await wait_result
                except Exception as close_error:
                    logger.warning(
                        f"Error closing Redis connection (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name}): {close_error}"
                    )
                finally:
                    self._redis = None
            
            # é‡Šæ”¾è¿æ¥æ± å¼•ç”¨ï¼ˆè¿æ¥æ± ç”±å…¨å±€ç®¡ç†å™¨ç®¡ç†ï¼‰
            self._redis_pool = None
            
            logger.debug(f"Redis è¿æ¥å·²é‡Šæ”¾ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})")
        except Exception as e:
            error_context = ErrorContext(
                context=f"é‡Šæ”¾ Redis è¿æ¥å¤±è´¥ (Project: {self.key_manager.project_name}, Spider: {self.key_manager.spider_name})"
            )
            ErrorHandler(__name__).handle_error(
                e,
                context=error_context,
                raise_error=False
            )

    def get_stats(self) -> dict:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        """
        stats = getattr(self, '_stats', {})
        stats['project_name'] = self.key_manager.project_name
        stats['spider_name'] = self.key_manager.spider_name
        stats['queue_name'] = self.queue_name
        return stats
