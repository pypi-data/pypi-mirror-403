#****************************************
#     Devices Visible Configuration     *
#****************************************
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7


#****************************************
#       Multi-node Configuration        *
#****************************************
master_ip=<MASTER IP>
worker_ip=<WORKER IP>


#****************************************
#          Ray Configuration            *
#****************************************
# Number of nodes in multi-node inference
node_num=<NUMBER OF NODES>


#****************************************
#   Ascend Multi-node Data Parallelism   *
#****************************************
export HCCL_OP_EXPANSION_MODE="AIV"
export OMP_PROC_BIND=false
export OMP_NUM_THREADS=100
export HCCL_BUFFSIZE=200
export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
export VLLM_ASCEND_ENABLE_MLAPO=1
export HCCL_INTRA_PCIE_ENABLE=1
export HCCL_INTRA_ROCE_ENABLE=0
dp_rpc_port=13389
dp_size_local=<NUMBER OF DP PER NODE>


#****************************************
#      Common vLLM Configuration        *
#****************************************
# For multi-node and multi-npu inference
export RAY_EXPERIMENTAL_NOSET_ASCEND_RT_VISIBLE_DEVICES=1 
export VLLM_ALLREDUCE_USE_SYMM_MEM=0
# For multi-node and multi-gpu inference
export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
# Run deepseek v3.1+ on CUDA
export VLLM_USE_DEEP_GEMM=0
export VLLM_LOGGING_LEVEL=INFO
model=/home/models/QwQ-32B
# served_model_name=QwQ-32B
server_host=0.0.0.0
server_port=7850
tp_size=4
dp_size=1
pp_size=1
seed=1024
enable_expert_parallel=false
enable_prefix_caching=false
max_model_len=20000
# max_num_batched_tokens=2048
# max_num_seqs=20
# block_size=128
gpu_memory_utilization=0.87
# NONE | PIECEWISE | FULL | FULL_DECODE_ONLY | FULL_AND_PIECEWISE
graph_mode=FULL_DECODE_ONLY
quantization=NONE
# mp | ray ; mp for single-node inference, ray for multi-node inference
distributed_executor_backend=mp
# async_scheduling=false

# speculative decoding configuration
enable_speculative_decoding=false
speculative_decode_model=NONE
speculative_decode_method=deepseek_mtp
num_speculative_tokens=1


#****************************************
#  extra vLLM Configuration for Ascend  *
#****************************************
enable_ascend_scheduler=false
# enable_torchair_graph=false


#****************************************
#          UCM  Configuration           *
#****************************************
# set true to enable UCM
ucm_enable=false
ucm_config_yaml_path=/vllm-workspace/unified-cache-management/examples/ucm_config_example.yaml


