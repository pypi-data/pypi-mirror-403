From 93d2b945ec463d1f0b71d177aa9d6a0933e45746 Mon Sep 17 00:00:00 2001
From: wenxinwang <wangwenxin21@huawei.com>
Date: Tue, 23 Dec 2025 19:21:33 -0800
Subject: [PATCH 1/5] sparse patch for vllm-ascend

---
 vllm_ascend/attention/attention_v1.py | 80 ++++++++++++++++++++++
 vllm_ascend/attention/mla_v1.py       | 14 +++-
 vllm_ascend/worker/model_runner_v1.py | 98 ++++++++++++++++++++++++---
 vllm_ascend/worker/worker_v1.py       | 25 +++++--
 4 files changed, 201 insertions(+), 16 deletions(-)

diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index 7d7f488f..ea982244 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -24,6 +24,9 @@ import torch_npu
 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionLayer, AttentionType)
 from vllm.attention.backends.utils import CommonAttentionState
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group,
+                                          is_v1_kv_transfer_group)
 from vllm.forward_context import ForwardContext, get_forward_context
 from vllm.utils import direct_register_custom_op
 from vllm.v1.core.sched.output import SchedulerOutput
@@ -33,6 +36,8 @@ from vllm_ascend.ops.attention import vanilla_chunked_prefill
 from vllm_ascend.utils import (ACL_FORMAT_FRACTAL_NZ, aligned_16, is_310p,
                                nd_to_nz_2d, nd_to_nz_spec)
 
+from ucm.sparse.state import get_ucm_sparse, has_ucm_sparse
+
 
 class AscendAttentionBackend(AttentionBackend):
     accept_output_buffer: bool = True
@@ -444,10 +449,14 @@ def unified_ascend_attention_with_output(
     output: torch.Tensor,
     layer_name: str,
 ) -> None:
+    wait_for_kv_layer_from_connector(layer_name)
+
     forward_context: ForwardContext = get_forward_context()
     attn_metadata = forward_context.attn_metadata
     self = forward_context.no_compile_layers[layer_name]
     kv_cache = self.kv_cache[forward_context.virtual_engine]
+    if not self.use_mla:
+        query, _, _, _ = maybe_execute_sparse_attention_begin(query, key, value, layer_name, forward_context)
     self.impl.forward(self,
                       query,
                       key,
@@ -456,8 +465,79 @@ def unified_ascend_attention_with_output(
                       attn_metadata,
                       output,
                       trace_flag=False)
+    if not self.use_mla:
+        maybe_execute_sparse_attention_finished(query, key, value, output, layer_name, forward_context)
+    maybe_save_kv_layer_to_connector(layer_name, kv_cache)
     return
 
+def wait_for_kv_layer_from_connector(layer_name: str):
+    if not has_kv_transfer_group() or not is_v1_kv_transfer_group():
+        return
+
+    connector = get_kv_transfer_group()
+
+    forward_context: ForwardContext = get_forward_context()
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+    connector.wait_for_layer_load(layer_name)
+
+def maybe_save_kv_layer_to_connector(
+    layer_name: str,
+    kv_cache_layer: List[torch.Tensor],
+):
+    if not has_kv_transfer_group() or not is_v1_kv_transfer_group():
+        return
+
+    connector = get_kv_transfer_group()
+
+    forward_context: ForwardContext = get_forward_context()
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+    connector.save_kv_layer(layer_name, kv_cache_layer,
+                            attn_metadata)
+
+def maybe_execute_sparse_attention_begin(
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        layer_name: str,
+        forward_context: ForwardContext,
+        output: Optional[torch.Tensor] = None,
+        phase: Optional[str] = None,
+):
+    if not has_ucm_sparse():
+            return query, key, value, output
+
+    ucm_sparse = get_ucm_sparse()
+
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return query, key, value, output
+
+    return ucm_sparse.attention_begin(
+        query, key, value, layer_name, forward_context, output, phase
+    )
+
+def maybe_execute_sparse_attention_finished(
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        attn_output: torch.Tensor,
+        layer_name: str,
+        forward_context: ForwardContext,
+):
+    if not has_ucm_sparse():
+        return
+
+    ucm_sparse = get_ucm_sparse()
+
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+
+    ucm_sparse.attention_finished(query, key, value, attn_output, layer_name, forward_context)
 
 def unified_attention_with_output_fake(
     query: torch.Tensor,
diff --git a/vllm_ascend/attention/mla_v1.py b/vllm_ascend/attention/mla_v1.py
index f50fe56e..ae8f50bf 100644
--- a/vllm_ascend/attention/mla_v1.py
+++ b/vllm_ascend/attention/mla_v1.py
@@ -13,10 +13,12 @@ from vllm.distributed import get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.linear import (LinearBase,
                                                UnquantizedLinearMethod)
 from vllm.utils import cdiv, round_down
+from vllm.forward_context import ForwardContext, get_forward_context
+from vllm.attention.layer import (maybe_execute_sparse_attention_begin, maybe_execute_sparse_attention_finished)
 
 from vllm_ascend.ascend_config import get_ascend_config
 from vllm_ascend.attention.attention import _ALLOWED_NUM_QUERIES_PER_KV
-from vllm_ascend.attention.attention_v1 import AscendAttentionState
+from vllm_ascend.attention.attention_v1 import AscendAttentionState, wait_for_kv_layer_from_connector, maybe_save_kv_layer_to_connector
 from vllm_ascend.multistream.base import MSAttentionMetadataSplitConfig
 from vllm_ascend.multistream.context import get_multistream_comm_context
 from vllm_ascend.multistream.ms_split import model_input_split_v1_mla_attn
@@ -1042,6 +1044,7 @@ class AscendMLAImpl(MLAAttentionImpl):
         enable_multistream_mla: bool = False,
         ckq: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
+        forward_context: ForwardContext = get_forward_context()
         assert output is not None, "Output tensor must be provided."
         if attn_metadata is None:
             # Profiling run.
@@ -1192,6 +1195,8 @@ class AscendMLAImpl(MLAAttentionImpl):
             # FIX: aicore move should be also placed on the comm stream in dbo,
             # otherwise it may affect the accuracy
             # TODO: use an elegant way to overlap
+            wait_for_kv_layer_from_connector(layer.layer_name)
+            prefill_q, _, _, _ =  maybe_execute_sparse_attention_begin(prefill_q, prefill_k_c_normed, k_pe, layer.layer_name, forward_context, output=output, phase="prefill")
             output_prefill = self._forward_prefill(prefill_q,
                                                    prefill_k_c_normed,
                                                    prefill_k_pe, kv_cache,
@@ -1203,8 +1208,11 @@ class AscendMLAImpl(MLAAttentionImpl):
                     current_ms_metadata.after_comm_event.record()
             else:
                 output[num_decode_tokens:] = output_prefill
-
+            maybe_execute_sparse_attention_finished(prefill_q, prefill_k_c_normed, prefill_k_pe, output[num_decode_tokens:], layer.layer_name, forward_context, "prefill")
+            maybe_save_kv_layer_to_connector(layer.layer_name, kv_cache)
         if has_decode:
+            wait_for_kv_layer_from_connector(layer.layer_name)
+            _, _, _, _ = maybe_execute_sparse_attention_begin(torch.cat([decode_ql_nope, decode_q_pe],dim=-1), decode_k_nope, k_pe, layer.layer_name, forward_context, output=output, phase="decode")
             if self.running_in_graph:
                 return self._forward_decode(decode_ql_nope, decode_q_pe,
                                             decode_k_nope, decode_k_pe,
@@ -1223,5 +1231,7 @@ class AscendMLAImpl(MLAAttentionImpl):
                     current_ms_metadata.after_comm_event.record()
             else:
                 output[:num_decode_tokens] = output_decode
+            maybe_execute_sparse_attention_finished(torch.cat([decode_ql_nope, decode_q_pe],dim=-1), decode_ql_nope, decode_q_pe, output[:num_decode_tokens], layer.layer_name, forward_context, "decode")
+            maybe_save_kv_layer_to_connector(layer.layer_name, kv_cache)
 
         return output_padded
diff --git a/vllm_ascend/worker/model_runner_v1.py b/vllm_ascend/worker/model_runner_v1.py
index eabcdbcc..782b9a3b 100644
--- a/vllm_ascend/worker/model_runner_v1.py
+++ b/vllm_ascend/worker/model_runner_v1.py
@@ -39,7 +39,10 @@ from vllm.config import CompilationLevel, VllmConfig
 from vllm.distributed import get_tensor_model_parallel_world_size
 from vllm.distributed.parallel_state import (get_dp_group, get_pp_group,
                                              get_tp_group)
-from vllm.forward_context import set_forward_context
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group)
+from vllm.distributed.kv_transfer.kv_connector.v1 import KVConnectorBase_V1
+from vllm.forward_context import set_forward_context, get_forward_context
 from vllm.inputs import INPUT_REGISTRY
 from vllm.logger import logger
 from vllm.model_executor.layers.fused_moe import FusedMoE
@@ -88,6 +91,9 @@ from vllm_ascend.worker.eagle_proposer_v1 import EagleProposer
 from vllm_ascend.worker.mtp_proposer_v1 import MtpProposer
 from vllm_ascend.worker.npu_input_batch import CachedRequestState, InputBatch
 
+from ucm.sparse.state import get_ucm_sparse, has_ucm_sparse
+from ucm.sparse.base import UcmSparseMetadata, INVALID_SLOT
+
 if TYPE_CHECKING:
     import xgrammar as xgr  # type: ignore[import-untyped]
     from vllm.v1.core.sched.output import SchedulerOutput
@@ -347,6 +353,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         """
         # Remove finished requests from the cached states.
         for req_id in scheduler_output.finished_req_ids:
+            self.ucm_sparse_request_finished_in_worker(req_id)
             self.requests.pop(req_id, None)
             self.encoder_cache.pop(req_id, None)
         # Remove the finished requests from the persistent batch.
@@ -453,12 +460,14 @@ class NPUModelRunner(LoRAModelRunnerMixin):
 
         # Update the states of the running/resumed requests.
         req_data = scheduler_output.scheduled_cached_reqs
+        req_sparsed_slots = scheduler_output.req_sparsed_slots
         is_last_rank = get_pp_group().is_last_rank
         for i, req_id in enumerate(req_data.req_ids):
             req_state = self.requests[req_id]
             num_computed_tokens = req_data.num_computed_tokens[i]
             new_block_ids = req_data.new_block_ids[i]
             resumed_from_preemption = req_data.resumed_from_preemption[i]
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
 
             req_state.num_computed_tokens = num_computed_tokens
             if not is_last_rank:
@@ -474,15 +483,15 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                     req_state.output_token_ids.extend(
                         new_token_ids[-num_new_tokens:])
             # Update the block IDs.
-            if not resumed_from_preemption:
+            if resumed_from_preemption or is_sparsed_request:
+                # The request is resumed from preemption.
+                # Replace the existing block IDs with the new ones.
+                req_state.block_ids = new_block_ids
+            else:
                 # Append the new blocks to the existing block IDs.
                 for block_ids, new_ids in zip(  # type: ignore[call-overload]
                         req_state.block_ids, new_block_ids):
                     block_ids.extend(new_ids)
-            else:
-                # The request is resumed from preemption.
-                # Replace the existing block IDs with the new ones.
-                req_state.block_ids = new_block_ids
 
             req_index = self.input_batch.req_id_to_index.get(req_id)
             if req_index is None:
@@ -496,6 +505,9 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             self.input_batch.num_computed_tokens_cpu[req_index] = (
                 num_computed_tokens)
 
+            if is_sparsed_request:
+                self.input_batch.block_table.reset_row(req_index)
+
             self.input_batch.block_table.append_row(new_block_ids, req_index)
 
             if not is_last_rank:
@@ -876,7 +888,8 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> tuple[Union[AscendMetadata, AscendMLAMetadata,
                      AscendTorchairMetadata], torch.Tensor, SpecDecodeMetadata,
-               torch.Tensor, int, torch.Tensor, torch.Tensor, np.ndarray]:
+               torch.Tensor, int, torch.Tensor, torch.Tensor, np.ndarray,
+               Optional[dict[str, list[str]]]]:
         # Check input valid
         total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
         assert total_num_scheduled_tokens > 0
@@ -955,12 +968,22 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             num_scheduled_tokens)
         seq_lens = self.seq_lens_cpu[:num_reqs]
 
+        # TODO: improve performance, no `positions_np.copy()`
+        sparsed_positions = positions_np.copy()
+        req_sparsed_slots = scheduler_output.req_sparsed_slots
+        for req_id in self.input_batch.req_id_to_index:
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
+            req_index = self.input_batch.req_id_to_index[req_id]
+            offset = 0 if req_index == 0 else cu_num_tokens[req_index - 1] # TODO: support MTP
+            if is_sparsed_request:
+                sparsed_positions[offset] = req_sparsed_slots[req_id] - 1
+
         block_table_indices = (req_indices * self.max_num_blocks_per_req +
-                               positions_np // self.block_size)
+                               sparsed_positions // self.block_size)
 
         block_table_cpu = self.input_batch.block_table[0].get_cpu_tensor()
         block_numbers = block_table_cpu.flatten()[block_table_indices].numpy()
-        block_offsets = positions_np % self.block_size
+        block_offsets = sparsed_positions % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,
                out=self.slot_mapping_np[:total_num_scheduled_tokens])
@@ -985,10 +1008,16 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         else:
             attn_state = AscendAttentionState.PrefillCacheHit
 
+        for req_id in self.input_batch.req_id_to_index:
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
+            req_index = self.input_batch.req_id_to_index[req_id]
+            if is_sparsed_request:
+                seq_lens[req_index] = req_sparsed_slots[req_id]
+
         self.attn_mask = self._make_attention_mask(
             seq_lens=seq_lens,
             query_lens=num_scheduled_tokens,
-            position=positions,
+            position=torch.tensor(sparsed_positions).npu(),
             attn_state=attn_state)
         self.attn_state = attn_state  # type: ignore
 
@@ -1125,6 +1154,8 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                     assert self.model is not None
                     maybe_converting_weight_acl_format(self.model,
                                                        ACL_FORMAT_FRACTAL_ND)
+                    self.maybe_setup_kv_connector(scheduler_output)
+                    self.maybe_execute_ucm_sparse_begin(scheduler_output, attn_metadata)
 
                     hidden_states = self.model(
                         input_ids=input_ids,
@@ -1133,6 +1164,8 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                         inputs_embeds=inputs_embeds,
                         **model_kwargs,
                     )
+                    self.maybe_wait_for_kv_save()
+                    logits_indices = self.maybe_execute_ucm_sparse_finished(logits_indices)
 
         use_spec_decode = len(
             scheduler_output.scheduled_spec_decode_tokens) > 0
@@ -2369,3 +2402,48 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             if batch_size <= padded_batch_size < selected_batch_size:
                 selected_batch_size = padded_batch_size
         return selected_batch_size
+
+    @staticmethod
+    def maybe_setup_kv_connector(scheduler_output: "SchedulerOutput"):
+        # Update KVConnector with the KVConnector metadata forward().
+        if has_kv_transfer_group():
+            kv_connector = get_kv_transfer_group()
+            assert isinstance(kv_connector, KVConnectorBase_V1)
+            assert scheduler_output.kv_connector_metadata is not None
+            kv_connector.bind_connector_metadata(
+                scheduler_output.kv_connector_metadata)
+
+            # Background KV cache transfers happen here.
+            # These transfers are designed to be async and the requests
+            # involved may be disjoint from the running requests.
+            # Do this here to save a collective_rpc.
+            kv_connector.start_load_kv(get_forward_context())
+
+    @staticmethod
+    def maybe_wait_for_kv_save():
+        if has_kv_transfer_group():
+            return get_kv_transfer_group().wait_for_save()
+
+    def maybe_execute_ucm_sparse_begin(self, scheduler_output: "SchedulerOutput", attn_metadata: CommonAttentionMetadata):
+        if not has_ucm_sparse():
+            return
+        if has_kv_transfer_group():
+            uc_connector = get_kv_transfer_group()
+            uc_setup_model = getattr(uc_connector, "setup_model", None)
+            if callable(uc_setup_model):
+                uc_setup_model(self.model)
+        ucm_sparse = get_ucm_sparse()
+        ucm_sparse.build_sparse_meta(scheduler_output, self.requests, self.input_batch, attn_metadata)
+        ucm_sparse.execute_begin(scheduler_output)
+
+    def maybe_execute_ucm_sparse_finished(self, logits_indices):
+        if not has_ucm_sparse():
+            return logits_indices
+        ucm_sparse = get_ucm_sparse()
+        return ucm_sparse.execute_finished(logits_indices)
+
+    def ucm_sparse_request_finished_in_worker(self, request_id: str | int):
+        if not has_ucm_sparse():
+            return
+        ucm_sparse = get_ucm_sparse()
+        ucm_sparse.request_finished_in_worker(request_id)
\ No newline at end of file
diff --git a/vllm_ascend/worker/worker_v1.py b/vllm_ascend/worker/worker_v1.py
index df03d508..5d5d9b5a 100644
--- a/vllm_ascend/worker/worker_v1.py
+++ b/vllm_ascend/worker/worker_v1.py
@@ -17,6 +17,7 @@
 # Adapted from vllm-project/vllm/vllm/worker/gpu_worker.py
 #
 
+import copy
 from typing import Optional
 
 import torch
@@ -27,7 +28,8 @@ from vllm import envs
 from vllm.config import VllmConfig
 from vllm.distributed import (ensure_model_parallel_initialized,
                               init_distributed_environment)
-from vllm.distributed.kv_transfer import ensure_kv_transfer_initialized
+from vllm.distributed.kv_transfer import (ensure_kv_transfer_initialized,
+                                          has_kv_transfer_group)
 from vllm.distributed.parallel_state import get_pp_group, get_tp_group
 from vllm.logger import logger
 from vllm.lora.request import LoRARequest
@@ -35,7 +37,7 @@ from vllm.sequence import IntermediateTensors
 from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE, GiB_bytes
 from vllm.v1.core.sched.output import SchedulerOutput
 from vllm.v1.kv_cache_interface import KVCacheConfig, KVCacheSpec
-from vllm.v1.outputs import ModelRunnerOutput
+from vllm.v1.outputs import EMPTY_MODEL_RUNNER_OUTPUT, ModelRunnerOutput
 from vllm.v1.worker.worker_base import WorkerBase
 
 import vllm_ascend.envs as envs_ascend
@@ -49,6 +51,7 @@ from vllm_ascend.utils import (check_kv_cache_bytes_cache_exist,
                                read_kv_cache_bytes_from_file,
                                sleep_mode_enabled, try_register_lib)
 from vllm_ascend.worker.model_runner_v1 import NPUModelRunner
+from ucm.sparse.state import ensure_ucm_sparse_initialized
 
 
 class NPUWorker(WorkerBase):
@@ -222,9 +225,22 @@ class NPUWorker(WorkerBase):
             assert isinstance(output, IntermediateTensors)
             get_pp_group().send_tensor_dict(output.tensors,
                                             all_gather_group=get_tp_group())
-            return None
+            if not has_kv_transfer_group():
+                return None
+
+            kv_connector_output = output.kv_connector_output
+            finished_sending = kv_connector_output.finished_sending
+            finished_recving = kv_connector_output.finished_recving
+
+            if not finished_sending and not finished_recving:
+                return EMPTY_MODEL_RUNNER_OUTPUT
+
+            new_output = copy.copy(EMPTY_MODEL_RUNNER_OUTPUT)
+            new_output.kv_connector_output = kv_connector_output
+            return new_output
+
         assert isinstance(output, ModelRunnerOutput)
-        return output if self.is_driver_worker else None
+        return output
 
     def load_model(self) -> None:
         if self.vllm_config.model_config.enable_sleep_mode:
@@ -321,6 +337,7 @@ class NPUWorker(WorkerBase):
             parallel_config.world_size_across_dp,
         )
         ensure_kv_transfer_initialized(self.vllm_config)
+        ensure_ucm_sparse_initialized(self.vllm_config)
 
     def _init_profiler(self):
         # Torch profiler. Enabled and configured through env vars:
-- 
2.34.1


From 35eb0e1c84ef9786a0c77561e3298002167c88c3 Mon Sep 17 00:00:00 2001
From: ldeng <ldeng.sjtu@gmail.com>
Date: Mon, 29 Dec 2025 17:59:28 +0800
Subject: [PATCH 2/5] update attention_v1 for kvcomp in NPU

---
 vllm_ascend/attention/attention_v1.py | 59 +++++++++++++++++++--------
 1 file changed, 43 insertions(+), 16 deletions(-)

diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index ea982244..b924d8ea 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -37,7 +37,7 @@ from vllm_ascend.utils import (ACL_FORMAT_FRACTAL_NZ, aligned_16, is_310p,
                                nd_to_nz_2d, nd_to_nz_spec)
 
 from ucm.sparse.state import get_ucm_sparse, has_ucm_sparse
-
+import os
 
 class AscendAttentionBackend(AttentionBackend):
     accept_output_buffer: bool = True
@@ -132,8 +132,9 @@ class AscendMetadata:
     # the computed tokens + new tokens None if it is a decoding.
     query_start_loc: torch.Tensor
     query_lens: torch.Tensor
+    query_lens_device: torch.Tensor # (ldeng) added for KVComp
     seq_lens: torch.Tensor
-
+    seq_lens_device: torch.Tensor # (ldeng) added for KVComp
     # max value of number of tokens across dp group
     max_num_tokens_across_dp: int = 0
 
@@ -182,15 +183,22 @@ class AscendAttentionMetadataBuilder:
             block_table[:num_reqs])
 
         query_lens = self.runner.query_lens
+        query_lens_device = query_lens.pin_memory().to(self.runner.device, non_blocking=True)
         seq_lens = self.runner.seq_lens_cpu[:num_reqs]
+        seq_lens_device = seq_lens.pin_memory().to(self.runner.device, non_blocking=True)
         slot_mapping = self.runner.slot_mapping_cpu[:num_actual_tokens].to(
             self.runner.device, non_blocking=True)
         attn_mask = self.runner.attn_mask
         attn_state = self.runner.attn_state
         query_start_loc_cpu = self.runner.query_start_loc_cpu[:num_reqs + 1]
-        query_start_loc = query_start_loc_cpu.to(self.runner.device,
+        query_start_loc = query_start_loc_cpu.pin_memory().to(self.runner.device,
                                                  non_blocking=True)
 
+        if has_ucm_sparse():
+            ucm_sparse = get_ucm_sparse()
+            if os.getenv("VLLM_HASH_ATTENTION", "0") == "1":
+                ucm_sparse.build_decode_attention_meta_npu(query_lens, seq_lens, block_table)
+
         if is_310p():
             if attn_state == AscendAttentionState.PrefillNoCache:
                 mask_nz = nd_to_nz_2d(attn_mask)
@@ -206,7 +214,9 @@ class AscendAttentionMetadataBuilder:
             block_tables=block_table,
             query_start_loc=query_start_loc,
             query_lens=query_lens,
+            query_lens_device=query_lens_device,
             seq_lens=seq_lens,
+            seq_lens_device=seq_lens_device,
             max_query_len=max_query_len,
             slot_mapping=slot_mapping,
             attn_mask=attn_mask,
@@ -279,8 +289,17 @@ class AscendAttentionBackendImpl(AttentionImpl):
             shape = [batch_size * seq_len, num_heads, head_size]
         """
         num_tokens = query.shape[0]
-        use_kv_cache_int8 = kv_cache.numel(
-        ) > 0 and kv_cache[0].dtype == torch.int8
+
+        # In NPU, forward could be called directly, not by unified_ascend_attention_with_output
+        actual_cache = kv_cache[0] if isinstance(kv_cache, tuple) else kv_cache
+        if actual_cache is not None:
+            use_kv_cache_int8 = actual_cache.numel() > 0 and actual_cache.dtype == torch.int8
+        else:
+            use_kv_cache_int8 = False
+        kv_cache = actual_cache 
+
+        #use_kv_cache_int8 = kv_cache.numel(
+        #) > 0 and kv_cache[0].dtype == torch.int8
         if output is None:
             output = torch.empty(num_tokens,
                                  self.num_heads,
@@ -449,14 +468,20 @@ def unified_ascend_attention_with_output(
     output: torch.Tensor,
     layer_name: str,
 ) -> None:
-    wait_for_kv_layer_from_connector(layer_name)
+    # wait_for_kv_layer_from_connector(layer_name)
 
     forward_context: ForwardContext = get_forward_context()
     attn_metadata = forward_context.attn_metadata
     self = forward_context.no_compile_layers[layer_name]
     kv_cache = self.kv_cache[forward_context.virtual_engine]
-    if not self.use_mla:
-        query, _, _, _ = maybe_execute_sparse_attention_begin(query, key, value, layer_name, forward_context)
+
+    # In NPU, during dummy_run, kv_cache could be a empty tensor, so we need to check the length of kv_cache
+    if os.getenv("VLLM_HASH_ATTENTION", "0") == "1" and len(kv_cache) > 0:
+        kv_cache, k_hash = kv_cache
+    else:
+        k_hash = None
+    if attn_metadata is not None:
+        maybe_execute_sparse_attention_begin(query, key, value, layer_name, forward_context, output, k_hash=k_hash)
     self.impl.forward(self,
                       query,
                       key,
@@ -465,9 +490,10 @@ def unified_ascend_attention_with_output(
                       attn_metadata,
                       output,
                       trace_flag=False)
-    if not self.use_mla:
+
+    if attn_metadata is not None:
         maybe_execute_sparse_attention_finished(query, key, value, output, layer_name, forward_context)
-    maybe_save_kv_layer_to_connector(layer_name, kv_cache)
+    # maybe_save_kv_layer_to_connector(layer_name, kv_cache)
     return
 
 def wait_for_kv_layer_from_connector(layer_name: str):
@@ -506,19 +532,20 @@ def maybe_execute_sparse_attention_begin(
         forward_context: ForwardContext,
         output: Optional[torch.Tensor] = None,
         phase: Optional[str] = None,
+        k_hash: Optional[torch.Tensor] = None,
+        decode_ql_nope: Optional[torch.Tensor] = None,
+        decode_q_pe: Optional[torch.Tensor] = None,
 ):
     if not has_ucm_sparse():
-            return query, key, value, output
+        return
 
     ucm_sparse = get_ucm_sparse()
 
     attn_metadata = forward_context.attn_metadata
     if attn_metadata is None:
-        return query, key, value, output
+        return
 
-    return ucm_sparse.attention_begin(
-        query, key, value, layer_name, forward_context, output, phase
-    )
+    ucm_sparse.attention_begin(query, key, value, layer_name, forward_context, output, phase, k_hash, decode_ql_nope, decode_q_pe)
 
 def maybe_execute_sparse_attention_finished(
         query: torch.Tensor,
@@ -555,4 +582,4 @@ direct_register_custom_op(
     mutates_args=["output"],
     fake_impl=unified_attention_with_output_fake,
     dispatch_key="PrivateUse1",
-)
+)
\ No newline at end of file
-- 
2.34.1


From 5c81688c4b5af2a474ac14c24e7c10abd7f9e4d2 Mon Sep 17 00:00:00 2001
From: ldeng <ldeng.sjtu@gmail.com>
Date: Mon, 29 Dec 2025 18:04:02 +0800
Subject: [PATCH 3/5] call initialize_kv_hash_cache_tensors_npu to allocate
 hashk cache in NPUModelRunner when VLLM_HASH_ATTENTION is enabled for KVComp

---
 vllm_ascend/worker/model_runner_v1.py | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/vllm_ascend/worker/model_runner_v1.py b/vllm_ascend/worker/model_runner_v1.py
index 782b9a3b..766316e1 100644
--- a/vllm_ascend/worker/model_runner_v1.py
+++ b/vllm_ascend/worker/model_runner_v1.py
@@ -1993,6 +1993,11 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                     # KV cache specs.
                     raise ValueError("Unknown KV cache spec type.")
 
+        if has_ucm_sparse():
+            ucm_sparse = get_ucm_sparse()
+            if os.getenv("VLLM_HASH_ATTENTION", "0") == "1":
+                ucm_sparse.initialize_kv_hash_cache_tensors_npu(kv_caches, self.device)
+
         bind_kv_cache(
             kv_caches,
             self.vllm_config.compilation_config.static_forward_context,
-- 
2.34.1


From 9d61017d7f144e4d8125b6cacb9dce39d5da45fc Mon Sep 17 00:00:00 2001
From: ldeng <ldeng.sjtu@gmail.com>
Date: Mon, 29 Dec 2025 18:04:50 +0800
Subject: [PATCH 4/5] uncomment connector

---
 vllm_ascend/attention/attention_v1.py | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index b924d8ea..ece7d173 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -468,7 +468,7 @@ def unified_ascend_attention_with_output(
     output: torch.Tensor,
     layer_name: str,
 ) -> None:
-    # wait_for_kv_layer_from_connector(layer_name)
+    wait_for_kv_layer_from_connector(layer_name)
 
     forward_context: ForwardContext = get_forward_context()
     attn_metadata = forward_context.attn_metadata
@@ -493,7 +493,7 @@ def unified_ascend_attention_with_output(
 
     if attn_metadata is not None:
         maybe_execute_sparse_attention_finished(query, key, value, output, layer_name, forward_context)
-    # maybe_save_kv_layer_to_connector(layer_name, kv_cache)
+    maybe_save_kv_layer_to_connector(layer_name, kv_cache)
     return
 
 def wait_for_kv_layer_from_connector(layer_name: str):
-- 
2.34.1


From 34cb655d94fc08f7853d6fd0fe95b78df978096b Mon Sep 17 00:00:00 2001
From: Clarence-1103 <indirashi@163.com>
Date: Tue, 6 Jan 2026 19:31:40 -0800
Subject: [PATCH 5/5] [bugfix] register_kv_caches

---
 vllm_ascend/worker/model_runner_v1.py | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/vllm_ascend/worker/model_runner_v1.py b/vllm_ascend/worker/model_runner_v1.py
index 766316e1..04b0f4e2 100644
--- a/vllm_ascend/worker/model_runner_v1.py
+++ b/vllm_ascend/worker/model_runner_v1.py
@@ -2003,6 +2003,9 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             self.vllm_config.compilation_config.static_forward_context,
             self.kv_caches)
 
+        if has_kv_transfer_group():
+            get_kv_transfer_group().register_kv_caches(kv_caches)
+
     def get_kv_cache_spec(self) -> dict[str, KVCacheSpec]:
         """
         Generates the KVCacheSpec by parsing the kv cache format from each
-- 
2.34.1

