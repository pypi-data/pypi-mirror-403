"""Analytics module for prompt usage analysis."""

import json
import csv
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import sqlite3

from .storage import Storage
from .filters import filter_prompts, is_system_message, is_likely_system_message, extract_real_prompt
from .classifier import classify_prompt, classify_prompt_with_scores, get_category_icon, PromptCategory, legacy_to_new_category
from .scoring import calculate_composite_score_v2


class PromptAnalytics:
    """Analyze prompt usage patterns."""

    def __init__(self, storage: Storage):
        """Initialize analytics.

        Args:
            storage: Storage instance
        """
        self.storage = storage

    def _resolve_project_name(self, project_name: Optional[str]) -> Optional[str]:
        """Resolve project name if None by selecting most recent project.

        Args:
            project_name: Project name or None

        Returns:
            Resolved project name or None if no projects exist
        """
        if project_name is not None:
            return project_name

        # Get most recent project
        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT p.name
                FROM projects p
                JOIN sessions s ON p.id = s.project_id
                ORDER BY s.modified_at DESC
                LIMIT 1
            """)
            row = cursor.fetchone()
            return row[0] if row else None

    def get_category_stats(self, project_name: Optional[str] = None) -> List[Dict]:
        """Get statistics by prompt category.

        Args:
            project_name: Project name to analyze

        Returns:
            List of category statistics
        """
        # Resolve project name if None
        project_name = self._resolve_project_name(project_name)
        if not project_name:
            return []  # No projects exist

        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    CASE 
                        WHEN lower(s.first_prompt) LIKE '%Î¶¨Î∑∞%' OR lower(s.first_prompt) LIKE '%review%' THEN 'ÏΩîÎìú Î¶¨Î∑∞'
                        WHEN lower(s.first_prompt) LIKE '%ÌÖåÏä§Ìä∏%' OR lower(s.first_prompt) LIKE '%test%' THEN 'ÌÖåÏä§Ìä∏'
                        WHEN lower(s.first_prompt) LIKE '%Î≤ÑÍ∑∏%' OR lower(s.first_prompt) LIKE '%bug%' OR lower(s.first_prompt) LIKE '%fix%' THEN 'Î≤ÑÍ∑∏ ÏàòÏ†ï'
                        WHEN lower(s.first_prompt) LIKE '%Íµ¨ÌòÑ%' OR lower(s.first_prompt) LIKE '%implement%' OR lower(s.first_prompt) LIKE '%add%' THEN 'Í∏∞Îä• Íµ¨ÌòÑ'
                        WHEN lower(s.first_prompt) LIKE '%Î¶¨Ìå©ÌÜ†ÎßÅ%' OR lower(s.first_prompt) LIKE '%refactor%' THEN 'Î¶¨Ìå©ÌÜ†ÎßÅ'
                        WHEN lower(s.first_prompt) LIKE '%Î¨∏ÏÑú%' OR lower(s.first_prompt) LIKE '%doc%' THEN 'Î¨∏ÏÑúÌôî'
                        ELSE 'Í∏∞ÌÉÄ'
                    END as category,
                    COUNT(DISTINCT s.session_id) as session_count,
                    COUNT(DISTINCT m.id) as total_messages,
                    COUNT(DISTINCT CASE WHEN m.type = 'user' THEN m.id END) as user_prompts,
                    COUNT(DISTINCT cs.id) as code_count,
                    ROUND(AVG(s.message_count), 1) as avg_messages_per_session,
                    ROUND(CAST(COUNT(DISTINCT cs.id) AS FLOAT) / NULLIF(COUNT(DISTINCT s.session_id), 0), 1) as avg_code_per_session
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                LEFT JOIN messages m ON s.session_id = m.session_id
                LEFT JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ?
                GROUP BY category
                ORDER BY session_count DESC
            """, (project_name,))
            return [dict(row) for row in cursor.fetchall()]

    def get_branch_productivity(self, project_name: Optional[str] = None) -> List[Dict]:
        """Get productivity metrics by branch type.

        Args:
            project_name: Project name to analyze

        Returns:
            List of branch productivity metrics
        """
        # Resolve project name if None
        project_name = self._resolve_project_name(project_name)
        if not project_name:
            return []  # No projects exist

        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    CASE 
                        WHEN s.git_branch LIKE 'feature/%' THEN 'Feature'
                        WHEN s.git_branch LIKE 'hotfix/%' THEN 'Hotfix'
                        WHEN s.git_branch = 'dev' THEN 'Dev'
                        WHEN s.git_branch = 'main' OR s.git_branch = 'master' THEN 'Main'
                        ELSE 'Other'
                    END as branch_type,
                    COUNT(DISTINCT s.session_id) as session_count,
                    COUNT(DISTINCT m.id) as total_messages,
                    COUNT(DISTINCT cs.id) as code_count,
                    ROUND(CAST(COUNT(DISTINCT cs.id) AS FLOAT) / NULLIF(COUNT(DISTINCT m.id), 0), 2) as code_per_message_ratio,
                    ROUND(AVG(s.message_count), 1) as avg_messages_per_session
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                LEFT JOIN messages m ON s.session_id = m.session_id
                LEFT JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ?
                GROUP BY branch_type
                ORDER BY session_count DESC
            """, (project_name,))
            return [dict(row) for row in cursor.fetchall()]

    def get_language_distribution(self, project_name: Optional[str] = None) -> List[Dict]:
        """Get code language distribution.

        Args:
            project_name: Project name to analyze

        Returns:
            List of language statistics
        """
        # Resolve project name if None
        project_name = self._resolve_project_name(project_name)
        if not project_name:
            return []  # No projects exist

        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    cs.language,
                    COUNT(*) as count,
                    ROUND(CAST(COUNT(*) AS FLOAT) * 100.0 / (
                        SELECT COUNT(*) 
                        FROM code_snippets cs2
                        JOIN sessions s2 ON cs2.session_id = s2.session_id
                        JOIN projects p2 ON s2.project_id = p2.id
                        WHERE p2.name = ?
                    ), 2) as percentage,
                    SUM(cs.line_count) as total_lines
                FROM code_snippets cs
                JOIN sessions s ON cs.session_id = s.session_id
                JOIN projects p ON s.project_id = p.id
                WHERE p.name = ?
                GROUP BY cs.language
                ORDER BY count DESC
                LIMIT 15
            """, (project_name, project_name))
            return [dict(row) for row in cursor.fetchall()]

    def get_time_based_analysis(self, project_name: Optional[str] = None, days: int = 30) -> Dict:
        """Get time-based usage analysis.

        Args:
            project_name: Project name to analyze
            days: Number of days to analyze

        Returns:
            Time-based statistics
        """
        # Resolve project name if None
        project_name = self._resolve_project_name(project_name)
        if not project_name:
            return {"daily_activity": [], "hourly_distribution": []}  # No projects exist

        with self.storage._get_connection() as conn:
            # Daily activity (convert UTC to KST/UTC+9)
            cursor = conn.execute("""
                SELECT
                    DATE(datetime(s.created_at, '+9 hours')) as date,
                    COUNT(DISTINCT s.session_id) as sessions,
                    COUNT(DISTINCT m.id) as messages,
                    COUNT(DISTINCT cs.id) as code_snippets
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                LEFT JOIN messages m ON s.session_id = m.session_id
                LEFT JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ?
                    AND datetime(s.created_at, '+9 hours') >= datetime('now', '+9 hours', '-' || ? || ' days')
                GROUP BY DATE(datetime(s.created_at, '+9 hours'))
                ORDER BY date DESC
            """, (project_name, days))
            daily_activity = [dict(row) for row in cursor.fetchall()]

            # Hour distribution (convert UTC to KST/UTC+9)
            cursor = conn.execute("""
                SELECT
                    CAST(strftime('%H', datetime(s.created_at, '+9 hours')) AS INTEGER) as hour,
                    COUNT(DISTINCT s.session_id) as sessions
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                WHERE p.name = ?
                GROUP BY hour
                ORDER BY sessions DESC
            """, (project_name,))
            hour_distribution = [dict(row) for row in cursor.fetchall()]

            # Most productive day (convert UTC to KST/UTC+9)
            cursor = conn.execute("""
                SELECT
                    DATE(datetime(s.created_at, '+9 hours')) as date,
                    COUNT(DISTINCT cs.id) as code_count
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                LEFT JOIN messages m ON s.session_id = m.session_id
                LEFT JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ?
                GROUP BY DATE(datetime(s.created_at, '+9 hours'))
                ORDER BY code_count DESC
                LIMIT 1
            """, (project_name,))
            most_productive = cursor.fetchone()

            return {
                "daily_activity": daily_activity,
                "hour_distribution": hour_distribution,
                "most_productive_day": dict(most_productive) if most_productive else None
            }

    def get_top_sessions(self, project_name: Optional[str] = None, limit: int = 10) -> List[Dict]:
        """Get most active sessions.
        
        Args:
            project_name: Project name to analyze
            limit: Max results
            
        Returns:
            List of top sessions
        """
        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    s.session_id,
                    s.first_prompt,
                    s.git_branch,
                    s.created_at,
                    COUNT(DISTINCT m.id) as message_count,
                    COUNT(DISTINCT cs.id) as code_count,
                    GROUP_CONCAT(DISTINCT cs.language) as languages
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                LEFT JOIN messages m ON s.session_id = m.session_id
                LEFT JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ?
                GROUP BY s.session_id
                ORDER BY message_count DESC
                LIMIT ?
            """, (project_name, limit))
            return [dict(row) for row in cursor.fetchall()]

    def get_sensitive_data_report(self, project_name: Optional[str] = None) -> Dict:
        """Get sensitive data detection report.
        
        Args:
            project_name: Project name to analyze
            
        Returns:
            Sensitive data statistics
        """
        with self.storage._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_snippets,
                    COUNT(CASE WHEN has_sensitive THEN 1 END) as sensitive_count,
                    ROUND(CAST(COUNT(CASE WHEN has_sensitive THEN 1 END) AS FLOAT) * 100.0 / COUNT(*), 2) as sensitive_percentage
                FROM code_snippets cs
                JOIN sessions s ON cs.session_id = s.session_id
                JOIN projects p ON s.project_id = p.id
                WHERE p.name = ?
            """, (project_name,))
            stats = dict(cursor.fetchone())

            # Get sessions with sensitive data
            cursor = conn.execute("""
                SELECT DISTINCT
                    s.session_id,
                    s.first_prompt,
                    s.git_branch,
                    COUNT(DISTINCT cs.id) as sensitive_snippet_count
                FROM sessions s
                JOIN projects p ON s.project_id = p.id
                JOIN messages m ON s.session_id = m.session_id
                JOIN code_snippets cs ON m.id = cs.message_id
                WHERE p.name = ? AND cs.has_sensitive = 1
                GROUP BY s.session_id
                ORDER BY sensitive_snippet_count DESC
            """, (project_name,))
            sensitive_sessions = [dict(row) for row in cursor.fetchall()]

            return {
                "statistics": stats,
                "affected_sessions": sensitive_sessions
            }

    def export_to_json(self, data: Dict, output_path: Path):
        """Export analytics data to JSON.
        
        Args:
            data: Data to export
            output_path: Output file path
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)

    def export_to_csv(self, data: List[Dict], output_path: Path):
        """Export analytics data to CSV.
        
        Args:
            data: Data to export (list of dicts)
            output_path: Output file path
        """
        if not data:
            return

        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)

    def analyze_prompt_quality(
        self,
        project_name: Optional[str] = None,
        include_nocode: bool = False,
        include_commands: bool = False,
        filter_system: bool = True
    ) -> List[Dict]:
        """Analyze prompt quality with scoring.

        Args:
            project_name: Project name to analyze
            include_nocode: Include prompts without code generation
            include_commands: Include command-only prompts
            filter_system: Filter out system/meta messages (default: True)

        Returns:
            List of prompts with quality scores
        """
        # Resolve project name if None
        project_name = self._resolve_project_name(project_name)
        if not project_name:
            return []  # No projects exist

        with self.storage._get_connection() as conn:
            query = """
                WITH user_prompts AS (
                    SELECT
                        m.id as message_id,
                        m.session_id,
                        m.content as prompt_text,
                        m.timestamp as prompt_ts,
                        s.git_branch,
                        s.created_at,
                        LEAD(m.timestamp) OVER (
                            PARTITION BY m.session_id
                            ORDER BY m.timestamp
                        ) as next_user_ts
                    FROM messages m
                    JOIN sessions s ON m.session_id = s.session_id
                    JOIN projects p ON s.project_id = p.id
                    WHERE p.name = ? AND m.type = 'user'
                ),
                turn_messages AS (
                    SELECT
                        up.message_id,
                        up.session_id,
                        up.prompt_text,
                        up.git_branch,
                        up.created_at,
                        m.id as msg_id,
                        m.type,
                        m.timestamp
                    FROM user_prompts up
                    JOIN messages m ON m.session_id = up.session_id
                    WHERE m.timestamp >= up.prompt_ts
                      AND (up.next_user_ts IS NULL OR m.timestamp < up.next_user_ts)
                ),
                turn_stats AS (
                    SELECT
                        tm.message_id as message_id,
                        tm.session_id,
                        tm.prompt_text,
                        tm.git_branch,
                        tm.created_at,
                        COUNT(DISTINCT msg_id) as message_count,
                        COUNT(DISTINCT CASE WHEN type = 'user' THEN msg_id END) as user_prompt_count,
                        COUNT(DISTINCT cs.id) as code_count,
                        SUM(cs.line_count) as total_lines,
                        COUNT(DISTINCT cs.language) as language_diversity,
                        COUNT(DISTINCT CASE WHEN cs.has_sensitive THEN cs.id END) as sensitive_count
                    FROM turn_messages tm
                    LEFT JOIN code_snippets cs ON cs.message_id = tm.msg_id
                    GROUP BY tm.message_id
                    {having_clause}
                ),
                prompt_metrics AS (
                    SELECT
                        ts.message_id,
                        ts.session_id,
                        ts.prompt_text,
                        ts.git_branch,
                        ts.created_at,
                        message_count,
                        user_prompt_count,
                        code_count,
                        total_lines,
                        language_diversity,
                        sensitive_count,
                        CASE
                            WHEN lower(prompt_text) LIKE '%Î¶¨Î∑∞%' OR lower(prompt_text) LIKE '%review%' THEN 'ÏΩîÎìú Î¶¨Î∑∞'
                            WHEN lower(prompt_text) LIKE '%ÌÖåÏä§Ìä∏%' OR lower(prompt_text) LIKE '%test%' THEN 'ÌÖåÏä§Ìä∏'
                            WHEN lower(prompt_text) LIKE '%Î≤ÑÍ∑∏%' OR lower(prompt_text) LIKE '%bug%' OR lower(prompt_text) LIKE '%fix%' THEN 'Î≤ÑÍ∑∏ ÏàòÏ†ï'
                            WHEN lower(prompt_text) LIKE '%Íµ¨ÌòÑ%' OR lower(prompt_text) LIKE '%implement%' OR lower(prompt_text) LIKE '%add%' THEN 'Í∏∞Îä• Íµ¨ÌòÑ'
                            WHEN lower(prompt_text) LIKE '%Î¶¨Ìå©ÌÜ†ÎßÅ%' OR lower(prompt_text) LIKE '%refactor%' THEN 'Î¶¨Ìå©ÌÜ†ÎßÅ'
                            ELSE 'Í∏∞ÌÉÄ'
                        END as category
                    FROM turn_stats ts
                )
                SELECT
                    message_id,
                    session_id,
                    prompt_text as first_prompt,
                    git_branch,
                    created_at,
                    category,
                    message_count,
                    user_prompt_count,
                    code_count,
                    total_lines,
                    language_diversity,
                    sensitive_count,
                    -- Efficiency: ÏΩîÎìú ÏÉùÏÑ±Îüâ / ÏÇ¨Ïö©Ïûê ÌîÑÎ°¨ÌîÑÌä∏ Ïàò
                    ROUND(CAST(code_count AS FLOAT) / NULLIF(user_prompt_count, 0), 2) as efficiency_score,
                    -- Clarity: ÏßßÏùÄ ÎåÄÌôîÏùºÏàòÎ°ù Î™ÖÌôïÌïú ÌîÑÎ°¨ÌîÑÌä∏
                    ROUND(100.0 / NULLIF(message_count, 0), 2) as clarity_score,
                    -- Productivity: Ï¥ù ÏÉùÏÑ± ÎùºÏù∏ Ïàò
                    total_lines as productivity_score,
                    -- Quality: ÎØºÍ∞ê Ï†ïÎ≥¥ ÏóÜÍ≥† Ïñ∏Ïñ¥ Îã§ÏñëÏÑ± ÎÜíÏúºÎ©¥ Ï¢ãÏùå
                    CASE
                        WHEN sensitive_count = 0 AND language_diversity >= 3 THEN 10
                        WHEN sensitive_count = 0 AND language_diversity >= 2 THEN 8
                        WHEN sensitive_count = 0 THEN 6
                        WHEN language_diversity >= 3 THEN 5
                        ELSE 3
                    END as quality_score
                FROM prompt_metrics
            """

            having_clause = "HAVING code_count > 0" if not include_nocode else ""
            query = query.format(having_clause=having_clause)

            cursor = conn.execute(query, (project_name,))

            results = [dict(row) for row in cursor.fetchall()]

            cleaned_results = []
            for r in results:
                prompt_text = r.get("first_prompt", "")

                # Try to extract real prompt from command args
                extracted_prompt = self._extract_command_args(prompt_text)
                if extracted_prompt:
                    r["first_prompt"] = extracted_prompt
                    prompt_text = extracted_prompt

                # Filter system/meta messages
                if filter_system:
                    if is_system_message(prompt_text):
                        continue
                    if is_likely_system_message(prompt_text):
                        continue

                # Filter command-only messages
                if not include_commands and self._is_command_only(r.get("first_prompt")):
                    continue

                # Skip empty prompts
                if not r.get("first_prompt"):
                    continue

                cleaned_results.append(r)

            if not cleaned_results:
                return []

            max_lines = max([x['total_lines'] or 0 for x in cleaned_results])

            # Calculate composite score and classification for each result
            for r in cleaned_results:
                prompt_text = r.get('first_prompt', '')

                # New category classification
                new_category = classify_prompt(prompt_text)
                r['category'] = new_category.value
                r['category_icon'] = get_category_icon(new_category)

                # New scoring model (v2)
                v2_scores = calculate_composite_score_v2(
                    prompt=prompt_text,
                    code_count=r.get('code_count', 0) or 0,
                    total_lines=r.get('total_lines', 0) or 0,
                    message_count=r.get('message_count', 0) or 0,
                    language_diversity=r.get('language_diversity', 0) or 0,
                    max_lines=max_lines,
                )
                r['structure_score'] = v2_scores['structure_score']
                r['context_score'] = v2_scores['context_score']
                r['efficiency_score_v2'] = v2_scores['efficiency_score']
                r['diversity_score'] = v2_scores['diversity_score']
                r['productivity_score_v2'] = v2_scores['productivity_score']
                r['composite_score_v2'] = v2_scores['composite_score']

                # Legacy scoring (for backwards compatibility)
                normalized_productivity = (r['productivity_score'] or 0) / max(max_lines, 1) * 10
                r['composite_score'] = round(
                    (r['efficiency_score'] or 0) * 0.4 +
                    (r['clarity_score'] or 0) * 0.3 +
                    normalized_productivity * 0.2 +
                    r['quality_score'] * 0.1,
                    2
                )

            return sorted(cleaned_results, key=lambda x: x['composite_score_v2'], reverse=True)

    def _extract_command_args(self, prompt_text: Optional[str]) -> Optional[str]:
        if not prompt_text:
            return None

        start = prompt_text.find("<command-args>")
        if start == -1:
            return None

        start += len("<command-args>")
        end = prompt_text.find("</command-args>", start)
        if end == -1:
            return None

        args = prompt_text[start:end].strip()
        if not args:
            return None

        if args.startswith('"') and args.endswith('"') and len(args) >= 2:
            args = args[1:-1].strip()

        return args or None

    def _is_command_only(self, prompt_text: Optional[str]) -> bool:
        if not prompt_text:
            return True

        if "<command-name>" not in prompt_text and "<command-message>" not in prompt_text:
            return False

        extracted = self._extract_command_args(prompt_text)
        return not extracted

    def get_best_prompts(
        self,
        project_name: Optional[str] = None,
        limit: int = 10,
        include_nocode: bool = False,
        include_commands: bool = False,
        filter_system: bool = True,
        min_structure: float = 2.0,
        min_context: float = 0.0,
        min_quality: Optional[float] = None,
        strict_mode: bool = False
    ) -> List[Dict]:
        """Get best performing prompts.

        Args:
            project_name: Project name to analyze
            limit: Number of top prompts
            include_nocode: Include prompts without code generation
            include_commands: Include command-only prompts
            filter_system: Filter out system/meta messages (default: True)
            min_structure: Minimum structure score (default: 2.0)
            min_context: Minimum context score (default: 0.0)
            min_quality: Minimum combined structure+context score (overrides individual mins)
            strict_mode: If True, use stricter thresholds (structure>=3.0, context>=2.0)

        Returns:
            List of best prompts with scores
        """
        all_prompts = self.analyze_prompt_quality(
            project_name,
            include_nocode=include_nocode,
            include_commands=include_commands,
            filter_system=filter_system
        )

        # Apply strict mode thresholds
        if strict_mode:
            min_structure = 3.0
            min_context = 2.0

        # Filter by quality thresholds
        filtered_prompts = []
        for p in all_prompts:
            structure = p.get('structure_score', 0)
            context = p.get('context_score', 0)

            # Check min_quality (combined threshold)
            if min_quality is not None:
                if structure + context < min_quality:
                    continue
            else:
                # Check individual thresholds
                if structure < min_structure:
                    continue
                if context < min_context:
                    continue

            filtered_prompts.append(p)

        return filtered_prompts[:limit]

    def get_worst_prompts(
        self,
        project_name: Optional[str] = None,
        limit: int = 10,
        include_nocode: bool = False,
        include_commands: bool = False,
        filter_system: bool = True
    ) -> List[Dict]:
        """Get worst performing prompts.

        Args:
            project_name: Project name to analyze
            limit: Number of bottom prompts
            include_nocode: Include prompts without code generation
            include_commands: Include command-only prompts
            filter_system: Filter out system/meta messages (default: True)

        Returns:
            List of worst prompts with scores
        """
        all_prompts = self.analyze_prompt_quality(
            project_name,
            include_nocode=include_nocode,
            include_commands=include_commands,
            filter_system=filter_system
        )
        return all_prompts[-limit:][::-1]  # Reverse to show worst first

    def export_prompt_library(self, project_name: Optional[str] = None, output_path: Optional[Path] = None):
        """Export prompt library as markdown.

        Args:
            project_name: Project name to analyze
            output_path: Output file path
        """
        if output_path is None:
            output_path = Path.home() / ".claude-x" / "prompt-library" / f"{project_name}-prompts.md"

        output_path.parent.mkdir(parents=True, exist_ok=True)

        best = self.get_best_prompts(project_name, 15)
        worst = self.get_worst_prompts(project_name, 10)

        # Group by category
        by_category = {}
        for prompt in best:
            cat = prompt['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(prompt)

        lines = [
            f"# ÌîÑÎ°¨ÌîÑÌä∏ ÎùºÏù¥Î∏åÎü¨Î¶¨: {project_name}",
            f"",
            f"ÏÉùÏÑ±Ïùº: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Ï¥ù Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏: {len(self.analyze_prompt_quality(project_name))}Í∞ú",
            f"",
            "---",
            "",
            "## üìä Ï†êÏàò Í≥ÑÏÇ∞ Î∞©Ïãù",
            "",
            "Í∞Å ÌîÑÎ°¨ÌîÑÌä∏Îäî Îã§Ïùå 4Í∞ÄÏßÄ ÏßÄÌëúÎ°ú ÌèâÍ∞ÄÎê©ÎãàÎã§:",
            "",
            "- **Ìö®Ïú®ÏÑ± (40%)**: ÏΩîÎìú ÏÉùÏÑ±Îüâ / ÌîÑÎ°¨ÌîÑÌä∏ Ïàò",
            "- **Î™ÖÌôïÏÑ± (30%)**: ÏßßÏùÄ ÎåÄÌôîÎ°ú Î™©Ìëú Îã¨ÏÑ± (Î©îÏãúÏßÄ ÏàòÏùò Ïó≠Ïàò)",
            "- **ÏÉùÏÇ∞ÏÑ± (20%)**: Ï¥ù ÏÉùÏÑ± ÏΩîÎìú ÎùºÏù∏ Ïàò",
            "- **ÌíàÏßà (10%)**: ÎØºÍ∞ê Ï†ïÎ≥¥ ÏóÜÏùå + Ïñ∏Ïñ¥ Îã§ÏñëÏÑ±",
            "",
            "**Ï¢ÖÌï© Ï†êÏàò = Ìö®Ïú®ÏÑ±√ó0.4 + Î™ÖÌôïÏÑ±√ó0.3 + ÏÉùÏÇ∞ÏÑ±√ó0.2 + ÌíàÏßà√ó0.1**",
            "",
            "---",
            "",
            "## üèÜ Î≤†Ïä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ (Top 15)",
            "",
            "ÏÑ±Í≥µÏ†ÅÏù∏ ÌîÑÎ°¨ÌîÑÌä∏ Ìå®ÌÑ¥ÏùÑ ÌïôÏäµÌïòÏÑ∏Ïöî.",
            ""
        ]

        for i, prompt in enumerate(best, 1):
            lines.extend([
                f"### {i}. {prompt['category']} (Ï†êÏàò: {prompt['composite_score']})",
                f"",
                f"**ÌîÑÎ°¨ÌîÑÌä∏:**",
                f"> {prompt['first_prompt'][:200]}{'...' if len(prompt['first_prompt']) > 200 else ''}",
                f"",
                f"**ÏÑ∏ÏÖò Ï†ïÎ≥¥:**",
                f"- ÏÑ∏ÏÖò ID: `{prompt['session_id'][:16]}...`",
                f"- Î∏åÎûúÏπò: `{prompt['git_branch'] or 'N/A'}`",
                f"- ÎÇ†Ïßú: {prompt['created_at'][:10] if prompt['created_at'] else 'N/A'}",
                f"",
                f"**ÏÑ±Í≥º ÏßÄÌëú:**",
                f"- Ï¥ù Î©îÏãúÏßÄ: {prompt['message_count']}Í∞ú",
                f"- ÏÇ¨Ïö©Ïûê ÌîÑÎ°¨ÌîÑÌä∏: {prompt['user_prompt_count']}Í∞ú",
                f"- ÏÉùÏÑ± ÏΩîÎìú: {prompt['code_count']}Í∞ú ({prompt['total_lines']}Ï§Ñ)",
                f"- ÏÇ¨Ïö© Ïñ∏Ïñ¥: {prompt['language_diversity']}Ï¢ÖÎ•ò",
                f"",
                f"**Ï†êÏàò Î∂ÑÏÑù:**",
                f"- Ìö®Ïú®ÏÑ±: {prompt['efficiency_score']} (ÏΩîÎìú/ÌîÑÎ°¨ÌîÑÌä∏)",
                f"- Î™ÖÌôïÏÑ±: {prompt['clarity_score']}",
                f"- ÏÉùÏÇ∞ÏÑ±: {prompt['total_lines']}Ï§Ñ",
                f"- ÌíàÏßà: {prompt['quality_score']}/10",
                f"",
                "---",
                ""
            ])

        lines.extend([
            "",
            "## üìö Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î≤†Ïä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏",
            ""
        ])

        for category, prompts in sorted(by_category.items()):
            lines.extend([
                f"### {category}",
                ""
            ])
            for p in prompts[:3]:  # Top 3 per category
                lines.extend([
                    f"- **Ï†êÏàò {p['composite_score']}**: {p['first_prompt'][:100]}...",
                    f"  - üíª ÏΩîÎìú {p['code_count']}Í∞ú, üìù {p['total_lines']}Ï§Ñ, üí¨ Î©îÏãúÏßÄ {p['message_count']}Í∞ú",
                    ""
                ])
            lines.append("")

        lines.extend([
            "## ‚ö†Ô∏è Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌïú ÌîÑÎ°¨ÌîÑÌä∏ (Bottom 10)",
            "",
            "Îã§Ïùå Ìå®ÌÑ¥ÏùÄ ÌîºÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.",
            ""
        ])

        for i, prompt in enumerate(worst, 1):
            lines.extend([
                f"### {i}. {prompt['category']} (Ï†êÏàò: {prompt['composite_score']})",
                f"",
                f"**ÌîÑÎ°¨ÌîÑÌä∏:**",
                f"> {prompt['first_prompt'][:200]}{'...' if len(prompt['first_prompt']) > 200 else ''}",
                f"",
                f"**Î¨∏Ï†úÏ†ê:**",
            ])

            issues = []
            if prompt['efficiency_score'] < 1:
                issues.append("- ÎÇÆÏùÄ Ìö®Ïú®ÏÑ±: ÌîÑÎ°¨ÌîÑÌä∏Îãπ ÏÉùÏÑ±Îêú ÏΩîÎìúÍ∞Ä Ï†ÅÏùå")
            if prompt['message_count'] > 100:
                issues.append("- Í∏¥ ÎåÄÌôî: Î™ÖÌôïÌïòÏßÄ ÏïäÏùÄ ÏßÄÏãúÎ°ú ÎßéÏùÄ ÎåÄÌôî ÌïÑÏöî")
            if prompt['sensitive_count'] > 0:
                issues.append(f"- Î≥¥Ïïà Ïù¥Ïäà: ÎØºÍ∞ê Ï†ïÎ≥¥ {prompt['sensitive_count']}Í±¥ Î∞úÍ≤¨")
            if prompt['language_diversity'] < 2:
                issues.append("- Ï†úÌïúÏ†ÅÏù∏ ÏÇ∞Ï∂úÎ¨º: Îã®Ïùº Ïñ∏Ïñ¥Îßå ÏÇ¨Ïö©")

            if not issues:
                issues.append("- Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú ÎÇÆÏùÄ ÏÑ±Í≥º ÏßÄÌëú")

            lines.extend(issues)
            lines.extend([
                f"",
                f"**Í∞úÏÑ† Î∞©Ìñ•:**",
                f"- Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ ÏöîÍµ¨ÏÇ¨Ìï≠ Î™ÖÏãú",
                f"- ÏòàÏÉÅ Í≤∞Í≥ºÎ¨º ÌòïÌÉú Ï†úÏãú",
                f"- Îã®Í≥ÑÎ≥ÑÎ°ú ÏûëÏóÖ Î∂ÑÎ¶¨",
                "",
                "---",
                ""
            ])

        lines.extend([
            "",
            "## üí° ÌîÑÎ°¨ÌîÑÌä∏ ÏûëÏÑ± ÌåÅ",
            "",
            "Î≤†Ïä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Ìïú Í∂åÏû•ÏÇ¨Ìï≠:",
            "",
            "1. **Î™ÖÌôïÌïú Î™©Ìëú ÏÑ§Ï†ï**: Î¨¥ÏóáÏùÑ ÎßåÎì§Í≥† Ïã∂ÏùÄÏßÄ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Î™ÖÏãú",
            "2. **Ïª®ÌÖçÏä§Ìä∏ Ï†úÍ≥µ**: ÌòÑÏû¨ ÏÉÅÌô©Í≥º Î∞∞Í≤Ω ÏÑ§Î™Ö",
            "3. **ÏòàÏãú Ï†úÍ≥µ**: ÏõêÌïòÎäî Í≤∞Í≥ºÎ¨ºÏùò ÏòàÏãúÎÇò Ï∞∏Í≥† ÏûêÎ£å",
            "4. **Ï†úÏïΩÏÇ¨Ìï≠ Î™ÖÏãú**: ÏßÄÏºúÏïº Ìï† Í∑úÏπôÏù¥ÎÇò Ï†úÌïúÏÇ¨Ìï≠",
            "5. **Îã®Í≥ÑÏ†Å Ï†ëÍ∑º**: ÌÅ∞ ÏûëÏóÖÏùÄ ÏûëÏùÄ Îã®ÏúÑÎ°ú Î∂ÑÎ¶¨",
            "",
            "---",
            "",
            f"üìù Ïù¥ Î¨∏ÏÑúÎäî `cx prompts --project {project_name} --export` Î™ÖÎ†πÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.",
            ""
        ])

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return output_path

    def generate_full_report(self, project_name: Optional[str] = None) -> Dict:
        """Generate comprehensive analytics report.

        Args:
            project_name: Project name to analyze

        Returns:
            Complete analytics report
        """
        return {
            "project": project_name,
            "generated_at": datetime.now().isoformat(),
            "category_stats": self.get_category_stats(project_name),
            "branch_productivity": self.get_branch_productivity(project_name),
            "language_distribution": self.get_language_distribution(project_name),
            "time_analysis": self.get_time_based_analysis(project_name),
            "top_sessions": self.get_top_sessions(project_name),
            "sensitive_data": self.get_sensitive_data_report(project_name)
        }
