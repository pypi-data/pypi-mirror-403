{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to EffDim","text":"<p>EffDim is a unified, research-oriented Python library designed to compute \"effective dimensionality\" (ED) across diverse data modalities.</p> <p>It aims to standardize the fragmented landscape of ED metrics found in statistics, physics, information theory, and machine learning into a single, cohesive interface.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modality Agnostic: Works with raw data, covariance matrices, and pre-computed spectra.</li> <li>Unified Interface: Simple <code>compute</code> and <code>analyze</code> functions.</li> <li>Extensive Estimators: PCA, Participation Ratio, Shannon Entropy, and more.</li> <li>Research Ready: Accurate implementations of metrics from literature.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install via pip:</p> <pre><code>pip install effdim\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import numpy as np\nimport effdim\n\n# Generate random high-dimensional data\ndata = np.random.randn(100, 50)\n\n# Compute Effective Dimension using PCA (95% variance)\ned = effdim.compute(data, method='pca', threshold=0.95)\nprint(f\"Effective Dimension (PCA): {ed}\")\n\n# Compute using Participation Ratio\npr = effdim.compute(data, method='participation_ratio')\nprint(f\"Participation Ratio: {pr}\")\n</code></pre> <p>Explore the User Guide for more examples.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-interface","title":"Main Interface","text":""},{"location":"api/#effdim.api.compute","title":"<code>compute(data, method='participation_ratio', **kwargs)</code>","text":"<p>Computes effective dimension using the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, Any]</code> <p>Input data.</p> required <code>method</code> <code>str</code> <p>Method name.</p> <code>'participation_ratio'</code> <code>**kwargs</code> <p>Arguments passed to the estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Estimated effective dimension.</p> Source code in <code>src/effdim/api.py</code> <pre><code>def compute(data: Union[np.ndarray, Any], method: str = 'participation_ratio', **kwargs) -&gt; float:\n    \"\"\"\n    Computes effective dimension using the specified method.\n\n    Args:\n        data: Input data.\n        method: Method name.\n        **kwargs: Arguments passed to the estimator.\n\n    Returns:\n        float: Estimated effective dimension.\n    \"\"\"\n    method = method.lower()\n\n    config = METHOD_CONFIG.get(method)\n    if not config:\n        raise ValueError(f\"Unknown method '{method}'. Available: {list(METHOD_CONFIG.keys())}\")\n\n    input_type = config['input_type']\n\n    # Branching logic for Data\n    if input_type == 'geometric':\n        # Geometric methods need raw data (N, D) or distance matrix\n        # For now, geometry.py assumes (N, D) points.\n        return config['func'](data, **kwargs)\n\n    # Spectral methods need singular values\n    s = adapters.get_singular_values(data)\n\n    if input_type == 'variance':\n        spectrum = s**2\n    else: # 'singular'\n        spectrum = s\n\n    return config['func'](spectrum, **kwargs)\n</code></pre>"},{"location":"api/#effdim.api.analyze","title":"<code>analyze(data, methods=None, **kwargs)</code>","text":"<p>Computes multiple effective dimension metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, Any]</code> <p>Input data.</p> required <code>methods</code> <code>Optional[List[str]]</code> <p>List of methods to compute. Defaults to generic set.</p> <code>None</code> <code>**kwargs</code> <p>Shared kwargs (e.g. threshold=0.95).        Note: Specific kwargs for specific methods not easily supported in this simple API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary of results.</p> Source code in <code>src/effdim/api.py</code> <pre><code>def analyze(data: Union[np.ndarray, Any], methods: Optional[List[str]] = None, **kwargs) -&gt; Dict[str, float]:\n    \"\"\"\n    Computes multiple effective dimension metrics.\n\n    Args:\n        data: Input data.\n        methods: List of methods to compute. Defaults to generic set.\n        **kwargs: Shared kwargs (e.g. threshold=0.95). \n                  Note: Specific kwargs for specific methods not easily supported in this simple API.\n\n    Returns:\n        Dict[str, float]: Dictionary of results.\n    \"\"\"\n    if methods is None:\n        methods = ['participation_ratio', 'shannon', 'effective_rank']\n\n    results = {}\n\n    # Cache singular values to avoid re-computing SVD for each method\n    # But compute() calls adapters.get_singular_values() every time.\n    # Optimization: We should split the logic.\n    # For now, simplistic approach is fine. For large data, we should optimize.\n    # Let's optimize:\n\n    # Optimize: Compute valid inputs once\n    s = None\n    s_sq = None\n\n    # Check if we need spectral computation at all\n    needs_spectral = False\n    for m in methods:\n        m_cleaned = m.lower()\n        if m_cleaned == 'pr': m_cleaned = 'participation_ratio'\n        if m_cleaned == 'entropy': m_cleaned = 'shannon'\n\n        cfg = METHOD_CONFIG.get(m_cleaned)\n        if cfg and cfg['input_type'] in ['variance', 'singular']:\n            needs_spectral = True\n            break\n\n    if needs_spectral:\n        s = adapters.get_singular_values(data)\n        s_sq = s**2\n\n    for method_name in methods:\n        orig_name = method_name\n        method_name = method_name.lower()\n\n        config = METHOD_CONFIG.get(method_name)\n        if not config:\n            # Check aliases in METHOD_CONFIG directly or manual map above?\n            # Creating a standard clean name helper would be better but keeping it simple.\n            # METHOD_CONFIG now has aliases.\n            if method_name not in METHOD_CONFIG:\n                 results[orig_name] = np.nan\n                 continue\n            config = METHOD_CONFIG[method_name] # Retrieve again if needed\n\n        input_type = config['input_type']\n\n        if input_type == 'geometric':\n             val = config['func'](data, **kwargs)\n        elif input_type == 'variance':\n            val = config['func'](s_sq, **kwargs)\n        else: # singular\n            val = config['func'](s, **kwargs)\n\n        results[orig_name] = val\n\n    return results\n</code></pre>"},{"location":"api/#metrics-spectral","title":"Metrics (Spectral)","text":""},{"location":"api/#effdim.metrics.effective_rank","title":"<code>effective_rank(spectrum)</code>","text":"<p>Computes Effective Rank (Roy &amp; Vetterli, 2007). This is effectively the Shannon Effective Dimension of the normalized spectrum. Alias for shannon_effective_dimension.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def effective_rank(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes Effective Rank (Roy &amp; Vetterli, 2007).\n    This is effectively the Shannon Effective Dimension of the normalized spectrum.\n    Alias for shannon_effective_dimension.\n    \"\"\"\n    return shannon_effective_dimension(spectrum)\n</code></pre>"},{"location":"api/#effdim.metrics.geometric_mean_dimension","title":"<code>geometric_mean_dimension(spectrum)</code>","text":"<p>Computes a dimension based on the ratio of arithmetic mean to geometric mean.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def geometric_mean_dimension(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes a dimension based on the ratio of arithmetic mean to geometric mean.\n    \"\"\"\n    # Filter strict positives\n    s = spectrum[spectrum &gt; 0]\n    if len(s) == 0:\n        return 0.0\n\n    arithmetic = np.mean(s)\n    geometric = np.exp(np.mean(np.log(s)))\n\n    # This ratio is 1 if all equal (max dim), and small if sparse.\n    # Not a standard 'dimension' count scalar like 5.4, but a ratio.\n    # However, some define a dimension proxy from it.\n    # For now, I'll return the raw ratio as a placeholder or looks for a specific 'Dimension' formula using it.\n    # Vardi's \"The effective dimension...\"?\n    # I will just return the ratio for now.\n    return arithmetic / geometric if geometric &gt; 0 else 0.0\n</code></pre>"},{"location":"api/#effdim.metrics.participation_ratio","title":"<code>participation_ratio(spectrum)</code>","text":"<p>Computes the Participation Ratio (PR). PR = (Sum lambda)^2 / Sum (lambda^2)</p> <p>Ref: Recanatesi et al.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def participation_ratio(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes the Participation Ratio (PR).\n    PR = (Sum lambda)^2 / Sum (lambda^2)\n\n    Ref: Recanatesi et al.\n    \"\"\"\n    # PR is usually defined on the eigenvalues of the covariance matrix.\n    # If spectrum are these eigenvalues:\n    s_sum = np.sum(spectrum)\n    s_sq_sum = np.sum(spectrum**2)\n    if s_sq_sum == 0:\n        return 0.0\n    return (s_sum**2) / s_sq_sum\n</code></pre>"},{"location":"api/#effdim.metrics.pca_explained_variance","title":"<code>pca_explained_variance(spectrum, threshold=0.95)</code>","text":"<p>Returns the number of components needed to explain <code>threshold</code> fraction of variance. Note: For singular values s, variance is proportional to s^2.</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def pca_explained_variance(spectrum: np.ndarray, threshold: float = 0.95) -&gt; float:\n    \"\"\"\n    Returns the number of components needed to explain `threshold` fraction of variance.\n    Note: For singular values s, variance is proportional to s^2.\n    \"\"\"\n    # If input is singular values (from SVD), eigenvalues are s^2.\n    # If input is eigenvalues (from Covariance), they are variance already.\n    # The adapter returns singular values for data matrix, eigenvalues for covariance.\n    # This ambiguity needs handling.\n    # Assumption for v0.1: The 'spectrum' passed here is assumed to be the \"importance\" metric directly.\n    # HOWEVER, standard PCA explained variance is on Eigenvalues of Covariance (s^2 / (N-1)).\n    # If users pass singular values (s), we should square them to get variance.\n    # But if they pass eigenvalues, we shouldn't. \n    # Let's assume the adapter output 's' or 'vals' is the \"magnitude of the mode\".\n    # For now, I will treat them as \"magnitudes\". If they are singular values, energy is s^2.\n    # If they are eigenvalues of covariance, energy is lambda.\n    # This is a tricky design point. \n    # DECISION: I will assume the input to these functions is strictly the \"Eigenvalues of the Correlation/Covariance Matrix\" or equivalent \"Power\".\n    # So if we had singular values s, we should convert to s^2 before calling this if we want \"Explained Variance\".\n    # BUT, to keep it simple, I will modify `adapters.py` later to always return \"Power/Variance\" spectrum?\n    # No, SVD returns singular values. \n    # Let's add a 'squared' argument or assume the user handles it? \n    # No, ease of use.\n    # I'll implement a helper that treats them as singular values by default (squaring them) if they seem to be s-values? \n    # Or I'll just document: \"Expects eigenvalues (variance)\".\n\n    # Actually, for PR and Entropy, we often operate on eigenvalues of covariance matrix.\n    # So, I should probably enforce that `components are energies`.\n\n    # Let's treat the input `spectrum` as strictly \"Variance/Energy\" distribution.\n    # I will update `adapters.py` to optionally return squared values, or I handle it here.\n    # Let's assume they are VARIANCES (Eigenvalues).\n\n    total_var = np.sum(spectrum)\n    if total_var == 0:\n        return 0.0\n\n    cumsum = np.cumsum(spectrum)\n    # Find index where cumsum &gt;= threshold * total_var\n    idx = np.searchsorted(cumsum, threshold * total_var)\n    return float(idx + 1)\n</code></pre>"},{"location":"api/#effdim.metrics.renyi_effective_dimension","title":"<code>renyi_effective_dimension(spectrum, alpha=2.0)</code>","text":"<p>Computes R\u00e9nyi Effective Dimension (Generalized). For alpha=1 -&gt; Shannon. For alpha=2 -&gt; Connected to Participation Ratio?   R_2 = 1/(1-2) * log(sum p^2) = -log(sum p^2)   Exp(R_2) = 1 / sum p^2.   PR = (sum lambda)^2 / sum lambda^2 = 1 / sum (lambda/sum lambda)^2 = 1 / sum p^2.   So Exp(Renyi_2) is exactly Participation Ratio!</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def renyi_effective_dimension(spectrum: np.ndarray, alpha: float = 2.0) -&gt; float:\n    \"\"\"\n    Computes R\u00e9nyi Effective Dimension (Generalized).\n    For alpha=1 -&gt; Shannon.\n    For alpha=2 -&gt; Connected to Participation Ratio?\n      R_2 = 1/(1-2) * log(sum p^2) = -log(sum p^2)\n      Exp(R_2) = 1 / sum p^2.\n      PR = (sum lambda)^2 / sum lambda^2 = 1 / sum (lambda/sum lambda)^2 = 1 / sum p^2.\n      So Exp(Renyi_2) is exactly Participation Ratio!\n    \"\"\"\n    if alpha == 1:\n        return shannon_effective_dimension(spectrum)\n\n    p = _normalize_spectrum(spectrum)\n    p_alpha = np.sum(p**alpha)\n    if p_alpha == 0:\n        return 0.0\n\n    entropy = (1 / (1 - alpha)) * np.log(p_alpha)\n    return np.exp(entropy)\n</code></pre>"},{"location":"api/#effdim.metrics.shannon_effective_dimension","title":"<code>shannon_effective_dimension(spectrum)</code>","text":"<p>Computes Shannon Effective Dimension: exp(Entropy). H = - sum p_i log p_i where p_i = lambda_i / sum(lambda)</p> Source code in <code>src/effdim/metrics.py</code> <pre><code>def shannon_effective_dimension(spectrum: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes Shannon Effective Dimension: exp(Entropy).\n    H = - sum p_i log p_i\n    where p_i = lambda_i / sum(lambda)\n    \"\"\"\n    p = _normalize_spectrum(spectrum)\n    # Filter zeros for log\n    p = p[p &gt; 0]\n    entropy = -np.sum(p * np.log(p))\n    return np.exp(entropy)\n</code></pre>"},{"location":"api/#geometry-spatial","title":"Geometry (Spatial)","text":""},{"location":"api/#effdim.geometry.knn_intrinsic_dimension","title":"<code>knn_intrinsic_dimension(data, k=5)</code>","text":"<p>Computes Intrinsic Dimension using Levina-Bickel MLE.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>(N, D) array of points.</p> required <code>k</code> <code>int</code> <p>Number of neighbors.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Estimated dimension.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def knn_intrinsic_dimension(data: np.ndarray, k: int = 5) -&gt; float:\n    \"\"\"\n    Computes Intrinsic Dimension using Levina-Bickel MLE.\n\n    Args:\n        data: (N, D) array of points.\n        k: Number of neighbors.\n\n    Returns:\n        float: Estimated dimension.\n    \"\"\"\n    data = np.asarray(data)\n    if data.ndim != 2:\n        raise ValueError(\"Data must be 2D array (N, D).\")\n\n    N = data.shape[0]\n    if N &lt; k + 1:\n        raise ValueError(f\"Not enough samples ({N}) for k={k}.\")\n\n    # Query k neighbors. k+1 because the point itself is included as distance 0.\n    tree = cKDTree(data)\n    dists, _ = tree.query(data, k=k+1)\n\n    # dists has shape (N, k+1). Column 0 is distance to self (0).\n    # We want neighbors 1 to k.\n    # The Levina-Bickel estimator uses distances up to k.\n    # Eq: inv( 1/(k-1) * sum_{j=1}^{k-1} log(Tk / Tj) ) ... wait average over N points first?\n    # MacKay/Levina-Bickel:\n    # For each point i: m_i = [ 1/(k-1) * sum_{j=1}^{k-1} log( T_k(x_i) / T_j(x_i) ) ]^-1\n    # Global estimator is the average of m_i? Or average the inverse?\n    # Levina-Bickel (2005) \"Maximum Likelihood Estimation...\":\n    # \\hat{m}_k = \\left[ \\frac{1}{N(k-1)} \\sum_{i=1}^N \\sum_{j=1}^{k-1} \\ln \\frac{T_k(x_i)}{T_j(x_i)} \\right]^{-1}\n    # Yes, one global inverse.\n\n    # Drop self\n    neighbors_dists = dists[:, 1:] # (N, k) - these are 1st to kth neighbors\n\n    # T_k is the distance to the k-th neighbor (last column)\n    T_k = neighbors_dists[:, -1] # (N,)\n\n    # T_j are distances 1 to k-1. (All columns excluding last)\n    T_j = neighbors_dists[:, :-1] # (N, k-1)\n\n    # Avoid log(0) - unlikely if points distinct, but clear duplicates\n    # If T_k or T_j is 0, we have duplicates.\n    # Add epsilon? Or filter?\n    # Simple fix: non-zero epsilon\n    epsilon = 1e-10\n    T_k = np.maximum(T_k, epsilon)\n    T_j = np.maximum(T_j, epsilon)\n\n    # Log ratios: log(T_k / T_j) = log(T_k) - log(T_j)\n    # Broadcast T_k: (N, 1) - (N, k-1)\n    log_sum = np.sum(np.log(T_k[:, None]) - np.log(T_j)) # Sum over all i, j\n\n    # Denominator: N * (k-1) is outside the sum if we sum over everything\n    # The formula is 1 / ( (1 / (N*(k-1))) * log_sum )\n    # = N*(k-1) / log_sum\n\n    # Actually, verify formula carefully.\n    # sum_{i=1}^N sum_{j=1}^{k-1} ... is the total sum.\n    # The term in bracket is Average of log ratios? No, explicit 1/N(k-1).\n    # So MLE = 1 / ( Mean of Log Ratios ).\n\n    estimator = (N * (k - 1)) / log_sum\n    return float(estimator)\n</code></pre>"},{"location":"api/#effdim.geometry.two_nn_intrinsic_dimension","title":"<code>two_nn_intrinsic_dimension(data)</code>","text":"<p>Computes ID using Two-NN method (Facco et al., 2017). Uses ratio of 2nd to 1st neighbor distances.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>(N, D) array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Estimated dimension.</p> Source code in <code>src/effdim/geometry.py</code> <pre><code>def two_nn_intrinsic_dimension(data: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes ID using Two-NN method (Facco et al., 2017).\n    Uses ratio of 2nd to 1st neighbor distances.\n\n    Args:\n        data: (N, D) array.\n\n    Returns:\n        float: Estimated dimension.\n    \"\"\"\n    data = np.asarray(data)\n    N = data.shape[0]\n    if N &lt; 3:\n        raise ValueError(\"Need at least 3 points for Two-NN.\")\n\n    tree = cKDTree(data)\n    dists, _ = tree.query(data, k=3) # Self, 1st, 2nd\n\n    # r1 is dists[:, 1], r2 is dists[:, 2]\n    r1 = dists[:, 1]\n    r2 = dists[:, 2]\n\n    # Filter valid ratios (r1 &gt; 0, r2 &gt; 0)\n    # If duplicates, r1=0.\n    mask = r1 &gt; 1e-10\n    r1 = r1[mask]\n    r2 = r2[mask]\n\n    # mu = r2 / r1\n    mu = r2 / r1\n\n    # Empirical cdf F(mu).\n    # The paper simplifies to a linear fit or directly the formula:\n    # d = N / sum_i ln(mu_i).\n    # Wait, the paper \"Estimating the intrinsic dimension of datasets by a minimal neighborhood information\"\n    # Section \"The Two-NN estimator\".\n    # \"d_hat = N / \\sum_{i=1}^N \\ln(\\mu_i)\"\n    # This is assuming the distribution is Pareto with alpha=d.\n    # Let's use this simple estimator.\n\n    if len(mu) == 0:\n        return 0.0\n\n    log_mu_sum = np.sum(np.log(mu))\n    if log_mu_sum == 0:\n        return 0.0\n\n    d_hat = len(mu) / log_mu_sum\n    return float(d_hat)\n</code></pre>"},{"location":"api/#adapters","title":"Adapters","text":""},{"location":"api/#effdim.adapters.get_singular_values","title":"<code>get_singular_values(data)</code>","text":"<p>Standardizes input data into Singular Values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[ndarray, spmatrix]</code> <p>Input data. - (N, D) array: Interpreted as raw data. Returns singular values. - (N, N) symmetric matrix: Interpreted as Covariance. Returns sqrt(abs(eigenvalues)).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 1D array of singular values, sorted descending.</p> Source code in <code>src/effdim/adapters.py</code> <pre><code>def get_singular_values(data: Union[np.ndarray, sparse.spmatrix]) -&gt; np.ndarray:\n    \"\"\"\n    Standardizes input data into Singular Values.\n\n    Args:\n        data: Input data.\n            - (N, D) array: Interpreted as raw data. Returns singular values.\n            - (N, N) symmetric matrix: Interpreted as Covariance. Returns sqrt(abs(eigenvalues)).\n\n    Returns:\n        np.ndarray: 1D array of singular values, sorted descending.\n    \"\"\"\n    data = np.asarray(data)\n\n    if data.ndim != 2:\n        raise ValueError(\"Input data must be 2-dimensional.\")\n\n    N, D = data.shape\n\n    # Heuristic for Symmetric Matrix (Covariance/Kernel)\n    if N == D and np.allclose(data, data.T):\n        vals = linalg.eigvalsh(data)\n        # Eigenvalues of Covariance are Variance = s^2.\n        # So s = sqrt(vals).\n        # We take abs just in case of numerical noise, though cov should be pos def.\n        return np.sqrt(np.abs(vals))[::-1]\n\n    # (N, D) Data Matrix -&gt; SVD\n    # For large matrices, this is slow. v0.2 will add randomized SVD.\n    _, s, _ = linalg.svd(data, full_matrices=False)\n    return s\n</code></pre>"},{"location":"theory/","title":"Theory &amp; Estimators","text":"<p>EffDim implements a variety of estimators for \"effective dimensionality\" (ED). These can be broadly categorized into Spectral Estimators, which operate on the eigenvalues (spectrum) of the data's covariance/correlation matrix, and Geometric Estimators, which operate on the distances between data points.</p>"},{"location":"theory/#spectral-estimators","title":"Spectral Estimators","text":"<p>These methods rely on the spectrum \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_D \\ge 0\\) of the covariance matrix (or the squared singular values of the data matrix). We define the normalized spectrum as \\(p_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}\\), which can be treated as a probability distribution.</p>"},{"location":"theory/#pca-explained-variance","title":"PCA Explained Variance","text":"<p>The classic approach used in Principal Component Analysis. It defines the effective dimension as the number of components required to explain a certain fraction (threshold) of the total variance.</p> \\[ ED_{PCA}(x) = \\min \\{ k \\mid \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^D \\lambda_j} \\ge x \\} \\] <p>where \\(x\\) is the threshold (default 0.95).</p>"},{"location":"theory/#participation-ratio-pr","title":"Participation Ratio (PR)","text":"<p>Widely used in physics and neuroscience to quantify the \"spread\" of the spectrum. If the variance is equally distributed across \\(N\\) dimensions, \\(PR=N\\). If it is concentrated in 1 dimension, \\(PR=1\\).</p> \\[ PR = \\frac{(\\sum_i \\lambda_i)^2}{\\sum_i \\lambda_i^2} = \\frac{1}{\\sum_i p_i^2} \\]"},{"location":"theory/#shannon-effective-dimension","title":"Shannon Effective Dimension","text":"<p>Based on the Shannon entropy of the spectral distribution. It corresponds to the exponential of the entropy.</p> \\[ H = - \\sum_i p_i \\ln p_i $$ $$ ED_{Shannon} = \\exp(H) \\]"},{"location":"theory/#renyi-effective-dimension-alpha-entropy","title":"R\u00e9nyi Effective Dimension (Alpha-Entropy)","text":"<p>A generalization of the Shannon dimension using R\u00e9nyi entropy of order \\(\\alpha\\).</p> \\[ H_\\alpha = \\frac{1}{1-\\alpha} \\ln (\\sum_i p_i^\\alpha) $$ $$ ED_{\\alpha} = \\exp(H_\\alpha) \\] <ul> <li>For \\(\\alpha \\to 1\\), this converges to Shannon Effective Dimension.</li> <li>For \\(\\alpha = 2\\), this is equivalent to the Participation Ratio.</li> </ul>"},{"location":"theory/#effective-rank","title":"Effective Rank","text":"<p>Often used in matrix completion and low-rank approximation contexts. For a covariance matrix, it is often defined identically to the Shannon Effective Dimension or related trace-norm metrics. EffDim implements it as an alias for Shannon Effective Dimension currently.</p>"},{"location":"theory/#geometric-mean-dimension","title":"Geometric Mean Dimension","text":"<p>Based on the ratio of the arithmetic mean to the geometric mean of the spectrum.</p>"},{"location":"theory/#geometric-estimators","title":"Geometric Estimators","text":"<p>These methods estimate the intrinsic dimension (ID) of the data manifold based on local neighborhoods, without relying on global projections like PCA.</p>"},{"location":"theory/#knn-intrinsic-dimension-mle","title":"kNN Intrinsic Dimension (MLE)","text":"<p>The Maximum Likelihood Estimator proposed by Levina and Bickel (2005). It estimates dimension by examining the ratio of distances to the \\(k\\)-th nearest neighbor.</p> \\[ \\hat{d}_k(x_i) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\ln \\frac{r_k(x_i)}{r_j(x_i)} \\right]^{-1} \\] <p>where \\(r_j(x_i)\\) is the distance from \\(x_i\\) to its \\(j\\)-th nearest neighbor. The final estimate is the average over all points \\(x_i\\).</p>"},{"location":"theory/#two-nn","title":"Two-NN","text":"<p>A robust estimator proposed by Facco et al. (2017) that relies only on the distances to the first two nearest neighbors. It is less sensitive to density variations and curvature than standard kNN.</p> <p>It assumes that the ratio of distances \\(\\mu_i = \\frac{r_2(x_i)}{r_1(x_i)}\\) follows a Pareto distribution depending on the intrinsic dimension \\(d\\).</p> \\[ d \\approx \\frac{\\ln(1-F(\\mu))}{-\\ln(\\mu)} \\]"},{"location":"tutorials/comparing_estimators/","title":"Comparing Estimators","text":"<p>Different fields use different definitions of \"effective dimension\". This tutorial highlights the differences.</p>"},{"location":"tutorials/comparing_estimators/#pca-vs-participation-ratio","title":"PCA vs Participation Ratio","text":"<ul> <li>PCA relies on a hard threshold (e.g., 95% variance). It answers \"how many axes do I need to keep?\".</li> <li>Participation Ratio (PR) is a \"soft\" count. It answers \"how spread out is the variance?\".</li> </ul> <p>Consider a spectrum where eigenvalues decay slowly: \\(\\lambda_i = 1/i\\).</p> <pre><code>import numpy as np\nimport effdim\nimport matplotlib.pyplot as plt\n\n# Simulate a slow decay spectrum directly\n# (We pass a diagonal matrix to simulate uncorrelated data with specific variances)\nD = 50\nlambdas = 1.0 / np.arange(1, D+1)\n# Create a covariance matrix\ncov = np.diag(lambdas)\n\n# effdim accepts covariance matrices directly if we assume these are eigenvalues\n# But currently `compute` expects data (N,D) and calls `adapters.get_singular_values`.\n# If we pass (D, D), it might treat it as N=D samples.\n# Ideally, we construct data that has this spectrum.\n# X = U * S * V.T.\n# Let's create data X (N=1000, D=50) with singular values s_i = sqrt(lambda_i * (N-1))\n\nN = 1000\ns = np.sqrt(lambdas * (N - 1))\n# Random orthogonal matrix U (N x D)\nU, _ = np.linalg.qr(np.random.randn(N, D))\n# Identity V (since we don't care about rotation)\nX = U @ np.diag(s)\n\npca_95 = effdim.compute(X, method='pca', threshold=0.95)\npr = effdim.compute(X, method='pr')\n\nprint(f\"PCA (95%): {pca_95}\")\nprint(f\"Participation Ratio: {pr:.2f}\")\n</code></pre> <p>In heavy-tailed distributions, PCA might suggest a very high dimension (to capture the tail), whereas PR might suggest a lower dimension because the mass is concentrated at the start.</p>"},{"location":"tutorials/comparing_estimators/#shannon-vs-renyi","title":"Shannon vs R\u00e9nyi","text":"<p>Shannon Entropy weights probabilities logarithmically. R\u00e9nyi entropy (with \\(\\alpha=2\\), which relates to PR) weights higher probabilities more heavily.</p> <ul> <li>Shannon is sensitive to the entire distribution.</li> <li>PR (R\u00e9nyi-2) is more dominated by the largest eigenvalues.</li> </ul> <p>If you have a dataset with many small noise directions, Shannon dimension might be higher than PR.</p>"},{"location":"tutorials/geometric_analysis/","title":"Geometric Analysis","text":"<p>Geometric estimators calculate the \"Intrinsic Dimension\" (ID) based on distances between points, rather than variance of global projections. This is crucial for manifolds that are non-linear (e.g., a Swiss Roll).</p>"},{"location":"tutorials/geometric_analysis/#the-swiss-roll-problem","title":"The Swiss Roll Problem","text":"<p>A \"Swiss Roll\" is a 2D plane rolled up in 3D. *   PCA will see it as 3D (because variance exists in x, y, z). *   Geometric ID should see it as 2D (locally, it's a plane).</p> <pre><code>import numpy as np\nimport effdim\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate Swiss Roll\nX, _ = make_swiss_roll(n_samples=2000, noise=0.0)\nX = X[:, [0, 2]] # Project to 2D for test? No, make_swiss_roll returns 3D.\n# Actually make_swiss_roll returns (N, 3).\nX, _ = make_swiss_roll(n_samples=2000, noise=0.01)\n\n# PCA\npca_dim = effdim.compute(X, method='pca', threshold=0.95)\nprint(f\"Global PCA Dimension: {pca_dim}\")\n# Likely 3, because the roll occupies 3D volume globally.\n\n# kNN Intrinsic Dimension\nknn_dim = effdim.compute(X, method='knn', k=5)\nprint(f\"kNN Intrinsic Dimension: {knn_dim:.2f}\")\n# Should be close to 2.0\n\n# Two-NN\ntwonn_dim = effdim.compute(X, method='twonn')\nprint(f\"Two-NN Intrinsic Dimension: {twonn_dim:.2f}\")\n# Should be close to 2.0\n</code></pre>"},{"location":"tutorials/geometric_analysis/#when-to-use-geometric-estimators","title":"When to use Geometric Estimators?","text":"<ol> <li>Non-linear manifolds: Image datasets (digits, faces) often lie on low-dimensional non-linear manifolds.</li> <li>Manifold Learning: Checking if your autoencoder latent space has matched the intrinsic dimension of the data.</li> <li>Local Analysis: kNN can be computed per-point (though <code>effdim</code> currently returns the average).</li> </ol>"},{"location":"tutorials/geometric_analysis/#limitations","title":"Limitations","text":"<ul> <li>Computational Cost: Requires computing nearest neighbors, which can be slow for large \\(N\\). <code>effdim</code> uses <code>scipy.spatial.cKDTree</code> for efficiency.</li> <li>Curse of Dimensionality: In extremely high dimensions, distance concentration can make geometric estimation unstable.</li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This guide will walk you through the basic usage of <code>effdim</code>.</p>"},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<p>Ensure <code>effdim</code> is installed:</p> <pre><code>pip install effdim\n</code></pre>"},{"location":"tutorials/getting_started/#basic-concepts","title":"Basic Concepts","text":"<p>EffDim revolves around two main functions:</p> <ul> <li><code>effdim.compute(data, method=...)</code>: Calculates a single dimension metric.</li> <li><code>effdim.analyze(data, methods=[...])</code>: Calculates multiple metrics at once.</li> </ul> <p>Data is typically passed as a N x D numpy array, where \\(N\\) is the number of samples and \\(D\\) is the number of features.</p>"},{"location":"tutorials/getting_started/#example-random-noise-vs-structured-data","title":"Example: Random Noise vs Structured Data","text":"<p>Let's see how effective dimension differs between random noise and structured data.</p>"},{"location":"tutorials/getting_started/#1-random-noise","title":"1. Random Noise","text":"<p>High-dimensional random noise should have a high effective dimension because the variance is spread out in all directions.</p> <pre><code>import numpy as np\nimport effdim\n\n# 1000 samples, 100 dimensions\nnoise = np.random.randn(1000, 100)\n\n# Participation Ratio\npr = effdim.compute(noise, method='participation_ratio')\nprint(f\"PR of Noise: {pr:.2f}\") \n# Expected: close to 100 (or slightly less due to finite sampling)\n</code></pre>"},{"location":"tutorials/getting_started/#2-structured-data-low-rank","title":"2. Structured Data (Low Rank)","text":"<p>If we create data that lies on a low-dimensional plane embedded in high-dimensional space, the effective dimension should be low.</p> <pre><code># Create 1000 samples with only 5 meaningful dimensions\nlatent = np.random.randn(1000, 5)\nprojection = np.random.randn(5, 100)\nstructured_data = latent @ projection\n\n# Add a tiny bit of noise\nstructured_data += 0.01 * np.random.randn(1000, 100)\n\npr = effdim.compute(structured_data, method='participation_ratio')\nprint(f\"PR of Structured Data: {pr:.2f}\")\n# Expected: close to 5\n</code></pre>"},{"location":"tutorials/getting_started/#available-methods","title":"Available Methods","text":"<p>You can check the available methods in the Theory section. Common ones include:</p> <ul> <li><code>'pca'</code>: PCA Explained Variance</li> <li><code>'participation_ratio'</code> (or <code>'pr'</code>)</li> <li><code>'shannon'</code> (or <code>'entropy'</code>)</li> <li><code>'effective_rank'</code> (or <code>'erank'</code>)</li> <li><code>'knn'</code>: k-Nearest Neighbors</li> <li><code>'twonn'</code>: Two-Nearest Neighbors</li> </ul>"},{"location":"tutorials/getting_started/#analyzing-multiple-metrics","title":"analyzing Multiple Metrics","text":"<p>Use <code>effdim.analyze</code> to get a report.</p> <pre><code>report = effdim.analyze(structured_data, methods=['pr', 'pca', 'shannon'])\nprint(report)\n# {'participation_ratio': ..., 'pca': ..., 'shannon': ...}\n</code></pre>"}]}