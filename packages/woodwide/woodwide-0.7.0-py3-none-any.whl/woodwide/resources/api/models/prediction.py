# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import Optional

import httpx

from ...._types import Body, Omit, Query, Headers, NotGiven, SequenceNotStr, omit, not_given
from ...._utils import maybe_transform, async_maybe_transform
from ...._compat import cached_property
from ...._resource import SyncAPIResource, AsyncAPIResource
from ...._response import (
    to_raw_response_wrapper,
    to_streamed_response_wrapper,
    async_to_raw_response_wrapper,
    async_to_streamed_response_wrapper,
)
from ...._base_client import make_request_options
from ....types.api.models import prediction_infer_params, prediction_train_params
from ....types.api.model_public import ModelPublic
from ....types.api.models.prediction_infer_response import PredictionInferResponse

__all__ = ["PredictionResource", "AsyncPredictionResource"]


class PredictionResource(SyncAPIResource):
    @cached_property
    def with_raw_response(self) -> PredictionResourceWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return
        the raw response object instead of the parsed content.

        For more information, see https://www.github.com/Wood-Wide-AI/wwai-python-sdk#accessing-raw-response-data-eg-headers
        """
        return PredictionResourceWithRawResponse(self)

    @cached_property
    def with_streaming_response(self) -> PredictionResourceWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.

        For more information, see https://www.github.com/Wood-Wide-AI/wwai-python-sdk#with_streaming_response
        """
        return PredictionResourceWithStreamingResponse(self)

    def infer(
        self,
        model_id: str,
        *,
        dataset_id: Optional[str] | Omit = omit,
        dataset_name: Optional[str] | Omit = omit,
        model_name: Optional[str] | Omit = omit,
        coerce_schema: bool | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> PredictionInferResponse:
        """
        Generates predictions for the provided dataset using a trained prediction model.
        Returns a JSON object mapping row IDs to predicted values (and probabilities if
        applicable).

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not model_id:
            raise ValueError(f"Expected a non-empty value for `model_id` but received {model_id!r}")
        return self._post(
            f"/api/models/prediction/{model_id}/infer",
            body=maybe_transform({"coerce_schema": coerce_schema}, prediction_infer_params.PredictionInferParams),
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=maybe_transform(
                    {
                        "dataset_id": dataset_id,
                        "dataset_name": dataset_name,
                        "model_name": model_name,
                    },
                    prediction_infer_params.PredictionInferParams,
                ),
            ),
            cast_to=PredictionInferResponse,
        )

    def train(
        self,
        *,
        label_column: Optional[str],
        model_name: str,
        dataset_id: Optional[str] | Omit = omit,
        dataset_name: Optional[str] | Omit = omit,
        hyperparameters: Optional[str] | Omit = omit,
        input_columns: Optional[SequenceNotStr[str]] | Omit = omit,
        overwrite: bool | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> ModelPublic:
        """Initiates training for a prediction model (regression or classification).

        This
        is an asynchronous operation that returns a model ID immediately. The model
        status will start as PENDING. You can poll the model status using the GET
        /models/{model_id} endpoint. Once the status is COMPLETE, you can use the model
        for inference via /models/prediction/{model_id}/infer.

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return self._post(
            "/api/models/prediction/train",
            body=maybe_transform(
                {
                    "label_column": label_column,
                    "model_name": model_name,
                    "hyperparameters": hyperparameters,
                    "input_columns": input_columns,
                    "overwrite": overwrite,
                },
                prediction_train_params.PredictionTrainParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=maybe_transform(
                    {
                        "dataset_id": dataset_id,
                        "dataset_name": dataset_name,
                    },
                    prediction_train_params.PredictionTrainParams,
                ),
            ),
            cast_to=ModelPublic,
        )


class AsyncPredictionResource(AsyncAPIResource):
    @cached_property
    def with_raw_response(self) -> AsyncPredictionResourceWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return
        the raw response object instead of the parsed content.

        For more information, see https://www.github.com/Wood-Wide-AI/wwai-python-sdk#accessing-raw-response-data-eg-headers
        """
        return AsyncPredictionResourceWithRawResponse(self)

    @cached_property
    def with_streaming_response(self) -> AsyncPredictionResourceWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.

        For more information, see https://www.github.com/Wood-Wide-AI/wwai-python-sdk#with_streaming_response
        """
        return AsyncPredictionResourceWithStreamingResponse(self)

    async def infer(
        self,
        model_id: str,
        *,
        dataset_id: Optional[str] | Omit = omit,
        dataset_name: Optional[str] | Omit = omit,
        model_name: Optional[str] | Omit = omit,
        coerce_schema: bool | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> PredictionInferResponse:
        """
        Generates predictions for the provided dataset using a trained prediction model.
        Returns a JSON object mapping row IDs to predicted values (and probabilities if
        applicable).

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not model_id:
            raise ValueError(f"Expected a non-empty value for `model_id` but received {model_id!r}")
        return await self._post(
            f"/api/models/prediction/{model_id}/infer",
            body=await async_maybe_transform(
                {"coerce_schema": coerce_schema}, prediction_infer_params.PredictionInferParams
            ),
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=await async_maybe_transform(
                    {
                        "dataset_id": dataset_id,
                        "dataset_name": dataset_name,
                        "model_name": model_name,
                    },
                    prediction_infer_params.PredictionInferParams,
                ),
            ),
            cast_to=PredictionInferResponse,
        )

    async def train(
        self,
        *,
        label_column: Optional[str],
        model_name: str,
        dataset_id: Optional[str] | Omit = omit,
        dataset_name: Optional[str] | Omit = omit,
        hyperparameters: Optional[str] | Omit = omit,
        input_columns: Optional[SequenceNotStr[str]] | Omit = omit,
        overwrite: bool | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> ModelPublic:
        """Initiates training for a prediction model (regression or classification).

        This
        is an asynchronous operation that returns a model ID immediately. The model
        status will start as PENDING. You can poll the model status using the GET
        /models/{model_id} endpoint. Once the status is COMPLETE, you can use the model
        for inference via /models/prediction/{model_id}/infer.

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return await self._post(
            "/api/models/prediction/train",
            body=await async_maybe_transform(
                {
                    "label_column": label_column,
                    "model_name": model_name,
                    "hyperparameters": hyperparameters,
                    "input_columns": input_columns,
                    "overwrite": overwrite,
                },
                prediction_train_params.PredictionTrainParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=await async_maybe_transform(
                    {
                        "dataset_id": dataset_id,
                        "dataset_name": dataset_name,
                    },
                    prediction_train_params.PredictionTrainParams,
                ),
            ),
            cast_to=ModelPublic,
        )


class PredictionResourceWithRawResponse:
    def __init__(self, prediction: PredictionResource) -> None:
        self._prediction = prediction

        self.infer = to_raw_response_wrapper(
            prediction.infer,
        )
        self.train = to_raw_response_wrapper(
            prediction.train,
        )


class AsyncPredictionResourceWithRawResponse:
    def __init__(self, prediction: AsyncPredictionResource) -> None:
        self._prediction = prediction

        self.infer = async_to_raw_response_wrapper(
            prediction.infer,
        )
        self.train = async_to_raw_response_wrapper(
            prediction.train,
        )


class PredictionResourceWithStreamingResponse:
    def __init__(self, prediction: PredictionResource) -> None:
        self._prediction = prediction

        self.infer = to_streamed_response_wrapper(
            prediction.infer,
        )
        self.train = to_streamed_response_wrapper(
            prediction.train,
        )


class AsyncPredictionResourceWithStreamingResponse:
    def __init__(self, prediction: AsyncPredictionResource) -> None:
        self._prediction = prediction

        self.infer = async_to_streamed_response_wrapper(
            prediction.infer,
        )
        self.train = async_to_streamed_response_wrapper(
            prediction.train,
        )
