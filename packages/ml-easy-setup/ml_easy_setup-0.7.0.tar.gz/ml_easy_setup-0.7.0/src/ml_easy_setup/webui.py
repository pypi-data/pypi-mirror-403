"""
ML Easy Setup - Web UI (Streamlit)
å›¾å½¢åŒ–é…ç½®ç•Œé¢
"""

import json
from pathlib import Path
from typing import Dict, Any

import streamlit as st

from ml_easy_setup.core.detector import HardwareDetector
from ml_easy_setup.core.template import TemplateManager
from ml_easy_setup.core.env_manager import EnvironmentManager
from ml_easy_setup.core.container import ContainerManager, ContainerConfig
from ml_easy_setup.core.distributed import DistributedConfigManager, DistributedConfig


# ============================================
# é¡µé¢é…ç½®
# ============================================
st.set_page_config(
    page_title="ML Easy Setup",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .feature-card {
        padding: 1rem;
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        margin: 0.5rem 0;
        background: #f8f9fa;
    }
    .template-selected {
        border: 2px solid #667eea;
        background: #e8eaf6;
    }
</style>
""", unsafe_allow_html=True)


# ============================================
# åˆå§‹åŒ– Session State
# ============================================
def init_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'step' not in st.session_state:
        st.session_state.step = 1
    if 'config' not in st.session_state:
        st.session_state.config = {}
    if 'created' not in st.session_state:
        st.session_state.created = False


init_session_state()


# ============================================
# ç¡¬ä»¶æ£€æµ‹
# ============================================
@st.cache_data
def detect_hardware() -> Dict[str, Any]:
    """ç¼“å­˜ç¡¬ä»¶æ£€æµ‹ç»“æœ"""
    detector = HardwareDetector()
    return {
        "system": detector.detect_all(verbose=False),
        "gpu_count": detector.detect_gpu_count(),
        "llm_report": detector.get_llm_hardware_report(),
    }


@st.cache_data
def get_templates() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰æ¨¡æ¿"""
    manager = TemplateManager()
    return {
        "templates": manager.list_templates(),
        "details": {t["name"]: manager.load_template(t["name"], "none")
                   for t in manager.list_templates()}
    }


# ============================================
# UI ç»„ä»¶
# ============================================
def render_header():
    """æ¸²æŸ“é¡µé¢æ ‡é¢˜"""
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ ML Easy Setup</h1>
        <p>ä¸€é”®é…ç½®æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç¯å¢ƒ - æ— éœ€å‘½ä»¤è¡Œ</p>
    </div>
    """, unsafe_allow_html=True)


def render_step_indicator():
    """æ¸²æŸ“æ­¥éª¤æŒ‡ç¤ºå™¨"""
    steps = ["ğŸ“‹ åŸºæœ¬ä¿¡æ¯", "ğŸ¯ æ¨¡æ¿é€‰æ‹©", "âš™ï¸ è®¡ç®—é…ç½®", "ğŸ”§ é«˜çº§é€‰é¡¹", "âœ… åˆ›å»ºé¡¹ç›®"]
    cols = st.columns(len(steps))

    for i, (col, step) in enumerate(zip(cols, steps), 1):
        if i == st.session_state.step:
            col.markdown(f"<b style='color:#667eea'>{step}</b>", unsafe_allow_html=True)
        elif i < st.session_state.step:
            col.markdown(f"<span style='color:#4caf50'>âœ“ {step}</span>", unsafe_allow_html=True)
        else:
            col.markdown(f"<span style='color:#9e9e9e'>{step}</span>", unsafe_allow_html=True)


def render_step1_basic_info():
    """æ­¥éª¤1: åŸºæœ¬ä¿¡æ¯"""
    st.subheader("ğŸ“‹ é¡¹ç›®åŸºæœ¬ä¿¡æ¯")

    col1, col2 = st.columns(2)

    with col1:
        project_name = st.text_input(
            "é¡¹ç›®åç§° *",
            placeholder="my-ml-project",
            help="é¡¹ç›®æ–‡ä»¶å¤¹åç§°ï¼ˆåªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ï¼‰"
        )

    with col2:
        project_path = st.text_input(
            "é¡¹ç›®è·¯å¾„",
            value=str(Path.home() / "ml-projects"),
            help="é¡¹ç›®å°†åˆ›å»ºåœ¨æ­¤ç›®å½•ä¸‹"
        )

    path_obj = Path(project_path) / project_name if project_name else Path(project_path)

    st.info(f"ğŸ“ é¡¹ç›®å°†åˆ›å»ºåœ¨: `{path_obj}`")

    return {
        "name": project_name,
        "path": str(path_obj),
    }


def render_step2_template_selection(templates_data: Dict):
    """æ­¥éª¤2: æ¨¡æ¿é€‰æ‹©"""
    st.subheader("ğŸ¯ é€‰æ‹©é¡¹ç›®æ¨¡æ¿")

    # æ¨¡æ¿åˆ†ç±»
    categories = {
        "ğŸ¤– LLM & GenAI": ["llm", "rag", "inference"],
        "ğŸ§  æ·±åº¦å­¦ä¹ ": ["pytorch", "tensorflow", "nlp", "cv", "rl"],
        "ğŸ”§ é«˜çº§åŠŸèƒ½": ["model-builder", "mlops", "algorithm-validator"],
        "ğŸ“Š æ•°æ®ç§‘å­¦": ["data-science", "timeseries", "graph", "gradient-boosting"],
        "ğŸ“¦ å…¶ä»–": ["minimal", "full"],
    }

    selected_template = None

    for category, template_names in categories.items():
        with st.expander(category, expanded=(category == "ğŸ¤– LLM & GenAI")):
            cols = st.columns(min(3, len(template_names)))

            for i, tmpl_name in enumerate(template_names):
                col = cols[i % len(cols)]
                tmpl_info = next(t for t in templates_data["templates"] if t["name"] == tmpl_name)

                with col:
                    if st.button(
                        f"**{tmpl_info['name']}**\n\n{tmpl_info['description'][:50]}...",
                        key=f"tmpl_{tmpl_name}",
                        use_container_width=True,
                    ):
                        selected_template = tmpl_name
                        st.session_state.config["template"] = tmpl_name

    # æ˜¾ç¤ºå·²é€‰æ‹©çš„æ¨¡æ¿
    if "template" in st.session_state.config:
        tmpl = st.session_state.config["template"]
        tmpl_detail = templates_data["details"][tmpl]

        st.success(f"âœ… å·²é€‰æ‹©: **{tmpl}**")

        with st.expander("ğŸ“¦ æŸ¥çœ‹ä¾èµ–è¯¦æƒ…", expanded=False):
            st.markdown(f"**æ ¸å¿ƒåŒ…**: {', '.join(tmpl_detail['core_packages'])}")
            st.markdown(f"**ä¾èµ–æ•°é‡**: {len(tmpl_detail['dependencies'])} ä¸ª")
            if st.button("æ˜¾ç¤ºå®Œæ•´ä¾èµ–åˆ—è¡¨", key="show_deps"):
                st.code("\n".join(tmpl_detail['dependencies']), language="text")

    return st.session_state.config.get("template")


def render_step3_compute_config(hardware_info: Dict):
    """æ­¥éª¤3: è®¡ç®—é…ç½®"""
    st.subheader("âš™ï¸ è®¡ç®—ç¯å¢ƒé…ç½®")

    # æ˜¾ç¤ºç¡¬ä»¶æ£€æµ‹ç»“æœ
    with st.expander("ğŸ” ç¡¬ä»¶æ£€æµ‹ç»“æœ", expanded=True):
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("æ“ä½œç³»ç»Ÿ", hardware_info["system"].get("æ“ä½œç³»ç»Ÿ", "Unknown"))

        with col2:
            gpu_count = hardware_info["gpu_count"]["total"]
            st.metric("GPU æ•°é‡", f"{gpu_count} å¡")

        with col3:
            cuda_version = hardware_info["system"].get("CUDA", "æœªå®‰è£…")
            st.metric("CUDA ç‰ˆæœ¬", cuda_version)

        if gpu_count > 0:
            gpu_type = "NVIDIA" if hardware_info["gpu_count"]["nvidia"] > 0 else "AMD"
            st.info(f"ğŸ® æ£€æµ‹åˆ° {gpu_type} GPU")

    # GPU ç±»å‹é€‰æ‹©
    gpu_type = st.radio(
        "GPU ç±»å‹",
        ["è‡ªåŠ¨æ£€æµ‹", "NVIDIA CUDA", "AMD ROCm", "CPU only"],
        horizontal=True,
        help="é€‰æ‹©è®¡ç®—åç«¯"
    )

    compute_config = {}

    if gpu_type in ["è‡ªåŠ¨æ£€æµ‹", "NVIDIA CUDA"]:
        # æ£€æµ‹ CUDA ç‰ˆæœ¬
        detected_cuda = hardware_info["system"].get("CUDA")
        cuda_options = ["è‡ªåŠ¨"] + (["11.8", "12.1", "12.4"] if detected_cuda else [])

        cuda_version = st.selectbox(
            "CUDA ç‰ˆæœ¬",
            cuda_options,
            index=0,
            help="PyTorch å°†æ ¹æ®æ­¤ç‰ˆæœ¬å®‰è£…"
        )
        compute_config["cuda"] = cuda_version if cuda_version != "è‡ªåŠ¨" else "auto"
        compute_config["rocm"] = False

    elif gpu_type == "AMD ROCm":
        rocm_version = st.selectbox(
            "ROCm ç‰ˆæœ¬",
            ["5.7", "5.6", "5.5", "5.4"],
            index=0,
            help="AMD GPU é©±åŠ¨ç‰ˆæœ¬"
        )
        compute_config["rocm"] = True
        compute_config["rocm_version"] = rocm_version
        compute_config["cuda"] = "none"

    else:  # CPU only
        compute_config["cuda"] = "none"
        compute_config["rocm"] = False

    # Python ç‰ˆæœ¬
    python_version = st.selectbox(
        "Python ç‰ˆæœ¬",
        ["3.10", "3.11", "3.12"],
        index=0,
        help="å»ºè®®ä½¿ç”¨ 3.10 ä»¥è·å¾—æœ€ä½³å…¼å®¹æ€§"
    )
    compute_config["python"] = python_version

    return compute_config


def render_step4_advanced_options(hardware_info: Dict):
    """æ­¥éª¤4: é«˜çº§é€‰é¡¹"""
    st.subheader("ğŸ”§ é«˜çº§åŠŸèƒ½")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ğŸ³ å®¹å™¨åŒ–")
        docker = st.checkbox("ç”Ÿæˆ Dockerfile", help="ç”Ÿæˆå¤šé˜¶æ®µ Docker æ„å»ºæ–‡ä»¶")
        devcontainer = st.checkbox(
            "ç”Ÿæˆ DevContainer é…ç½®",
            help="VS Code è¿œç¨‹å¼€å‘ç¯å¢ƒé…ç½®"
        )

        st.markdown("#### ğŸ“¦ ä¾èµ–ç®¡ç†")
        pyproject = st.checkbox(
            "ä½¿ç”¨ pyproject.toml",
            value=True,
            help="ç°ä»£ Python æ‰“åŒ…æ ‡å‡†ï¼Œæ”¯æŒ UV dependency groups"
        )

    with col2:
        st.markdown("#### ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ")
        gpu_count = hardware_info["gpu_count"]["total"]

        if gpu_count > 0:
            distributed = st.checkbox(
                "ç”Ÿæˆåˆ†åˆ†å¸ƒå¼è®­ç»ƒé…ç½®",
                value=(gpu_count >= 2),
                help=f"æ£€æµ‹åˆ° {gpu_count} ä¸ª GPUï¼Œè‡ªåŠ¨ç”Ÿæˆ Accelerate/DeepSpeed é…ç½®"
            )

            if distributed:
                st.info(f"å°†ç”Ÿæˆé€‚åˆ {gpu_count} GPU çš„é…ç½®")
        else:
            distributed = False
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œåˆ†å¸ƒå¼é…ç½®ä¸å¯ç”¨")

        st.markdown("#### ğŸ” LLM ä¼˜åŒ–")
        if gpu_count > 0:
            llm_report = hardware_info["llm_report"]
            if llm_report.get("flash_attention", {}).get("compatible"):
                st.success("âœ… æ”¯æŒ Flash Attention 2.x")
            else:
                st.warning("âš ï¸ GPU ä¸æ”¯æŒ Flash Attention")

    return {
        "docker": docker,
        "devcontainer": devcontainer,
        "pyproject": pyproject,
        "distributed": distributed,
    }


def render_step5_create_project():
    """æ­¥éª¤5: åˆ›å»ºé¡¹ç›®"""
    st.subheader("âœ… å‡†å¤‡åˆ›å»º")

    # æ˜¾ç¤ºé…ç½®æ‘˜è¦
    config = st.session_state.config

    st.markdown("### ğŸ“‹ é…ç½®æ‘˜è¦")

    summary_col1, summary_col2 = st.columns(2)

    with summary_col1:
        st.markdown("**åŸºæœ¬ä¿¡æ¯**")
        st.markdown(f"- é¡¹ç›®åç§°: `{config['name']}`")
        st.markdown(f"- æ¨¡æ¿: `{config['template']}`")
        st.markdown(f"- Python: `{config['compute']['python']}`")

    with summary_col2:
        st.markdown("**è®¡ç®—é…ç½®**")
        if config['compute'].get('rocm'):
            st.markdown(f"- GPU: AMD ROCm {config['compute']['rocm_version']}")
        elif config['compute']['cuda'] != 'none':
            st.markdown(f"- GPU: CUDA {config['compute']['cuda']}")
        else:
            st.markdown("- GPU: CPU only")

        st.markdown("**é«˜çº§åŠŸèƒ½**")
        features = []
        if config['options']['docker']:
            features.append("Docker")
        if config['options']['devcontainer']:
            features.append("DevContainer")
        if config['options']['pyproject']:
            features.append("pyproject.toml")
        if config['options']['distributed']:
            features.append("åˆ†å¸ƒå¼")
        st.markdown(f"- {', '.join(features) if features else 'æ— '}")

    st.markdown("---")

    # åˆ›å»ºæŒ‰é’®
    col1, col2, col3 = st.columns([1, 2, 1])

    with col2:
        if st.button("ğŸš€ å¼€å§‹åˆ›å»ºé¡¹ç›®", type="primary", use_container_width=True):
            create_project()


def create_project():
    """æ‰§è¡Œé¡¹ç›®åˆ›å»º"""
    config = st.session_state.config

    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        # æ­¥éª¤ 1: åˆå§‹åŒ–
        status_text.text("åˆå§‹åŒ–ç¯å¢ƒ...")
        progress_bar.progress(10)

        project_path = Path(config["path"])
        env_manager = EnvironmentManager(project_path)
        template_manager = TemplateManager()
        detector = HardwareDetector()

        # æ­¥éª¤ 2: åˆ›å»ºé¡¹ç›®ç»“æ„
        status_text.text("åˆ›å»ºé¡¹ç›®ç»“æ„...")
        env_manager.create_project_structure(config["name"])
        progress_bar.progress(30)

        # æ­¥éª¤ 3: åŠ è½½æ¨¡æ¿
        status_text.text("åŠ è½½æ¨¡æ¿å¹¶è§£æä¾èµ–...")
        compute_cfg = config["compute"]

        template_config = template_manager.load_template(
            config["template"],
            compute_cfg.get("cuda", "none"),
            use_rocm=compute_cfg.get("rocm", False),
            rocm_version=compute_cfg.get("rocm_version", "5.7")
        )
        progress_bar.progress(50)

        # æ­¥éª¤ 4: åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
        status_text.text("åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–...")
        dependencies = template_config.get("dependencies", [])
        dev_dependencies = template_config.get("dev_dependencies", [])
        env_manager.create_environment(compute_cfg["python"], dependencies, dev_dependencies)
        progress_bar.progress(70)

        # æ­¥éª¤ 5: ç”Ÿæˆé¡¹ç›®æ–‡ä»¶
        status_text.text("ç”Ÿæˆé¡¹ç›®æ–‡ä»¶...")
        env_manager.generate_project_files(
            config["name"],
            template_config,
            use_pyproject=config["options"]["pyproject"]
        )
        progress_bar.progress(80)

        # æ­¥éª¤ 6: å®¹å™¨é…ç½®
        options = config["options"]
        if options["docker"] or options["devcontainer"]:
            status_text.text("ç”Ÿæˆå®¹å™¨é…ç½®...")
            container_manager = ContainerManager(project_path)
            container_config = ContainerConfig(
                project_name=config["name"],
                template_type=template_config.get("type", "minimal"),
                cuda_version=compute_cfg.get("cuda", "none"),
                python_version=compute_cfg["python"],
                dependencies=dependencies,
                include_devcontainer=options["devcontainer"]
            )

            if options["docker"]:
                container_manager.generate_dockerfile(container_config)
                container_manager.generate_dockerignore()
                container_manager.generate_readme_addon(container_config)

            if options["devcontainer"]:
                container_manager.generate_devcontainer(container_config)

        # æ­¥éª¤ 7: åˆ†å¸ƒå¼é…ç½®
        if options["distributed"]:
            gpu_count = detector.detect_gpu_count()["total"]
            if gpu_count > 0:
                status_text.text("ç”Ÿæˆåˆ†åˆ†å¸ƒå¼è®­ç»ƒé…ç½®...")
                dist_manager = DistributedConfigManager(project_path)
                dist_config = DistributedConfig(
                    project_path=project_path,
                    num_gpus=gpu_count,
                    gpu_type="amd" if compute_cfg.get("rocm") else "nvidia",
                    template_type=template_config.get("type", "minimal"),
                )

                dist_manager.generate_accelerate_config(gpu_count, "nvidia")
                dist_manager.generate_deepspeed_config(gpu_count)
                dist_manager.generate_training_script(
                    template_type=template_config.get("type", "minimal"),
                    num_gpus=gpu_count,
                )
                dist_manager.generate_launch_config(gpu_count, template_config.get("type", "minimal"))
                dist_manager.generate_multi_gpu_example()

        progress_bar.progress(100)
        status_text.text("")

        # æˆåŠŸæ¶ˆæ¯
        st.session_state.created = True

        st.success("""
        ## ğŸ‰ é¡¹ç›®åˆ›å»ºæˆåŠŸï¼

        é¡¹ç›®å·²æˆåŠŸåˆ›å»ºåœ¨æŒ‡å®šè·¯å¾„ã€‚
        """)

        # ä¸‹ä¸€æ­¥æ“ä½œ
        st.markdown("### ğŸ“Œ ä¸‹ä¸€æ­¥æ“ä½œ")

        st.code(f"""
# è¿›å…¥é¡¹ç›®ç›®å½•
cd {config['path']}

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\\Scripts\\activate   # Windows

# éªŒè¯å®‰è£…
python -c 'import torch; print(torch.__version__)'
        """, language="bash")

        # å®¹å™¨æ“ä½œæç¤º
        if options["docker"]:
            st.markdown("#### ğŸ³ Docker æ„å»º")
            st.code(f"""
# æ„å»ºé•œåƒ
docker build -t {config['name']}:latest .

# è¿è¡Œå®¹å™¨
docker run -it --rm --gpus all -p 8888:8888 {config['name']}:latest
            """, language="bash")

        # åˆ†å¸ƒå¼è®­ç»ƒæç¤º
        if options["distributed"]:
            st.markdown("#### ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ")
            st.code("""
# ä½¿ç”¨ Accelerate
accelerate launch --config_file accelerate_config.yaml src/train.py

# ä½¿ç”¨ DeepSpeed
deepspeed --num_gpus=all src/train.py --deepspeed ds_config.json
            """, language="bash")

        # é‡ç½®æŒ‰é’®
        if st.button("åˆ›å»ºæ–°é¡¹ç›®"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            init_session_state()
            st.rerun()

    except Exception as e:
        st.error(f"""
        ## âŒ åˆ›å»ºå¤±è´¥

        **é”™è¯¯ä¿¡æ¯**: {str(e)}

        è¯·æ£€æŸ¥ï¼š
        1. é¡¹ç›®è·¯å¾„æ˜¯å¦æœ‰å†™å…¥æƒé™
        2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
        3. Python ç¯å¢ƒæ˜¯å¦æ­£ç¡®
        """)
        st.exception(e)


# ============================================
# ä¸»æµç¨‹
# ============================================
def main():
    """ä¸»ç•Œé¢"""
    render_header()
    render_step_indicator()

    st.markdown("---")

    # ç¼“å­˜æ•°æ®
    hardware_info = detect_hardware()
    templates_data = get_templates()

    # æ­¥éª¤å¯¼èˆª
    if st.session_state.step == 1:
        basic_info = render_step1_basic_info()

        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            if st.button("ä¸‹ä¸€æ­¥ â†’", use_container_width=True):
                if basic_info["name"]:
                    st.session_state.config.update(basic_info)
                    st.session_state.step = 2
                    st.rerun()
                else:
                    st.warning("è¯·è¾“å…¥é¡¹ç›®åç§°")

    elif st.session_state.step == 2:
        template = render_step2_template_selection(templates_data)

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            if st.button("â† ä¸Šä¸€æ­¥", use_container_width=True):
                st.session_state.step = 1
                st.rerun()
        with col2:
            if st.button("ä¸‹ä¸€æ­¥ â†’", use_container_width=True, disabled=(not template)):
                st.session_state.step = 3
                st.rerun()

    elif st.session_state.step == 3:
        compute_config = render_step3_compute_config(hardware_info)
        st.session_state.config["compute"] = compute_config

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            if st.button("â† ä¸Šä¸€æ­¥", use_container_width=True):
                st.session_state.step = 2
                st.rerun()
        with col2:
            if st.button("ä¸‹ä¸€æ­¥ â†’", use_container_width=True):
                st.session_state.step = 4
                st.rerun()

    elif st.session_state.step == 4:
        options = render_step4_advanced_options(hardware_info)
        st.session_state.config["options"] = options

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            if st.button("â† ä¸Šä¸€æ­¥", use_container_width=True):
                st.session_state.step = 3
                st.rerun()
        with col2:
            if st.button("ä¸‹ä¸€æ­¥ â†’", use_container_width=True):
                st.session_state.step = 5
                st.rerun()

    elif st.session_state.step == 5:
        render_step5_create_project()

        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            if st.button("â† ä¸Šä¸€æ­¥", use_container_width=True):
                st.session_state.step = 4
                st.rerun()

    # ä¾§è¾¹æ ï¼šå¸®åŠ©ä¿¡æ¯
    with st.sidebar:
        st.markdown("## ğŸ“– ä½¿ç”¨æŒ‡å—")

        st.markdown("""
        ### å¿«é€Ÿå¼€å§‹
        1. è¾“å…¥é¡¹ç›®åç§°å’Œè·¯å¾„
        2. é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        3. é…ç½®è®¡ç®—ç¯å¢ƒ
        4. é€‰æ‹©é«˜çº§åŠŸèƒ½
        5. ç‚¹å‡»åˆ›å»º

        ### æ¨¡æ¿è¯´æ˜
        - **llm**: å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ
        - **rag**: æ£€ç´¢å¢å¼ºç”Ÿæˆ
        - **pytorch**: PyTorch æ·±åº¦å­¦ä¹ 
        - **inference**: LLM æ¨ç†æœåŠ¡
        """)

        st.markdown("---")

        st.markdown("### ğŸ’¡ æç¤º")
        st.markdown("""
        - é¦–æ¬¡ä½¿ç”¨å»ºè®®é€‰æ‹©é»˜è®¤é…ç½®
        - GPU ç±»å‹é€‰æ‹©"è‡ªåŠ¨æ£€æµ‹"æœ€å®‰å…¨
        - Docker é€‚åˆéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
        - åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦å¤š GPU æ”¯æŒ
        """)

        st.markdown("---")

        st.markdown("""
        ### ğŸ”— ç›¸å…³é“¾æ¥
        - [GitHub ä»“åº“](https://github.com/YuanyuanMa03/ml-easy-setup)
        - [PyPI é¡µé¢](https://pypi.org/project/ml-easy-setup/)
        """)


if __name__ == "__main__":
    main()
