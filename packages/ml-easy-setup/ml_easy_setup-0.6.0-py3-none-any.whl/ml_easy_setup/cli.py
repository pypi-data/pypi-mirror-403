"""
ML Easy Setup CLI - å‘½ä»¤è¡Œæ¥å£
"""

import click
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

from ml_easy_setup.core.env_manager import EnvironmentManager
from ml_easy_setup.core.template import TemplateManager
from ml_easy_setup.core.detector import HardwareDetector
from ml_easy_setup.core.health import check_command
from ml_easy_setup.core.container import ContainerManager, ContainerConfig
from ml_easy_setup.core.distributed import DistributedConfigManager, DistributedConfig

console = Console()


@click.group()
@click.version_option(version="0.4.0", prog_name="ml-easy-setup")
def main():
    """
    ML Easy Setup - ä¸€é”®é…ç½®æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç¯å¢ƒ

    è®©ç§‘ç ”å·¥ä½œæ›´ä¸“æ³¨äºç®—æ³•æœ¬èº«ï¼Œè€Œä¸æ˜¯ç¯å¢ƒé…ç½®ã€‚
    """
    pass


@main.command()
@click.argument("name", type=str)
@click.option(
    "--template", "-t",
    type=click.Choice([
        "minimal", "pytorch", "tensorflow", "nlp", "cv", "rl",
        "model-builder", "algorithm-validator", "data-science",
        "gradient-boosting", "mlops", "timeseries", "graph",
        "llm", "rag", "inference",  # æ–°å¢ LLM/GenAI æ¨¡æ¿
        "full"
    ]),
    default="minimal",
    help="é¢„é…ç½®ç¯å¢ƒæ¨¡æ¿"
)
@click.option(
    "--cuda", "-c",
    type=click.Choice(["none", "cpu", "auto", "11.8", "12.1", "12.4"]),
    default="auto",
    help="CUDA ç‰ˆæœ¬ (auto è‡ªåŠ¨æ£€æµ‹)"
)
@click.option(
    "--python", "-p",
    type=str,
    default="3.10",
    help="Python ç‰ˆæœ¬"
)
@click.option(
    "--path", "-d",
    type=click.Path(),
    default=None,
    help="é¡¹ç›®è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰"
)
@click.option(
    "--docker",
    is_flag=True,
    help="ç”Ÿæˆ Dockerfile å’Œå®¹å™¨é…ç½®"
)
@click.option(
    "--devcontainer",
    is_flag=True,
    help="ç”Ÿæˆ VS Code DevContainer é…ç½®"
)
@click.option(
    "--pyproject",
    is_flag=True,
    help="ä½¿ç”¨ pyproject.toml (ç°ä»£ Python æ‰“åŒ…æ ‡å‡†)"
)
@click.option(
    "--rocm",
    is_flag=True,
    help="ä½¿ç”¨ ROCm (AMD GPU æ”¯æŒ)"
)
@click.option(
    "--rocm-version",
    type=click.Choice(["5.4", "5.5", "5.6", "5.7"]),
    default="5.7",
    help="ROCm ç‰ˆæœ¬ (é»˜è®¤: 5.7)"
)
@click.option(
    "--distributed",
    is_flag=True,
    help="ç”Ÿæˆåˆ†å¸ƒå¼è®­ç»ƒé…ç½® (accelerate/DeepSpeed)"
)
def create(
    name: str,
    template: str,
    cuda: str,
    python: str,
    path: str | None,
    docker: bool,
    devcontainer: bool,
    pyproject: bool,
    rocm: bool,
    rocm_version: str,
    distributed: bool
):
    """
    åˆ›å»ºæ–°çš„ ML é¡¹ç›®ç¯å¢ƒ

    ç¤ºä¾‹:
        mlsetup create my-project --template pytorch --cuda auto
        mlsetup create my-project --template pytorch --docker
        mlsetup create my-project --template nlp --devcontainer
        mlsetup create my-project --template llm --pyproject --cuda 12.1
        mlsetup create my-project --template llm --rocm --distributed
        mlsetup create my-project --template llm --pyproject --cuda 12.1 --distributed
    """
    project_path = Path(path) if path else Path.cwd() / name
    project_path = project_path.resolve()

    # é…ç½®æç¤º
    hints = []
    if docker or devcontainer:
        container_hint = 'Docker' if docker else ''
        container_hint += ' + ' if docker and devcontainer else ''
        container_hint += 'DevContainer' if devcontainer else ''
        hints.append(f"å®¹å™¨: {container_hint}")

    if pyproject:
        hints.append("ä¾èµ–: pyproject.toml")

    if rocm:
        hints.append(f"ROCm: {rocm_version}")

    if distributed:
        hints.append("åˆ†å¸ƒå¼: æ˜¯")

    hint_str = "\n" + "\n".join(hints) if hints else ""
    compute_str = f"ROCm {rocm_version}" if rocm else f"CUDA {cuda}"

    console.print(Panel.fit(
        f"[bold cyan]åˆ›å»ºé¡¹ç›®: {name}[/bold cyan]\n"
        f"æ¨¡æ¿: {template} | {compute_str} | Python: {python}{hint_str}\n"
        f"è·¯å¾„: {project_path}",
        title="ML Easy Setup"
    ))

    try:
        # æ£€æµ‹ç¡¬ä»¶
        detector = HardwareDetector()

        # GPU æ•°é‡æ£€æµ‹ (ç”¨äºåˆ†å¸ƒå¼é…ç½®)
        gpu_count = detector.detect_gpu_count()
        gpu_type = "amd" if rocm else "nvidia"

        # CUDA/ROCm ç‰ˆæœ¬æ£€æµ‹
        if rocm:
            # ROCm æ¨¡å¼
            detected_rocm = detector.detect_rocm()
            if detected_rocm:
                console.print(f"ğŸ¯ æ£€æµ‹åˆ° ROCm ç‰ˆæœ¬: [green]{detected_rocm}[/green]")
                rocm_version = detected_rocm
            cuda = "none"  # ROCm æ¨¡å¼ä¸‹ç¦ç”¨ CUDA
        elif cuda == "auto":
            # CUDA è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
            detected_cuda = detector.detect_cuda()
            if detected_cuda:
                console.print(f"ğŸ¯ æ£€æµ‹åˆ° CUDA ç‰ˆæœ¬: [green]{detected_cuda}[/green]")
                cuda = detected_cuda
            else:
                # æ£€æµ‹æ˜¯å¦æœ‰ AMD GPU
                detected_rocm = detector.detect_rocm()
                if detected_rocm:
                    console.print(f"ğŸ¯ æ£€æµ‹åˆ° AMD GPUï¼Œä½¿ç”¨ ROCm [green]{detected_rocm}[/green]")
                    rocm = True
                    rocm_version = detected_rocm
                    cuda = "none"
        elif cuda == "cpu":
            cuda = "none"

        # åˆ›å»ºç¯å¢ƒ
        env_manager = EnvironmentManager(project_path)
        template_manager = TemplateManager()

        console.print("\n[bold]æ­¥éª¤ 1/3:[/bold] åˆ›å»ºé¡¹ç›®ç»“æ„...")
        env_manager.create_project_structure(name)

        console.print("[bold]æ­¥éª¤ 2/3:[/bold] åŠ è½½æ¨¡æ¿å¹¶è§£æä¾èµ–...")
        template_config = template_manager.load_template(
            template,
            cuda,
            use_rocm=rocm,
            rocm_version=rocm_version
        )
        dependencies = template_config.get("dependencies", [])
        dev_dependencies = template_config.get("dev_dependencies", [])

        console.print(f"   éœ€è¦å®‰è£… [cyan]{len(dependencies)}[/cyan] ä¸ªæ ¸å¿ƒä¾èµ–")
        console.print(f"   éœ€è¦å®‰è£… [cyan]{len(dev_dependencies)}[/cyan] ä¸ªå¼€å‘ä¾èµ–")

        console.print("\n[bold]æ­¥éª¤ 3/4:[/bold] åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–...")
        env_manager.create_environment(python, dependencies, dev_dependencies)

        # ç”Ÿæˆé¡¹ç›®æ–‡ä»¶
        console.print("\n[bold]æ­¥éª¤ 4/4:[/bold] ç”Ÿæˆé¡¹ç›®æ–‡ä»¶...")
        env_manager.generate_project_files(name, template_config, use_pyproject=pyproject)

        # ç”Ÿæˆå®¹å™¨é…ç½®
        if docker or devcontainer:
            container_manager = ContainerManager(project_path)
            container_config = ContainerConfig(
                project_name=name,
                template_type=template_config.get("type", "minimal"),
                cuda_version=cuda,
                python_version=python,
                dependencies=dependencies,
                include_devcontainer=devcontainer
            )

            if docker:
                console.print("   [dim]ç”Ÿæˆ Dockerfile...[/dim]")
                container_manager.generate_dockerfile(container_config)
                container_manager.generate_dockerignore()
                container_manager.generate_readme_addon(container_config)

            if devcontainer:
                console.print("   [dim]ç”Ÿæˆ DevContainer é…ç½®...[/dim]")
                container_manager.generate_devcontainer(container_config)

        # ç”Ÿæˆåˆ†å¸ƒå¼é…ç½®
        if distributed:
            num_gpus = gpu_count["total"]
            if num_gpus > 0:
                console.print("\n[bold]æ­¥éª¤ 5/5:[/bold] ç”Ÿæˆåˆ†å¸ƒå¼è®­ç»ƒé…ç½®...")
                console.print(f"   æ£€æµ‹åˆ° {num_gpus} ä¸ª {gpu_type.upper()} GPU")

                dist_manager = DistributedConfigManager(project_path)
                dist_config = DistributedConfig(
                    project_path=project_path,
                    num_gpus=num_gpus,
                    gpu_type=gpu_type,
                    template_type=template_config.get("type", "minimal"),
                )

                # ç”Ÿæˆ accelerate é…ç½®
                dist_manager.generate_accelerate_config(
                    num_gpus=num_gpus,
                    gpu_type=gpu_type,
                )

                # ç”Ÿæˆ DeepSpeed é…ç½®
                dist_manager.generate_deepspeed_config(num_gpus=num_gpus)

                # ç”Ÿæˆ FSDP é…ç½® (PyTorch æ¨¡æ¿)
                if template_config.get("type") == "pytorch":
                    dist_manager.generate_fsdp_config(num_gpus=num_gpus)

                # ç”Ÿæˆè®­ç»ƒè„šæœ¬
                dist_manager.generate_training_script(
                    template_type=template_config.get("type", "minimal"),
                    num_gpus=num_gpus,
                )

                # ç”Ÿæˆå¯åŠ¨é…ç½®
                dist_manager.generate_launch_config(
                    num_gpus=num_gpus,
                    template_type=template_config.get("type", "minimal"),
                )

                # ç”Ÿæˆå¤š GPU è®­ç»ƒç¤ºä¾‹
                dist_manager.generate_multi_gpu_example()
            else:
                console.print("\n[yellow]æœªæ£€æµ‹åˆ° GPUï¼Œè·³è¿‡åˆ†å¸ƒå¼é…ç½®[/yellow]")

        console.print("\n" + "=" * 50)
        console.print("[bold green]âœ“ ç¯å¢ƒé…ç½®å®Œæˆï¼[/bold green]")
        console.print("=" * 50)
        console.print(f"\nä¸‹ä¸€æ­¥æ“ä½œ:")
        console.print(f"  [cyan]cd {name}[/cyan]")
        console.print(f"  [cyan]source .venv/bin/activate[/cyan]  # Linux/Mac")
        console.print(f"  [cyan]\\.venv\\Scripts\\activate[/cyan]  # Windows")
        console.print(f"  [cyan]python -c 'import torch; print(torch.__version__)'[/cyan]")

        # å®¹å™¨ç›¸å…³æç¤º
        if docker:
            console.print(f"\n[bold]å®¹å™¨åŒ–éƒ¨ç½²:[/bold]")
            console.print(f"  [cyan]docker build -t {name}:latest .[/cyan]")
            console.print(f"  [cyan]docker run -it --rm --gpus all -p 8888:8888 {name}:latest[/cyan]")

        if devcontainer:
            console.print(f"\n[bold]DevContainer:[/bold]")
            console.print(f"  åœ¨ VS Code ä¸­æŒ‰ [cyan]F1[/cyan] â†’ [cyan]Dev Containers: Reopen in Container[/cyan]")

        # åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³æç¤º
        if distributed and gpu_count["total"] > 0:
            console.print(f"\n[bold]åˆ†å¸ƒå¼è®­ç»ƒ:[/bold]")
            console.print(f"  [cyan]accelerate launch --config_file accelerate_config.yaml src/train.py[/cyan]")
            console.print(f"  [cyan]bash scripts/train_distributed.sh[/cyan]")
            console.print(f"  [cyan]deepspeed --num_gpus={gpu_count['total']} src/train.py --deepspeed ds_config.json[/cyan]")

        # pyproject ç›¸å…³æç¤º
        if pyproject:
            console.print(f"\n[bold]pyproject.toml:[/bold]")
            console.print(f"  [cyan]uv sync --group dev[/cyan]  # å®‰è£…å¼€å‘ä¾èµ–")
            console.print(f"  [cyan]uv lock[/cyan]  # ç”Ÿæˆé”æ–‡ä»¶")

        # ROCm ç›¸å…³æç¤º
        if rocm:
            console.print(f"\n[bold]ROCm (AMD GPU):[/bold]")
            console.print(f"  PyTorch ROCm ç‰ˆæœ¬å·²é…ç½®åœ¨ requirements.txt ä¸­")
            console.print(f"  ç¡®ä¿ç³»ç»Ÿå·²å®‰è£… ROCm: [cyan]rocm-smi --showversion[/cyan]")

        console.print(f"\n[yellow]æ›´å¤šå®¹å™¨åŒ–æ–‡æ¡£: [cyan]README_CONTAINER.md[/cyan][/yellow]")

    except Exception as e:
        console.print(f"\n[bold red]âœ— åˆ›å»ºå¤±è´¥:[/bold red] {e}")
        raise click.ClickException(str(e))


@main.command()
def list_templates():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç¯å¢ƒæ¨¡æ¿"""
    template_manager = TemplateManager()
    templates = template_manager.list_templates()

    table = Table(title="å¯ç”¨çš„ç¯å¢ƒæ¨¡æ¿")
    table.add_column("æ¨¡æ¿åç§°", style="cyan")
    table.add_column("æè¿°", style="white")
    table.add_column("æ ¸å¿ƒåº“", style="yellow")

    for template in templates:
        table.add_row(
            template["name"],
            template["description"],
            ", ".join(template["core_packages"][:3]) + ("..." if len(template["core_packages"]) > 3 else "")
        )

    console.print(table)


@main.command()
@click.option("--verbose", "-v", is_flag=True, help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")
def detect(verbose: bool):
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶å’Œè½¯ä»¶ç¯å¢ƒ"""
    detector = HardwareDetector()
    info = detector.detect_all(verbose)

    table = Table(title="ç³»ç»Ÿç¯å¢ƒæ£€æµ‹")
    table.add_column("é¡¹ç›®", style="cyan")
    table.add_column("æ£€æµ‹ç»“æœ", style="green")

    for key, value in info.items():
        table.add_row(key, str(value))

    console.print(table)


@main.command()
def llm_check():
    """
    LLM ç¡¬ä»¶å…¼å®¹æ€§æ£€æŸ¥

    æ£€æŸ¥ GPUã€CUDAã€Flash Attention ç­‰ä¸ LLM è®­ç»ƒç›¸å…³çš„ç¡¬ä»¶ä¿¡æ¯
    """
    from rich.panel import Panel
    from rich.syntax import Syntax

    detector = HardwareDetector()
    report = detector.get_llm_hardware_report()

    console.print("\n[bold cyan]ğŸ” LLM ç¡¬ä»¶å…¼å®¹æ€§æ£€æŸ¥[/bold cyan]\n")

    # GPU ä¿¡æ¯
    console.print("[bold]GPU ä¿¡æ¯:[/bold]")
    if report["gpu_available"]:
        console.print(f"  GPU: [green]{report['gpu_name']}[/green]")
        console.print(f"  Compute Capability: [cyan]{report['compute_capability']}[/cyan]")
    else:
        console.print("  [yellow]æœªæ£€æµ‹åˆ° NVIDIA GPU[/yellow]")

    # CUDA ä¿¡æ¯
    console.print(f"\n[bold]CUDA ç‰ˆæœ¬:[/bold]")
    if report["cuda_version"]:
        console.print(f"  [green]{report['cuda_version']}[/green]")
    else:
        console.print("  [yellow]æœªå®‰è£… CUDA[/yellow]")

    # Flash Attention ä¿¡æ¯
    console.print(f"\n[bold]Flash Attention:[/bold]")
    if report["flash_attention"]["compatible"]:
        console.print("  [green]âœ“ å…¼å®¹[/green] - " + report["flash_attention"]["reason"])
        if report["flash_attention"]["install_command"]:
            console.print("\n  [dim]å®‰è£…å‘½ä»¤:[/dim]")
            install_cmd = report["flash_attention"]["install_command"]
            for line in install_cmd.split("\n"):
                console.print(f"    {line}")
    else:
        console.print("  [yellow]âœ— ä¸å…¼å®¹[/yellow] - " + report["flash_attention"]["reason"])

    # æ¨èè®¾ç½®
    if report["recommended_settings"]:
        console.print(f"\n[bold]æ¨èè®­ç»ƒè®¾ç½®:[/bold]")
        for key, value in report["recommended_settings"].items():
            value_str = "[green]" + str(value) + "[/green]" if value is True else str(value)
            console.print(f"  {key}: {value_str}")

    console.print()


@main.command()
@click.argument("packages", nargs=-1, required=True)
@click.option("--dev", is_flag=True, help="å®‰è£…åˆ°å¼€å‘ä¾èµ–")
def add(packages: tuple[str, ...], dev: bool):
    """
    æ·»åŠ é¢å¤–çš„åŒ…åˆ°å½“å‰é¡¹ç›®

    ç¤ºä¾‹:
        mlsetup add numpy pandas
        mlsetup add pytest --dev
    """
    env_manager = EnvironmentManager(Path.cwd())

    console.print(f"[bold]æ·»åŠ åŒ…:[/bold] {', '.join(packages)}")

    try:
        env_manager.add_packages(list(packages), dev=dev)
        console.print("[bold green]âœ“ åŒ…å®‰è£…å®Œæˆ[/bold green]")
    except Exception as e:
        console.print(f"[bold red]âœ— å®‰è£…å¤±è´¥:[/bold red] {e}")
        raise click.ClickException(str(e))


@main.command()
@click.option("--path", "-p", type=click.Path(), default=".", help="é¡¹ç›®è·¯å¾„")
@click.option("--auto-fix", is_flag=True, help="è‡ªåŠ¨ä¿®å¤å‘ç°çš„é—®é¢˜")
def health(path: str, auto_fix: bool) -> None:
    """
    ç¯å¢ƒå¥åº·æ£€æŸ¥

    æ£€æŸ¥é¡¹ç›®ç¯å¢ƒçŠ¶æ€ï¼Œå‘ç°æ½œåœ¨é—®é¢˜å¹¶æä¾›ä¿®å¤å»ºè®®ã€‚

    ç¤ºä¾‹:
        mlsetup health
        mlsetup health --auto-fix
    """
    from rich.prompt import Confirm

    project_path = Path(path).resolve()

    # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬çš„é¡¹ç›®ï¼ˆæœ‰ .venv æˆ– requirements.txtï¼‰
    has_venv = (project_path / ".venv").exists()
    has_req = (project_path / "requirements.txt").exists()

    if not (has_venv or has_req):
        console.print("[yellow]æç¤º:[/yellow] å½“å‰ç›®å½•å¯èƒ½ä¸æ˜¯ ML Easy Setup é¡¹ç›®")
        console.print("å°è¯•è¿è¡Œæ£€æŸ¥...")
    else:
        console.print("[green]âœ“[/green] æ£€æµ‹åˆ° ML Easy Setup é¡¹ç›®ç»“æ„")

    from ml_easy_setup.core.health import HealthChecker

    checker = HealthChecker(project_path)
    results = checker.check_all()
    checker.print_report(results)

    # è¯¢é—®æ˜¯å¦è‡ªåŠ¨ä¿®å¤
    if results["status"] in ["warning", "critical"]:
        if auto_fix or Confirm.ask("\næ˜¯å¦å°è¯•è‡ªåŠ¨ä¿®å¤è¿™äº›é—®é¢˜ï¼Ÿ"):
            actions = checker.auto_fix(results, dry_run=not auto_fix)

            if not auto_fix:
                console.print("\n[bold]å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ:[/bold]")
                for i, action in enumerate(actions, 1):
                    console.print(f"  {i}. {action}")

                if Confirm.ask("\næ˜¯å¦ç»§ç»­ï¼Ÿ"):
                    actions = checker.auto_fix(results, dry_run=False)
                    console.print(f"\n[green]âœ“ å·²æ‰§è¡Œ {len(actions)} é¡¹ä¿®å¤æ“ä½œ[/green]")


if __name__ == "__main__":
    main()
