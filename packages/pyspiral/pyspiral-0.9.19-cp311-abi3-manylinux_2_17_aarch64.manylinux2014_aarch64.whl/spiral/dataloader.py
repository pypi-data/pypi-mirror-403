from __future__ import annotations

import os
import random
from collections.abc import Callable, Iterator
from dataclasses import dataclass
from functools import partial
from multiprocessing import get_context
from typing import Any

import pyarrow as pa

from spiral.core.client import Shard
from spiral.scan import Scan


@dataclass(frozen=True)
class World:
    """Distributed training configuration.
    Attributes:
        rank: Process rank (0 to world_size-1).
        world_size: Total number of processes.
    """

    rank: int
    world_size: int

    def shards(
        self,
        shards: list[Shard],
        shuffle_seed: int | None = None,
    ) -> list[Shard]:
        """Partition shards for distributed training.

        Args:
            shards: List of Shard objects to partition.
            shuffle_seed: Optional seed to shuffle before partitioning.

        Returns:
            Subset of shards for this rank (round-robin partitioning).
        """
        if shuffle_seed is not None:
            shards = World._shuffle(shards, shuffle_seed)

        return shards[self.rank :: self.world_size]

    @classmethod
    def from_torch(cls) -> World:
        """Auto-detect world configuration from PyTorch distributed."""
        try:
            import torch.distributed as dist

            if dist.is_available() and dist.is_initialized():
                return cls(
                    rank=dist.get_rank(),
                    world_size=dist.get_world_size(),
                )
        except ImportError:
            pass

        return cls(
            rank=int(os.environ.get("RANK", 0)),
            world_size=int(os.environ.get("WORLD_SIZE", 1)),
        )

    @classmethod
    def _shuffle(cls, shards: list[Shard], seed: int) -> list[Shard]:
        """Shuffle shards deterministically with given seed."""
        shuffled = list(shards)
        random.Random(seed).shuffle(shuffled)
        return shuffled


# Top level so we can pickle this function
def _len_and_transform(batch: pa.RecordBatch, transform_fn: Callable) -> tuple[int, Any]:
    return (len(batch), transform_fn(batch))


class SpiralDataLoader:
    """DataLoader optimized for Spiral's multi-threaded streaming architecture.

    Unlike PyTorch's DataLoader which uses multiprocessing for I/O (num_workers),
    SpiralDataLoader leverages Spiral's efficient Rust-based streaming and only
    uses multiprocessing for CPU-bound post-processing transforms.

    Key differences from PyTorch DataLoader:
    - No num_workers for I/O (Spiral's Rust layer is already multi-threaded)
    - map_workers for parallel post-processing (tokenization, decoding, etc.)
    - Built-in checkpoint support via skip_samples
    - Explicit shard-based architecture for distributed training

    Simple usage:

    ```python
    def train_step(batch):
        pass

    loader = SpiralDataLoader(scan, batch_size=32)
    for batch in loader:
        train_step(batch)
    ```

    With parallel transforms:

    ```python
    def tokenize_batch(batch):
        # ...
        return batch

    loader = SpiralDataLoader(
        scan,
        batch_size=32,
        transform_fn=tokenize_batch,
        map_workers=4,
    )
    ```
    """

    def __init__(
        self,
        scan: Scan,
        *,
        shards: list[Shard] | None = None,
        shuffle_shards: bool = True,
        seed: int = 42,
        skip_samples: int = 0,
        shuffle_buffer_size: int = 0,
        batch_size: int = 32,
        batch_readahead: int | None = None,
        # TODO(os): accept vortex arrays here instead of Arrow
        transform_fn: Callable[[pa.RecordBatch], Any] | None = None,
        map_workers: int = 0,
        infinite: bool = False,
    ):
        """Initialize SpiralDataLoader.

        Args:
            scan: Spiral scan to load data from.
            shards: Optional list of Shard objects to read. If None, uses
                scan's natural sharding based on physical layout.
            shuffle_shards: Whether to shuffle the list of shards.
                Uses the provided seed.
            seed: Base random seed for deterministic shuffling and checkpointing.
            skip_samples: Number of samples to skip at the beginning (for resuming
                from checkpoint).
            shuffle_buffer_size: Size of shuffle buffer for within-shard shuffling.
                0 means no shuffling.
            batch_size: Number of rows per batch.
            batch_readahead: Number of batches to prefetch in background. If None,
                uses a sensible default based on whether transforms are applied.
            transform_fn: Optional function to transform each batch. Takes a PyArrow
                RecordBatch and returns any type. Users can call batch.to_pydict()
                inside the function if they need a dict. If map_workers > 0, this
                function must be picklable.
            map_workers: Number of worker processes for parallel transform_fn
                application. 0 means single-process (no parallelism). Use this for
                CPU-bound transforms like tokenization or audio decoding.
            infinite: Whether to cycle through the dataset infinitely. If True,
                the dataloader will repeat the dataset indefinitely. If False,
                the dataloader will stop after going through the dataset once.
        """
        self.scan = scan
        self.shards = shards if shards is not None else scan.shards()
        if shuffle_shards:
            self.shards = World._shuffle(self.shards, seed)
        self.seed = seed
        self.skip_samples = skip_samples
        self.shuffle_buffer_size = shuffle_buffer_size
        self.batch_size = batch_size
        self.batch_readahead = batch_readahead
        self.transform_fn = transform_fn
        self.map_workers = map_workers
        self.infinite = infinite

        self._samples_yielded = 0

    def __iter__(self) -> Iterator[Any]:
        """Iterate over batches."""
        from spiral.core.client import ShuffleConfig

        shuffle = None
        if self.shuffle_buffer_size > 0:
            shuffle = ShuffleConfig(
                buffer_size=self.shuffle_buffer_size,
                seed=self.seed,
            )

        stream = self.scan.core.to_shuffled_record_batches(
            shards=self.shards,
            shuffle=shuffle,
            max_batch_size=self.batch_size,
            batch_readahead=self.batch_readahead,
            infinite=self.infinite,
        )

        if self.skip_samples > 0:

            def skip(s: Iterator[pa.RecordBatch], skip_count: int) -> Iterator[pa.RecordBatch]:
                """Skip samples from stream, yielding remaining batches."""
                skipped = 0
                for batch in s:
                    batch_size = len(batch)
                    if skipped + batch_size <= skip_count:
                        # Skip entire batch
                        skipped += batch_size
                        continue
                    elif skipped < skip_count:
                        # Partial skip - discard first N samples, yield remainder
                        skip_in_batch = skip_count - skipped
                        skipped = skip_count
                        yield batch[skip_in_batch:]
                    else:
                        # take the entire batch
                        yield batch

            stream = skip(stream, self.skip_samples)

        if self.transform_fn is None:
            for batch in stream:
                self._samples_yielded += len(batch)
                yield batch
        elif self.map_workers == 0:
            # Single-process transform
            for batch in stream:
                result = self.transform_fn(batch)
                self._samples_yielded += len(batch)
                yield result
        else:
            with get_context("spawn").Pool(self.map_workers) as pool:
                for batch_len, result in pool.imap(partial(_len_and_transform, transform_fn=self.transform_fn), stream):
                    self._samples_yielded += batch_len
                    yield result

    def state_dict(self) -> dict[str, Any]:
        """Get checkpoint state for resuming.

        Returns:
            Dictionary containing samples_yielded, seed, and shards.

        Example checkpoint:
        ```python
        loader = SpiralDataLoader(scan, batch_size=32, seed=42)
        for i, batch in enumerate(loader):
            if i == 10:
                checkpoint = loader.state_dict()
                break
        ```

        Example resume:
        ```python
        loader = SpiralDataLoader.from_state_dict(scan, checkpoint, batch_size=32)
        ```
        """
        return {
            "samples_yielded": self._samples_yielded,
            "seed": self.seed,
            "shards": self.shards,  # Will be pickled automatically
        }

    @classmethod
    def from_state_dict(
        cls,
        scan: Scan,
        state: dict[str, Any],
        **kwargs,
    ) -> SpiralDataLoader:
        """Create a DataLoader from checkpoint state, resuming from where it left off.

        This is the recommended way to resume training from a checkpoint. It extracts
        the seed, samples_yielded, and shards from the state dict and creates a new
        DataLoader that will skip the already-processed samples.

        Args:
            scan: Spiral scan to load data from.
            state: Checkpoint state from state_dict().
            **kwargs: Additional arguments to pass to SpiralDataLoader constructor.
                These will override values in the state dict where applicable.

        Returns:
            New SpiralDataLoader instance configured to resume from the checkpoint.

        Save checkpoint during training:
        ```python
        loader = scan.to_distributed_data_loader(batch_size=32, seed=42)
        checkpoint = loader.state_dict()
        ```

        Resume later using the same shards from checkpoint:
        ```python
        resumed_loader = SpiralDataLoader.from_state_dict(
            scan,
            checkpoint,
            batch_size=32,
            # An optional transform_fn may be provided:
            # transform_fn=my_transform,
        )
        ```
        """

        # Extract resume parameters from state
        seed = state.get("seed", 42)
        skip_samples = state.get("samples_yielded", 0)
        shards = state.get("shards")

        # Allow kwargs to override state dict values
        seed = kwargs.pop("seed", seed)
        skip_samples = kwargs.pop("skip_samples", skip_samples)
        shards = kwargs.pop("shards", shards)

        return cls(scan, seed=seed, skip_samples=skip_samples, shards=shards, **kwargs)
