"""
Multi-Server MCP Tool Orchestration - Production Implementation

Demonstrates sophisticated orchestration of multiple MCP servers to create complex,
distributed tool ecosystems using Kailash SDK's production-ready infrastructure.
"""

import asyncio
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from kaizen.core.base_agent import BaseAgent, BaseAgentConfig
from kaizen.signatures import InputField, OutputField, Signature

try:
    from kailash.mcp_server import MCPClient
    from kailash.mcp_server.registry import ServiceRegistry
    from kailash.mcp_server.service_mesh import ServiceMesh

    KAILASH_MCP_AVAILABLE = True
except ImportError:
    KAILASH_MCP_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# TASK STATUS AND DATA STRUCTURES
# ================================================================================


class TaskStatus(Enum):
    """Task execution status."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class WorkflowTask:
    """Individual task in multi-server workflow."""

    task_id: str
    server: str
    tool: str
    arguments: Dict[str, Any]
    dependencies: List[str] = field(default_factory=list)
    status: TaskStatus = TaskStatus.PENDING
    result: Any = None
    error: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    retry_count: int = 0
    max_retries: int = 3

    @property
    def execution_time(self) -> Optional[float]:
        """Calculate execution time in seconds."""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


@dataclass
class ExecutionPlan:
    """Workflow execution plan with parallelization strategy."""

    parallel_groups: List[List[str]] = field(default_factory=list)
    sequential_groups: List[List[str]] = field(default_factory=list)
    estimated_duration: float = 0.0
    dependency_levels: Dict[str, int] = field(default_factory=dict)


# ================================================================================
# SIGNATURES FOR ORCHESTRATION
# ================================================================================


class WorkflowPlanningSignature(Signature):
    """Signature for workflow planning and optimization."""

    workflow_description: str = InputField(
        description="Description of workflow to execute"
    )
    available_servers: str = InputField(
        description="Available MCP servers and capabilities"
    )
    performance_requirements: str = InputField(
        description="Performance and reliability requirements"
    )

    execution_strategy: str = OutputField(description="Optimal execution strategy")
    parallelization_plan: str = OutputField(
        description="Parallel vs sequential task grouping"
    )
    estimated_completion: str = OutputField(description="Estimated completion time")
    resource_allocation: str = OutputField(
        description="Server and resource allocation plan"
    )


class DependencyResolutionSignature(Signature):
    """Signature for analyzing and resolving tool dependencies."""

    task_list: str = InputField(description="List of tasks with dependencies")
    capability_constraints: str = InputField(
        description="Server capability constraints"
    )

    dependency_graph: str = OutputField(description="Complete dependency graph")
    execution_order: str = OutputField(description="Optimal execution order")
    critical_path: str = OutputField(description="Critical path through dependencies")
    parallel_opportunities: str = OutputField(
        description="Tasks that can run in parallel"
    )


class ExecutionCoordinationSignature(Signature):
    """Signature for coordinating multi-server execution."""

    execution_plan: str = InputField(description="Execution plan to coordinate")
    server_health_status: str = InputField(description="Current server health status")
    resource_availability: str = InputField(
        description="Available resources per server"
    )

    coordination_strategy: str = OutputField(description="Coordination approach")
    failover_plan: str = OutputField(description="Failover and retry strategy")
    monitoring_points: str = OutputField(description="Key monitoring checkpoints")
    optimization_recommendations: str = OutputField(
        description="Performance optimization suggestions"
    )


# ================================================================================
# SERVER REGISTRY
# ================================================================================


@dataclass
class ServerInfo:
    """Information about registered MCP server."""

    name: str
    transport: str
    url: Optional[str] = None
    command: Optional[str] = None
    args: Optional[List[str]] = None
    capabilities: List[str] = field(default_factory=list)
    tools: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    healthy: bool = True
    last_health_check: Optional[float] = None
    response_time_ms: float = 0.0


class MCPServerRegistry:
    """Registry for managing multiple MCP servers."""

    def __init__(self):
        """Initialize empty server registry."""
        self.servers: Dict[str, ServerInfo] = {}
        self.capability_map: Dict[str, List[str]] = {}
        self.tool_catalog: Dict[str, str] = {}
        self.service_registry: Optional[ServiceRegistry] = None

        if KAILASH_MCP_AVAILABLE:
            self.service_registry = ServiceRegistry()

    async def register_server(self, server_config: Dict[str, Any]) -> ServerInfo:
        """Register and discover capabilities of MCP server."""
        server_info = ServerInfo(
            name=server_config["name"],
            transport=server_config.get("transport", "http"),
            url=server_config.get("url"),
            command=server_config.get("command"),
            args=server_config.get("args"),
            capabilities=server_config.get("capabilities", []),
            tools=server_config.get("tools", []),
            metadata=server_config.get("metadata", {}),
        )

        # Register with Kailash ServiceRegistry
        if self.service_registry:
            await self.service_registry.register_service(
                service_id=server_info.name,
                service_type="mcp_server",
                endpoint=server_info.url
                or f"{server_info.command} {' '.join(server_info.args or [])}",
                metadata={
                    "capabilities": server_info.capabilities,
                    "tools": server_info.tools,
                    **server_info.metadata,
                },
            )

        # Build capability map
        for capability in server_info.capabilities:
            if capability not in self.capability_map:
                self.capability_map[capability] = []
            self.capability_map[capability].append(server_info.name)

        # Build tool catalog
        for tool in server_info.tools:
            self.tool_catalog[tool] = server_info.name

        self.servers[server_info.name] = server_info

        logger.info(
            f"Registered server: {server_info.name} with {len(server_info.tools)} tools"
        )
        return server_info

    async def health_check(self, server_name: str) -> bool:
        """Check health of specific server."""
        if server_name not in self.servers:
            return False

        server = self.servers[server_name]

        # Simulate health check (in production, this would ping the actual server)
        # Using Kailash's health checking mechanism
        current_time = time.time()
        server.last_health_check = current_time

        # For this example, assume servers are healthy
        # In production, you'd use actual health check via MCPClient
        server.healthy = True
        server.response_time_ms = 50.0 + (hash(server_name) % 100)

        return server.healthy

    async def discover_tools(self, server_name: str) -> List[str]:
        """Discover available tools from server."""
        if server_name in self.servers:
            return self.servers[server_name].tools
        return []

    def get_server_for_tool(self, tool_name: str) -> Optional[str]:
        """Get server that provides specific tool."""
        return self.tool_catalog.get(tool_name)

    def get_servers_with_capability(self, capability: str) -> List[str]:
        """Get all servers with specific capability."""
        return self.capability_map.get(capability, [])

    def get_all_servers(self) -> Dict[str, ServerInfo]:
        """Get all registered servers."""
        return self.servers

    async def health_check_all(self) -> Dict[str, bool]:
        """Health check all registered servers."""
        health_status = {}
        for server_name in self.servers:
            health_status[server_name] = await self.health_check(server_name)
        return health_status


# ================================================================================
# ORCHESTRATION ENGINE
# ================================================================================


class MCPOrchestrationEngine:
    """Engine for coordinating multi-server tool execution."""

    def __init__(self, registry: MCPServerRegistry):
        """Initialize orchestration engine with server registry."""
        self.registry = registry
        self.execution_history: List[Dict[str, Any]] = []
        self.service_mesh: Optional[ServiceMesh] = None
        self.mcp_clients: Dict[str, MCPClient] = {}

        if KAILASH_MCP_AVAILABLE:
            self.service_mesh = ServiceMesh(registry=registry.service_registry)

    async def execute_workflow(self, tasks: List[WorkflowTask]) -> Dict[str, Any]:
        """Execute complex workflow across multiple servers."""
        start_time = time.time()

        # 1. Create execution plan
        plan = await self._create_execution_plan(tasks)

        # 2. Resolve dependencies
        resolved_tasks = self._resolve_dependencies(tasks, plan)

        # 3. Coordinate execution
        results = await self._coordinate_execution(resolved_tasks, plan)

        # 4. Calculate metrics
        end_time = time.time()
        metrics = self._calculate_metrics(tasks, plan, start_time, end_time)

        # 5. Store in history
        self.execution_history.append(
            {
                "timestamp": datetime.now().isoformat(),
                "tasks": len(tasks),
                "duration": end_time - start_time,
                "success_rate": metrics["success_rate"],
                "results": results,
            }
        )

        return {
            "success": True,
            "results": results,
            "metrics": metrics,
            "execution_plan": plan,
        }

    async def _create_execution_plan(self, tasks: List[WorkflowTask]) -> ExecutionPlan:
        """Create optimized execution plan with parallelization."""
        plan = ExecutionPlan()

        # Build dependency levels
        task_dict = {task.task_id: task for task in tasks}
        dependency_levels = self._analyze_task_dependencies(tasks)
        plan.dependency_levels = dependency_levels

        # Group tasks by dependency level for sequential execution
        levels: Dict[int, List[str]] = {}
        for task_id, level in dependency_levels.items():
            if level not in levels:
                levels[level] = []
            levels[level].append(task_id)

        # Each level can be executed in parallel, levels are sequential
        plan.sequential_groups = [levels.get(i, []) for i in sorted(levels.keys())]

        # Estimate duration (simplified)
        plan.estimated_duration = (
            len(plan.sequential_groups) * 2.0
        )  # 2 seconds per level

        logger.info(
            f"Created execution plan: {len(plan.sequential_groups)} sequential groups"
        )
        return plan

    def _analyze_task_dependencies(self, tasks: List[WorkflowTask]) -> Dict[str, int]:
        """Analyze task dependencies and assign execution levels."""
        task_dict = {task.task_id: task for task in tasks}
        levels = {}

        def get_level(task_id: str) -> int:
            if task_id in levels:
                return levels[task_id]

            task = task_dict.get(task_id)
            if not task or not task.dependencies:
                levels[task_id] = 0
                return 0

            max_dep_level = -1
            for dep_id in task.dependencies:
                if dep_id in task_dict:
                    dep_level = get_level(dep_id)
                    max_dep_level = max(max_dep_level, dep_level)

            levels[task_id] = max_dep_level + 1
            return levels[task_id]

        for task in tasks:
            get_level(task.task_id)

        return levels

    def _resolve_dependencies(
        self, tasks: List[WorkflowTask], plan: ExecutionPlan
    ) -> List[WorkflowTask]:
        """Validate and resolve task dependencies."""
        task_ids = {task.task_id for task in tasks}

        for task in tasks:
            # Check all dependencies exist
            for dep in task.dependencies:
                if dep not in task_ids:
                    logger.warning(f"Task {task.task_id} has missing dependency: {dep}")
                    task.status = TaskStatus.SKIPPED

        return tasks

    async def _coordinate_execution(
        self, tasks: List[WorkflowTask], plan: ExecutionPlan
    ) -> List[Dict[str, Any]]:
        """Coordinate parallel and sequential task execution."""
        task_dict = {task.task_id: task for task in tasks}
        results = []

        # Execute each sequential group (dependency level)
        for group_index, group_task_ids in enumerate(plan.sequential_groups):
            logger.info(
                f"Executing group {group_index + 1}/{len(plan.sequential_groups)} with {len(group_task_ids)} tasks"
            )

            # Execute all tasks in this group in parallel
            group_tasks = [task_dict[tid] for tid in group_task_ids if tid in task_dict]
            group_results = await self._execute_parallel_tasks(group_tasks)
            results.extend(group_results)

        return results

    async def _execute_parallel_tasks(
        self, tasks: List[WorkflowTask]
    ) -> List[Dict[str, Any]]:
        """Execute multiple tasks in parallel."""
        if not tasks:
            return []

        # Create coroutines for parallel execution
        coroutines = [self._execute_single_task(task) for task in tasks]

        # Execute in parallel
        results = await asyncio.gather(*coroutines, return_exceptions=True)

        # Process results
        processed_results = []
        for task, result in zip(tasks, results):
            if isinstance(result, Exception):
                processed_results.append(
                    {"task_id": task.task_id, "success": False, "error": str(result)}
                )
            else:
                processed_results.append(result)

        return processed_results

    async def _execute_single_task(self, task: WorkflowTask) -> Dict[str, Any]:
        """Execute single task with retry logic."""
        task.status = TaskStatus.RUNNING
        task.start_time = time.time()

        for attempt in range(task.max_retries):
            try:
                # Get server for this task
                server = self.registry.servers.get(task.server)
                if not server or not server.healthy:
                    raise RuntimeError(f"Server {task.server} not available")

                # Simulate tool execution (in production, use MCPClient)
                await asyncio.sleep(0.1)  # Simulate network latency

                # Mock successful execution
                result = {
                    "tool": task.tool,
                    "server": task.server,
                    "output": f"Result from {task.tool} on {task.server}",
                    "arguments": task.arguments,
                }

                task.status = TaskStatus.COMPLETED
                task.result = result
                task.end_time = time.time()

                logger.info(
                    f"Task {task.task_id} completed in {task.execution_time:.2f}s"
                )

                return {
                    "task_id": task.task_id,
                    "success": True,
                    "result": result,
                    "execution_time": task.execution_time,
                }

            except Exception as e:
                task.retry_count += 1
                logger.warning(
                    f"Task {task.task_id} failed (attempt {task.retry_count}/{task.max_retries}): {e}"
                )

                if task.retry_count >= task.max_retries:
                    task.status = TaskStatus.FAILED
                    task.error = str(e)
                    task.end_time = time.time()

                    return {
                        "task_id": task.task_id,
                        "success": False,
                        "error": str(e),
                        "retry_count": task.retry_count,
                    }

                # Exponential backoff
                await asyncio.sleep(2**attempt)

        # Should not reach here
        return {
            "task_id": task.task_id,
            "success": False,
            "error": "Max retries exceeded",
        }

    def _calculate_metrics(
        self,
        tasks: List[WorkflowTask],
        plan: ExecutionPlan,
        start_time: float,
        end_time: float,
    ) -> Dict[str, Any]:
        """Calculate comprehensive orchestration metrics."""
        completed = sum(1 for t in tasks if t.status == TaskStatus.COMPLETED)
        failed = sum(1 for t in tasks if t.status == TaskStatus.FAILED)
        total_retries = sum(t.retry_count for t in tasks)

        # Calculate parallel efficiency
        parallel_efficiency = []
        for group in plan.sequential_groups:
            if len(group) > 1:
                # Parallel efficiency = speedup / ideal_speedup
                # For simplicity, assume ideal case
                efficiency = min(1.0, len(group) / max(len(group), 1))
                parallel_efficiency.append(efficiency)

        return {
            "total_tasks": len(tasks),
            "completed_tasks": completed,
            "failed_tasks": failed,
            "success_rate": completed / len(tasks) if tasks else 0,
            "total_execution_time": end_time - start_time,
            "average_task_time": (
                sum(t.execution_time or 0 for t in tasks) / len(tasks) if tasks else 0
            ),
            "total_retries": total_retries,
            "parallel_groups": len(plan.sequential_groups),
            "parallel_efficiency": parallel_efficiency,
            "dependency_levels": len(set(plan.dependency_levels.values())),
        }

    def get_orchestration_metrics(self) -> Dict[str, Any]:
        """Get comprehensive orchestration metrics across all workflows."""
        if not self.execution_history:
            return {"total_workflows": 0}

        total_workflows = len(self.execution_history)
        successful_workflows = sum(
            1 for h in self.execution_history if h.get("success_rate", 0) > 0.95
        )

        total_tasks = sum(h.get("tasks", 0) for h in self.execution_history)
        avg_duration = (
            sum(h.get("duration", 0) for h in self.execution_history) / total_workflows
        )

        return {
            "total_workflows": total_workflows,
            "successful_workflows": successful_workflows,
            "total_tasks": total_tasks,
            "average_workflow_duration": avg_duration,
            "workflow_success_rate": (
                successful_workflows / total_workflows if total_workflows else 0
            ),
        }


# ================================================================================
# CONFIGURATION
# ================================================================================


@dataclass
class MultiServerOrchestrationConfig(BaseAgentConfig):
    """Configuration for multi-server orchestration agent."""

    # Server configuration
    servers: List[Dict[str, Any]] = field(default_factory=list)

    # Execution settings
    parallel_execution: bool = True
    max_concurrent_tasks: int = 10
    task_timeout: float = 300.0

    # Resilience settings
    circuit_breaker_enabled: bool = True
    max_retries: int = 3
    retry_delay: float = 1.0

    # Load balancing
    load_balancing_strategy: str = "round_robin"
    health_check_interval: float = 30.0

    # Monitoring
    enable_metrics: bool = True
    enable_audit_trail: bool = True
    log_level: str = "INFO"


# ================================================================================
# ORCHESTRATION AGENT
# ================================================================================


class MultiServerOrchestrationAgent(BaseAgent):
    """Agent for orchestrating complex workflows across multiple MCP servers."""

    def __init__(self, config: MultiServerOrchestrationConfig):
        """Initialize multi-server orchestration agent."""
        super().__init__(config=config, signature=WorkflowPlanningSignature())

        self.registry = MCPServerRegistry()
        self.engine = MCPOrchestrationEngine(self.registry)
        self.initialized = False

    async def initialize_servers(self) -> Dict[str, Any]:
        """Initialize and register all MCP servers."""
        logger.info(f"Initializing {len(self.config.servers)} MCP servers...")

        registered_servers = []
        for server_config in self.config.servers:
            try:
                server_info = await self.registry.register_server(server_config)
                registered_servers.append(server_info.name)
            except Exception as e:
                logger.error(
                    f"Failed to register server {server_config.get('name')}: {e}"
                )

        # Perform initial health check
        health_status = await self.registry.health_check_all()

        self.initialized = True

        initialization_result = {
            "registered_servers": registered_servers,
            "health_status": health_status,
            "total_tools": len(self.registry.tool_catalog),
            "capability_map": self.registry.capability_map,
        }

        # Store in shared memory
        self.write_to_memory(
            content=initialization_result,
            tags=["initialization", "servers"],
            importance=0.9,
        )

        logger.info(
            f"Initialized {len(registered_servers)} servers with {len(self.registry.tool_catalog)} tools"
        )

        return initialization_result

    async def orchestrate_workflow(
        self, workflow_spec: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Orchestrate complex multi-server workflow."""
        if not self.initialized:
            await self.initialize_servers()

        # Convert workflow spec to WorkflowTask objects
        tasks = []
        for spec in workflow_spec:
            task = WorkflowTask(
                task_id=spec["task_id"],
                server=spec["server"],
                tool=spec["tool"],
                arguments=spec.get("arguments", {}),
                dependencies=spec.get("dependencies", []),
                max_retries=self.config.max_retries,
            )
            tasks.append(task)

        logger.info(f"Orchestrating workflow with {len(tasks)} tasks")

        # Execute workflow
        result = await self.engine.execute_workflow(tasks)

        # Store execution in shared memory
        self.write_to_memory(
            content={
                "workflow_size": len(tasks),
                "success_rate": result["metrics"]["success_rate"],
                "execution_time": result["metrics"]["total_execution_time"],
            },
            tags=["workflow", "execution"],
            importance=0.8,
        )

        return result

    def get_orchestration_metrics(self) -> Dict[str, Any]:
        """Get comprehensive orchestration metrics."""
        return self.engine.get_orchestration_metrics()

    def get_server_status(self) -> Dict[str, Any]:
        """Get current status of all servers."""
        servers = self.registry.get_all_servers()

        status = {}
        for name, info in servers.items():
            status[name] = {
                "healthy": info.healthy,
                "tools": len(info.tools),
                "capabilities": info.capabilities,
                "response_time_ms": info.response_time_ms,
                "last_health_check": info.last_health_check,
            }

        return status


# ================================================================================
# EXAMPLE WORKFLOWS
# ================================================================================


async def example_data_processing_pipeline():
    """Example: Data processing pipeline with ML training."""
    print("\n" + "=" * 80)
    print("Example 1: Data Processing Pipeline")
    print("=" * 80 + "\n")

    # Configure servers
    servers = [
        {
            "name": "data-server",
            "transport": "http",
            "url": "http://localhost:8001",
            "capabilities": ["database", "analytics"],
            "tools": ["sql_query", "table_schema", "data_export", "analytics_query"],
        },
        {
            "name": "compute-server",
            "transport": "http",
            "url": "http://compute.internal:8002",
            "capabilities": ["compute", "ml"],
            "tools": ["train_model", "predict", "data_processing", "gpu_status"],
        },
        {
            "name": "storage-server",
            "transport": "http",
            "url": "http://storage.internal:8003",
            "capabilities": ["storage", "backup"],
            "tools": ["upload_file", "download_file", "list_files", "backup_data"],
        },
        {
            "name": "security-server",
            "transport": "http",
            "url": "http://security.internal:8005",
            "capabilities": ["security", "audit"],
            "tools": ["authenticate", "authorize", "encrypt", "audit_log"],
        },
        {
            "name": "notification-server",
            "transport": "http",
            "url": "http://notify.internal:8006",
            "capabilities": ["notification", "messaging"],
            "tools": ["send_email", "send_slack", "create_alert", "dashboard_update"],
        },
    ]

    # Create agent
    config = MultiServerOrchestrationConfig(servers=servers)
    agent = MultiServerOrchestrationAgent(config)

    # Initialize
    init_result = await agent.initialize_servers()
    print(f"‚úÖ Initialized {len(init_result['registered_servers'])} servers")
    print(f"   Total tools available: {init_result['total_tools']}")

    # Define workflow: Extract ‚Üí Store ‚Üí Process ‚Üí Train ‚Üí Encrypt ‚Üí Notify
    workflow = [
        {
            "task_id": "extract_data",
            "server": "data-server",
            "tool": "sql_query",
            "arguments": {"query": "SELECT * FROM customer_transactions"},
            "dependencies": [],
        },
        {
            "task_id": "audit_access",
            "server": "security-server",
            "tool": "audit_log",
            "arguments": {"action": "data_access", "resource": "customer_transactions"},
            "dependencies": [],
        },
        {
            "task_id": "store_data",
            "server": "storage-server",
            "tool": "upload_file",
            "arguments": {"file": "customer_data.parquet"},
            "dependencies": ["extract_data"],
        },
        {
            "task_id": "process_data",
            "server": "compute-server",
            "tool": "data_processing",
            "arguments": {"operations": ["clean", "normalize"]},
            "dependencies": ["store_data"],
        },
        {
            "task_id": "train_model",
            "server": "compute-server",
            "tool": "train_model",
            "arguments": {"algorithm": "collaborative_filtering"},
            "dependencies": ["process_data"],
        },
        {
            "task_id": "encrypt_model",
            "server": "security-server",
            "tool": "encrypt",
            "arguments": {"target": "model_v2.1"},
            "dependencies": ["train_model"],
        },
        {
            "task_id": "notify_completion",
            "server": "notification-server",
            "tool": "send_slack",
            "arguments": {"channel": "#ml-team", "message": "Model training complete"},
            "dependencies": ["encrypt_model"],
        },
    ]

    # Execute workflow
    print(f"\nüöÄ Executing workflow with {len(workflow)} tasks...")
    result = await agent.orchestrate_workflow(workflow)

    # Display results
    print("\nüìä Workflow Results:")
    print(f"   Success rate: {result['metrics']['success_rate']*100:.1f}%")
    print(f"   Total execution time: {result['metrics']['total_execution_time']:.2f}s")
    print(f"   Average task time: {result['metrics']['average_task_time']:.2f}s")
    print(f"   Dependency levels: {result['metrics']['dependency_levels']}")
    print(f"   Parallel groups: {result['metrics']['parallel_groups']}")

    if result["metrics"]["parallel_efficiency"]:
        avg_efficiency = sum(result["metrics"]["parallel_efficiency"]) / len(
            result["metrics"]["parallel_efficiency"]
        )
        print(f"   Parallel efficiency: {avg_efficiency*100:.1f}%")

    print("\n‚úÖ Example 1 complete")


async def example_ml_training_workflow():
    """Example: Parallel ML training workflow."""
    print("\n" + "=" * 80)
    print("Example 2: Parallel ML Training Workflow")
    print("=" * 80 + "\n")

    # Configure servers
    servers = [
        {
            "name": "data-server",
            "transport": "http",
            "url": "http://localhost:8001",
            "capabilities": ["database"],
            "tools": ["sql_query", "data_export"],
        },
        {
            "name": "compute-server",
            "transport": "http",
            "url": "http://compute.internal:8002",
            "capabilities": ["compute", "ml"],
            "tools": ["train_model", "data_processing"],
        },
        {
            "name": "notification-server",
            "transport": "http",
            "url": "http://notify.internal:8006",
            "capabilities": ["notification"],
            "tools": ["send_email"],
        },
    ]

    # Create agent
    config = MultiServerOrchestrationConfig(servers=servers)
    agent = MultiServerOrchestrationAgent(config)

    # Initialize
    await agent.initialize_servers()

    # Define workflow: Parallel data loading ‚Üí Parallel processing ‚Üí Parallel training ‚Üí Notify
    workflow = [
        {
            "task_id": "load_dataset_a",
            "server": "data-server",
            "tool": "sql_query",
            "arguments": {"query": "SELECT * FROM dataset_a"},
            "dependencies": [],
        },
        {
            "task_id": "load_dataset_b",
            "server": "data-server",
            "tool": "sql_query",
            "arguments": {"query": "SELECT * FROM dataset_b"},
            "dependencies": [],
        },
        {
            "task_id": "process_a",
            "server": "compute-server",
            "tool": "data_processing",
            "arguments": {"dataset": "a"},
            "dependencies": ["load_dataset_a"],
        },
        {
            "task_id": "process_b",
            "server": "compute-server",
            "tool": "data_processing",
            "arguments": {"dataset": "b"},
            "dependencies": ["load_dataset_b"],
        },
        {
            "task_id": "train_model_a",
            "server": "compute-server",
            "tool": "train_model",
            "arguments": {"model": "a"},
            "dependencies": ["process_a"],
        },
        {
            "task_id": "train_model_b",
            "server": "compute-server",
            "tool": "train_model",
            "arguments": {"model": "b"},
            "dependencies": ["process_b"],
        },
        {
            "task_id": "notify",
            "server": "notification-server",
            "tool": "send_email",
            "arguments": {"subject": "Training complete"},
            "dependencies": ["train_model_a", "train_model_b"],
        },
    ]

    # Execute
    result = await agent.orchestrate_workflow(workflow)

    print("‚úÖ Parallel ML workflow complete")
    print(f"   Parallel efficiency: {result['metrics'].get('parallel_efficiency', [])}")


async def example_complex_enterprise_workflow():
    """Example: Complex enterprise workflow with all 6 servers."""
    print("\n" + "=" * 80)
    print("Example 3: Complex Enterprise Workflow (All 6 Servers)")
    print("=" * 80 + "\n")

    # Configure all 6 servers
    servers = [
        {
            "name": "data-server",
            "transport": "http",
            "url": "http://localhost:8001",
            "capabilities": ["database", "analytics"],
            "tools": ["sql_query", "table_schema", "data_export", "analytics_query"],
        },
        {
            "name": "compute-server",
            "transport": "http",
            "url": "http://compute.internal:8002",
            "capabilities": ["compute", "ml"],
            "tools": ["train_model", "predict", "data_processing", "gpu_status"],
        },
        {
            "name": "storage-server",
            "transport": "http",
            "url": "http://storage.internal:8003",
            "capabilities": ["storage", "backup"],
            "tools": ["upload_file", "download_file", "list_files", "backup_data"],
        },
        {
            "name": "integration-server",
            "transport": "http",
            "url": "http://api.internal:8004",
            "capabilities": ["integration", "api"],
            "tools": ["rest_call", "webhook_setup", "api_key_mgmt", "rate_limit"],
        },
        {
            "name": "security-server",
            "transport": "http",
            "url": "http://security.internal:8005",
            "capabilities": ["security", "audit"],
            "tools": ["authenticate", "authorize", "encrypt", "audit_log"],
        },
        {
            "name": "notification-server",
            "transport": "http",
            "url": "http://notify.internal:8006",
            "capabilities": ["notification", "messaging"],
            "tools": ["send_email", "send_slack", "create_alert", "dashboard_update"],
        },
    ]

    # Create agent
    config = MultiServerOrchestrationConfig(servers=servers, parallel_execution=True)
    agent = MultiServerOrchestrationAgent(config)

    # Initialize
    init_result = await agent.initialize_servers()
    print(f"‚úÖ Initialized {len(init_result['registered_servers'])} servers")
    print(f"   Total tools: {init_result['total_tools']}")
    print(f"   Capabilities: {list(init_result['capability_map'].keys())}")

    # Define complex workflow using all 6 servers
    workflow = [
        # Stage 1: Authentication and data loading (parallel)
        {
            "task_id": "authenticate",
            "server": "security-server",
            "tool": "authenticate",
            "arguments": {"user": "system"},
            "dependencies": [],
        },
        {
            "task_id": "load_data",
            "server": "data-server",
            "tool": "sql_query",
            "arguments": {"query": "SELECT * FROM production_data"},
            "dependencies": [],
        },
        # Stage 2: Audit and storage (depends on stage 1)
        {
            "task_id": "audit_log",
            "server": "security-server",
            "tool": "audit_log",
            "arguments": {"action": "data_access"},
            "dependencies": ["authenticate", "load_data"],
        },
        {
            "task_id": "store_data",
            "server": "storage-server",
            "tool": "upload_file",
            "arguments": {"file": "production_data.parquet"},
            "dependencies": ["load_data"],
        },
        # Stage 3: Processing and integration setup (parallel)
        {
            "task_id": "process_data",
            "server": "compute-server",
            "tool": "data_processing",
            "arguments": {"operations": ["clean", "transform"]},
            "dependencies": ["store_data"],
        },
        {
            "task_id": "setup_webhook",
            "server": "integration-server",
            "tool": "webhook_setup",
            "arguments": {"endpoint": "/model/status"},
            "dependencies": ["audit_log"],
        },
        # Stage 4: Model training
        {
            "task_id": "train_model",
            "server": "compute-server",
            "tool": "train_model",
            "arguments": {"algorithm": "deep_learning"},
            "dependencies": ["process_data"],
        },
        # Stage 5: Encryption and deployment (parallel)
        {
            "task_id": "encrypt_model",
            "server": "security-server",
            "tool": "encrypt",
            "arguments": {"target": "model.onnx"},
            "dependencies": ["train_model"],
        },
        {
            "task_id": "deploy_api",
            "server": "integration-server",
            "tool": "rest_call",
            "arguments": {"endpoint": "/deploy", "method": "POST"},
            "dependencies": ["train_model", "setup_webhook"],
        },
        # Stage 6: Notifications (parallel)
        {
            "task_id": "send_email",
            "server": "notification-server",
            "tool": "send_email",
            "arguments": {"subject": "Model deployed"},
            "dependencies": ["encrypt_model", "deploy_api"],
        },
        {
            "task_id": "send_slack",
            "server": "notification-server",
            "tool": "send_slack",
            "arguments": {"channel": "#ops", "message": "Production model deployed"},
            "dependencies": ["encrypt_model", "deploy_api"],
        },
        {
            "task_id": "update_dashboard",
            "server": "notification-server",
            "tool": "dashboard_update",
            "arguments": {"metric": "deployment_status"},
            "dependencies": ["deploy_api"],
        },
    ]

    # Execute
    print(
        f"\nüöÄ Executing complex workflow with {len(workflow)} tasks across 6 servers..."
    )
    result = await agent.orchestrate_workflow(workflow)

    # Display detailed results
    print("\nüìä Complex Workflow Results:")
    print(f"   ‚úÖ Success rate: {result['metrics']['success_rate']*100:.1f}%")
    print(
        f"   ‚è±Ô∏è  Total execution time: {result['metrics']['total_execution_time']:.2f}s"
    )
    print(f"   üìà Dependency levels: {result['metrics']['dependency_levels']}")
    print(f"   üîÄ Parallel groups: {result['metrics']['parallel_groups']}")
    print(f"   üîÅ Total retries: {result['metrics']['total_retries']}")

    if result["metrics"]["parallel_efficiency"]:
        avg_efficiency = sum(result["metrics"]["parallel_efficiency"]) / len(
            result["metrics"]["parallel_efficiency"]
        )
        print(f"   ‚ö° Average parallel efficiency: {avg_efficiency*100:.1f}%")

    # Display server status
    server_status = agent.get_server_status()
    print("\nüñ•Ô∏è  Server Status:")
    for server_name, status in server_status.items():
        health_icon = "‚úÖ" if status["healthy"] else "‚ùå"
        print(
            f"   {health_icon} {server_name}: {status['tools']} tools, {status['response_time_ms']:.1f}ms"
        )

    # Display orchestration metrics
    metrics = agent.get_orchestration_metrics()
    print("\nüìà Orchestration Metrics:")
    print(f"   Total workflows executed: {metrics['total_workflows']}")
    print(f"   Workflow success rate: {metrics['workflow_success_rate']*100:.1f}%")

    print("\n‚úÖ Complex enterprise workflow complete")


# ================================================================================
# MAIN EXECUTION
# ================================================================================


async def main():
    """Run all multi-server orchestration examples."""
    print("\n" + "=" * 80)
    print("Multi-Server MCP Tool Orchestration Examples")
    print("Production Implementation using Kailash SDK")
    print("=" * 80)

    # Run examples
    await example_data_processing_pipeline()
    await example_ml_training_workflow()
    await example_complex_enterprise_workflow()

    print("\n" + "=" * 80)
    print("All examples complete!")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    if not KAILASH_MCP_AVAILABLE:
        print("‚ö†Ô∏è  Warning: kailash.mcp_server not available. Install with:")
        print("   pip install kailash")
        print("\nRunning in demonstration mode with simulated MCP infrastructure.\n")

    asyncio.run(main())
