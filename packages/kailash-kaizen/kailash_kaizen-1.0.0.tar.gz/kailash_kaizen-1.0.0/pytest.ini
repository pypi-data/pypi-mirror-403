[pytest]
pythonpath = src
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = strict
asyncio_default_fixture_loop_scope = function
env_files = .env
addopts =
    -v
    --tb=short
    --strict-markers
    --maxfail=10
    --durations=5
    --color=yes
filterwarnings =
    # Ignore known third-party library warnings
    ignore::DeprecationWarning:pydantic.*
    ignore::DeprecationWarning:langchain.*
    ignore::DeprecationWarning:httpx.*
    ignore::DeprecationWarning:httpcore.*
    ignore::DeprecationWarning:google.*
    ignore::DeprecationWarning:openai.*
    ignore::DeprecationWarning:anthropic.*
    ignore::PendingDeprecationWarning:pydantic.*
    # Ignore websockets library deprecation warnings
    ignore::DeprecationWarning:websockets.*
    ignore::DeprecationWarning:nexus.*
    # Ignore kailash SDK and DataFlow warnings (external libraries)
    ignore::DeprecationWarning:kailash.*
    ignore::DeprecationWarning:dataflow.*
    ignore:LocalRuntime.execute\(\) without context manager:DeprecationWarning
    ignore:datetime.datetime.utcnow\(\) is deprecated:DeprecationWarning
    # Ignore asyncio loop warnings in tests
    ignore:The loop argument is deprecated:DeprecationWarning
    ignore:There is no current event loop:DeprecationWarning
    # Ignore ResourceWarning from unclosed sockets (common in tests)
    ignore::ResourceWarning
    # Ignore numpy RuntimeWarnings about division (occurs in correlation calculations)
    ignore:invalid value encountered in divide:RuntimeWarning:numpy.*
    # Ignore RuntimeWarnings about unawaited coroutines (mock-related in tests)
    ignore:coroutine .* was never awaited:RuntimeWarning
    # Ignore pytest-asyncio internal warnings
    ignore::pytest.PytestUnraisableExceptionWarning
    # Ignore pytest collection warnings for utility classes with Test prefix
    ignore::pytest.PytestCollectionWarning
    # Ignore internal kaizen deprecation warnings during tests
    ignore::DeprecationWarning:kaizen.*
    # Ignore deprecation warnings from kaizen modules (by message)
    ignore:kaizen\..* is deprecated:DeprecationWarning
    # Ignore internal kaizen UserWarnings about API usage patterns during tests
    ignore:Instance-based API usage detected:UserWarning:kaizen.*
    # Ignore max_tokens deprecation warnings
    ignore:'max_tokens' is deprecated:DeprecationWarning
    # Ignore websockets library specific deprecation messages
    ignore:websockets.WebSocketServerProtocol is deprecated:DeprecationWarning
    ignore:websockets.legacy is deprecated:DeprecationWarning
markers =
    unit: Unit tests (fast, isolated, can use mocks) - Tier 1
    integration: Integration tests (real services, no mocking) - Tier 2
    integration_llm: Integration tests using real LLM providers
    e2e: End-to-end tests (complete workflows, real infrastructure) - Tier 3
    tier3: Tier 3 end-to-end tests (alias for e2e)
    performance: Performance benchmark tests
    slow: Slow running tests (timeout > 5s)
    load: Load tests (stress testing with real infrastructure)
    migration: Migration tests (version compatibility, database migrations)
    requires_postgres: Tests requiring PostgreSQL
    requires_redis: Tests requiring Redis
    requires_docker: Tests requiring Docker services
    requires_ollama: Tests requiring Ollama service
    requires_db: Tests requiring database (DataFlow)
    requires_llm: Tests requiring real LLM provider
    memory_intensive: Tests that use significant memory
    flaky: Tests that may occasionally fail (retry candidates)
    llm_execution: Tests that use real LLM inference (paid API calls)
    mcp: Tests for MCP (Model Context Protocol) integration
    server: Tests for server-related functionality
    real_llm: Tests that use real LLM providers (OpenAI, Anthropic, etc.)
    ollama_validation: Tests validating Ollama integration
    summary: Summary tests that aggregate results
    ollama: Tests using Ollama provider (free, local)
    openai: Tests using OpenAI provider (requires API key, incurs cost)
    anthropic: Tests using Anthropic Claude provider (requires API key, incurs cost)
    google: Tests using Google Gemini provider (requires API key, incurs cost)
    requires_google: Tests requiring Google Gemini credentials
    landing_ai: Tests using Landing AI provider (requires API key)
    cost: Tests that incur API costs (requires API keys)
    batch_processing: Batch document processing tests
    cost_optimization: Cost optimization scenario tests
    rag_workflow: RAG workflow integration tests
    observability: Tests for observability system (Systems 3-7)

# Timeout configurations by tier
timeout = 120
# Individual test timeouts managed by @pytest.mark.timeout decorators:
# - Unit tests: @pytest.mark.timeout(1) - 1 second max
# - Integration tests: @pytest.mark.timeout(5) - 5 seconds max
# - E2E tests: @pytest.mark.timeout(10) - 10 seconds max

# Performance thresholds (validated in conftest.py)
# Unit: <1000ms, Integration: <5000ms, E2E: <10000ms
