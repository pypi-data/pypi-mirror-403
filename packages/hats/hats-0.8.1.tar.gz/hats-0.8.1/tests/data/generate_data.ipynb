{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test data\n",
    "\n",
    "This directory contains very small, toy, data sets that are used\n",
    "for unit tests.\n",
    "\n",
    "## Object catalog: small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values. \n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "The following are imports and paths that are used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T21:43:06.003910Z",
     "start_time": "2025-10-29T21:43:01.940332Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from hats_import import (\n",
    "    ImportArguments,\n",
    "    pipeline_with_client,\n",
    "    IndexArguments,\n",
    "    CollectionArguments,\n",
    ")\n",
    "\n",
    "import hats\n",
    "from hats.catalog import PartitionInfo, TableProperties\n",
    "from hats.io.file_io import remove_directory\n",
    "from hats.pixel_math import HealpixPixel\n",
    "import lsdb\n",
    "from hats.pixel_math.spatial_index import healpix_to_spatial_index\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "## Assumes you also have a working local branch of hats-import\n",
    "hats_import_dir = \"../../../hats-import/tests/data/\"\n",
    "client = Client(n_workers=1, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values. \n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        output_artifact_name=\"small_sky\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_order1\n",
    "\n",
    "This catalog has the same data points as other small sky catalogs,\n",
    "but is coerced to spreading these data points over partitions at order 1, instead\n",
    "of order 0.\n",
    "\n",
    "This means there are 4 leaf partition files, instead of just 1, and so can\n",
    "be useful for confirming reads/writes over multiple leaf partition files.\n",
    "\n",
    "NB: Setting `constant_healpix_order` coerces the import pipeline to create\n",
    "leaf partitions at order 1.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T21:43:12.775463Z",
     "start_time": "2025-10-29T21:43:07.583039Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_o1_collection\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = (\n",
    "        CollectionArguments(\n",
    "            output_artifact_name=\"small_sky_o1_collection\",\n",
    "            output_path=\".\",\n",
    "            tmp_dir=pipeline_tmp,\n",
    "            addl_hats_properties={\"obs_regime\": \"Optical\", \"default_index\": \"id\"},\n",
    "        )\n",
    "        .catalog(\n",
    "            input_path=Path(hats_import_dir) / \"small_sky\",\n",
    "            file_reader=\"csv\",\n",
    "            output_artifact_name=\"small_sky_order1\",\n",
    "            constant_healpix_order=1,\n",
    "        )\n",
    "        .add_margin(margin_threshold=7200, output_artifact_name=\"small_sky_order1_margin\", is_default=True)\n",
    "        .add_margin(\n",
    "            margin_threshold=10, output_artifact_name=\"small_sky_order1_margin_10arcs\", is_default=False\n",
    "        )\n",
    "        .add_index(\n",
    "            indexing_column=\"id\",\n",
    "            output_artifact_name=\"small_sky_order1_id_index\",\n",
    "            include_healpix_29=False,\n",
    "            compute_partition_size=200_000,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_to_small_sky_order1\n",
    "\n",
    "Association table that maps (pretty naively) the `small_sky` to `small_sky_order1`. Note that these are the *same* catalog data, but the stored pixels are at different healpix orders.\n",
    "\n",
    "Note also that this doesn't really create a catalog! This is faking out a \"soft\" association catalog, which just contains the partition join information, and not the actual matching rows. It's not generated by any \"import pipeline\", but just through writing the files directly.\n",
    "\n",
    "This catalog was generated using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_catalog_name = \"small_sky_to_small_sky_order1\"\n",
    "\n",
    "remove_directory(out_catalog_name, ignore_errors=True)\n",
    "Path(out_catalog_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "partition_info = PartitionInfo.from_healpix([HealpixPixel(1, p) for p in np.arange(44, 48)])\n",
    "partition_info.write_to_file(f\"{out_catalog_name}/partition_info.csv\")\n",
    "\n",
    "properties = TableProperties(\n",
    "    catalog_name=out_catalog_name,\n",
    "    catalog_type=\"association\",\n",
    "    total_rows=131,\n",
    "    hats_primary_table_url=\"small_sky\",\n",
    "    hats_col_assn_primary=\"id\",\n",
    "    hats_col_assn_primary_assn=\"id_small_sky\",\n",
    "    hats_assn_join_table_url=\"small_sky_order1\",\n",
    "    hats_col_assn_join=\"id\",\n",
    "    hats_col_assn_join_assn=\"id_small_sky_order1\",\n",
    "    hats_assn_leaf_files=False,\n",
    "    assn_max_separation=0,\n",
    ").to_properties_file(out_catalog_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_npix_alt_suffix\n",
    "\n",
    "Copies small_sky but changes the parquet file suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hats does not constrain the suffix,\n",
    "# but the suffix should make the file recognizable as parquet for compatibility with other libraries.\n",
    "npix_suffix = \".parq\"  # could also include the compression, e.g., \".snappy.parquet\"\n",
    "\n",
    "sso = hats.read_hats(\"small_sky\")\n",
    "paths = [hats.io.paths.pixel_catalog_file(sso.catalog_base_dir, pixel) for pixel in sso.get_healpix_pixels()]\n",
    "\n",
    "out_catalog_name = \"small_sky_npix_alt_suffix\"\n",
    "out_catalog_info = sso.catalog_info.copy_and_update(catalog_name=out_catalog_name, npix_suffix=npix_suffix)\n",
    "out_paths = [\n",
    "    hats.io.paths.pixel_catalog_file(out_catalog_name, pixel, npix_suffix=npix_suffix)\n",
    "    for pixel in sso.get_healpix_pixels()\n",
    "]\n",
    "\n",
    "for path, out_path in zip(paths, out_paths):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(path, out_path)\n",
    "hats.io.write_parquet_metadata(out_catalog_name)\n",
    "out_catalog_info.to_properties_file(out_catalog_name)\n",
    "sso.partition_info.write_to_file(hats.io.paths.get_partition_info_pointer(out_catalog_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_npix_as_dir\n",
    "\n",
    "Copies small_sky but makes Npix a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import hats\n",
    "\n",
    "npix_suffix = \"/\"\n",
    "\n",
    "sso = hats.read_hats(\"small_sky\")\n",
    "paths = [hats.io.paths.pixel_catalog_file(sso.catalog_base_dir, pixel) for pixel in sso.get_healpix_pixels()]\n",
    "\n",
    "out_catalog_name = \"small_sky_npix_as_dir\"\n",
    "out_catalog_info = sso.catalog_info.copy_and_update(catalog_name=out_catalog_name, npix_suffix=npix_suffix)\n",
    "out_dirs = [\n",
    "    hats.io.paths.pixel_catalog_file(out_catalog_name, pixel, npix_suffix=npix_suffix)\n",
    "    for pixel in sso.get_healpix_pixels()\n",
    "]\n",
    "\n",
    "for path, out_dir in zip(paths, out_dirs):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # hats will only care about `out_dir`. It will be agnostic to filenames, given `npix_suffix = \"/\"`.\n",
    "    shutil.copy(path, out_dir / \"part0.parquet\")\n",
    "hats.io.write_parquet_metadata(out_catalog_name)\n",
    "out_catalog_info.to_properties_file(out_catalog_name)\n",
    "sso.partition_info.write_to_file(hats.io.paths.get_partition_info_pointer(out_catalog_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source catalog: small_sky_source\n",
    "\n",
    "This \"source catalog\" is 131 detections at each of the 131 objects\n",
    "in the \"small_sky\" catalog. These have a random magnitude, MJD, and \n",
    "band (selected from ugrizy). The full script that generated the values\n",
    "can be found [here](https://github.com/delucchi-cmu/hipscripts/blob/main/twiddling/small_sky_source.py)\n",
    "\n",
    "The catalog was generated with the following snippet, using raw data \n",
    "from the `hats-import` file.\n",
    "\n",
    "NB: `pixel_threshold=3000` is set just to make sure that we're generating\n",
    "a handful of files at various healpix orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_source\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky_source\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        ra_column=\"source_ra\",\n",
    "        dec_column=\"source_dec\",\n",
    "        catalog_type=\"source\",\n",
    "        pixel_threshold=3000,\n",
    "        row_group_kwargs={\"num_rows\": 1_000},\n",
    "        highest_healpix_order=6,\n",
    "        drop_empty_siblings=False,\n",
    "        output_artifact_name=\"small_sky_source\",\n",
    "        skymap_alt_orders=[2, 4],\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_source_object_index\n",
    "\n",
    "This catalog exists as an index of the SOURCE table, using the OBJECT ID\n",
    "as the indexed column. This means you should be able to quickly find\n",
    "partions of SOURCES for a given OBJECT ID.\n",
    "\n",
    "NB: \n",
    "\n",
    "- Setting `compute_partition_size` to something less than `1_000_000` \n",
    "  coerces the import pipeline to create smaller result partitions, \n",
    "  and so we have three distinct index partitions.\n",
    "- Setting `include_healpix_29=False` keeps us from needing a row for every \n",
    "  source and lets the indexing pipeline create only one row per \n",
    "  unique objectId/Norder/Npix\n",
    "\n",
    "This catalog was generated using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_source_object_index\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = IndexArguments(\n",
    "        input_catalog_path=\"small_sky_source\",\n",
    "        indexing_column=\"object_id\",\n",
    "        output_path=\".\",\n",
    "        output_artifact_name=\"small_sky_source_object_index\",\n",
    "        include_healpix_29=False,\n",
    "        compute_partition_size=200_000,\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapCatalog - square map\n",
    "\n",
    "Silly little map catalog that contains the count of number of stars in each pixel (the count is just the square of the pixel number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pixels = np.arange(0, 12)\n",
    "\n",
    "healpix_29 = healpix_to_spatial_index(0, target_pixels)\n",
    "\n",
    "square_vals = target_pixels * target_pixels\n",
    "value_frame = pd.DataFrame({\"_healpix_29\": healpix_29, \"star_count\": square_vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./square_map\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    csv_file = Path(pipeline_tmp) / \"square_map.csv\"\n",
    "    value_frame.to_csv(csv_file, index=False)\n",
    "    args = ImportArguments(\n",
    "        constant_healpix_order=1,\n",
    "        catalog_type=\"map\",\n",
    "        use_healpix_29=True,\n",
    "        ra_column=None,\n",
    "        dec_column=None,\n",
    "        file_reader=\"csv\",\n",
    "        input_file_list=[csv_file],\n",
    "        output_artifact_name=\"square_map\",\n",
    "        output_path=\".\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested catalog: small_sky_nested\n",
    "\n",
    "Nests light curves from `small_sky_source` into `small_sky_order1` object catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_nested\")\n",
    "\n",
    "small_sky_object = lsdb.read_hats(\"small_sky_o1_collection\")\n",
    "small_sky_source = lsdb.read_hats(\"small_sky_source\")\n",
    "small_sky_nested = small_sky_object.join_nested(\n",
    "    small_sky_source, left_on=\"id\", right_on=\"object_id\", nested_column_name=\"lc\"\n",
    ")\n",
    "small_sky_nested = small_sky_nested.map_partitions(\n",
    "    lambda df, p: df.assign(Norder=p.order, Npix=p.pixel, Dir=p.pixel // 10000), include_pixel=True\n",
    ")\n",
    "lsdb.io.to_hats(\n",
    "    small_sky_nested,\n",
    "    base_catalog_path=\"small_sky_nested\",\n",
    "    catalog_name=\"small_sky_nested\",\n",
    "    histogram_order=5,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_healpix13\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_file_list=Path(hats_import_dir) / \"test_formats\" / \"small_sky_healpix13.csv\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        pixel_threshold=3000,\n",
    "        row_group_kwargs={\"num_rows\": 1_000},\n",
    "        highest_healpix_order=2,\n",
    "        output_artifact_name=\"small_sky_healpix13\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "        add_healpix_29=False,\n",
    "        addl_hats_properties={\"hats_col_healpix\": \"healpix13\", \"hats_col_healpix_order\": 13},\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "tmp_path.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
