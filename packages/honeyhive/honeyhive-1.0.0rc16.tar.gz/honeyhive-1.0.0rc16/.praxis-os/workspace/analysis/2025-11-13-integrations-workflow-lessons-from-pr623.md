# Critical Lessons from PR #623: Integrations Workflow Design

**Date:** 2025-11-13  
**Context:** Designing `integrations_workflow_v1` for testing new instrumentor support  
**PR Reference:** https://github.com/honeyhiveai/hive-kube/pull/623  
**Related Analysis:** `2025-11-13-honeyhive-event-schema-frontend-usage.md`

---

## üéØ Executive Summary

**Problem:** PR #623 revealed that **fixture-driven testing** of new instrumentors led to incorrect expectations and customer-blocking bugs because fixtures were created **manually** without running them through the actual ingestion pipeline.

**Solution Needed:** An **integrations workflow** that:
1. Captures real instrumentor spans
2. Runs them through ingestion service
3. Validates against expected patterns
4. Creates fixtures FROM validated results (not manual guessing)

**Key Insight:** The event schema is **flexible by design** (`.passthrough()`), but the **attribute router** must be **explicitly programmed** to handle each instrumentor's patterns. Testing must validate the FULL pipeline: Instrumentor ‚Üí Ingestion ‚Üí Schema ‚Üí Frontend.

---

## üìä What PR #623 Fixed

### Summary

**Title:** "fix(ingestion): Support instrumentor edge cases for OpenInference, Traceloop, OpenLIT"

**Key Changes:**
1. **Fixed tool span detection** - Distinguish TOOL spans from LLM tool events
2. **Preserved `outputs.message`** for tool spans (don't flatten to role/content)
3. **Added indexed message parsing** for Traceloop (`gen_ai.prompt.0.content`)
4. **Added OpenLIT metadata routing** (sdk_version, instrumentor, server_*)
5. **Fixed OpenInference tool fixture** expectations

**Result:** 
- ‚úÖ All 1203 tests passing (29 failures ‚Üí 0)
- ‚úÖ Customer-reported bugs fixed
- ‚úÖ 422 lines added, 160 lines removed

---

## üî¥ Critical Mistakes Made (Lessons for Workflow)

### Mistake #1: Manual Fixture Creation Without Validation

**What Happened:**
- Created `openinference_google_adk_unknown_tool_001.json` fixture manually
- Guessed that tool inputs should be wrapped in `chat_history`
- **Reality:** Tool spans should have direct parameters, NOT chat messages

**Fixture (WRONG):**
```json
{
  "expected": {
    "inputs": {
      "chat_history": [
        {
          "role": "user",
          "content": "{\"city\": \"New York\"}"
        }
      ]
    }
  }
}
```

**After Fix (CORRECT):**
```json
{
  "expected": {
    "inputs": {
      "city": "New York"  // Direct tool arguments!
    }
  }
}
```

**Lesson:** üéì **NEVER manually create fixture expectations. Capture real ingestion output.**

---

### Mistake #2: Assuming Universal Flattening

**What Happened:**
- Ingestion service flattened `outputs.message` ‚Üí `{role, content}` for ALL events
- **Reality:** TOOL spans need `message` intact, only MODEL events get flattened

**Code Before (WRONG):**
```typescript
// Universal flattening: Convert outputs.message ‚Üí {role, content} for all model events
if (outputs.message && typeof outputs.message === 'string' && !outputs.content) {
  outputs.role = 'assistant';
  outputs.content = outputs.message;
  delete outputs.message;
}
```

**Code After (CORRECT):**
```typescript
// Universal flattening: Convert outputs.message ‚Üí {role, content} for MODEL events
// BUT: Skip this for TOOL events - they should keep outputs.message intact
const isToolSpan = attributes['openinference.span.kind'] === 'TOOL' || eventType === 'tool';
if (outputs.message && typeof outputs.message === 'string' && !outputs.content && !isToolSpan) {
  outputs.role = 'assistant';
  outputs.content = outputs.message;
  delete outputs.message;
}
```

**Lesson:** üéì **Event type matters! Different span kinds need different processing logic.**

---

### Mistake #3: Missing Instrumentor-Specific Parsing

**What Happened:**
- Traceloop uses indexed attributes (`gen_ai.prompt.0.content`)
- OpenLIT uses flat attributes (`gen_ai.prompt`)
- Ingestion service only handled OpenInference/flat formats
- **Result:** Traceloop spans had no inputs/outputs!

**Code Added:**
```typescript
/**
 * Extract indexed gen_ai.prompt.* attributes into chat_history array
 * Traceloop/OpenLLMetry uses indexed gen_ai.prompt attributes:
 * - gen_ai.prompt.0.role = "user"
 * - gen_ai.prompt.0.content = "[{\"type\": \"text\", \"text\": \"...\"}]"
 */
function extractFromIndexedPrompt(attributes: Record<string, any>):
  { chat_history: Array<{ role: string; content: string }>; consumedKeys: Set<string> } | null {
  // Filter keys matching pattern: gen_ai.prompt.<index>.*
  const indexedKeys = Object.keys(attributes).filter((key) => /^gen_ai\.prompt\.\d+\./.test(key));
  
  if (indexedKeys.length === 0) {
    return null;
  }
  
  // Group attributes by message index
  const messagesByIndex = new Map<number, Record<string, any>>();
  // ... parsing logic ...
  
  return { chat_history, consumedKeys };
}
```

**Lesson:** üéì **Each instrumentor has unique patterns. Ingestion must explicitly handle each one.**

---

### Mistake #4: Missing OpenLIT-Specific Attributes

**What Happened:**
- OpenLIT adds unique attributes:
  - `gen_ai.usage.cost` (automatic cost calculation)
  - `gen_ai.server.ttft` (time to first token)
  - `telemetry.sdk.name` (self-identification)
- Ingestion service had no routes for these
- **Result:** Critical metrics lost!

**Code Added:**
```typescript
// Special: Cost and timing metrics (go to metrics bucket, NOT metadata)
if (key === 'gen_ai.usage.cost') {
  result.metrics.cost = typeof value === 'number' ? value : parseFloat(value as string);
  consumedKeys.add(key);
  continue;
} else if (key === 'gen_ai.server.ttft') {
  result.metrics.ttft_ms = typeof value === 'number' ? value : parseFloat(value as string);
  consumedKeys.add(key);
  continue;
}

// Special: telemetry.sdk.name ‚Üí metadata.instrumentor (OpenLIT SDK identifier)
else if (key === 'telemetry.sdk.name') {
  result.metadata.instrumentor = value;
  consumedKeys.add(key);
  continue;
}
```

**Lesson:** üéì **Instrumentors add unique value-add attributes. Don't assume standard conventions only.**

---

### Mistake #5: No Input Parsing for Google ADK CHAIN Spans

**What Happened:**
- Google ADK sends CHAIN span inputs as:
  ```json
  {
    "new_message": {
      "parts": [{"text": "Explain the concept..."}],
      "role": "user"
    }
  }
  ```
- Ingestion service only looked for `.messages` array
- **Result:** User queries completely lost!

**Analysis Document:** `.praxis-os/workspace/analysis/2025-11-12-google-adk-input-parsing-bug.md`

**Lesson:** üéì **Framework-level spans (CHAIN, AGENT) have different formats than LLM spans.**

---

## üîç Why Event Schema Knowledge is Critical

### The Flexible Schema Philosophy

**From:** `hive-kube/packages/core/src/schemas/events/honeyhive_event.schema.ts`

```typescript
export const HoneyHiveEventSchema = z
  .object({
    event_id: z.string().uuid(),              // REQUIRED
    event_type: z.enum(['model', 'tool', 'chain', 'session']), // REQUIRED
    inputs: z.record(z.unknown()).optional(), // FLEXIBLE (any shape!)
    outputs: z.union([...]).optional(),       // FLEXIBLE (object or array!)
    metadata: z.record(z.unknown()).optional(), // FLEXIBLE
    // ...
  })
  .passthrough(); // ‚ö†Ô∏è Critical: Preserves unknown fields!
```

**Key Design Principles:**
1. **Validate Structure** - Enforce event_id, event_type, relationships
2. **Allow Flexible Data** - inputs, outputs, config, metadata accept any shape
3. **Preserve Unknown Fields** - `.passthrough()` prevents data loss

**Why This Matters:**
- ‚úÖ **Flexibility:** Supports ALL instrumentors without schema changes
- ‚ö†Ô∏è **Complexity:** Attribute router must be explicitly programmed for each instrumentor
- üéØ **Testing:** Must validate FULL pipeline (instrumentor ‚Üí ingestion ‚Üí schema ‚Üí frontend)

---

### The Attribute Router is the Real Schema

**The event schema says:** "I accept anything in `inputs`"  
**The attribute router says:** "I know how to parse OpenInference, Traceloop, OpenLIT, etc."

**File:** `hive-kube/kubernetes/ingestion_service/app/utils/attribute_router.ts`

**Critical Functions:**
1. `normalizeModelInputs()` - Parse LLM inputs (chat_history, functions)
2. `normalizeModelOutputs()` - Parse LLM outputs (role, content, tool_calls)
3. `applyUniversalRouting()` - Route generic attributes to correct buckets
4. `extractFromIndexedMessages()` - Parse Traceloop/Autogen indexed format
5. `extractFromIndexedPrompt()` - Parse Traceloop `gen_ai.prompt.*.*`
6. `extractContentFromNested()` - Parse Google ADK nested content

**The Pattern:**
```typescript
// Explicit switch for per-instrumentor normalization
switch (instrumentor) {
  case 'openinference':
    // Handle OpenInference patterns
    break;
  case 'traceloop':
    // Handle Traceloop patterns
    break;
  case 'openlit':
    // Handle OpenLIT patterns
    break;
  case 'standard-genai':
    // Handle standard GenAI conventions
    break;
}
```

**The Reality:**
- **No automatic mapping** - Each instrumentor pattern must be explicitly coded
- **Easy to miss edge cases** - Framework spans (CHAIN, AGENT) differ from LLM spans
- **Fixtures don't catch bugs** - If fixtures are wrong, tests pass but customers fail

---

## üöÄ Required: Integrations Workflow V1

### Goals

1. **Capture Real Instrumentor Spans** - From actual instrumentor executions
2. **Validate Ingestion Pipeline** - Run through attribute_router.ts
3. **Compare Against Expected Patterns** - Verify optimal patterns detected
4. **Create Fixtures FROM Results** - Fixtures = validated ingestion output

### Workflow Phases

#### Phase 1: Span Capture
```
INPUT: Instrumentor (e.g., "openinference-google-adk")
ACTION: 
  - Run instrumentor example code
  - Capture OTel spans
  - Save as raw JSON (unprocessed)
OUTPUT: Raw span files (*.raw.json)
```

#### Phase 2: Ingestion Test
```
INPUT: Raw span files
ACTION:
  - Run through hive-kube ingestion service
  - Apply attribute_router.ts logic
  - Validate against HoneyHiveEventSchema
OUTPUT: Processed event JSON (*.processed.json)
```

#### Phase 3: Pattern Validation
```
INPUT: Processed event JSON
ACTION:
  - Check for optimal patterns:
    * inputs.chat_history (model events)
    * outputs.role/content (model events)
    * outputs.message (tool events)
  - Check for expected metadata:
    * prompt_tokens, completion_tokens
    * model, provider, temperature
  - Check for instrumentor-specific fields:
    * OpenLIT: cost, ttft_ms
    * Traceloop: indexed messages
    * OpenInference ADK: gcp.vertex.agent.*
OUTPUT: Validation report (pass/fail per pattern)
```

#### Phase 4: Fixture Generation
```
INPUT: Validation report + Processed event JSON
ACTION:
  - IF validation PASS:
      - Create fixture with processed output as "expected"
      - Add fixture to test suite
  - IF validation FAIL:
      - Document missing mappings
      - Create GitHub issue for hive-kube
      - Add to "pending" fixtures
OUTPUT: Test fixtures (*.fixture.json) OR pending issues
```

#### Phase 5: Frontend Validation
```
INPUT: Test fixtures
ACTION:
  - Simulate frontend rendering
  - Check dynamic column discovery
  - Verify side view rendering
  - Validate JSON view completeness
OUTPUT: Frontend compatibility report
```

---

### Workflow Tool Requirements

#### Tool 1: Span Capturer
```python
# Example usage
from honeyhive.integrations.testing import SpanCapturer

capturer = SpanCapturer(instrumentor="openinference-google-adk")
spans = capturer.run_example("examples/google_adk_basic.py")
capturer.save(spans, "openinference_google_adk_basic.raw.json")
```

#### Tool 2: Ingestion Tester
```python
# Example usage
from honeyhive.integrations.testing import IngestionTester

tester = IngestionTester(
    ingestion_service_url="http://localhost:3000",  # Local hive-kube
    raw_span_file="openinference_google_adk_basic.raw.json"
)

result = tester.run()
# result = {
#   "success": True,
#   "processed_event": { ... },
#   "validation_errors": []
# }
```

#### Tool 3: Pattern Validator
```python
# Example usage
from honeyhive.integrations.testing import PatternValidator

validator = PatternValidator(
    event=processed_event,
    instrumentor="openinference-google-adk",
    event_type="model"
)

report = validator.validate()
# report = {
#   "optimal_patterns": {
#     "inputs.chat_history": True,  # ‚úì Found
#     "outputs.role": True,         # ‚úì Found
#     "outputs.content": True       # ‚úì Found
#   },
#   "required_metadata": {
#     "prompt_tokens": True,        # ‚úì Found
#     "model": True                 # ‚úì Found
#   },
#   "instrumentor_specific": {
#     "gcp.vertex.agent.invocation_id": False  # ‚úó Missing
#   }
# }
```

#### Tool 4: Fixture Generator
```python
# Example usage
from honeyhive.integrations.testing import FixtureGenerator

generator = FixtureGenerator(
    raw_span=raw_span,
    processed_event=processed_event,
    validation_report=report
)

if report.all_passed():
    fixture = generator.create_fixture()
    generator.save(fixture, "tests/fixtures/instrumentor_spans/openinference_google_adk_basic.json")
else:
    issue = generator.create_github_issue()
    print(f"Missing mappings: {issue.url}")
```

---

## üéØ Integration Test Matrix

### Instrumentor Support Matrix

| Instrumentor              | Model | Tool | Chain | Agent | Session | Fixtures | Status |
|---------------------------|-------|------|-------|-------|---------|----------|--------|
| **OpenInference ADK**     | ‚úÖ    | ‚úÖ   | ‚úÖ    | ‚úÖ    | ‚ùå      | 7        | ‚úÖ DONE |
| **Traceloop google-gen**  | ‚úÖ    | ‚ùå   | ‚ùå    | ‚ùå    | ‚ùå      | 1        | ‚ö†Ô∏è PARTIAL |
| **OpenLIT google-genai**  | ‚úÖ    | ‚ùå   | ‚ùå    | ‚ùå    | ‚ùå      | 1        | ‚ö†Ô∏è PARTIAL |
| **OpenInference OpenAI**  | ‚úÖ    | ‚ùå   | ‚ùå    | ‚ùå    | ‚ùå      | 2        | ‚ö†Ô∏è PARTIAL |
| **Traceloop OpenAI**      | ‚úÖ    | ‚ùå   | ‚ùå    | ‚ùå    | ‚ùå      | 1        | ‚ö†Ô∏è PARTIAL |
| **OpenLIT OpenAI**        | ‚úÖ    | ‚ùå   | ‚ùå    | ‚ùå    | ‚ùå      | 1        | ‚ö†Ô∏è PARTIAL |
| **Pydantic AI**           | ‚úÖ    | ‚úÖ   | ‚ùå    | ‚úÖ    | ‚ùå      | 3        | ‚ö†Ô∏è PARTIAL |

**Key:**
- ‚úÖ = Fully supported with validated fixtures
- ‚ö†Ô∏è = Partially supported (only model events tested)
- ‚ùå = Not supported or no fixtures

---

### Required Test Cases Per Instrumentor

**Minimum Coverage:**
1. **Model Event (Simple)** - Single-turn chat completion
2. **Model Event (Chat)** - Multi-turn conversation with history
3. **Model Event (Tool Calls)** - LLM requesting tool execution
4. **Tool Event** - Actual tool execution with inputs/outputs
5. **Chain Event** (if framework) - Multi-step orchestration
6. **Agent Event** (if framework) - Agent invocation

**Extended Coverage:**
7. **Model Event (Streaming)** - Streaming response
8. **Model Event (Error)** - LLM call failure
9. **Tool Event (Error)** - Tool execution failure
10. **Session Event** (if supported) - Full conversation trace

---

## üìù Workflow Specification Template

### Example: Testing Pydantic AI Integration

```yaml
integration_test:
  name: "pydantic-ai-anthropic"
  instrumentor: "pydantic-ai"
  provider: "anthropic"
  model: "claude-3-5-sonnet"
  
  phases:
    - name: "capture"
      example_file: "examples/pydantic_ai/anthropic_agent.py"
      output: "pydantic_ai_anthropic_agent.raw.json"
      
    - name: "ingest"
      service: "http://localhost:3000/v1/traces"
      input: "pydantic_ai_anthropic_agent.raw.json"
      output: "pydantic_ai_anthropic_agent.processed.json"
      
    - name: "validate"
      patterns:
        inputs:
          - "chat_history"  # Expected: Array of messages
          - "system_prompt"  # Pydantic AI specific
        outputs:
          - "role"  # Expected: "assistant"
          - "content"  # Expected: String
        metadata:
          - "prompt_tokens"
          - "completion_tokens"
          - "model"
          - "provider"
        instrumentor_specific:
          - "gen_ai.agent.name"  # Pydantic AI v3
          - "gen_ai.system_instructions"  # Pydantic AI v3
      
    - name: "generate_fixture"
      condition: "validation.passed"
      output: "tests/fixtures/instrumentor_spans/pydantic_ai_anthropic_agent_001.json"
```

---

## üö® Critical Lessons for Workflow Design

### 1. **Test Against Reality, Not Expectations**

**Wrong Approach:**
```python
# Manually create fixture
fixture = {
    "expected": {
        "inputs": {"chat_history": [...]},  # Guessing!
        "outputs": {"role": "assistant", "content": "..."}  # Guessing!
    }
}
```

**Right Approach:**
```python
# Capture real span, run through ingestion, use actual output
raw_span = capture_instrumentor_span()
processed = ingestion_service.process(raw_span)
fixture = {
    "attributes": raw_span.attributes,
    "expected": processed  # ACTUAL ingestion output!
}
```

---

### 2. **Validate FULL Pipeline**

**Layers to Test:**
1. **Instrumentor ‚Üí OTel Span** - Does instrumentor produce valid spans?
2. **OTel Span ‚Üí Ingestion** - Does ingestion parse attributes correctly?
3. **Ingestion ‚Üí Schema** - Does processed event pass HoneyHiveEventSchema?
4. **Schema ‚Üí Frontend** - Does frontend render data correctly?

**One broken layer = customer-blocking bug!**

---

### 3. **Span Kind Matters**

**Event Types:**
- `model` = LLM completion
- `tool` = Tool execution
- `chain` = Multi-step workflow
- `session` = Conversation container

**OpenInference Span Kinds:**
- `LLM` ‚Üí `event_type: "model"`
- `TOOL` ‚Üí `event_type: "tool"`
- `CHAIN` ‚Üí `event_type: "chain"`
- `AGENT` ‚Üí `event_type: "chain"` (usually)

**Different kinds need different parsing logic!**

---

### 4. **Instrumentors Have Unique Value-Adds**

**Don't just test standard GenAI conventions. Test unique features:**

**OpenLIT:**
- Automatic cost calculation (`gen_ai.usage.cost`)
- Latency metrics (`gen_ai.server.ttft`, `gen_ai.server.tbt`)
- Self-identification (`telemetry.sdk.name: "openlit"`)

**Pydantic AI v3:**
- Agent name (`gen_ai.agent.name`)
- System instructions (`gen_ai.system_instructions`)
- Tool calls with IDs (`gen_ai.tool.call.id`)

**OpenInference ADK:**
- Framework metadata (`gcp.vertex.agent.*`)
- Full request/response payloads
- Multi-span hierarchies (CHAIN ‚Üí AGENT ‚Üí LLM ‚Üí TOOL)

---

### 5. **Frontend is Part of the Contract**

**The pipeline is:**
```
Instrumentor ‚Üí Ingestion ‚Üí Schema ‚Üí Frontend
```

**Frontend expectations:**
- **Table View:** Needs `event_type`, `event_name`, `start_time`, `duration`, `error`
- **Side View Inputs:** Prefers `chat_history` array for model events
- **Side View Outputs:** Prefers `{role, content}` for model events, `message` for tool events
- **Metrics:** Needs `prompt_tokens`, `completion_tokens`, `cost`, `latency`
- **Config:** Needs `provider`, `model`, `temperature`

**If ingestion produces data in wrong shape, frontend breaks!**

---

## üìä Workflow Success Metrics

### Coverage Goals

**Tier 1 (Launch Blocking):**
- ‚úÖ OpenInference (OpenAI, Anthropic, Google ADK)
- ‚úÖ Traceloop (OpenAI, Anthropic, Google)
- ‚úÖ OpenLIT (OpenAI, Anthropic, Google)

**Tier 2 (Post-Launch):**
- ‚ö†Ô∏è LangChain (via OpenInference)
- ‚ö†Ô∏è LlamaIndex (via OpenInference)
- ‚ö†Ô∏è Vercel AI SDK
- ‚ö†Ô∏è Pydantic AI

**Tier 3 (Nice to Have):**
- ‚ùå Haystack
- ‚ùå AutoGen
- ‚ùå CrewAI

---

### Quality Gates

**Per Instrumentor:**
1. ‚úÖ At least 1 model event fixture
2. ‚úÖ At least 1 tool event fixture (if applicable)
3. ‚úÖ All fixtures pass ingestion validation
4. ‚úÖ All optimal patterns detected
5. ‚úÖ Frontend rendering validated
6. ‚úÖ Customer example code tested

---

## üéØ Next Steps

### 1. Design `integrations_workflow_v1`

**Spec:** `.praxis-os/specs/pending/integrations-workflow-v1.md`

**Phases:**
1. **Planning** - Understand workflow requirements
2. **Design** - Define workflow schema and tool interfaces
3. **Implementation** - Build workflow + tools
4. **Testing** - Run against existing instrumentors
5. **Documentation** - Guide for adding new instrumentors

---

### 2. Build Integration Testing Tools

**Required Tools:**
1. `SpanCapturer` - Capture real instrumentor spans
2. `IngestionTester` - Test against hive-kube ingestion service
3. `PatternValidator` - Validate optimal patterns
4. `FixtureGenerator` - Generate test fixtures from validated output
5. `FrontendSimulator` - Validate frontend rendering

---

### 3. Validate Existing Instrumentors

**Run workflow against:**
1. OpenInference (OpenAI, Anthropic, Google ADK) - 10+ fixtures
2. Traceloop (OpenAI, Anthropic, Google) - 3+ fixtures
3. OpenLIT (OpenAI, Anthropic, Google) - 3+ fixtures

**Goal:** Identify missing mappings in hive-kube ingestion service

---

### 4. Create Integration Guide

**Documentation:** `docs/development/testing/integration-testing.md`

**Topics:**
- How to add new instrumentor support
- How to capture spans
- How to validate ingestion
- How to create fixtures
- How to test frontend rendering

---

## üîó Related Documents

### PR Analysis
- **PR #623:** https://github.com/honeyhiveai/hive-kube/pull/623
- **Analysis Docs:**
  - `.praxis-os/workspace/analysis/2025-11-12-google-adk-input-parsing-bug.md`
  - `.praxis-os/workspace/analysis/2025-11-12-google-adk-complete-coverage.md`
  - `.praxis-os/workspace/analysis/2025-11-12-openinference-fixture-bugs.md`

### Event Schema
- **Analysis:** `.praxis-os/workspace/analysis/2025-11-13-honeyhive-event-schema-frontend-usage.md`
- **Schema Source:** `hive-kube/packages/core/src/schemas/events/honeyhive_event.schema.ts`
- **Attribute Router:** `hive-kube/kubernetes/ingestion_service/app/utils/attribute_router.ts`

### Pydantic AI Analysis
- **Integration Analysis:** `integrations-analysis/PYDANTIC_AI_ANALYSIS.md`
- **Ingestion Compatibility:** `integrations-analysis/PYDANTIC_AI_INGESTION_COMPATIBILITY_ANALYSIS.md`

---

**Analysis Complete!** This document provides the foundation for designing `integrations_workflow_v1`. üöÄ

