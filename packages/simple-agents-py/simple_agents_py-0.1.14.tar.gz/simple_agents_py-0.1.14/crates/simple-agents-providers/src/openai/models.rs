//! OpenAI API request and response types.

use serde::{Deserialize, Serialize};
use simple_agent_type::prelude::Message;
use simple_agent_type::request::ResponseFormat;
use simple_agent_type::tool::{ToolChoice, ToolDefinition};

/// OpenAI chat completion request
///
/// This struct borrows messages to avoid cloning during serialization.
#[derive(Debug, Serialize)]
pub struct OpenAICompletionRequest<'a> {
    /// Model identifier (e.g., "gpt-4", "gpt-3.5-turbo")
    pub model: &'a str,

    /// List of messages in the conversation (borrowed to avoid cloning)
    pub messages: &'a [Message],

    /// Temperature (0.0-2.0)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// Maximum tokens to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens: Option<u32>,

    /// Top-p sampling (0.0-1.0)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// Number of completions to generate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<u32>,

    /// Whether to stream the response
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// Stop sequences (borrowed when possible)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<&'a Vec<String>>,

    /// Response format for structured outputs
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<&'a ResponseFormat>,
    /// Tool definitions for tool calling
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<&'a Vec<ToolDefinition>>,
    /// Tool choice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<&'a ToolChoice>,
}

/// OpenAI chat completion response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAICompletionResponse {
    /// Unique identifier for the completion
    pub id: String,

    /// Object type (always "chat.completion")
    pub object: String,

    /// Unix timestamp of creation
    pub created: u64,

    /// Model used for completion
    pub model: String,

    /// List of completion choices
    pub choices: Vec<OpenAIChoice>,

    /// Token usage information
    pub usage: OpenAIUsage,

    /// System fingerprint
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,
}

/// A single completion choice
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIChoice {
    /// Index of this choice
    pub index: u32,

    /// The message generated by the model
    pub message: Message,

    /// Reason for completion finish
    pub finish_reason: Option<String>,
}

/// Token usage information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIUsage {
    /// Number of tokens in the prompt
    pub prompt_tokens: u32,

    /// Number of tokens in the completion
    pub completion_tokens: u32,

    /// Total tokens used
    pub total_tokens: u32,
}

/// OpenAI error response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIErrorResponse {
    /// Error details
    pub error: OpenAIErrorDetails,
}

/// OpenAI error details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIErrorDetails {
    /// Error message
    pub message: String,

    /// Error type
    #[serde(rename = "type")]
    pub error_type: String,

    /// Error code (optional)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,

    /// Parameter that caused the error (optional)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub param: Option<String>,
}

/// OpenAI streaming chunk (SSE format)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIStreamChunk {
    /// Unique identifier for the completion
    pub id: String,

    /// Object type (always "chat.completion.chunk")
    pub object: String,

    /// Unix timestamp of creation
    pub created: u64,

    /// Model used for completion
    pub model: String,

    /// List of streaming choices
    pub choices: Vec<OpenAIStreamChoice>,

    /// System fingerprint
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,
}

/// A single streaming choice
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIStreamChoice {
    /// Index of this choice
    pub index: u32,

    /// The delta for this chunk
    pub delta: OpenAIDelta,

    /// Reason for completion finish (only in last chunk)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<String>,
}

/// Message delta in a streaming chunk
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIDelta {
    /// Role (only in first chunk)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub role: Option<String>,

    /// Incremental content
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
}

#[cfg(test)]
mod tests {
    use super::*;
    use simple_agent_type::prelude::Role;

    #[test]
    fn test_serialize_request() {
        let messages = vec![Message {
            role: Role::User,
            content: "Hello".to_string(),
            name: None,
            tool_call_id: None,
            tool_calls: None,
        }];

        let request = OpenAICompletionRequest {
            model: "gpt-4",
            messages: &messages,
            temperature: Some(0.7),
            max_tokens: Some(100),
            top_p: None,
            n: None,
            stream: Some(false),
            stop: None,
            response_format: None,
            tools: None,
            tool_choice: None,
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("gpt-4"));
        assert!(json.contains("Hello"));
        assert!(json.contains("0.7"));
    }

    #[test]
    fn test_deserialize_response() {
        let json = r#"{
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-4",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello there!"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 20,
                "total_tokens": 30
            }
        }"#;

        let response: OpenAICompletionResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.id, "chatcmpl-123");
        assert_eq!(response.model, "gpt-4");
        assert_eq!(response.choices.len(), 1);
        assert_eq!(response.choices[0].message.content, "Hello there!");
        assert_eq!(response.usage.total_tokens, 30);
    }

    #[test]
    fn test_deserialize_error_response() {
        let json = r#"{
            "error": {
                "message": "Invalid API key",
                "type": "invalid_request_error",
                "code": "invalid_api_key"
            }
        }"#;

        let response: OpenAIErrorResponse = serde_json::from_str(json).unwrap();
        assert_eq!(response.error.message, "Invalid API key");
        assert_eq!(response.error.error_type, "invalid_request_error");
    }
}
