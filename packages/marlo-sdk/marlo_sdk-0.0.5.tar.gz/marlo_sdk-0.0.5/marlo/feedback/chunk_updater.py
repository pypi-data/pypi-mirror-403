"""Chunk updater for feedback-based prompt customization."""

from __future__ import annotations

import logging
from typing import Any

from marlo.feedback.prompts import (
    REWARD_CHUNK_UPDATE_PROMPT,
    LEARNING_CHUNK_UPDATE_PROMPT,
    LEARNING_EDIT_CONTEXT,
    LEARNING_REJECT_CONTEXT,
)
from marlo.runtime.llm_client import LLMClient

logger = logging.getLogger(__name__)

CHUNK_UPDATE_MODEL = "gemini/gemini-3-flash-preview"


async def update_reward_chunk(
    current_chunk: str,
    rationale: str,
    user_feedback: str,
) -> str:
    """
    Update the reward guidelines chunk based on user feedback.

    Args:
        current_chunk: The current guidelines chunk (may be empty)
        rationale: The rationale that was generated by the reward system
        user_feedback: The user's feedback on the evaluation

    Returns:
        The updated guidelines chunk
    """
    if not user_feedback or not user_feedback.strip():
        return current_chunk

    chunk_display = current_chunk.strip() if current_chunk else "(No guidelines yet)"

    prompt = REWARD_CHUNK_UPDATE_PROMPT.format(
        current_chunk=chunk_display,
        rationale=rationale or "(No rationale provided)",
        user_feedback=user_feedback.strip(),
    )

    try:
        client = LLMClient(model=CHUNK_UPDATE_MODEL, params={"temperature": 0.1})
        response = await client.acomplete(
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.content
        if not content:
            logger.warning("Chunk update returned empty content, keeping original")
            return current_chunk

        updated_chunk = content.strip()
        if not updated_chunk:
            return current_chunk

        return updated_chunk

    except Exception as exc:
        logger.warning("Failed to update reward chunk: %s", exc)
        return current_chunk


async def update_learning_chunk(
    current_chunk: str,
    feedback_type: str,
    *,
    original_learning: str | None = None,
    edited_learning: str | None = None,
    rejected_learning: str | None = None,
    rejection_reason: str | None = None,
) -> str:
    """
    Update the learning guidelines chunk based on user feedback.

    Args:
        current_chunk: The current guidelines chunk (may be empty)
        feedback_type: Either 'edit' or 'reject'
        original_learning: For edits, the original learning text
        edited_learning: For edits, the edited learning text
        rejected_learning: For rejections, the rejected learning text
        rejection_reason: For rejections, the user's reason

    Returns:
        The updated guidelines chunk
    """
    chunk_display = current_chunk.strip() if current_chunk else "(No guidelines yet)"

    if feedback_type == "edit":
        if not original_learning or not edited_learning:
            return current_chunk
        if original_learning.strip() == edited_learning.strip():
            return current_chunk

        context_section = LEARNING_EDIT_CONTEXT.format(
            original_learning=original_learning.strip(),
            edited_learning=edited_learning.strip(),
        )
    elif feedback_type == "reject":
        if not rejected_learning:
            return current_chunk

        context_section = LEARNING_REJECT_CONTEXT.format(
            rejected_learning=rejected_learning.strip(),
            rejection_reason=(rejection_reason or "No reason provided").strip(),
        )
    else:
        logger.warning("Unknown feedback type: %s", feedback_type)
        return current_chunk

    prompt = LEARNING_CHUNK_UPDATE_PROMPT.format(
        current_chunk=chunk_display,
        context_section=context_section,
    )

    try:
        client = LLMClient(model=CHUNK_UPDATE_MODEL, params={"temperature": 0.1})
        response = await client.acomplete(
            messages=[{"role": "user", "content": prompt}]
        )
        content = response.content
        if not content:
            logger.warning("Chunk update returned empty content, keeping original")
            return current_chunk

        updated_chunk = content.strip()
        if not updated_chunk:
            return current_chunk

        return updated_chunk

    except Exception as exc:
        logger.warning("Failed to update learning chunk: %s", exc)
        return current_chunk


__all__ = ["update_reward_chunk", "update_learning_chunk"]
