# Wareflow Analysis v0.2.0 - Implementation Plan

**Version:** 0.2.0
**Status:** Implementation Roadmap
**Target Release:** Q1 2025
**Last Updated:** 2025-01-23

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current State Analysis](#current-state-analysis)
3. [Missing Features Inventory](#missing-features-inventory)
4. [Implementation Strategy](#implementation-strategy)
5. [Development Roadmap](#development-roadmap)
6. [Technical Specifications](#technical-specifications)
7. [Testing Strategy](#testing-strategy)
8. [Risk Assessment](#risk-assessment)

---

## Executive Summary

### Current Status

Wareflow Analysis v0.1.x has successfully implemented the **foundation layer** of the warehouse analytics system:
- âœ… CLI infrastructure with Typer
- âœ… Project initialization (`init`)
- âœ… Data import with excel-to-sql Auto-Pilot (`import-data`)
- âœ… Comprehensive validation system (`validate`)
- âœ… Database status reporting (`status`)

### Critical Gap

The **business value layer** is completely missing:
- âŒ No analytics capabilities (zero analysis implemented)
- âŒ No reporting functionality (no export to Excel)
- âŒ No pipeline orchestration (no full automation)

**Impact:** The system can ingest and validate data but cannot produce any insights or reports for business users.

### v0.2.0 Objective

Complete the **core analytics engine** to deliver business value:
1. Implement `analyze` command with 3 core analyses
2. Implement `export` command with Excel report generation
3. Implement `run` command for end-to-end pipeline automation

**Goal:** Transform from "data ingestion tool" to "warehouse analytics system" ready for initial production use.

---

## Current State Analysis

### Implemented Features (v0.1.x)

| Feature | Status | Quality | Notes |
|---------|--------|---------|-------|
| **CLI Infrastructure** | âœ… Complete | Excellent | Typer-based, well-structured |
| **Project Initialization** | âœ… Complete | Excellent | Creates full project structure |
| **Data Import** | âœ… Complete | Excellent | Auto-Pilot integration, pandas, sqlite3 |
| **Validation System** | âœ… Complete | Excellent | 6 validation modules, comprehensive |
| **Status Reporting** | âœ… Complete | Good | Basic database status display |

**Code Quality:** The implemented features are well-designed, tested (basic), and production-ready for their scope.

### Missing Features (v0.2.0 Scope)

| Feature | Status | Priority | Estimation |
|---------|--------|----------|------------|
| **Analyze Command** | âŒ Not Implemented | **CRITICAL** | 3-5 days |
| **Export Command** | âŒ Not Implemented | **CRITICAL** | 2-3 days |
| **Run Command** | âŒ Not Implemented | HIGH | 1-2 days |
| **Core Analyses (3)** | âŒ Not Implemented | **CRITICAL** | 2-3 days each |
| **Test Coverage** | âš ï¸ Partial | HIGH | 2-3 days |

**Note:** Documentation is comprehensive (80%+) but code implementation lags significantly behind.

### Progress Metrics

```
Overall Completion: 35%

CLI Foundation:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  40%
  â”œâ”€ init              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
  â”œâ”€ import-data       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
  â”œâ”€ validate          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
  â”œâ”€ status            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
  â”œâ”€ analyze           â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
  â”œâ”€ export            â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
  â””â”€ run               â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%

Business Logic:       â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%

Testing:              â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   15%

Documentation:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  80%
```

---

## Missing Features Inventory

### 1. Analyze Command - CRITICAL

**Current State:**
```python
@app.command()
def analyze() -> None:
    """Run all analyses."""
    typer.echo("Analyze command not implemented yet")
```

**Required Implementation:**

#### Architecture
```
src/wareflow_analysis/analyze/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ loader.py          # Analysis discovery and loading
â”œâ”€â”€ executor.py        # SQL query execution
â”œâ”€â”€ calculators.py     # KPI calculations
â””â”€â”€ analyses/          # Analysis implementations
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ abc.py         # ABC Classification
    â”œâ”€â”€ kpis.py        # Product Performance KPIs
    â”œâ”€â”€ movements.py   # Movement Analytics
    â””â”€â”€ orders.py      # Order Analytics (future)
```

#### Core Analyses (v0.2.0 MVP)

**1. ABC Classification** (ANALYZE-001)
- Pareto analysis: 20% products = 80% movements
- SQL: NTILE percentile ranking
- Output: Class A/B/C distribution with recommendations

**2. Product Performance KPIs** (ANALYZE-002)
- Movement frequency, velocity, trends
- Activity status (ACTIVE, SLOW, STALE)
- Top N products by picks

**3. Movement Analytics** (ANALYZE-003)
- Type distribution (inbound/outbound/transfer)
- Temporal patterns (daily, hourly, day-of-week)
- Peak identification

#### CLI Interface
```bash
# Run all analyses
wareflow analyze

# Run specific analysis
wareflow analyze --name abc

# List available analyses
wareflow analyze --list

# Verbose output
wareflow analyze --verbose

# Custom lookback period
wareflow analyze --days 60
```

**Dependencies:**
- pandas (already installed)
- sqlite3 (standard library)
- No new dependencies required

**Success Criteria:**
- âœ… All 3 analyses execute successfully
- âœ… Results display in formatted terminal output
- âœ… Complete execution in < 5 seconds
- âœ… Handle empty databases gracefully

---

### 2. Export Command - CRITICAL

**Current State:**
```python
@app.command()
def export() -> None:
    """Generate Excel reports."""
    typer.echo("Export command not implemented yet")
```

**Required Implementation:**

#### Architecture
```
src/wareflow_analysis/export/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ builder.py         # Excel workbook builder
â”œâ”€â”€ formatters.py      # Cell formatting and styling
â””â”€â”€ templates/         # Report templates
    â”œâ”€â”€ abc_report.py
    â”œâ”€â”€ kpis_report.py
    â””â”€â”€ movements_report.py
```

#### Features

**v0.2.0 MVP:**
1. **Multi-sheet Excel workbooks**
   - Sheet 1: Summary/Dashboard
   - Sheet 2-N: Individual analysis data

2. **Formatting and Styling**
   - Header rows with bold/fill colors
   - Number formatting (commas, decimals)
   - Column width auto-adjustment
   - Freeze panes for large datasets

3. **Data Export**
   - Export analysis results to Excel
   - Preserve calculated KPIs
   - Include timestamps and metadata

**Future (v0.3.0):**
- Charts and graphs (bar, pie, line)
- Conditional formatting (color scales)
- Pivot tables
- Automated report scheduling

#### CLI Interface
```bash
# Export all analyses
wareflow export

# Export specific analysis
wareflow export --analysis abc

# Custom output filename
wareflow export --output quarterly_report.xlsx

# Output directory
wareflow export --dir reports/q1/
```

**Dependencies:**
- openpyxl (already installed)
- xlsxwriter (optional, for advanced formatting)

**Success Criteria:**
- âœ… Generate valid Excel files (.xlsx)
- âœ… Multiple sheets with proper formatting
- âœ… Include all analysis results
- âœ… Professional appearance suitable for business distribution

---

### 3. Run Command - HIGH PRIORITY

**Current State:**
```python
@app.command()
def run() -> None:
    """Run full pipeline (import -> analyze -> export)."""
    typer.echo("Run command not implemented yet")
```

**Required Implementation:**

#### Architecture
```python
# src/wareflow_analysis/orchestrator.py

class PipelineOrchestrator:
    """Orchestrate the full analysis pipeline."""

    def run_pipeline(self, project_dir: Path, options: dict):
        """
        Execute: import â†’ validate â†’ analyze â†’ export

        Handles:
        - Dependency management (each step depends on previous)
        - Error handling (stop on failure, continue on warning)
        - Progress reporting
        - Rollback on critical failures
        """
        steps = [
            ("import", self.run_import),
            ("validate", self.run_validate),
            ("analyze", self.run_analyze),
            ("export", self.run_export)
        ]

        results = {}
        for step_name, step_func in steps:
            try:
                results[step_name] = step_func()
            except Exception as e:
                if options.get("stop_on_error", True):
                    self.handle_failure(step_name, e, results)
                    raise
                else:
                    results[step_name] = {"status": "failed", "error": str(e)}

        return results
```

#### CLI Interface
```bash
# Run full pipeline
wareflow run

# Continue on errors
wareflow run --continue-on-error

# Skip specific steps
wareflow run --skip-import

# Verbose progress
wareflow run --verbose

# Dry run (show what would be done)
wareflow run --dry-run
```

**Features:**
1. **Sequential Execution**
   - import â†’ validate â†’ analyze â†’ export
   - Stop on failure (configurable)

2. **Progress Reporting**
   - Clear step-by-step output
   - Execution time per step
   - Total pipeline time

3. **Error Handling**
   - Graceful failure with clear messages
   - Optional continue-on-error mode
   - Rollback on critical failures

**Success Criteria:**
- âœ… Execute all 4 steps in correct order
- âœ… Stop gracefully on step failure
- âœ… Provide clear progress reporting
- âœ… Complete full pipeline in < 30 seconds (typical dataset)

---

### 4. Test Coverage - HIGH PRIORITY

**Current State:**
- âœ… Test infrastructure exists (pytest)
- âŒ No tests for import functionality
- âŒ No tests for validate functionality
- âŒ No tests for analyze (not implemented)
- âŒ No tests for export (not implemented)
- âŒ No integration tests

**Required Implementation:**

#### Test Structure
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_analyze_abc.py
â”‚   â”œâ”€â”€ test_analyze_kpis.py
â”‚   â”œâ”€â”€ test_analyze_movements.py
â”‚   â”œâ”€â”€ test_export_builder.py
â”‚   â””â”€â”€ test_orchestrator.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_full_pipeline.py
â”‚   â”œâ”€â”€ test_import_validate_analyze.py
â”‚   â””â”€â”€ test_error_handling.py
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_warehouse.db
    â”œâ”€â”€ test_data/
    â”‚   â”œâ”€â”€ produits.xlsx
    â”‚   â”œâ”€â”€ mouvements.xlsx
    â”‚   â””â”€â”€ commandes.xlsx
    â””â”€â”€ expected_outputs/
        â”œâ”€â”€ abc_analysis.json
        â””â”€â”€ export_template.xlsx
```

#### Coverage Goals

**v0.2.0 MVP:**
- Unit tests: 70%+ coverage for new code
- Integration tests: Full pipeline test
- Edge cases: Empty database, missing files, corrupt data

**Dependencies:**
- pytest (already installed)
- pytest-cov (for coverage reports)
- pytest-mock (for mocking)

---

## Implementation Strategy

### Development Principles

1. **Incremental Delivery**
   - Implement 1 analysis completely before starting next
   - Each analysis must be independently testable
   - Progressive enhancement: basic â†’ advanced

2. **Pragmatic Architecture**
   - Start with hardcoded SQL queries
   - Don't build plugin system until 3+ analyses exist
   - Optimize based on real usage patterns

3. **Test-First for New Code**
   - Write test before implementation for analyze/export
   - Integration tests for pipeline
   - Regression tests for existing features

4. **Documentation-Code Parity**
   - Update docs as code is implemented
   - Remove documentation for unimplemented features
   - Keep README in sync with reality

### Development Phases

#### Phase 1: Analyze Foundation (Week 1)

**Goal:** Get `analyze` command working with 1 analysis

**Tasks:**
1. Create `analyze/` module structure
2. Implement database connector and loader
3. Implement ABC Classification analysis
4. Add CLI integration
5. Write tests for ABC analysis
6. Update documentation

**Deliverables:**
- âœ… `wareflow analyze --name abc` works
- âœ… Terminal output shows ABC classification
- âœ… Tests pass
- âœ… Documentation updated

**Success Criteria:**
- Command executes without errors
- Results are accurate and well-formatted
- Test coverage > 70%

---

#### Phase 2: Core Analyses (Week 2)

**Goal:** Complete remaining 2 core analyses

**Tasks:**
1. Implement Product Performance KPIs analysis
2. Implement Movement Analytics analysis
3. Add analysis selection logic
4. Implement `--list` command
5. Write tests for new analyses
6. Performance optimization

**Deliverables:**
- âœ… `wareflow analyze --name kpis` works
- âœ… `wareflow analyze --name movements` works
- âœ… `wareflow analyze --list` shows all analyses
- âœ… `wareflow analyze` runs all 3 analyses
- âœ… All tests pass

**Success Criteria:**
- All 3 analyses produce accurate results
- Total execution time < 5 seconds
- Test coverage > 70% for all analyses

---

#### Phase 3: Export Implementation (Week 2-3)

**Goal:** Generate Excel reports from analysis results

**Tasks:**
1. Create `export/` module structure
2. Implement Excel workbook builder
3. Implement cell formatting and styling
4. Add multi-sheet support
5. Implement export templates
6. Add CLI integration
7. Write tests

**Deliverables:**
- âœ… `wareflow export` generates Excel file
- âœ… Multiple sheets (one per analysis)
- âœ… Professional formatting
- âœ… Tests pass

**Success Criteria:**
- Generated Excel files are valid and readable
- Formatting is professional
- All analysis results included
- Test coverage > 70%

---

#### Phase 4: Pipeline Orchestration (Week 3)

**Goal:** Implement end-to-end automation

**Tasks:**
1. Create orchestrator module
2. Implement pipeline execution logic
3. Add error handling and recovery
4. Implement progress reporting
5. Add CLI integration
6. Write integration tests
7. Update documentation

**Deliverables:**
- âœ… `wareflow run` executes full pipeline
- âœ… Error handling works correctly
- âœ… Progress reporting is clear
- âœ… Integration tests pass

**Success Criteria:**
- Full pipeline completes successfully
- Clear progress reporting
- Graceful error handling
- Test coverage > 70%

---

#### Phase 5: Testing & Polish (Week 3-4)

**Goal:** Production-ready quality

**Tasks:**
1. Complete test coverage gaps
2. Performance optimization
3. Error message improvements
4. Documentation updates
5. Edge case handling
6. Code review and refactoring

**Deliverables:**
- âœ… Test coverage > 80% overall
- âœ… Performance benchmarks met
- âœ… Documentation complete and accurate
- âœ… All known issues resolved

**Success Criteria:**
- All tests pass consistently
- Performance targets met
- Documentation is accurate
- Code is clean and maintainable

---

## Development Roadmap

### Timeline Summary

| Phase | Duration | Start | End | Deliverables |
|-------|----------|-------|-----|--------------|
| **Phase 1** | 3-5 days | Week 1 | Week 1 | Analyze foundation + ABC |
| **Phase 2** | 4-5 days | Week 2 | Week 2 | KPIs + Movements analyses |
| **Phase 3** | 2-3 days | Week 2 | Week 3 | Export implementation |
| **Phase 4** | 1-2 days | Week 3 | Week 3 | Pipeline orchestration |
| **Phase 5** | 2-3 days | Week 3 | Week 4 | Testing & polish |
| **Total** | **12-18 days** | | | **v0.2.0 complete** |

### Milestone Definitions

**Milestone 1: First Analysis (Day 5)**
- `wareflow analyze --name abc` works end-to-end
- Proof of concept for analysis system

**Milestone 2: Core Analytics Complete (Day 10)**
- All 3 core analyses implemented
- `wareflow analyze` runs successfully
- Business value delivery begins

**Milestone 3: Reporting Ready (Day 13)**
- `wareflow export` generates Excel reports
- System can produce deliverables for stakeholders

**Milestone 4: Production Pipeline (Day 15)**
- `wareflow run` orchestrates full pipeline
- Ready for initial production deployment

**Milestone 5: v0.2.0 Release (Day 18)**
- All features complete and tested
- Documentation updated
- Ready for broader rollout

---

## Technical Specifications

### Analyze Command Specification

#### Module Structure

```python
# src/wareflow_analysis/analyze/__init__.py
from .loader import AnalysisLoader
from .executor import AnalysisExecutor
from .analyses.abc import ABCClassification
from .analyses.kpis import ProductKPIs
from .analyses.movements import MovementAnalytics

__all__ = [
    "AnalysisLoader",
    "AnalysisExecutor",
    "ABCClassification",
    "ProductKPIs",
    "MovementAnalytics",
]
```

#### Analysis Interface

```python
# src/wareflow_analysis/analyze/base.py

from abc import ABC, abstractmethod
import sqlite3
from typing import Dict, Any

class BaseAnalysis(ABC):
    """Abstract base class for all analyses."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Unique identifier for this analysis."""
        pass

    @property
    @abstractmethod
    def display_name(self) -> str:
        """Human-readable name."""
        pass

    @property
    @abstractmethod
    def description(self) -> str:
        """What this analysis does."""
        pass

    @abstractmethod
    def run(self, conn: sqlite3.Connection, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the analysis.

        Args:
            conn: Database connection
            config: Analysis configuration (lookback_days, limits, etc.)

        Returns:
            Dictionary with analysis results
        """
        pass

    @abstractmethod
    def format_output(self, results: Dict[str, Any]) -> str:
        """
        Format results for terminal display.

        Args:
            results: Analysis results from run()

        Returns:
            Formatted string for terminal output
        """
        pass
```

#### Example Implementation: ABC Classification

```python
# src/wareflow_analysis/analyze/analyses/abc.py

from ..base import BaseAnalysis
import pandas as pd

class ABCClassification(BaseAnalysis):
    """ABC Classification analysis using Pareto principle."""

    @property
    def name(self) -> str:
        return "abc"

    @property
    def display_name(self) -> str:
        return "ABC Classification"

    @property
    def description(self) -> str:
        return "Pareto analysis: 20% products = 80% movements"

    def run(self, conn: sqlite3.Connection, config: Dict[str, Any]) -> Dict[str, Any]:
        lookback_days = config.get("lookback_days", 90)

        query = f"""
        WITH product_movement AS (
            SELECT
                no_produit,
                nom_produit,
                COUNT(*) as total_movements,
                SUM(CASE WHEN type = 'SORTIE' THEN quantite ELSE 0 END) as total_picked,
                MAX(date_heure) as last_movement
            FROM mouvements
            WHERE date_heure >= date('now', '-{lookback_days} days')
            GROUP BY no_produit
        ),
        with_percentile AS (
            SELECT *,
                NTILE(100) OVER (ORDER BY total_picked DESC) as percentile_rank
            FROM product_movement
        )
        SELECT
            no_produit,
            nom_produit,
            total_movements,
            total_picked,
            percentile_rank,
            CASE
                WHEN percentile_rank <= 20 THEN 'A'
                WHEN percentile_rank <= 50 THEN 'B'
                ELSE 'C'
            END as abc_class,
            last_movement
        FROM with_percentile
        ORDER BY total_picked DESC
        """

        df = pd.read_sql_query(query, conn)

        # Calculate summary statistics
        summary = {
            "class_a": {
                "count": int(len(df[df['abc_class'] == 'A'])),
                "picks": int(df[df['abc_class'] == 'A']['total_picked'].sum())
            },
            "class_b": {
                "count": int(len(df[df['abc_class'] == 'B'])),
                "picks": int(df[df['abc_class'] == 'B']['total_picked'].sum())
            },
            "class_c": {
                "count": int(len(df[df['abc_class'] == 'C'])),
                "picks": int(df[df['abc_class'] == 'C']['total_picked'].sum())
            }
        }

        return {
            "classification": df.to_dict('records'),
            "summary": summary,
            "total_products": len(df),
            "total_picks": int(df['total_picked'].sum())
        }

    def format_output(self, results: Dict[str, Any]) -> str:
        summary = results["summary"]
        total = results["total_picks"]

        output = []
        output.append(f"\n{'='*60}")
        output.append("ğŸ“Š ABC CLASSIFICATION ANALYSIS")
        output.append(f"{'='*60}")
        output.append(f"\nTotal Products: {results['total_products']:,}")
        output.append(f"Total Picks: {total:,}")

        # Class A
        a_pct = (summary["class_a"]["picks"] / total * 100) if total > 0 else 0
        output.append(f"\nClass A (Top 20%):")
        output.append(f"  Products: {summary['class_a']['count']}")
        output.append(f"  Picks: {summary['class_a']['picks']:,} ({a_pct:.1f}%)")

        # Class B
        b_pct = (summary["class_b"]["picks"] / total * 100) if total > 0 else 0
        output.append(f"\nClass B (Next 30%):")
        output.append(f"  Products: {summary['class_b']['count']}")
        output.append(f"  Picks: {summary['class_b']['picks']:,} ({b_pct:.1f}%)")

        # Class C
        c_pct = (summary["class_c"]["picks"] / total * 100) if total > 0 else 0
        output.append(f"\nClass C (Bottom 50%):")
        output.append(f"  Products: {summary['class_c']['count']}")
        output.append(f"  Picks: {summary['class_c']['picks']:,} ({c_pct:.1f}%)")

        output.append(f"\n{'='*60}\n")

        return "\n".join(output)
```

### Export Command Specification

#### Module Structure

```python
# src/wareflow_analysis/export/__init__.py
from .builder import ExcelReportBuilder
from .formatters import CellFormatter

__all__ = [
    "ExcelReportBuilder",
    "CellFormatter",
]
```

#### Excel Builder Interface

```python
# src/wareflow_analysis/export/builder.py

from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from typing import Dict, Any, List
import pandas as pd

class ExcelReportBuilder:
    """Build Excel reports from analysis results."""

    def __init__(self, output_path: str):
        self.output_path = output_path
        self.workbook = Workbook()
        self.formatter = CellFormatter()

    def add_summary_sheet(self, results: Dict[str, Any]):
        """Add dashboard/summary sheet."""
        ws = self.workbook.active
        ws.title = "Summary"

        # Header
        ws['A1'] = "Wareflow Analysis Report"
        ws['A1'].font = Font(size=16, bold=True)
        ws['A2'] = f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}"

        # Summary statistics
        row = 4
        for analysis_name, analysis_results in results.items():
            ws[f'A{row}'] = analysis_name
            ws[f'A{row}'].font = Font(bold=True)
            row += 1

            for key, value in analysis_results.get('summary', {}).items():
                ws[f'A{row}'] = f"  {key}: {value}"
                row += 1

            row += 1

    def add_analysis_sheet(self, name: str, df: pd.DataFrame):
        """Add sheet with analysis data."""
        ws = self.workbook.create_sheet(title=name)

        # Write header
        for col_idx, column in enumerate(df.columns, 1):
            cell = ws.cell(row=1, column=col_idx)
            cell.value = column
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
            cell.alignment = Alignment(horizontal="center")

        # Write data
        for row_idx, row_data in enumerate(df.itertuples(index=False), 2):
            for col_idx, value in enumerate(row_data, 1):
                ws.cell(row=row_idx, column=col_idx, value=value)

        # Auto-adjust column widths
        for col in ws.columns:
            max_length = 0
            column = col[0].column_letter
            for cell in col:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column].width = adjusted_width

        # Freeze header row
        ws.freeze_panes = 'A2'

    def save(self):
        """Save workbook to file."""
        self.workbook.save(self.output_path)
        return self.output_path
```

---

## Testing Strategy

### Unit Tests

**Coverage Requirements:**
- Each analysis: > 70% coverage
- Export builder: > 80% coverage
- Orchestrator: > 80% coverage

**Test Categories:**
1. **Happy Path:** Normal operation with valid data
2. **Edge Cases:** Empty results, single row, extreme values
3. **Error Handling:** Missing tables, invalid SQL, connection failures

**Example Test Structure:**

```python
# tests/unit/test_analyze_abc.py

import pytest
import sqlite3
import pandas as pd
from wareflow_analysis.analyze.analyses.abc import ABCClassification

@pytest.fixture
def sample_database(tmp_path):
    """Create a test database with sample data."""
    db_path = tmp_path / "test.db"
    conn = sqlite3.connect(db_path)

    # Create schema
    conn.execute("""
        CREATE TABLE mouvements (
            oid INTEGER PRIMARY KEY,
            no_produit INTEGER,
            type TEXT,
            quantite INTEGER,
            date_heure DATETIME
        )
    """)

    # Insert test data
    test_data = [
        (1, 1001, 'SORTIE', 100, '2024-01-01 10:00:00'),
        (2, 1001, 'SORTIE', 150, '2024-01-02 10:00:00'),
        (3, 1002, 'SORTIE', 50, '2024-01-01 11:00:00'),
        # ... more test data
    ]

    conn.executemany(
        "INSERT INTO mouvements VALUES (?, ?, ?, ?, ?)",
        test_data
    )
    conn.commit()
    conn.close()

    return db_path

def test_abc_analysis_basic(sample_database):
    """Test ABC analysis with basic data."""
    conn = sqlite3.connect(sample_database)
    analysis = ABCClassification()

    results = analysis.run(conn, {"lookback_days": 90})

    # Assertions
    assert "classification" in results
    assert "summary" in results
    assert "class_a" in results["summary"]
    assert results["total_products"] > 0

def test_abc_analysis_empty_database(tmp_path):
    """Test ABC analysis with empty database."""
    db_path = tmp_path / "empty.db"
    conn = sqlite3.connect(db_path)
    conn.execute("CREATE TABLE mouvements (oid INTEGER, no_produit INTEGER, type TEXT)")

    analysis = ABCClassification()
    results = analysis.run(conn, {})

    assert results["total_products"] == 0
    assert len(results["classification"]) == 0

def test_abc_format_output(sample_database):
    """Test output formatting."""
    conn = sqlite3.connect(sample_database)
    analysis = ABCClassification()

    results = analysis.run(conn, {})
    output = analysis.format_output(results)

    assert "ABC CLASSIFICATION" in output
    assert "Class A" in output
    assert "Class B" in output
    assert "Class C" in output
```

### Integration Tests

**Full Pipeline Test:**

```python
# tests/integration/test_full_pipeline.py

import pytest
from pathlib import Path
import subprocess

@pytest.mark.integration
def test_full_pipeline_flow(tmp_path):
    """Test complete pipeline: init â†’ import â†’ analyze â†’ export"""
    project_dir = tmp_path / "test_warehouse"

    # Step 1: Initialize project
    result = subprocess.run(
        ["wareflow", "init", str(project_dir)],
        capture_output=True
    )
    assert result.returncode == 0

    # Step 2: Copy test data
    # ... (copy Excel files to project_dir / "data")

    # Step 3: Import data
    result = subprocess.run(
        ["wareflow", "import-data"],
        cwd=project_dir,
        capture_output=True
    )
    assert result.returncode == 0

    # Step 4: Run analysis
    result = subprocess.run(
        ["wareflow", "analyze"],
        cwd=project_dir,
        capture_output=True
    )
    assert result.returncode == 0
    assert "ABC CLASSIFICATION" in result.stdout.decode()

    # Step 5: Export results
    result = subprocess.run(
        ["wareflow", "export"],
        cwd=project_dir,
        capture_output=True
    )
    assert result.returncode == 0

    # Verify Excel file created
    export_file = project_dir / "output" / "warehouse_analysis.xlsx"
    assert export_file.exists()

    # Step 6: Run full pipeline
    result = subprocess.run(
        ["wareflow", "run"],
        cwd=project_dir,
        capture_output=True
    )
    assert result.returncode == 0
```

### Performance Tests

**Benchmarks:**

```python
# tests/performance/test_benchmarks.py

import pytest
import time
from wareflow_analysis.analyze.executor import AnalysisExecutor

@pytest.mark.benchmark
def test_abc_analysis_performance():
    """Test ABC analysis completes within target time."""
    conn = sqlite3.connect("large_test_database.db")  # 100K+ movements
    executor = AnalysisExecutor()

    start_time = time.time()
    results = executor.run_analysis("abc", conn, {})
    execution_time = time.time() - start_time

    assert execution_time < 5.0  # Must complete in < 5 seconds
```

---

## Risk Assessment

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **SQL query performance** on large datasets | Medium | High | Use indexes, LIMIT clauses, test with real data |
| **Excel file size** becomes too large | Medium | Medium | Implement pagination, data sampling |
| **Database schema changes** break analyses | Low | High | Version schema, backward compatibility checks |
| **Memory issues** with large datasets | Low | Medium | Process data in chunks, use generators |

### Development Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Timeline underestimation** | High | Medium | Add 20% buffer, phase delivery |
| **Scope creep** (adding too many features) | Medium | High | Strict adherence to MVP scope |
| **Test data availability** | Medium | Medium | Create synthetic test data early |
| **Dependency conflicts** (openpyxl, pandas) | Low | Low | Use virtual environments, pin versions |

### Operational Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **User data quality issues** | High | Medium | Robust validation, clear error messages |
| **Concurrent access** to SQLite database | Low | Medium | Document single-user limitation |
| **Excel compatibility** issues | Low | Medium | Test with multiple Excel versions |

---

## Success Criteria

### v0.2.0 Release Requirements

**Must Have:**
- âœ… All 3 core analyses implemented and tested
- âœ… Export to Excel working with professional formatting
- âœ… Full pipeline orchestration functional
- âœ… Test coverage > 70% for new code
- âœ… Documentation updated and accurate
- âœ… Performance benchmarks met (< 5s per analysis)

**Should Have:**
- âœ… Comprehensive error messages
- âœ… Progress reporting for long operations
- âœ… Integration tests passing
- âœ… Code review completed

**Nice to Have:**
- Charts in Excel exports
- Analysis comparison features
- Automated report scheduling

### Definition of Done

A feature is "done" when:
1. Code is implemented and follows project standards
2. Unit tests written and passing (> 70% coverage)
3. Integration tests included where applicable
4. Documentation updated (code comments, user docs)
5. Peer review completed
6. Performance benchmarks met
7. Edge cases handled appropriately

---

## Post-v0.2.0 Preview

### v0.3.0 - GUI & Advanced Analytics

**Planned Features:**
1. CustomTkinter GUI for non-technical users
2. Advanced analyses (Picking Efficiency, Slow-Moving Inventory)
3. Chart generation in Excel exports
4. Analysis plugin system
5. Scheduled report generation

**Timeline:** Q2 2025 (4-6 weeks after v0.2.0)

### v1.0.0 - Production Ready

**Planned Features:**
1. Multi-warehouse support
2. Cross-warehouse comparison
3. Consolidated reporting
4. User management and permissions
5. Cloud deployment options

**Timeline:** Q3-Q4 2025

---

## Appendix

### Dependencies Matrix

| Package | Current Version | v0.2.0 Required | Purpose |
|---------|----------------|-----------------|---------|
| typer | âœ… Installed | No change | CLI framework |
| pandas | âœ… Installed | No change | Data manipulation |
| openpyxl | âœ… Installed | No change | Excel generation |
| sqlite3 | âœ… StdLib | No change | Database |
| pytest | âœ… Installed | No change | Testing |
| customtkinter | âŒ Not installed | v0.3.0 only | GUI (future) |

### File Structure (Post-v0.2.0)

```
wareflow-analysis/
â”œâ”€â”€ src/wareflow_analysis/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ init.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_import/          âœ… (v0.1.x)
â”‚   â”‚   â”œâ”€â”€ autopilot.py
â”‚   â”‚   â”œâ”€â”€ config_refiner.py
â”‚   â”‚   â””â”€â”€ importer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/           âœ… (v0.1.x)
â”‚   â”‚   â”œâ”€â”€ checks/
â”‚   â”‚   â”œâ”€â”€ auto_fix/
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ schema_parser.py
â”‚   â”‚   â”œâ”€â”€ validator.py
â”‚   â”‚   â””â”€â”€ reporters.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analyze/              ğŸ†• (v0.2.0)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ executor.py
â”‚   â”‚   â””â”€â”€ analyses/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ abc.py
â”‚   â”‚       â”œâ”€â”€ kpis.py
â”‚   â”‚       â””â”€â”€ movements.py
â”‚   â”‚
â”‚   â”œâ”€â”€ export/               ğŸ†• (v0.2.0)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”œâ”€â”€ formatters.py
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚
â”‚   â””â”€â”€ orchestrator/         ğŸ†• (v0.2.0)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ tests/                    âœ… (Enhanced v0.2.0)
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â””â”€â”€ docs/                     âœ… (Updated v0.2.0)
    â”œâ”€â”€ version/
    â”‚   â”œâ”€â”€ 1.0.0.md         # Architecture vision
    â”‚   â””â”€â”€ 0.2.0.md         # This document
    â””â”€â”€ features/
        â”œâ”€â”€ analyze.md       # Updated with implementation
        â””â”€â”€ export.md        # Updated with implementation
```

---

## Document Control

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 0.2.0 | 2025-01-23 | System Architecture | Initial implementation plan |

**Next Review:** After Phase 1 completion (approx. Week 1)

---

**This document is a living roadmap and will be updated as implementation progresses.**
