use crate::omega::{Candidate, DiscoverySource};
use crate::variant_catalog::{Variant, VariantConstraints, VariantType};

/// Context provided to the Observer (LLM) to inform its suggestions
#[derive(Debug, Clone)]
pub struct ObserverContext {
    pub recent_telemetry: Vec<String>, // e.g., "Step 100: quality=0.8"
    pub current_config: String,        // e.g., JSON of current params
    pub goal_description: String,      // e.g., "Maximize stability"
}

/// Helper trait for components that act as an "Observer" (Source of invention)
pub trait Observer {
    /// Analyze context and propose a new candidate
    fn propose(&self, context: &ObserverContext) -> Option<Candidate>;
}

/// A Mock Observer that simulates an LLM returning a suggestion
pub struct MockLlmObserver {
    // In a real impl, this would hold API keys, model names, etc.
    pub model_name: String,
}

impl MockLlmObserver {
    pub fn new(model_name: &str) -> Self {
        Self {
            model_name: model_name.to_string(),
        }
    }

    /// Simulate the LLM "thinking" and generating a JSON proposal
    fn simulate_llm_response(&self, _context: &ObserverContext) -> String {
        // Mock response: A JSON string representing a new Kernel variant
        // In reality, this string comes from the API.
        r#"{
            "hypothesis": "Increasing kernel radius slightly might smooth out high-frequency noise while maintaining edge stability.",
            "variant_name": "kernel_5x5_adaptive_mock",
            "metadata": {
                "kernel_radius": "2",
                "optimizer_bias": "stability"
            }
        }"#.to_string()
    }
}

impl Observer for MockLlmObserver {
    fn propose(&self, context: &ObserverContext) -> Option<Candidate> {
        // 1. "Build Prompt" (Mocked)
        let _prompt = format!(
            "System: You are an AI optimization assistant.\nContext: {:?}\nGoal: {}\nPropose a Variant JSON.",
            context.recent_telemetry, context.goal_description
        );

        // 2. Call LLM (Mocked)
        let response_json = self.simulate_llm_response(context);

        // 3. Parse Response
        // We use a simple untyped parse for the mock, then map to Candidate
        let parsed: serde_json::Value = match serde_json::from_str(&response_json) {
            Ok(v) => v,
            Err(_) => return None,
        };

        // 4. Construct Candidate
        let variant = Variant {
            id: 0, // Assigned by catalog later
            name: parsed["variant_name"]
                .as_str()
                .unwrap_or("unknown")
                .to_string(),
            version: "1.0-llm".to_string(),
            variant_type: VariantType::Kernel,
            constraints: VariantConstraints::default(),
            expected_latency_us: 500, // LLM estimates? Or we benchmark it.
            is_default: false,
            metadata: parsed["metadata"]
                .as_object()
                .unwrap()
                .iter()
                .map(|(k, v)| (k.clone(), v.as_str().unwrap_or("").to_string()))
                .collect(),
        };

        Some(Candidate {
            id: format!("cand_llm_{}", rand::random::<u32>()),
            source: DiscoverySource::LlmObserver,
            variant,
            hypothesis: parsed["hypothesis"].as_str().unwrap_or("").to_string(),
        })
    }
}
