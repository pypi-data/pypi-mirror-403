
import os
from openai import AzureOpenAI

# load API keys
from data_store_tools.tools.load_env import load_env_file

class LargeLanguageModelTools:
    
    def __init__(self,):
        
        load_env_file(".env")
        self.LLM_API_KEY = os.getenv('OPENAI_API_KEY')
        self.LLM_API_VERSION = os.getenv('OPENAI_VERSION')
        self.LLM_DEPLOYMENT = os.getenv('DEPLOYMENT')        
        self.LLM_ENPOINT = os.getenv('ENDPOINT')   
        
        print(f"model deployment - {self.LLM_DEPLOYMENT}")
        print(f"model version - {self.LLM_API_VERSION}")                 
        
        self.client = AzureOpenAI(
            api_version=self.LLM_API_VERSION,
            azure_endpoint=self.LLM_ENPOINT,
            api_key=self.LLM_API_KEY,
        )
        
        
    def query_large_language_model(self,
                                   role_text="You are a helpful assistant.",
                                   content_text="What is 2+2?"
                                   ):
        '''
        This function will allow you to query the large language model deffined in the .env file
        
        Inputs:
        role_text: Str - this is the role or persona you want the LLM to take. By default this is as an assistant but you may wan't to adjust this to be domain specific to help with the langauge returned
        content_text: Str - the bulk of your prompt. You are advised to follow the GCSE framework - Goal, Context, Source material, Expectation (of output format)
        
        Outputs:
        Response: Str -  the verbatim generated by the AI in response to the prompt
        '''
        
        payload = [
                    {
                        "role": "system",
                        "content": role_text,
                    },
                    {
                        "role": "user",
                        "content": content_text,
                    }
                ]
        
        if self.LLM_DEPLOYMENT == "gpt-4o-mini":

            response = self.client.chat.completions.create(
                messages=payload,
                max_tokens=4096,
                temperature=0.0,
                top_p=1.0,
                model=self.LLM_DEPLOYMENT
            )
             
            
        elif self.LLM_DEPLOYMENT == "gpt-5-mini":

            response = self.client.chat.completions.create(
                messages=payload,
                max_completion_tokens=128000,
                model=self.LLM_DEPLOYMENT
            )            
            
        elif self.LLM_DEPLOYMENT == "gpt-5-chat":

            response = self.client.chat.completions.create(
                messages=payload,
                max_tokens=16384,
                temperature=0.0,
                top_p=1.0,
                model=self.LLM_DEPLOYMENT
            )
            
        elif self.LLM_DEPLOYMENT == "gpt-5.1-chat":
            response = self.client.chat.completions.create(
                messages=payload,
                max_completion_tokens = 16384,
                top_p=1.0,
                model=self.LLM_DEPLOYMENT
            )
        
        return response.choices[0].message.content
    
    