"""
QA Dataset Loader
=================

This module provides tools for loading Question-Answering datasets and knowledge bases.
Designed for RAG benchmarks and QA system evaluation.
"""

import json
from pathlib import Path
from typing import Iterator, Optional, Union


class QADataLoader:
    """
    Load and manage QA datasets and knowledge bases.

    Supports multiple file formats:
    - queries.jsonl: JSONL format with query and id fields
    - knowledge bases: txt, pdf, docx formats

    Examples:
        >>> loader = QADataLoader()
        >>> queries = loader.load_queries()
        >>> knowledge = loader.load_knowledge_base()
    """

    def __init__(self, data_dir: Optional[Union[str, Path]] = None):
        """
        Initialize QA data loader.

        Args:
            data_dir: Path to QA data directory. Defaults to current module directory.
        """
        if data_dir is None:
            data_dir = Path(__file__).parent
        self.data_dir = Path(data_dir)

        if not self.data_dir.exists():
            raise FileNotFoundError(f"QA data directory not found: {self.data_dir}")

    def load_queries(self, filename: str = "queries.jsonl") -> list[dict[str, str]]:
        """
        Load queries from JSONL file.

        Args:
            filename: Name of the queries file (default: queries.jsonl)

        Returns:
            List of query dictionaries with 'query' and 'id' fields

        Example:
            >>> loader = QADataLoader()
            >>> queries = loader.load_queries()
            >>> print(queries[0])
            {'query': '什么是 ChromaDB？', 'id': 'q1'}
        """
        file_path = self.data_dir / filename

        if not file_path.exists():
            raise FileNotFoundError(f"Queries file not found: {file_path}")

        queries = []
        with open(file_path, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    queries.append(json.loads(line))

        return queries

    def iter_queries(self, filename: str = "queries.jsonl") -> Iterator[dict[str, str]]:
        """
        Iterate over queries without loading all into memory.

        Args:
            filename: Name of the queries file

        Yields:
            Query dictionary with 'query' and 'id' fields

        Example:
            >>> loader = QADataLoader()
            >>> for query in loader.iter_queries():
            ...     print(query['query'])
        """
        file_path = self.data_dir / filename

        if not file_path.exists():
            raise FileNotFoundError(f"Queries file not found: {file_path}")

        with open(file_path, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    yield json.loads(line)

    def load_knowledge_base(self, filename: str = "qa_knowledge_base.txt") -> str:
        """
        Load knowledge base from text file.

        Args:
            filename: Name of the knowledge base file

        Returns:
            Content of the knowledge base as string

        Example:
            >>> loader = QADataLoader()
            >>> kb = loader.load_knowledge_base()
            >>> print(kb[:100])
        """
        file_path = self.data_dir / filename

        if not file_path.exists():
            raise FileNotFoundError(f"Knowledge base file not found: {file_path}")

        with open(file_path, encoding="utf-8") as f:
            return f.read()

    def load_knowledge_chunks(
        self, filename: str = "qa_knowledge_base.txt", chunk_separator: str = "\n\n"
    ) -> list[str]:
        """
        Load knowledge base and split into chunks.

        Args:
            filename: Name of the knowledge base file
            chunk_separator: Separator to split chunks (default: double newline)

        Returns:
            List of knowledge chunks

        Example:
            >>> loader = QADataLoader()
            >>> chunks = loader.load_knowledge_chunks()
            >>> print(f"Total chunks: {len(chunks)}")
        """
        content = self.load_knowledge_base(filename)
        chunks = [chunk.strip() for chunk in content.split(chunk_separator)]
        return [chunk for chunk in chunks if chunk]

    def list_files(self, pattern: str = "*") -> list[Path]:
        """
        List all files in the QA data directory matching pattern.

        Args:
            pattern: Glob pattern to match files (default: all files)

        Returns:
            List of Path objects

        Example:
            >>> loader = QADataLoader()
            >>> txt_files = loader.list_files("*.txt")
            >>> for file in txt_files:
            ...     print(file.name)
        """
        return list(self.data_dir.glob(pattern))

    def get_sample_data(self, sample_name: str) -> str:
        """
        Load data from the sample directory.

        Args:
            sample_name: Name of the sample file (e.g., 'question.txt')

        Returns:
            Content of the sample file

        Example:
            >>> loader = QADataLoader()
            >>> sample = loader.get_sample_data("question.txt")
        """
        sample_path = self.data_dir / "sample" / sample_name

        if not sample_path.exists():
            raise FileNotFoundError(f"Sample file not found: {sample_path}")

        if sample_path.suffix == ".json":
            with open(sample_path, encoding="utf-8") as f:
                return json.load(f)
        else:
            with open(sample_path, encoding="utf-8") as f:
                return f.read()

    def get_statistics(self) -> dict[str, any]:
        """
        Get statistics about the QA dataset.

        Returns:
            Dictionary with dataset statistics

        Example:
            >>> loader = QADataLoader()
            >>> stats = loader.get_statistics()
            >>> print(f"Total queries: {stats['num_queries']}")
        """
        stats = {
            "data_dir": str(self.data_dir),
            "num_queries": 0,
            "knowledge_base_size": 0,
            "available_files": [],
        }

        # Count queries
        queries_file = self.data_dir / "queries.jsonl"
        if queries_file.exists():
            stats["num_queries"] = len(self.load_queries())

        # Get knowledge base size
        kb_file = self.data_dir / "qa_knowledge_base.txt"
        if kb_file.exists():
            stats["knowledge_base_size"] = kb_file.stat().st_size
            stats["knowledge_base_chunks"] = len(self.load_knowledge_chunks())

        # List available files
        stats["available_files"] = [f.name for f in self.list_files() if f.is_file()]

        return stats


# Usage example
if __name__ == "__main__":
    print("=" * 60)
    print("QA Dataset Loader - Usage Example")
    print("=" * 60)

    # Initialize loader
    loader = QADataLoader()

    # 1. Load queries
    print("\n1. Loading queries:")
    print("-" * 60)
    queries = loader.load_queries()
    print(f"Total queries: {len(queries)}")
    for i, query in enumerate(queries[:3], 1):
        print(f"  {i}. [{query['id']}] {query['query']}")
    if len(queries) > 3:
        print(f"  ... and {len(queries) - 3} more queries")

    # 2. Load knowledge base
    print("\n2. Loading knowledge base:")
    print("-" * 60)
    kb = loader.load_knowledge_base()
    print(f"Knowledge base size: {len(kb)} characters")
    print(f"Preview (first 200 chars):\n{kb[:200]}...")

    # 3. Load knowledge chunks
    print("\n3. Loading knowledge chunks:")
    print("-" * 60)
    chunks = loader.load_knowledge_chunks()
    print(f"Total chunks: {len(chunks)}")
    for i, chunk in enumerate(chunks[:2], 1):
        preview = chunk[:80].replace("\n", " ")
        print(f"  Chunk {i}: {preview}...")

    # 4. Get statistics
    print("\n4. Dataset statistics:")
    print("-" * 60)
    stats = loader.get_statistics()
    for key, value in stats.items():
        if isinstance(value, list):
            print(f"  {key}: {len(value)} items")
        else:
            print(f"  {key}: {value}")

    print("\n" + "=" * 60)
    print("Done!")
    print("=" * 60)
