"""
Control Plane Benchmark DataLoader

This module provides a data loader for the Control Plane scheduling benchmark,
supporting LLM workloads, hybrid workloads, and test prompts/texts.

The benchmark evaluates scheduling policies under various workload conditions:
- Pure LLM scheduling: Testing FIFO, Priority, SLO-aware, Adaptive, etc.
- Hybrid scheduling: Mixed LLM + Embedding requests with HybridSchedulingPolicy
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterator, Literal, Optional, overload

from pydantic import BaseModel, Field, field_validator, model_validator

try:
    from datasets import load_dataset

    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False

# ============================================================================
# Data Models
# ============================================================================


class WorkloadConfig(BaseModel):
    """Base workload configuration."""

    workload_id: str = Field(..., description="Unique workload identifier")
    request_count: int = Field(..., ge=1, description="Total number of requests")
    rate_per_second: float = Field(..., gt=0, description="Target request rate")
    arrival_pattern: str = Field(
        default="poisson",
        pattern="^(uniform|poisson|burst|wave)$",
        description="Request arrival pattern",
    )
    priority_distribution: dict[str, float] = Field(
        default_factory=lambda: {"HIGH": 0.2, "NORMAL": 0.6, "LOW": 0.2},
        description="Priority distribution",
    )

    @field_validator("priority_distribution")
    @classmethod
    def validate_priority_sum(cls, v: dict[str, float]) -> dict[str, float]:
        """Ensure priority probabilities sum to approximately 1.0."""
        total = sum(v.values())
        if not (0.99 <= total <= 1.01):
            raise ValueError(f"Priority distribution must sum to 1.0, got {total}")
        return v


class LLMWorkloadConfig(WorkloadConfig):
    """LLM-specific workload configuration."""

    model_distribution: dict[str, float] = Field(
        default_factory=lambda: {"default": 1.0},
        description="Distribution across models",
    )
    prompt_len_range: tuple[int, int] = Field(
        default=(50, 200), description="Prompt length range in tokens"
    )
    output_len_range: tuple[int, int] = Field(
        default=(100, 500), description="Output length range in tokens"
    )
    slo_deadlines: dict[str, int] = Field(
        default_factory=lambda: {"HIGH": 500, "NORMAL": 1000, "LOW": 2000},
        description="SLO deadlines in ms by priority",
    )

    @field_validator("model_distribution")
    @classmethod
    def validate_model_sum(cls, v: dict[str, float]) -> dict[str, float]:
        """Ensure model probabilities sum to approximately 1.0."""
        total = sum(v.values())
        if not (0.99 <= total <= 1.01):
            raise ValueError(f"Model distribution must sum to 1.0, got {total}")
        return v


class HybridWorkloadConfig(WorkloadConfig):
    """Hybrid (LLM + Embedding) workload configuration."""

    llm_ratio: float = Field(..., ge=0, le=1, description="Proportion of LLM requests")
    embedding_ratio: float = Field(..., ge=0, le=1, description="Proportion of Embedding requests")

    # LLM settings
    llm_model_distribution: dict[str, float] = Field(
        default_factory=lambda: {"default": 1.0},
        description="LLM model distribution",
    )
    llm_slo_deadlines: dict[str, int] = Field(
        default_factory=lambda: {"HIGH": 500, "NORMAL": 1000, "LOW": 2000},
        description="LLM SLO deadlines in ms",
    )

    # Embedding settings
    embedding_model: str = Field(default="BAAI/bge-m3", description="Embedding model to use")
    embedding_batch_sizes: list[int] = Field(
        default_factory=lambda: [1, 8, 16, 32],
        description="Allowed embedding batch sizes",
    )
    embedding_slo_deadline_ms: int = Field(
        default=200, ge=50, description="Embedding SLO deadline in ms"
    )

    # Burst configuration (optional)
    burst_config: Optional[dict[str, Any]] = Field(
        default=None, description="Burst pattern configuration"
    )

    @model_validator(mode="after")
    def validate_ratios(self) -> HybridWorkloadConfig:
        """Ensure LLM and Embedding ratios sum to 1.0."""
        total = self.llm_ratio + self.embedding_ratio
        if not (0.99 <= total <= 1.01):
            raise ValueError(f"llm_ratio + embedding_ratio must equal 1.0, got {total}")
        return self


class LLMPrompt(BaseModel):
    """LLM test prompt data."""

    prompt_id: str = Field(..., pattern=r"^llm_\d{3,6}$", description="Prompt ID")
    text: str = Field(..., min_length=10, description="Prompt text")
    expected_tokens: int = Field(default=200, ge=10, le=4096, description="Expected output tokens")
    priority: str = Field(
        default="NORMAL",
        pattern="^(HIGH|NORMAL|LOW)$",
        description="Suggested priority",
    )
    category: str = Field(default="general", description="Prompt category")
    difficulty: str = Field(
        default="medium",
        pattern="^(easy|medium|hard)$",
        description="Difficulty level",
    )


class EmbeddingText(BaseModel):
    """Embedding test text data."""

    text_id: str = Field(..., pattern=r"^embed_\d{3,6}$", description="Text batch ID")
    texts: list[str] = Field(..., min_length=1, max_length=128, description="Texts to embed")
    model: Optional[str] = Field(default=None, description="Specific model")
    batch_size: int = Field(default=1, ge=1, le=128, description="Suggested batch size")
    category: str = Field(default="general", description="Text category")


# ============================================================================
# DataLoader
# ============================================================================


@dataclass
class WorkloadStats:
    """Statistics for loaded workloads."""

    llm_workloads: int = 0
    hybrid_workloads: int = 0
    llm_prompts: int = 0
    embed_texts: int = 0


class ControlPlaneBenchmarkDataLoader:
    """
    DataLoader for Control Plane Benchmark dataset.

    This loader provides access to:
    - LLM workload configurations (light, medium, heavy)
    - Hybrid workload configurations (balanced, llm_heavy, embed_heavy, burst)
    - Test prompts for LLM requests
    - Test texts for Embedding requests

    Example:
        >>> loader = ControlPlaneBenchmarkDataLoader()
        >>> stats = loader.get_stats()
        >>> print(f"LLM workloads: {stats.llm_workloads}")
        >>>
        >>> # Load a workload
        >>> workload = loader.load_workload("llm_medium")
        >>> print(f"Requests: {workload.request_count}")
        >>>
        >>> # Iterate over prompts
        >>> for prompt in loader.iter_prompts("llm"):
        ...     print(f"{prompt.prompt_id}: {prompt.text[:50]}...")
    """

    WORKLOAD_CATEGORIES = ["llm", "hybrid"]
    LLM_WORKLOADS = ["light", "medium", "heavy"]
    HYBRID_WORKLOADS = ["balanced", "llm_heavy", "embed_heavy", "burst"]

    def __init__(self, data_dir: Optional[str | Path] = None):
        """
        Initialize the Control Plane Benchmark data loader.

        Args:
            data_dir: Directory containing benchmark data.
                     If None, uses the default directory relative to this file.
        """
        if data_dir is None:
            data_dir = Path(__file__).parent
        self.data_dir = Path(data_dir)

        # Data subdirectories
        self.llm_workloads_dir = self.data_dir / "data" / "llm_workloads"
        self.hybrid_workloads_dir = self.data_dir / "data" / "hybrid_workloads"
        self.prompts_dir = self.data_dir / "data" / "prompts"
        self.metadata_dir = self.data_dir / "metadata"

        # Caches
        self._workload_cache: dict[str, WorkloadConfig] = {}
        self._prompts_cache: dict[str, list[LLMPrompt]] = {}
        self._embed_cache: dict[str, list[EmbeddingText]] = {}

        # HF Hub datasets (lazy loaded)
        self._hf_llm_workloads = None
        self._hf_hybrid_workloads = None
        self._hf_prompts = None
        self._using_hf_hub = False

        # Check if data exists locally, try HF Hub if not
        if not self.llm_workloads_dir.exists() and HF_AVAILABLE:
            self._load_from_hf_hub()

    def _load_from_hf_hub(self) -> None:
        """Load datasets from Hugging Face Hub if local data not available."""
        try:
            print(f"Local data not found at {self.data_dir / 'data'}")
            print("Attempting to load from Hugging Face Hub...")

            # Load LLM workloads
            print("  - Loading LLM workloads...")
            self._hf_llm_workloads = load_dataset(
                "intellistream/sage-control-plane-llm-workloads", split="train"
            )
            print(f"    ✓ Loaded {len(self._hf_llm_workloads)} LLM workload configurations")

            # Load Hybrid workloads
            print("  - Loading Hybrid workloads...")
            self._hf_hybrid_workloads = load_dataset(
                "intellistream/sage-control-plane-hybrid-workloads", split="train"
            )
            print(f"    ✓ Loaded {len(self._hf_hybrid_workloads)} Hybrid workload configurations")

            # Load prompts
            print("  - Loading prompts...")
            self._hf_prompts = load_dataset(
                "intellistream/sage-control-plane-prompts", split="train"
            )
            print(f"    ✓ Loaded {len(self._hf_prompts)} test prompts")

            self._using_hf_hub = True
            print("✓ Successfully loaded from Hugging Face Hub")
        except Exception as e:
            print(f"⚠ Failed to load from HF Hub: {e}")
            print("Please ensure data files exist locally or install 'datasets' package.")

    # ========================================================================
    # Workload Methods
    # ========================================================================

    def list_workloads(self, category: Optional[Literal["llm", "hybrid"]] = None) -> list[str]:
        """
        List available workload configurations.

        Args:
            category: Filter by category ('llm' or 'hybrid'). If None, returns all.

        Returns:
            List of workload identifiers (e.g., ['llm_light', 'hybrid_balanced'])
        """
        workloads = []

        # Use HF Hub data if available
        if self._using_hf_hub:
            if (category is None or category == "llm") and self._hf_llm_workloads is not None:
                for item in self._hf_llm_workloads:
                    workloads.append(item.get("workload_id", ""))

            if (category is None or category == "hybrid") and self._hf_hybrid_workloads is not None:
                for item in self._hf_hybrid_workloads:
                    workloads.append(item.get("workload_id", ""))
        else:
            # Use local files
            if category is None or category == "llm":
                for name in self.LLM_WORKLOADS:
                    file_path = self.llm_workloads_dir / f"{name}.jsonl"
                    if file_path.exists():
                        workloads.append(f"llm_{name}")

            if category is None or category == "hybrid":
                for name in self.HYBRID_WORKLOADS:
                    file_path = self.hybrid_workloads_dir / f"{name}.jsonl"
                    if file_path.exists():
                        workloads.append(f"hybrid_{name}")

        return sorted(workloads)

    def load_workload(self, workload_id: str) -> LLMWorkloadConfig | HybridWorkloadConfig:
        """
        Load a workload configuration by ID.

        Args:
            workload_id: Workload identifier (e.g., 'llm_medium', 'hybrid_balanced')

        Returns:
            Workload configuration object

        Raises:
            ValueError: If workload not found or invalid format
        """
        # Check cache first
        if workload_id in self._workload_cache:
            return self._workload_cache[workload_id]  # type: ignore

        # Determine config class from workload ID
        if workload_id.startswith("llm_"):
            config_class = LLMWorkloadConfig
        elif workload_id.startswith("hybrid_"):
            config_class = HybridWorkloadConfig
        else:
            raise ValueError(
                f"Invalid workload_id format: {workload_id}. Expected 'llm_*' or 'hybrid_*'"
            )

        # Use HF Hub data if available
        if self._using_hf_hub:
            # Choose the right dataset based on workload type
            hf_dataset = None
            if workload_id.startswith("llm_") and self._hf_llm_workloads is not None:
                hf_dataset = self._hf_llm_workloads
            elif workload_id.startswith("hybrid_") and self._hf_hybrid_workloads is not None:
                hf_dataset = self._hf_hybrid_workloads

            if hf_dataset is not None:
                for item in hf_dataset:
                    if item.get("workload_id") == workload_id:
                        # Data is already in correct types from HF Hub
                        # Convert and clean up None values in dicts
                        data = {}
                        for key, value in item.items():
                            if isinstance(value, dict):
                                # Remove None values from dicts (artifacts from schema merging)
                                data[key] = {k: v for k, v in value.items() if v is not None}
                            else:
                                data[key] = value

                        config = config_class(**data)
                        self._workload_cache[workload_id] = config
                        return config

            raise ValueError(
                f"Workload not found in HF Hub: {workload_id}. "
                f"Available workloads: {self.list_workloads()}"
            )
        else:
            # Use local files
            if workload_id.startswith("llm_"):
                name = workload_id[4:]  # Remove 'llm_' prefix
                file_path = self.llm_workloads_dir / f"{name}.jsonl"
            else:  # hybrid
                name = workload_id[7:]  # Remove 'hybrid_' prefix
                file_path = self.hybrid_workloads_dir / f"{name}.jsonl"

            if not file_path.exists():
                raise ValueError(
                    f"Workload file not found: {file_path}. "
                    f"Available workloads: {self.list_workloads()}"
                )

            # Load from JSONL (first line contains the config)
            with open(file_path, encoding="utf-8") as f:
                first_line = f.readline().strip()
                if not first_line:
                    raise ValueError(f"Empty workload file: {file_path}")
            data = json.loads(first_line)

        config = config_class(**data)
        self._workload_cache[workload_id] = config
        return config

    def iter_workloads(
        self, category: Optional[Literal["llm", "hybrid"]] = None
    ) -> Iterator[LLMWorkloadConfig | HybridWorkloadConfig]:
        """
        Iterate over all workload configurations.

        Args:
            category: Filter by category ('llm' or 'hybrid')

        Yields:
            Workload configuration objects
        """
        for workload_id in self.list_workloads(category):
            yield self.load_workload(workload_id)

    # ========================================================================
    # Prompt Methods
    # ========================================================================

    @overload
    def load_prompts(
        self,
        prompt_type: Literal["llm"],
        limit: Optional[int] = None,
    ) -> list[LLMPrompt]: ...

    @overload
    def load_prompts(
        self,
        prompt_type: Literal["embedding"],
        limit: Optional[int] = None,
    ) -> list[EmbeddingText]: ...

    def load_prompts(
        self,
        prompt_type: Literal["llm", "embedding"],
        limit: Optional[int] = None,
    ) -> list[LLMPrompt] | list[EmbeddingText]:
        """
        Load test prompts or embedding texts.

        Args:
            prompt_type: Type of prompts ('llm' or 'embedding')
            limit: Maximum number of prompts to load

        Returns:
            List of prompt or text objects
        """
        if prompt_type == "llm":
            return self._load_llm_prompts(limit)
        elif prompt_type == "embedding":
            return self._load_embed_texts(limit)
        else:
            raise ValueError(f"Invalid prompt_type: {prompt_type}")

    def _load_llm_prompts(self, limit: Optional[int] = None) -> list[LLMPrompt]:
        """Load LLM test prompts."""
        cache_key = f"llm_{limit}"
        if cache_key in self._prompts_cache:
            return self._prompts_cache[cache_key]

        file_path = self.prompts_dir / "llm_prompts.jsonl"
        if not file_path.exists():
            raise ValueError(f"Prompts file not found: {file_path}")

        prompts = []
        with open(file_path, encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    prompts.append(LLMPrompt(**data))
                    if limit and len(prompts) >= limit:
                        break

        self._prompts_cache[cache_key] = prompts
        return prompts

    def _load_embed_texts(self, limit: Optional[int] = None) -> list[EmbeddingText]:
        """Load embedding test texts."""
        cache_key = f"embed_{limit}"
        if cache_key in self._embed_cache:
            return self._embed_cache[cache_key]

        file_path = self.prompts_dir / "embed_texts.jsonl"
        if not file_path.exists():
            raise ValueError(f"Embed texts file not found: {file_path}")

        texts = []
        with open(file_path, encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    texts.append(EmbeddingText(**data))
                    if limit and len(texts) >= limit:
                        break

        self._embed_cache[cache_key] = texts
        return texts

    def iter_prompts(
        self, prompt_type: Literal["llm", "embedding"]
    ) -> Iterator[LLMPrompt | EmbeddingText]:
        """
        Iterate over test prompts.

        Args:
            prompt_type: Type of prompts ('llm' or 'embedding')

        Yields:
            Prompt or text objects
        """
        if prompt_type == "llm":
            file_path = self.prompts_dir / "llm_prompts.jsonl"
            model_class: type[LLMPrompt] | type[EmbeddingText] = LLMPrompt
        else:
            file_path = self.prompts_dir / "embed_texts.jsonl"
            model_class = EmbeddingText

        if not file_path.exists():
            return

        with open(file_path, encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    yield model_class(**data)

    # ========================================================================
    # Statistics Methods
    # ========================================================================

    def get_stats(self) -> WorkloadStats:
        """
        Get dataset statistics.

        Returns:
            WorkloadStats object with counts for each category
        """
        stats = WorkloadStats()

        # Count workloads
        stats.llm_workloads = len(self.list_workloads("llm"))
        stats.hybrid_workloads = len(self.list_workloads("hybrid"))

        # Count prompts
        llm_prompts_file = self.prompts_dir / "llm_prompts.jsonl"
        if llm_prompts_file.exists():
            with open(llm_prompts_file, encoding="utf-8") as f:
                stats.llm_prompts = sum(1 for line in f if line.strip())

        embed_texts_file = self.prompts_dir / "embed_texts.jsonl"
        if embed_texts_file.exists():
            with open(embed_texts_file, encoding="utf-8") as f:
                stats.embed_texts = sum(1 for line in f if line.strip())

        return stats

    def get_schema(self) -> dict[str, Any]:
        """
        Load the JSON schema for validation.

        Returns:
            JSON schema dictionary
        """
        schema_file = self.metadata_dir / "schema.json"
        if not schema_file.exists():
            return {}

        with open(schema_file, encoding="utf-8") as f:
            return json.load(f)

    # ========================================================================
    # Utility Methods
    # ========================================================================

    def get_sample_prompts(
        self,
        prompt_type: Literal["llm", "embedding"],
        count: int = 10,
        category: Optional[str] = None,
        difficulty: Optional[str] = None,
    ) -> list[LLMPrompt] | list[EmbeddingText]:
        """
        Get a sample of prompts with optional filtering.

        Args:
            prompt_type: Type of prompts ('llm' or 'embedding')
            count: Number of samples to return
            category: Filter by category
            difficulty: Filter by difficulty (LLM only)

        Returns:
            List of sampled prompts
        """
        import random

        if prompt_type == "llm":
            all_prompts = self.load_prompts("llm")
            filtered: list[LLMPrompt] = list(all_prompts)
            if category:
                filtered = [p for p in filtered if p.category == category]
            if difficulty:
                filtered = [p for p in filtered if p.difficulty == difficulty]
            if len(filtered) <= count:
                return filtered
            return random.sample(filtered, count)
        else:
            all_texts = self.load_prompts("embedding")
            filtered_texts: list[EmbeddingText] = list(all_texts)
            if category:
                filtered_texts = [t for t in filtered_texts if t.category == category]
            if len(filtered_texts) <= count:
                return filtered_texts
            return random.sample(filtered_texts, count)

    def validate_data(self) -> dict[str, list[str]]:
        """
        Validate all data files against schema.

        Returns:
            Dictionary mapping file names to validation errors (empty if valid)
        """
        errors: dict[str, list[str]] = {}

        # Validate workloads
        for workload_id in self.list_workloads():
            try:
                self.load_workload(workload_id)
            except Exception as e:
                errors[workload_id] = [str(e)]

        # Validate prompts
        for prompt_type in ["llm", "embedding"]:
            try:
                prompts = self.load_prompts(prompt_type)  # type: ignore
                if not prompts:
                    errors[f"{prompt_type}_prompts"] = ["No prompts found"]
            except Exception as e:
                errors[f"{prompt_type}_prompts"] = [str(e)]

        return errors


# ============================================================================
# Convenience Functions
# ============================================================================


def load_workload(workload_id: str) -> LLMWorkloadConfig | HybridWorkloadConfig:
    """
    Convenience function to load a workload configuration.

    Args:
        workload_id: Workload identifier

    Returns:
        Workload configuration object
    """
    loader = ControlPlaneBenchmarkDataLoader()
    return loader.load_workload(workload_id)


def load_prompts(
    prompt_type: Literal["llm", "embedding"],
    limit: Optional[int] = None,
) -> list[LLMPrompt] | list[EmbeddingText]:
    """
    Convenience function to load test prompts.

    Args:
        prompt_type: Type of prompts
        limit: Maximum number to load

    Returns:
        List of prompt objects
    """
    loader = ControlPlaneBenchmarkDataLoader()
    return loader.load_prompts(prompt_type, limit)
