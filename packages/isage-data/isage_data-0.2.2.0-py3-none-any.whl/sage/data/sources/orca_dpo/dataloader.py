"""
Orca DPO Pairs DataLoader

This module provides a data loader for the Orca DPO (Direct Preference Optimization) 
Pairs dataset from Intel, used for alignment experiments and preference learning.

The dataset contains question-answer pairs with chosen and rejected responses,
suitable for training models with preference-based optimization methods.
"""

import warnings
from typing import Dict, Iterator, List, Optional

try:
    from datasets import load_dataset
    HF_DATASETS_AVAILABLE = True
except ImportError:
    HF_DATASETS_AVAILABLE = False
    warnings.warn(
        "datasets library not found. Install it with: pip install datasets"
    )


class OrcaDPODataLoader:
    """
    DataLoader for Intel Orca DPO Pairs dataset.
    
    This dataset is designed for Direct Preference Optimization (DPO) and contains
    question-answer pairs where each question has both a chosen (preferred) and 
    rejected (non-preferred) response. This format is ideal for alignment research
    and preference learning experiments.
    
    Dataset Structure:
    - system: System prompt/instruction
    - question: The input question
    - chosen: The preferred/better response
    - rejected: The less preferred/worse response
    
    Attributes:
        dataset_name (str): Hugging Face dataset identifier
    """
    
    def __init__(self, dataset_name: str = "Intel/orca_dpo_pairs"):
        """
        Initialize the Orca DPO DataLoader.
        
        Args:
            dataset_name: Hugging Face dataset identifier (default: "Intel/orca_dpo_pairs")
        
        Raises:
            ImportError: If datasets library is not installed
        """
        if not HF_DATASETS_AVAILABLE:
            raise ImportError(
                "The 'datasets' library is required to use OrcaDPODataLoader. "
                "Install it with: pip install datasets"
            )
        
        self.dataset_name = dataset_name
        self._cache = {}
    
    def load_data(
        self,
        split: str = "train",
        cache: bool = True,
        streaming: bool = False
    ) -> List[Dict]:
        """
        Load the Orca DPO Pairs dataset.
        
        Args:
            split: Dataset split to load (default: "train")
            cache: Whether to cache the loaded data
            streaming: Whether to use streaming mode (for large datasets)
        
        Returns:
            List of examples, each containing:
                - system: System prompt
                - question: Input question
                - chosen: Preferred response
                - rejected: Non-preferred response
        
        Raises:
            ValueError: If split is invalid or data cannot be loaded
        """
        # Check cache
        cache_key = f"{split}"
        if cache and cache_key in self._cache:
            return self._cache[cache_key]
        
        # Load from Hugging Face
        try:
            if streaming:
                dataset = load_dataset(self.dataset_name, split=split, streaming=True)
                # For streaming, return an iterator wrapper
                return self._create_streaming_iterator(dataset)
            else:
                dataset = load_dataset(self.dataset_name, split=split)
        except Exception as e:
            raise ValueError(f"Failed to load dataset: {e}")
        
        # Convert to list of dictionaries
        examples = []
        for item in dataset:
            example = {
                "system": item.get("system", ""),
                "question": item.get("question", ""),
                "chosen": item.get("chosen", ""),
                "rejected": item.get("rejected", ""),
            }
            examples.append(example)
        
        # Cache if requested
        if cache:
            self._cache[cache_key] = examples
        
        return examples
    
    def _create_streaming_iterator(self, dataset) -> Iterator[Dict]:
        """
        Create an iterator for streaming mode.
        
        Args:
            dataset: Streaming dataset
        
        Yields:
            Example dictionaries
        """
        for item in dataset:
            yield {
                "system": item.get("system", ""),
                "question": item.get("question", ""),
                "chosen": item.get("chosen", ""),
                "rejected": item.get("rejected", ""),
            }
    
    def iter_examples(
        self,
        split: str = "train",
        batch_size: Optional[int] = None
    ) -> Iterator[Dict]:
        """
        Iterate over examples in the dataset.
        
        Args:
            split: Dataset split to iterate over
            batch_size: If specified, yield batches of examples instead of individual ones
        
        Yields:
            Example dictionaries or batches of examples
        """
        examples = self.load_data(split=split, cache=True)
        
        if batch_size is None:
            # Yield individual examples
            for example in examples:
                yield example
        else:
            # Yield batches
            for i in range(0, len(examples), batch_size):
                yield examples[i:i + batch_size]
    
    def get_statistics(self, split: str = "train") -> Dict:
        """
        Get statistics about the dataset.
        
        Args:
            split: Dataset split to analyze
        
        Returns:
            Dictionary containing:
                - num_examples: Total number of examples
                - avg_question_length: Average question length (characters)
                - avg_chosen_length: Average chosen response length
                - avg_rejected_length: Average rejected response length
                - has_system_prompts: Whether system prompts are present
        """
        examples = self.load_data(split=split, cache=True)
        
        if not examples:
            return {
                "num_examples": 0,
                "avg_question_length": 0,
                "avg_chosen_length": 0,
                "avg_rejected_length": 0,
                "has_system_prompts": False,
            }
        
        total_question_len = sum(len(ex["question"]) for ex in examples)
        total_chosen_len = sum(len(ex["chosen"]) for ex in examples)
        total_rejected_len = sum(len(ex["rejected"]) for ex in examples)
        has_system = any(ex["system"] for ex in examples)
        
        return {
            "num_examples": len(examples),
            "avg_question_length": total_question_len / len(examples),
            "avg_chosen_length": total_chosen_len / len(examples),
            "avg_rejected_length": total_rejected_len / len(examples),
            "has_system_prompts": has_system,
        }
    
    def sample_examples(
        self,
        n: int = 5,
        split: str = "train",
        seed: Optional[int] = None
    ) -> List[Dict]:
        """
        Sample random examples from the dataset.
        
        Args:
            n: Number of examples to sample
            split: Dataset split to sample from
            seed: Random seed for reproducibility
        
        Returns:
            List of sampled examples
        """
        import random
        
        examples = self.load_data(split=split, cache=True)
        
        if seed is not None:
            random.seed(seed)
        
        return random.sample(examples, min(n, len(examples)))
    
    def format_for_dpo(self, example: Dict, include_system: bool = True) -> Dict:
        """
        Format an example for DPO training.
        
        Args:
            example: Example dictionary from load_data()
            include_system: Whether to include system prompt
        
        Returns:
            Formatted dictionary with:
                - prompt: The full prompt (system + question)
                - chosen: The preferred response
                - rejected: The non-preferred response
        """
        if include_system and example["system"]:
            prompt = f"{example['system']}\n\n{example['question']}"
        else:
            prompt = example["question"]
        
        return {
            "prompt": prompt,
            "chosen": example["chosen"],
            "rejected": example["rejected"],
        }
    
    def export_for_training(
        self,
        output_file: str,
        split: str = "train",
        format_type: str = "jsonl",
        include_system: bool = True,
        max_examples: Optional[int] = None
    ):
        """
        Export dataset in a format suitable for training.
        
        Args:
            output_file: Path to output file
            split: Dataset split to export
            format_type: Output format ("jsonl" or "json")
            include_system: Whether to include system prompts
            max_examples: Maximum number of examples to export (None for all)
        """
        import json
        
        examples = self.load_data(split=split, cache=True)
        
        if max_examples:
            examples = examples[:max_examples]
        
        formatted_examples = [
            self.format_for_dpo(ex, include_system) for ex in examples
        ]
        
        if format_type == "jsonl":
            with open(output_file, "w", encoding="utf-8") as f:
                for example in formatted_examples:
                    f.write(json.dumps(example, ensure_ascii=False) + "\n")
        elif format_type == "json":
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(formatted_examples, f, ensure_ascii=False, indent=2)
        else:
            raise ValueError(f"Unsupported format: {format_type}")
    
    def clear_cache(self):
        """Clear the internal cache."""
        self._cache.clear()
    
    def compare_responses(self, example: Dict) -> Dict:
        """
        Compare chosen and rejected responses.
        
        Args:
            example: Example dictionary
        
        Returns:
            Comparison dictionary with length statistics
        """
        return {
            "question": example["question"],
            "chosen_length": len(example["chosen"]),
            "rejected_length": len(example["rejected"]),
            "length_difference": len(example["chosen"]) - len(example["rejected"]),
        }
