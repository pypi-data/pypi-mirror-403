# Configuration for GenAI OpenTelemetry instrumentation

# The name of the service. Defaults to "genai-app".
OTEL_SERVICE_NAME=genai-app

# The OTLP exporter endpoint. Defaults to "http://localhost:4318".
# If not set or empty, telemetry will be exported to console instead of OTLP.
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# OTLP exporter protocol. Defaults to "http/protobuf" (HTTP).
# Options:
#   - "http/protobuf" or unset: Use HTTP protocol (default)
#   - "grpc": Use gRPC protocol
# Note: This setting is checked at import time, so it must be set before importing genai_otel.
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf

# Timeout for OTLP exporter in seconds. Defaults to 60 seconds.
# This is the maximum time the exporter will wait for a response from the OTLP endpoint.
# OTEL_EXPORTER_OTLP_TIMEOUT=60

# Optional headers for the OTLP exporter, in key=value format, separated by commas.
# Example: OTEL_EXPORTER_OTLP_HEADERS=my-header=my-value,another-header=another-value
# OTEL_EXPORTER_OTLP_HEADERS=

# Optional service instance identifier (e.g., container ID, pod name, hostname).
# Useful for distinguishing between multiple instances of the same service.
# OTEL_SERVICE_INSTANCE_ID=

# Deployment environment (e.g., dev, staging, production).
# OTEL_ENVIRONMENT=

# Comma-separated list of instrumentors to enable. If not set, all supported instrumentors are enabled by default.
# Example: GENAI_ENABLED_INSTRUMENTORS=openai,langchain,llama_index
# GENAI_ENABLED_INSTRUMENTORS=

# Enable GPU metrics collection. Defaults to "true".
# GENAI_ENABLE_GPU_METRICS=true

# GPU metrics collection interval in seconds. Defaults to 5 seconds.
# Controls how frequently GPU metrics (utilization, memory, temperature, power) are collected.
# GENAI_GPU_COLLECTION_INTERVAL=5

# Enable cost tracking. Defaults to "true".
# GENAI_ENABLE_COST_TRACKING=true

# Custom pricing for models not in llm_pricing.json
# Provide a JSON string with the same structure as llm_pricing.json
# Custom prices will be merged with default pricing, with custom taking precedence
# Format: {"chat": {"model-name": {"promptPrice": 0.001, "completionPrice": 0.002}}}
# Example for chat models:
# GENAI_CUSTOM_PRICING_JSON='{"chat":{"my-custom-model":{"promptPrice":0.001,"completionPrice":0.002}}}'
# Example for embeddings:
# GENAI_CUSTOM_PRICING_JSON='{"embeddings":{"my-custom-embeddings":0.00005}}'
# Example for multiple categories:
# GENAI_CUSTOM_PRICING_JSON='{"chat":{"custom-chat":{"promptPrice":0.001,"completionPrice":0.002}},"embeddings":{"custom-embed":0.00005}}'
# GENAI_CUSTOM_PRICING_JSON=

# Enable MCP (Model Context Protocol) instrumentation. Defaults to "true".
# This includes databases, vector DBs, Redis, Kafka, etc.
# GENAI_ENABLE_MCP_INSTRUMENTATION=true

# Enable HTTP/API instrumentation (requests/httpx). Defaults to "false".
# Disabled by default to avoid conflicts with OTLP HTTP exporters.
# GENAI_ENABLE_HTTP_INSTRUMENTATION=false

# Logging configuration
# Set the logging level for the genai-otel library. Defaults to INFO.
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
GENAI_OTEL_LOG_LEVEL=INFO

# If set to "true", the instrumentation will raise an error if it fails. Defaults to "false".
# GENAI_FAIL_ON_ERROR=false

# Enable CO2 tracking. Defaults to "false".
# When enabled, CO2 emissions are calculated based on GPU power consumption.
# If codecarbon is installed, it provides automatic region-based carbon intensity.
# Install codecarbon: pip install genai-otel-instrument[co2]
# GENAI_ENABLE_CO2_TRACKING=false

# Carbon intensity in gCO2e/kWh. Defaults to "475.0".
# Used as fallback when codecarbon is not installed or fails to initialize.
# This value represents the CO2 emissions per kilowatt-hour of electricity.
# Examples: US average ~420, UK ~233, Germany ~350, France ~56, China ~555
# GENAI_CARBON_INTENSITY=475.0

# Electricity cost per kilowatt-hour (kWh) in USD. Defaults to "0.12".
# Used to calculate power consumption cost for GPU usage tracking.
# This metric (gen_ai.power.cost) tracks cumulative electricity costs based on GPU power draw.
# Common rates: US average ~$0.12/kWh, Europe ~$0.20/kWh, Industrial ~$0.07/kWh
# GENAI_POWER_COST_PER_KWH=0.12

# --- Codecarbon Integration Settings (for CO2 tracking) ---
# These settings are used when GENAI_ENABLE_CO2_TRACKING=true and codecarbon is installed.
# Codecarbon provides more accurate CO2 calculations using region-specific carbon intensity data.

# Country ISO code (3-letter). Examples: "USA", "GBR", "DEU", "FRA", "CHN", "IND"
# IMPORTANT: If not set, defaults to "USA". Set this to match your actual location for accurate carbon intensity.
# Full list: https://github.com/mlco2/codecarbon/blob/master/codecarbon/data/private_infra/2022/global_energy_mix.json
# GENAI_CO2_COUNTRY_ISO_CODE=USA
# GENAI_CO2_COUNTRY_ISO_CODE=IND  # For India

# Region/state within country (optional). Examples: "california", "texas", "new york"
# Provides more accurate carbon intensity for US states.
# GENAI_CO2_REGION=

# Cloud provider name (optional). Examples: "aws", "gcp", "azure"
# Used for cloud-specific carbon intensity data.
# GENAI_CO2_CLOUD_PROVIDER=

# Cloud region (optional). Examples: "us-east-1", "europe-west1", "eastus"
# Used together with cloud_provider for accurate cloud carbon intensity.
# GENAI_CO2_CLOUD_REGION=

# Run codecarbon in offline mode (no external API calls). Defaults to "true".
# When true, uses local carbon intensity data based on country_iso_code.
# When false, may fetch real-time carbon intensity from APIs (requires internet).
# GENAI_CO2_OFFLINE_MODE=true

# Tracking mode. Options: "machine" (all processes) or "process" (current only). Defaults to "machine".
# "machine": Tracks total machine power consumption (more accurate for dedicated GPU servers)
# "process": Tracks only the current process (useful for shared environments)
# GENAI_CO2_TRACKING_MODE=machine

# Force manual CO2 calculation. Defaults to "false".
# When true, uses GENAI_CARBON_INTENSITY for CO2 calculation even if codecarbon is installed.
# Useful when you want to use your own carbon intensity value instead of codecarbon's region-based data.
# GENAI_CO2_USE_MANUAL=false

# Codecarbon logging level. Options: "debug", "info", "warning", "error", "critical". Defaults to "error".
# Controls verbosity of codecarbon's internal logging (warnings about CPU tracking mode, multiple instances, etc.)
# Set to "error" (default) to suppress informational warnings
# Set to "warning" or "info" to see codecarbon's diagnostic messages
# GENAI_CODECARBON_LOG_LEVEL=error

# --- OpenTelemetry Semantic Conventions ---

# OpenTelemetry semantic convention stability opt-in. Defaults to "gen_ai".
# Options:
#   - "gen_ai": Use new semantic conventions only (default)
#   - "gen_ai/dup": Emit both old and new token attributes for migration compatibility
# When set to "gen_ai/dup", both attribute sets are emitted:
#   - New: gen_ai.usage.prompt_tokens, gen_ai.usage.completion_tokens
#   - Old: gen_ai.usage.input_tokens, gen_ai.usage.output_tokens
# OTEL_SEMCONV_STABILITY_OPT_IN=gen_ai

# Enable content capture as span events. Defaults to "false".
# WARNING: When enabled, this captures full prompt and completion content as span events.
#          This may expose sensitive data. Use with caution and ensure proper data handling.
# GENAI_ENABLE_CONTENT_CAPTURE=false

# --- Ollama Server Metrics Configuration ---

# Enable automatic Ollama server metrics polling. Defaults to "true".
# When enabled, polls Ollama's /api/ps endpoint to get VRAM usage and running models.
# This populates the gen_ai.server.kv_cache.usage metric automatically.
# GENAI_ENABLE_OLLAMA_SERVER_METRICS=true

# Ollama server base URL. Defaults to "http://localhost:11434".
# Change this if Ollama is running on a different host or port.
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama server metrics polling interval in seconds. Defaults to "5.0".
# How frequently to query /api/ps for running model metrics.
# GENAI_OLLAMA_METRICS_INTERVAL=5.0

# Maximum VRAM in GB for your GPU. OPTIONAL - Auto-detected if not set.
# The system will auto-detect GPU VRAM using nvidia-ml-py or nvidia-smi.
# Only set this if auto-detection fails or you want to override the detected value.
# Example: GENAI_OLLAMA_MAX_VRAM_GB=24 for a 24GB GPU (e.g., RTX 3090, 4090, A5000)
# Auto-detection requires: pip install genai-otel-instrument[gpu] (for nvidia-ml-py)
# GENAI_OLLAMA_MAX_VRAM_GB=

# --- API Keys and Connection Strings ---

# LLM Provider API Keys
# OPENAI_API_KEY=your_openai_api_key
# OPENROUTER_API_KEY=your_openrouter_api_key
# ANTHROPIC_API_KEY=your_anthropic_api_key
# GOOGLE_API_KEY=your_google_api_key
# AWS_ACCESS_KEY_ID=your_aws_access_key_id
# AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
# AWS_REGION=your_aws_region
# AZURE_OPENAI_API_KEY=your_azure_openai_api_key
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
# COHERE_API_KEY=your_cohere_api_key
# MISTRAL_API_KEY=your_mistral_api_key
# TOGETHER_API_KEY=your_together_api_key
# GROQ_API_KEY=your_groq_api_key
# REPLICATE_API_TOKEN=your_replicate_api_token
# ANYSCALE_API_KEY=your_anyscale_api_key
# For Vertex AI, ensure GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path of your service account key file.

# MCP Instrumentation Connection Details

# Databases
# POSTGRES_USER=your_postgres_user
# POSTGRES_PASSWORD=your_postgres_password
# POSTGRES_HOST=your_postgres_host
# POSTGRES_PORT=5432
# POSTGRES_DB=your_postgres_db

# MYSQL_USER=your_mysql_user
# MYSQL_PASSWORD=your_mysql_password
# MYSQL_HOST=your_mysql_host
# MYSQL_PORT=3306
# MYSQL_DB=your_mysql_db

# MONGODB_URI=your_mongodb_connection_string

# Caching
# REDIS_HOST=your_redis_host
# REDIS_PORT=6379
# REDIS_PASSWORD=your_redis_password

# Message Queues
# KAFKA_BOOTSTRAP_SERVERS=your_kafka_bootstrap_servers

# Vector Databases
# PINECONE_API_KEY=your_pinecone_api_key
# PINECONE_ENVIRONMENT=your_pinecone_environment

# WEAVIATE_URL=your_weaviate_url
# WEAVIATE_API_KEY=your_weaviate_api_key

# QDRANT_URL=your_qdrant_url
# QDRANT_API_KEY=your_qdrant_api_key

# MILVUS_HOST=your_milvus_host
# MILVUS_PORT=19530

# --- Evaluation and Safety Features ---
# These features provide content moderation, compliance checking, and quality monitoring
# for LLM applications. All features are opt-in and disabled by default.

# === PII Detection ===
# Detect and protect Personally Identifiable Information (PII) in prompts and responses.
# Uses Microsoft Presidio for entity recognition and anonymization.
# Requirements: pip install genai-otel-instrument[pii]

# Enable PII detection. Defaults to "false".
# GENAI_ENABLE_PII_DETECTION=false

# PII detection mode. Options: "detect", "redact", or "block". Defaults to "detect".
# - "detect": Only detect and log PII (no modification)
# - "redact": Replace detected PII with asterisks in telemetry
# - "block": Block requests/responses containing PII (raises exception)
# GENAI_PII_MODE=detect

# PII detection confidence threshold (0.0-1.0). Defaults to "0.5".
# Lower values = more sensitive (may have false positives)
# Higher values = less sensitive (may miss some PII)
# Note: Presidio typically returns scores of 0.5-0.7 for valid PII matches
# GENAI_PII_THRESHOLD=0.5

# Enable GDPR compliance mode (adds EU-specific entity types). Defaults to "false".
# Adds: IBAN codes, UK NHS numbers, EU passport numbers
# GENAI_PII_GDPR_MODE=false

# Enable HIPAA compliance mode (adds healthcare entity types). Defaults to "false".
# Adds: Medical license numbers, US passport numbers, dates (Protected Health Information)
# GENAI_PII_HIPAA_MODE=false

# Enable PCI-DSS compliance mode (ensures credit card detection). Defaults to "false".
# Adds: Credit card numbers, bank account numbers
# GENAI_PII_PCI_DSS_MODE=false

# === Toxicity Detection ===
# Detect toxic, harmful, or offensive content in prompts and responses.
# Supports both Google Perspective API and local Detoxify model.
# Requirements: pip install genai-otel-instrument[toxicity]

# Enable toxicity detection. Defaults to "false".
# GENAI_ENABLE_TOXICITY_DETECTION=false

# Toxicity score threshold (0.0-1.0). Defaults to "0.7".
# Scores above this threshold are flagged as toxic.
# GENAI_TOXICITY_THRESHOLD=0.7

# Use Google Perspective API for toxicity detection. Defaults to "false".
# If false, falls back to local Detoxify model (no API key required).
# GENAI_TOXICITY_USE_PERSPECTIVE_API=false

# Google Perspective API key (required if USE_PERSPECTIVE_API=true).
# Get your API key at: https://perspectiveapi.com/
# GENAI_TOXICITY_PERSPECTIVE_API_KEY=your_perspective_api_key

# Block requests when toxicity is detected. Defaults to "false".
# When true, sets span status to ERROR and records blocked metrics.
# GENAI_TOXICITY_BLOCK_ON_DETECTION=false

# === Bias Detection ===
# Detect demographic and other biases in prompts and responses.
# Checks for gender, race, ethnicity, religion, age, disability, sexual orientation, and political bias.

# Enable bias detection. Defaults to "false".
# GENAI_ENABLE_BIAS_DETECTION=false

# Bias detection threshold (0.0-1.0). Defaults to "0.4".
# Pattern-based detection typically scores 0.3-0.5 for clear bias indicators.
# GENAI_BIAS_THRESHOLD=0.4

# Block requests when bias is detected. Defaults to "false".
# When true, sets span status to ERROR and records blocked metrics.
# GENAI_BIAS_BLOCK_ON_DETECTION=false

# === Prompt Injection Detection ===
# Detect and block prompt injection attacks (jailbreaks, system overrides, etc.).
# Protects against attempts to manipulate system prompts or extract sensitive information.

# Enable prompt injection detection. Defaults to "false".
# GENAI_ENABLE_PROMPT_INJECTION_DETECTION=false

# Prompt injection detection threshold (0.0-1.0). Defaults to "0.5".
# Pattern matching typically scores 0.5-0.7 for common injection techniques.
# GENAI_PROMPT_INJECTION_THRESHOLD=0.5

# Block requests when prompt injection is detected. Defaults to "false".
# When true, sets span status to ERROR and records blocked metrics.
# GENAI_PROMPT_INJECTION_BLOCK_ON_DETECTION=false

# === Restricted Topics Detection ===
# Detect and optionally block sensitive or inappropriate topics.
# Monitors for: medical advice, legal advice, financial advice, violence, self-harm,
# illegal activities, adult content, personal information requests, political manipulation.

# Enable restricted topics detection. Defaults to "false".
# GENAI_ENABLE_RESTRICTED_TOPICS=false

# Restricted topics threshold (0.0-1.0). Defaults to "0.5".
# GENAI_RESTRICTED_TOPICS_THRESHOLD=0.5

# Block requests when restricted topics are detected. Defaults to "false".
# When true, sets span status to ERROR and records blocked metrics.
# GENAI_RESTRICTED_TOPICS_BLOCK_ON_DETECTION=false

# === Hallucination Detection ===
# Detect potential hallucinations and factual inaccuracies in LLM responses.
# Analyzes: citation quality, hedge words, unsupported claims, vague references.
# Useful for RAG applications and fact-based Q&A systems.

# Enable hallucination detection. Defaults to "false".
# GENAI_ENABLE_HALLUCINATION_DETECTION=false

# Hallucination detection threshold (0.0-1.0). Defaults to "0.7".
# Higher scores indicate higher likelihood of hallucination.
# GENAI_HALLUCINATION_THRESHOLD=0.7

# === Evaluation Metrics ===
# All enabled evaluation features automatically export metrics to Prometheus:
# - genai.evaluation.pii.detections (PII detection counter)
# - genai.evaluation.toxicity.detections (Toxicity detection counter)
# - genai.evaluation.bias.detections (Bias detection counter)
# - genai.evaluation.prompt_injection.detections (Injection attempt counter)
# - genai.evaluation.restricted_topics.detections (Topic detection counter)
# - genai.evaluation.hallucination.detections (Hallucination counter)
#
# See examples/ directory for detailed usage examples of each feature.
