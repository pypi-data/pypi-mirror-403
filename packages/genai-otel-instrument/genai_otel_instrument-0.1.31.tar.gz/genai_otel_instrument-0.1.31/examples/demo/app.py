"""GenAI OTel Instrumentation Demo Application

This demo showcases automatic instrumentation of multiple LLM providers
and frameworks with cost tracking, metrics, and distributed tracing.
"""

import time

import genai_otel

# Enable auto-instrumentation for all supported libraries
print("ğŸš€ Starting GenAI OTel Demo...")
print("ğŸ“Š Auto-instrumenting all LLM libraries...")
genai_otel.instrument()
print("âœ… Instrumentation enabled!\n")

# Demo 1: OpenAI
print("=" * 60)
print("DEMO 1: OpenAI GPT-3.5 Turbo")
print("=" * 60)
try:
    from openai import OpenAI

    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is OpenTelemetry in one sentence?"},
        ],
        max_tokens=100,
    )

    print(f"âœ… OpenAI Response: {response.choices[0].message.content}")
    print(f"ğŸ“Š Tokens: {response.usage.total_tokens}")
    print(f"ğŸ’° Estimated cost captured in metrics\n")
except Exception as e:
    print(f"âš ï¸  OpenAI demo skipped: {e}\n")

time.sleep(1)

# Demo 2: Anthropic
print("=" * 60)
print("DEMO 2: Anthropic Claude")
print("=" * 60)
try:
    from anthropic import Anthropic

    anthropic_client = Anthropic()
    message = anthropic_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=100,
        messages=[{"role": "user", "content": "What is distributed tracing?"}],
    )

    print(f"âœ… Claude Response: {message.content[0].text}")
    print(f"ğŸ“Š Tokens: {message.usage.input_tokens + message.usage.output_tokens}")
    print(f"ğŸ’° Estimated cost captured in metrics\n")
except Exception as e:
    print(f"âš ï¸  Anthropic demo skipped: {e}\n")

time.sleep(1)

# Demo 3: LangChain
print("=" * 60)
print("DEMO 3: LangChain with OpenAI")
print("=" * 60)
try:
    from langchain.chains import LLMChain
    from langchain.prompts import PromptTemplate
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = PromptTemplate(
        input_variables=["topic"], template="Explain {topic} in simple terms in one sentence."
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.run(topic="observability")

    print(f"âœ… LangChain Response: {result}")
    print(f"ğŸ“Š Chain execution traced automatically\n")
except Exception as e:
    print(f"âš ï¸  LangChain demo skipped: {e}\n")

print("=" * 60)
print("ğŸ‰ Demo Complete!")
print("=" * 60)
print("\nğŸ“ˆ View traces and metrics:")
print("   - Jaeger UI: http://localhost:16686")
print("   - Prometheus: http://localhost:9091")
print("   - Grafana: http://localhost:3000")
print("\nğŸ” What was captured:")
print("   âœ… Distributed traces with parent-child relationships")
print("   âœ… Token usage metrics (prompt, completion, total)")
print("   âœ… Cost calculations for each request")
print("   âœ… Model and provider metadata")
print("   âœ… Request/response timing and latency")
print("   âœ… Error tracking (if any)")
print("\nğŸ’¡ Check the Jaeger UI to see all the telemetry data!")
print("\nKeeping container alive for 5 minutes to explore traces...")
time.sleep(300)
