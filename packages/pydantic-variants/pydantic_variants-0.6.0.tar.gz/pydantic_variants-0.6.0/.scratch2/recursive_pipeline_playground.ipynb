{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d7a2d7",
   "metadata": {},
   "source": [
    "# Recursive Pipeline Playground\n",
    "\n",
    "This notebook explores recursive pipeline patterns with objects that have `open()` and `build()` methods. We'll implement a simple recursive pipeline and demonstrate it with an encrypted dictionary example.\n",
    "\n",
    "## Key Concepts:\n",
    "- **Recursive Pipeline**: A pipeline where each step can itself be a pipeline\n",
    "- **Open/Build Pattern**: Objects that can be \"opened\" for modification and then \"built\" into final form\n",
    "- **Encrypted Dict**: A practical example showing secure data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce0545",
   "metadata": {},
   "source": [
    "## 1. Define the Pipeline Object\n",
    "\n",
    "Let's start with a simple recursive pipeline class that can chain operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976b825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple pipeline: Pipeline(add_one -> multiply_by_two -> subtract_three)\n",
      "add_one: 5 -> 6\n",
      "multiply_by_two: 6 -> 12\n",
      "subtract_three: 12 -> 9\n",
      "Result: 9\n"
     ]
    }
   ],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Simple recursive pipeline where each step can be a function or another Pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, step=None):\n",
    "        self.step = step\n",
    "        self.next = None\n",
    "    \n",
    "    def then(self, step_or_pipeline):\n",
    "        \"\"\"Add a step (function) or another pipeline to the chain\"\"\"\n",
    "        if self.next is None:\n",
    "            if callable(step_or_pipeline):\n",
    "                self.next = Pipeline(step_or_pipeline)\n",
    "            else:\n",
    "                self.next = step_or_pipeline\n",
    "        else:\n",
    "            self.next.then(step_or_pipeline)\n",
    "        return self  # Always return root pipeline\n",
    "    \n",
    "    def process(self, obj):\n",
    "        \"\"\"Execute the pipeline on an object\"\"\"\n",
    "        if self.step:\n",
    "            obj = self.step(obj)\n",
    "        if self.next:\n",
    "            obj = self.next.process(obj)\n",
    "        return obj\n",
    "    \n",
    "    def __repr__(self):\n",
    "        steps = []\n",
    "        current = self\n",
    "        while current:\n",
    "            if current.step:\n",
    "                steps.append(getattr(current.step, '__name__', str(current.step)))\n",
    "            current = current.next\n",
    "        return f\"Pipeline({' -> '.join(steps)})\"\n",
    "\n",
    "# Test with simple functions\n",
    "def add_one(x):\n",
    "    print(f\"add_one: {x} -> {x + 1}\")\n",
    "    return x + 1\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    print(f\"multiply_by_two: {x} -> {x * 2}\")\n",
    "    return x * 2\n",
    "\n",
    "def subtract_three(x):\n",
    "    print(f\"subtract_three: {x} -> {x - 3}\")\n",
    "    return x - 3\n",
    "\n",
    "# Create a simple pipeline\n",
    "simple_pipeline = Pipeline(add_one).then(multiply_by_two).then(subtract_three)\n",
    "print(\"Simple pipeline:\", simple_pipeline)\n",
    "print(\"Result:\", simple_pipeline.process(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655df748",
   "metadata": {},
   "source": [
    "## 2. Implement open and build Methods\n",
    "\n",
    "Now let's create a pipeline that supports the open/build pattern - where you can \"open\" an object for modification and \"build\" it into its final form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4392464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing BuildablePipeline:\n",
      "Opening: {'name': 'john', 'email': 'john@example.com', 'status': 'active'}\n",
      "add_timestamp: added timestamp\n",
      "uppercase_values: {'name': 'john', 'email': 'john@example.com', 'status': 'active', 'timestamp': '2025-07-30T01:41:04.673103'} -> {'name': 'JOHN', 'email': 'JOHN@EXAMPLE.COM', 'status': 'ACTIVE', 'timestamp': '2025-07-30T01:41:04.673103'}\n",
      "Building final result: {'name': 'JOHN', 'email': 'JOHN@EXAMPLE.COM', 'status': 'ACTIVE', 'timestamp': '2025-07-30T01:41:04.673103'}\n",
      "Final result: {'name': 'JOHN', 'email': 'JOHN@EXAMPLE.COM', 'status': 'ACTIVE', 'timestamp': '2025-07-30T01:41:04.673103'}\n"
     ]
    }
   ],
   "source": [
    "class BuildablePipeline:\n",
    "    \"\"\"\n",
    "    Simple pipeline with open/build pattern - same linking structure as Pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, step=None, opener=None, builder=None):\n",
    "        self.step = step\n",
    "        self.opener = opener    # Function to \"open\" the object\n",
    "        self.builder = builder  # Function to \"build\" the final object\n",
    "        self.next = None\n",
    "    \n",
    "    def then(self, step_or_pipeline):\n",
    "        \"\"\"Add a step (function) or another pipeline to the chain\"\"\"\n",
    "        if self.next is None:\n",
    "            if callable(step_or_pipeline):\n",
    "                self.next = BuildablePipeline(step_or_pipeline)\n",
    "            else:\n",
    "                self.next = step_or_pipeline\n",
    "        else:\n",
    "            self.next.then(step_or_pipeline)\n",
    "        return self  # Always return root pipeline\n",
    "    \n",
    "    def process(self, obj):\n",
    "        \"\"\"Process an object through the pipeline\"\"\"\n",
    "        # 1. Open the object (only at the root)\n",
    "        if self.opener:\n",
    "            obj = self.opener(obj)\n",
    "        \n",
    "        # 2. Apply this step\n",
    "        if self.step:\n",
    "            obj = self.step(obj)\n",
    "        \n",
    "        # 3. Continue to next pipeline (recursive)\n",
    "        if self.next:\n",
    "            obj = self.next.process(obj)\n",
    "        \n",
    "        # 4. Build final object (only at the root)\n",
    "        if self.builder:\n",
    "            obj = self.builder(obj)\n",
    "        \n",
    "        return obj\n",
    "\n",
    "# Example transformations for testing\n",
    "def uppercase_values(data):\n",
    "    \"\"\"Transform all string values to uppercase\"\"\"\n",
    "    result = {}\n",
    "    for k, v in data.items():\n",
    "        if isinstance(v, str):\n",
    "            result[k] = v.upper()\n",
    "        else:\n",
    "            result[k] = v\n",
    "    print(f\"uppercase_values: {data} -> {result}\")\n",
    "    return result\n",
    "\n",
    "def add_timestamp(data):\n",
    "    \"\"\"Add a timestamp to the data\"\"\"\n",
    "    from datetime import datetime\n",
    "    result = data.copy()\n",
    "    result['timestamp'] = datetime.now().isoformat()\n",
    "    print(\"add_timestamp: added timestamp\")\n",
    "    return result\n",
    "\n",
    "# Test the buildable pipeline\n",
    "def simple_opener(obj):\n",
    "    print(f\"Opening: {obj}\")\n",
    "    return obj.copy() if hasattr(obj, 'copy') else obj\n",
    "\n",
    "def simple_builder(obj):\n",
    "    print(f\"Building final result: {obj}\")\n",
    "    return obj\n",
    "\n",
    "# Create a simple buildable pipeline - same linking as section 1\n",
    "buildable_pipeline = BuildablePipeline(add_timestamp, simple_opener, simple_builder) \\\n",
    "    .then(uppercase_values)\n",
    "\n",
    "# Test data\n",
    "test_data = {\"name\": \"john\", \"email\": \"john@example.com\", \"status\": \"active\"}\n",
    "print(\"\\nTesting BuildablePipeline:\")\n",
    "result = buildable_pipeline.process(test_data)\n",
    "print(\"Final result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e579f",
   "metadata": {},
   "source": [
    "## 5. Complex File Operations Pipeline\n",
    "\n",
    "Let's create a sophisticated example that demonstrates file operations with automatic resource management, data processing, and error handling through the open/build pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3914f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created test files:\n",
      "- test_files/users.json\n",
      "- test_files/products.csv\n",
      "\\nFile processor created: FileProcessor('test_files/users.json', format=json, status=closed)\n",
      "Peek at content: {'users': [{'id': 1, 'name': 'John Doe', 'email': 'john@example.com', 'status': 'active'}, {'id': 2, 'name': 'Jane Smith', 'email': 'jane@example.com', 'status': 'pending'}], 'metadata': {'version': '1.0', 'created': '2025-07-30'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import csv\n",
    "from typing import Dict, Any\n",
    "import shutil\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"\n",
    "    Complex file processor that uses open/build pattern for safe file operations\n",
    "    Supports multiple file formats and automatic cleanup\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str, backup_enabled: bool = True):\n",
    "        self.file_path = file_path\n",
    "        self.backup_enabled = backup_enabled\n",
    "        self.temp_dir = None\n",
    "        self.working_file = None\n",
    "        self.backup_file = None\n",
    "        self.is_open = False\n",
    "        self.file_format = self._detect_format()\n",
    "        self.metadata = {\n",
    "            'operations': [],\n",
    "            'created_temp_files': [],\n",
    "            'warnings': []\n",
    "        }\n",
    "    \n",
    "    def _detect_format(self) -> str:\n",
    "        \"\"\"Detect file format from extension\"\"\"\n",
    "        ext = os.path.splitext(self.file_path)[1].lower()\n",
    "        format_map = {\n",
    "            '.json': 'json',\n",
    "            '.csv': 'csv',\n",
    "            '.txt': 'text',\n",
    "            '.log': 'text'\n",
    "        }\n",
    "        return format_map.get(ext, 'text')\n",
    "    \n",
    "    def open(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Open file for processing - creates working copy and backup\n",
    "        Returns a working data structure\n",
    "        \"\"\"\n",
    "        if self.is_open:\n",
    "            raise RuntimeError(\"FileProcessor is already open\")\n",
    "        \n",
    "        print(f\"üìÇ Opening file: {self.file_path}\")\n",
    "        \n",
    "        # Create temporary directory for operations\n",
    "        self.temp_dir = tempfile.mkdtemp(prefix=\"fileprocessor_\")\n",
    "        self.metadata['temp_dir'] = self.temp_dir\n",
    "        \n",
    "        # Create backup if enabled and file exists\n",
    "        if self.backup_enabled and os.path.exists(self.file_path):\n",
    "            self.backup_file = os.path.join(self.temp_dir, f\"backup_{os.path.basename(self.file_path)}\")\n",
    "            shutil.copy2(self.file_path, self.backup_file)\n",
    "            print(f\"üíæ Created backup: {self.backup_file}\")\n",
    "            self.metadata['operations'].append('backup_created')\n",
    "        \n",
    "        # Create working file\n",
    "        self.working_file = os.path.join(self.temp_dir, f\"working_{os.path.basename(self.file_path)}\")\n",
    "        \n",
    "        # Load data based on format\n",
    "        working_data = {'content': None, 'metadata': self.metadata}\n",
    "        \n",
    "        if os.path.exists(self.file_path):\n",
    "            working_data['content'] = self._load_file_content()\n",
    "        else:\n",
    "            working_data['content'] = self._create_empty_content()\n",
    "            self.metadata['warnings'].append('file_not_found_created_empty')\n",
    "        \n",
    "        self.is_open = True\n",
    "        self.metadata['operations'].append('file_opened')\n",
    "        \n",
    "        print(f\"‚úÖ File opened successfully. Format: {self.file_format}\")\n",
    "        return working_data\n",
    "    \n",
    "    def _load_file_content(self) -> Any:\n",
    "        \"\"\"Load file content based on detected format\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                if self.file_format == 'json':\n",
    "                    return json.load(f)\n",
    "                elif self.file_format == 'csv':\n",
    "                    reader = csv.DictReader(f)\n",
    "                    return list(reader)\n",
    "                else:  # text\n",
    "                    return f.read().splitlines()\n",
    "        except Exception as e:\n",
    "            self.metadata['warnings'].append(f'load_error: {str(e)}')\n",
    "            return self._create_empty_content()\n",
    "    \n",
    "    def _create_empty_content(self) -> Any:\n",
    "        \"\"\"Create empty content based on format\"\"\"\n",
    "        if self.file_format == 'json':\n",
    "            return {}\n",
    "        elif self.file_format == 'csv':\n",
    "            return []\n",
    "        else:  # text\n",
    "            return []\n",
    "    \n",
    "    def build(self, working_data: Dict[str, Any]) -> 'FileProcessor':\n",
    "        \"\"\"\n",
    "        Build final file from working data and cleanup\n",
    "        \"\"\"\n",
    "        if not self.is_open:\n",
    "            raise RuntimeError(\"FileProcessor is not open\")\n",
    "        \n",
    "        print(f\"üî® Building final file: {self.file_path}\")\n",
    "        \n",
    "        # Save working data to final file\n",
    "        self._save_file_content(working_data['content'])\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata = working_data['metadata']\n",
    "        self.metadata['operations'].append('file_built')\n",
    "        \n",
    "        # Cleanup\n",
    "        self._cleanup()\n",
    "        \n",
    "        print(\"‚úÖ File built successfully\")\n",
    "        \n",
    "        # Return new processor instance pointing to the updated file\n",
    "        new_processor = FileProcessor(self.file_path, self.backup_enabled)\n",
    "        new_processor.metadata = self.metadata.copy()\n",
    "        return new_processor\n",
    "    \n",
    "    def _save_file_content(self, content: Any):\n",
    "        \"\"\"Save content to file based on format\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.file_path, 'w', encoding='utf-8') as f:\n",
    "            if self.file_format == 'json':\n",
    "                json.dump(content, f, indent=2, ensure_ascii=False)\n",
    "            elif self.file_format == 'csv':\n",
    "                if content and len(content) > 0:\n",
    "                    fieldnames = content[0].keys() if isinstance(content[0], dict) else ['value']\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(content)\n",
    "            else:  # text\n",
    "                if isinstance(content, list):\n",
    "                    f.write('\\\\n'.join(content))\n",
    "                else:\n",
    "                    f.write(str(content))\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "            print(f\"üßπ Cleaned up temp directory: {self.temp_dir}\")\n",
    "        self.is_open = False\n",
    "    \n",
    "    def peek_content(self) -> Any:\n",
    "        \"\"\"Peek at file content without opening for modification\"\"\"\n",
    "        if os.path.exists(self.file_path):\n",
    "            return self._load_file_content()\n",
    "        return self._create_empty_content()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        status = \"open\" if self.is_open else \"closed\"\n",
    "        return f\"FileProcessor('{self.file_path}', format={self.file_format}, status={status})\"\n",
    "\n",
    "# Create test files for demonstration\n",
    "os.makedirs('test_files', exist_ok=True)\n",
    "\n",
    "# Create sample JSON file\n",
    "sample_json = {\n",
    "    \"users\": [\n",
    "        {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\", \"status\": \"active\"},\n",
    "        {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"status\": \"pending\"}\n",
    "    ],\n",
    "    \"metadata\": {\"version\": \"1.0\", \"created\": \"2025-07-30\"}\n",
    "}\n",
    "\n",
    "with open('test_files/users.json', 'w') as f:\n",
    "    json.dump(sample_json, f, indent=2)\n",
    "\n",
    "# Create sample CSV file\n",
    "sample_csv_data = [\n",
    "    {\"product\": \"laptop\", \"price\": \"1200\", \"category\": \"electronics\"},\n",
    "    {\"product\": \"book\", \"price\": \"25\", \"category\": \"education\"},\n",
    "    {\"product\": \"phone\", \"price\": \"800\", \"category\": \"electronics\"}\n",
    "]\n",
    "\n",
    "with open('test_files/products.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['product', 'price', 'category'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_csv_data)\n",
    "\n",
    "print(\"üìÅ Created test files:\")\n",
    "print(\"- test_files/users.json\")\n",
    "print(\"- test_files/products.csv\")\n",
    "\n",
    "# Test basic file processor\n",
    "processor = FileProcessor('test_files/users.json')\n",
    "print(\"\\\\nFile processor created:\", processor)\n",
    "print(\"Peek at content:\", processor.peek_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5dca093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== Testing JSON File Processing Pipeline ===\n",
      "Before processing: {'users': [{'id': 1, 'name': 'John Doe', 'email': 'john@example.com', 'status': 'active'}, {'id': 2, 'name': 'Jane Smith', 'email': 'jane@example.com', 'status': 'pending'}], 'metadata': {'version': '1.0', 'created': '2025-07-30'}}\n",
      "üìÇ Opening file: test_files/users.json\n",
      "üíæ Created backup: C:\\Users\\flash\\AppData\\Local\\Temp\\fileprocessor_7xe5shsn\\backup_users.json\n",
      "‚úÖ File opened successfully. Format: json\n",
      "üíæ Created original data backup\n",
      "‚úÖ JSON structure validated\n",
      "üìù Added user metadata\n",
      "üìä Updated file metadata\n",
      "üî® Building final file: test_files/users.json\n",
      "üßπ Cleaned up temp directory: C:\\Users\\flash\\AppData\\Local\\Temp\\fileprocessor_7xe5shsn\n",
      "‚úÖ File built successfully\n",
      "\\nAfter processing: {'users': [{'id': 1, 'name': 'John Doe', 'email': 'john@example.com', 'status': 'active', 'processed_at': '2025-07-30T12:00:00Z', 'processor_version': '2.0'}, {'id': 2, 'name': 'Jane Smith', 'email': 'jane@example.com', 'status': 'pending', 'processed_at': '2025-07-30T12:00:00Z', 'processor_version': '2.0'}], 'metadata': {'version': '1.0', 'created': '2025-07-30', 'last_processed': '2025-07-30T12:00:00Z', 'processor': 'FileProcessingPipeline', 'operations_count': 6}}\n",
      "Processing metadata: {'operations': ['backup_created', 'file_opened', 'original_data_backed_up', 'json_structure_validated', 'emails_normalized', 'user_metadata_added', 'file_metadata_updated', 'file_built'], 'created_temp_files': [], 'warnings': [], 'temp_dir': 'C:\\\\Users\\\\flash\\\\AppData\\\\Local\\\\Temp\\\\fileprocessor_7xe5shsn', 'original_backup': {'users': [{'id': 1, 'name': 'John Doe', 'email': 'john@example.com', 'status': 'active'}, {'id': 2, 'name': 'Jane Smith', 'email': 'jane@example.com', 'status': 'pending'}], 'metadata': {'version': '1.0', 'created': '2025-07-30'}}}\n"
     ]
    }
   ],
   "source": [
    "# File Processing Pipeline that works with FileProcessor\n",
    "class FileProcessingPipeline(BuildablePipeline):\n",
    "    \"\"\"\n",
    "    Specialized pipeline for FileProcessor with automatic resource management\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            opener=lambda file_processor: file_processor.open(),\n",
    "            builder=lambda working_data: self.source_processor.build(working_data)\n",
    "        )\n",
    "        self.source_processor = None\n",
    "    \n",
    "    def add_transform(self, transform_func):\n",
    "        \"\"\"Add a transformation function to the pipeline\"\"\"\n",
    "        return self.then(transform_func)\n",
    "    \n",
    "    def process(self, file_processor: FileProcessor):\n",
    "        \"\"\"Process a FileProcessor through the pipeline\"\"\"\n",
    "        self.source_processor = file_processor\n",
    "        return super().process(file_processor)\n",
    "\n",
    "# Complex file transformation functions\n",
    "def validate_json_structure(working_data):\n",
    "    \"\"\"Validate JSON file has required structure\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if not isinstance(content, dict):\n",
    "        raise ValueError(\"JSON content must be a dictionary\")\n",
    "    \n",
    "    required_keys = ['users', 'metadata']\n",
    "    for key in required_keys:\n",
    "        if key not in content:\n",
    "            content[key] = [] if key == 'users' else {}\n",
    "            metadata['warnings'].append(f'added_missing_key: {key}')\n",
    "    \n",
    "    metadata['operations'].append('json_structure_validated')\n",
    "    print(\"‚úÖ JSON structure validated\")\n",
    "    return working_data\n",
    "\n",
    "def normalize_user_emails(working_data):\n",
    "    \"\"\"Normalize email addresses in user data\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if 'users' in content:\n",
    "        for user in content['users']:\n",
    "            if 'email' in user:\n",
    "                old_email = user['email']\n",
    "                user['email'] = user['email'].lower().strip()\n",
    "                if old_email != user['email']:\n",
    "                    print(f\"üìß Normalized email: {old_email} -> {user['email']}\")\n",
    "    \n",
    "    metadata['operations'].append('emails_normalized')\n",
    "    return working_data\n",
    "\n",
    "def add_user_metadata(working_data):\n",
    "    \"\"\"Add processing metadata to users\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if 'users' in content:\n",
    "        for user in content['users']:\n",
    "            user['processed_at'] = '2025-07-30T12:00:00Z'\n",
    "            user['processor_version'] = '2.0'\n",
    "    \n",
    "    metadata['operations'].append('user_metadata_added')\n",
    "    print(\"üìù Added user metadata\")\n",
    "    return working_data\n",
    "\n",
    "def update_file_metadata(working_data):\n",
    "    \"\"\"Update file-level metadata\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if isinstance(content, dict) and 'metadata' in content:\n",
    "        content['metadata']['last_processed'] = '2025-07-30T12:00:00Z'\n",
    "        content['metadata']['processor'] = 'FileProcessingPipeline'\n",
    "        content['metadata']['operations_count'] = len(metadata['operations'])\n",
    "    \n",
    "    metadata['operations'].append('file_metadata_updated')\n",
    "    print(\"üìä Updated file metadata\")\n",
    "    return working_data\n",
    "\n",
    "def backup_original_data(working_data):\n",
    "    \"\"\"Create a backup of original data in the working structure\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    # Store original data reference\n",
    "    if 'original_backup' not in metadata:\n",
    "        metadata['original_backup'] = json.loads(json.dumps(content))  # Deep copy\n",
    "        metadata['operations'].append('original_data_backed_up')\n",
    "        print(\"üíæ Created original data backup\")\n",
    "    \n",
    "    return working_data\n",
    "\n",
    "# CSV-specific transformations\n",
    "def normalize_csv_prices(working_data):\n",
    "    \"\"\"Normalize price data in CSV\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if isinstance(content, list):\n",
    "        for row in content:\n",
    "            if 'price' in row:\n",
    "                try:\n",
    "                    # Convert to float and back to normalized string\n",
    "                    price_val = float(row['price'])\n",
    "                    row['price'] = f\"{price_val:.2f}\"\n",
    "                    row['price_normalized'] = True\n",
    "                except ValueError:\n",
    "                    metadata['warnings'].append(f'invalid_price: {row.get(\"price\", \"unknown\")}')\n",
    "    \n",
    "    metadata['operations'].append('csv_prices_normalized')\n",
    "    print(\"üí∞ Normalized CSV prices\")\n",
    "    return working_data\n",
    "\n",
    "def add_csv_categories(working_data):\n",
    "    \"\"\"Add category analysis to CSV data\"\"\"\n",
    "    content = working_data['content']\n",
    "    metadata = working_data['metadata']\n",
    "    \n",
    "    if isinstance(content, list):\n",
    "        categories = {}\n",
    "        for row in content:\n",
    "            category = row.get('category', 'unknown')\n",
    "            categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        # Add category summary to metadata\n",
    "        metadata['category_summary'] = categories\n",
    "        metadata['operations'].append('csv_categories_analyzed')\n",
    "        print(f\"üìä Analyzed categories: {categories}\")\n",
    "    \n",
    "    return working_data\n",
    "\n",
    "# Create pipelines for different file types\n",
    "json_processing_pipeline = FileProcessingPipeline() \\\n",
    "    .add_transform(backup_original_data) \\\n",
    "    .add_transform(validate_json_structure) \\\n",
    "    .add_transform(normalize_user_emails) \\\n",
    "    .add_transform(add_user_metadata) \\\n",
    "    .add_transform(update_file_metadata)\n",
    "\n",
    "csv_processing_pipeline = FileProcessingPipeline() \\\n",
    "    .add_transform(backup_original_data) \\\n",
    "    .add_transform(normalize_csv_prices) \\\n",
    "    .add_transform(add_csv_categories)\n",
    "\n",
    "print(\"\\\\n=== Testing JSON File Processing Pipeline ===\")\n",
    "json_processor = FileProcessor('test_files/users.json')\n",
    "print(\"Before processing:\", json_processor.peek_content())\n",
    "\n",
    "processed_json = json_processing_pipeline.process(json_processor)\n",
    "print(\"\\\\nAfter processing:\", processed_json.peek_content())\n",
    "print(\"Processing metadata:\", processed_json.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
