# æ‰€ä»¥MS2BioTextçš„è¾“å…¥åº”è¯¥æ˜¯åŒ…æ‹¬MS2ä¸BioTextã€‚ç„¶è€Œä¸€ä¸ªBioTextå¯¹åº”ä¸€ä¸ªåˆ†å­ï¼Œä¸€ä¸ªåˆ†å­å¯¹åº”å¤šä¸ªMS2ã€‚æ‰€ä»¥åœ¨datasetæ„å»ºçš„æ—¶å€™ï¼Œéœ€è¦å‚¨å­˜ï¼š
# MS2çš„åˆ—è¡¨
# MS2å¯¹åº”çš„åˆ†å­çš„åˆ—è¡¨
# åˆ†å­å¯¹åº”çš„BioTextçš„åˆ—è¡¨
# æœ€åitemçš„æ—¶å€™è¿”å›input_idsï¼ˆm/zï¼‰ï¼Œintensityï¼ŒBioText
# ï¼ˆè¿™é‡Œæ·»åŠ ä¸€ç³»åˆ—ç±»å†…æ–¹æ³•ï¼Œåœ¨initçš„æ—¶å€™è¾“å…¥å‚æ•°å¯ä»¥é€‰æ‹©å¤„ç†BioTextçš„ç±»å†…æ–¹æ³•ï¼‰
# ä½†æ˜¯é—®é¢˜æ˜¯ä½†æ˜¯åœ¨ä¹‹åçš„å®éªŒé‡Œï¼Œè¿˜è¦è®°å½•å¯¹äºæ¯ä¸ªMS2çš„å…¶ä»–ä¿¡æ¯ï¼Œè¿™ä¸ªæ€ä¹ˆå‚¨å­˜ã€‚


# å…ˆå®Œæˆdatasetç„¶ååˆ›å»ºå®ä¾‹
# è¯»å–HMDBæ•°æ®é›†
# HMDB.h5æ˜¯MS2æ•°æ®è¯»å–ä¸ºlistï¼Ÿï¼ŒHMDB.parquetæ˜¯metaæ•°æ®ï¼Œè¯»å–ä¸ºï¼Ÿç„¶åBiotextæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹åŒ…å«çš„ä¸€ç³»åˆ—txtæ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºHMDBçš„idï¼Œè¯»å–ä¸ºï¼Ÿ
# ï¼Œè¯»å–test dataè·‘é€šä¸¤ä¸ªæ¨¡å‹è¯•è¯•

import pickle
import h5py
import pandas as pd
import os
import torch
from torch.utils.data import Dataset
import numpy as np
import random
from pathlib import Path
from sklearn.model_selection import train_test_split
import json
from torch.utils.data import Sampler
import random, itertools
import math
from collections.abc import Iterator
from typing import Optional, TypeVar, Dict, List, Tuple
import random
import argparse
import torch
import torch.distributed as dist
from torch.utils.data.dataset import Dataset
from torch.utils.data.sampler import Sampler


_T_co = TypeVar("_T_co", covariant=True)

class MS2MoleculeDistributedSampler(Sampler[_T_co]):
    """
    ä¸“ä¸ºMS2å¯¹æ¯”å­¦ä¹ è®¾è®¡çš„åˆ†å¸ƒå¼é‡‡æ ·å™¨
    
    æ–°å¢åŠŸèƒ½ï¼šä¸ºæ¯ä¸ªbatchä¸­çš„æ ·æœ¬åˆ†é…ä¸å†²çªçš„text
    - ç¡®ä¿batchå†…æ¯ä¸ªmoleculeé€‰æ‹©çš„textä¸å‡ºç°åœ¨å…¶ä»–moleculeçš„å€™é€‰åˆ—è¡¨ä¸­
    - ä¿è¯batchå†…åªæœ‰å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬
    """
    
    def __init__(
        self,
        dataset: Dataset,
        batch_size: int,
        num_replicas: Optional[int] = None,
        rank: Optional[int] = None,
        shuffle: bool = True,
        seed: int = 0,
        drop_last: bool = False,
    ) -> None:
        if num_replicas is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            num_replicas = dist.get_world_size()
        if rank is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            rank = dist.get_rank()
        if rank >= num_replicas or rank < 0:
            raise ValueError(
                f"Invalid rank {rank}, rank should be in the interval [0, {num_replicas - 1}]"
            )
            
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_replicas = num_replicas
        self.rank = rank
        self.epoch = 0
        self.drop_last = drop_last
        self.shuffle = shuffle
        self.seed = seed
        
        # æ„å»ºmolecule -> textsçš„æ˜ å°„
        self.molecule_to_texts = self._build_molecule_text_mapping()
        
        # æŒ‰moleculeåˆ†ç»„å¹¶æŒ‰MS2æ•°é‡æ’åº
        self.molecule_groups = self._group_by_molecule()
        self.sorted_molecules = self._sort_molecules_by_ms2_count()
        
        # ç”Ÿæˆbatchåˆ†é…æ–¹æ¡ˆ
        self.batch_indices = self._create_batch_allocation()
        
        # ç¡®ä¿èƒ½è¢«GPUæ•°æ•´é™¤
        self._adjust_for_distributed()
        
        # è®¡ç®—æ¯ä¸ªè¿›ç¨‹çš„æ ·æœ¬æ•°
        total_batches = len(self.batch_indices)
        batches_per_replica = total_batches // self.num_replicas
        self.num_samples = batches_per_replica * self.batch_size
        
    def _build_molecule_text_mapping(self) -> Dict[str, List[str]]:
        """æ„å»ºæ¯ä¸ªmoleculeçš„å€™é€‰textåˆ—è¡¨"""
        molecule_to_texts = {}
        
        for mol_id, entry in self.dataset.biotext_data.items():
            texts = []
            if isinstance(entry, list):
                texts = [record['text'] for record in entry]
            elif isinstance(entry, dict):
                original = entry.get("original", "")
                paraphrases = entry.get("paraphrases", [])
                texts = [original] + paraphrases
            elif isinstance(entry, str):
                texts = [entry]
            
            molecule_to_texts[mol_id] = texts
        
        print(f"Built text mapping for {len(molecule_to_texts)} molecules")
        return molecule_to_texts
    
    def _group_by_molecule(self) -> Dict[str, List[int]]:
        """æŒ‰molecule IDå¯¹MS2æ•°æ®è¿›è¡Œåˆ†ç»„"""
        molecule_groups = {}
        for idx in range(len(self.dataset)):
            ms2_id = self.dataset.ms2_ids[idx]
            molecule_id = self.dataset.preprocessed_ms2_tensors[ms2_id]['molecule_id']
            
            if molecule_id not in molecule_groups:
                molecule_groups[molecule_id] = []
            molecule_groups[molecule_id].append(idx)
        
        return molecule_groups
    
    def _sort_molecules_by_ms2_count(self) -> List[Tuple[str, List[int]]]:
        """æŒ‰æ¯ä¸ªmoleculeæ‹¥æœ‰çš„MS2æ•°é‡ä»å¤šåˆ°å°‘æ’åº"""
        molecule_items = [(mol_id, indices) for mol_id, indices in self.molecule_groups.items()]
        sorted_items = sorted(molecule_items, key=lambda x: len(x[1]), reverse=True)
        
        print(f"Molecule MS2 count distribution:")
        print(f"Max MS2 per molecule: {len(sorted_items[0][1])}")
        print(f"Min MS2 per molecule: {len(sorted_items[-1][1])}")
        print(f"Total molecules: {len(sorted_items)}")
        print(f"Total MS2 spectra: {sum(len(indices) for _, indices in sorted_items)}")
        
        return sorted_items
    
    def _assign_texts_for_batch(self, batch_molecule_ids: List[str]) -> Dict[str, int]:
        """
        ä¸ºbatchä¸­çš„æ¯ä¸ªmoleculeåˆ†é…ä¸€ä¸ªtext index
        ç¡®ä¿é€‰ä¸­çš„textä¸åœ¨å…¶ä»–moleculeçš„å€™é€‰åˆ—è¡¨ä¸­
        
        è¿”å›: {molecule_id: text_index}
        """
        mol_to_text_idx = {}
        occupied_texts = set()  # å·²è¢«å ç”¨çš„text
        
        # æŒ‰ç…§å€™é€‰textæ•°é‡ä»å°‘åˆ°å¤šæ’åºï¼Œä¼˜å…ˆå¤„ç†é€‰æ‹©ç©ºé—´å°çš„molecule
        sorted_mols = sorted(
            batch_molecule_ids,
            key=lambda mol_id: len(self.molecule_to_texts.get(mol_id, []))
        )
        
        for mol_id in sorted_mols:
            candidate_texts = self.molecule_to_texts.get(mol_id, [])
            
            if not candidate_texts:
                print(f"âš ï¸ Warning: Molecule {mol_id} has no candidate texts")
                mol_to_text_idx[mol_id] = 0
                continue
            
            # æ‰¾åˆ°æ‰€æœ‰æœªè¢«å ç”¨ä¸”ä¸åœ¨å…¶ä»–moleculeå€™é€‰ä¸­çš„text
            available_indices = []
            for i, text in enumerate(candidate_texts):
                if text not in occupied_texts:
                    # æ£€æŸ¥è¿™ä¸ªtextæ˜¯å¦åœ¨å…¶ä»–moleculeçš„å€™é€‰ä¸­
                    text_in_others = False
                    for other_mol_id in batch_molecule_ids:
                        if other_mol_id != mol_id:
                            other_texts = self.molecule_to_texts.get(other_mol_id, [])
                            if text in other_texts:
                                text_in_others = True
                                break
                    
                    if not text_in_others:
                        available_indices.append(i)
            
            # å¦‚æœæœ‰å¯ç”¨çš„textï¼Œéšæœºé€‰ä¸€ä¸ª
            if available_indices:
                chosen_idx = random.choice(available_indices)
                mol_to_text_idx[mol_id] = chosen_idx
                occupied_texts.add(candidate_texts[chosen_idx])
            else:
                # Fallback: éšæœºé€‰ä¸€ä¸ªæœªè¢«å ç”¨çš„ï¼ˆå¯èƒ½åœ¨å…¶ä»–moleculeçš„å€™é€‰ä¸­ï¼‰
                fallback_indices = [i for i, text in enumerate(candidate_texts) 
                                  if text not in occupied_texts]
                if fallback_indices:
                    chosen_idx = random.choice(fallback_indices)
                    mol_to_text_idx[mol_id] = chosen_idx
                    occupied_texts.add(candidate_texts[chosen_idx])
                    print(f"âš ï¸ Fallback: Molecule {mol_id} text may conflict with others")
                else:
                    # æç«¯æƒ…å†µï¼šæ‰€æœ‰textéƒ½è¢«å ç”¨äº†
                    chosen_idx = random.randint(0, len(candidate_texts) - 1)
                    mol_to_text_idx[mol_id] = chosen_idx
                    print(f"âš ï¸ Extreme fallback: All texts occupied for {mol_id}")
        
        return mol_to_text_idx
    
    def _create_batch_allocation(self) -> List[List[int]]:
        """åˆ›å»ºbatchåˆ†é…æ–¹æ¡ˆ"""
        molecule_ms2_usage = {}
        for mol_id, indices in self.sorted_molecules:
            molecule_ms2_usage[mol_id] = {
                'indices': indices.copy(),
                'used_count': 0
            }
        
        batch_indices = []
        total_ms2_count = sum(len(indices) for _, indices in self.sorted_molecules)
        used_ms2_count = 0
        
        print(f"Starting batch allocation for {total_ms2_count} MS2 spectra...")
        
        while used_ms2_count < total_ms2_count:
            current_batch = []
            used_molecules_in_batch = set()
            
            for mol_id, mol_data in molecule_ms2_usage.items():
                if len(current_batch) >= self.batch_size:
                    break
                    
                if mol_id in used_molecules_in_batch:
                    continue
                
                if mol_data['used_count'] < len(mol_data['indices']):
                    ms2_idx = mol_data['indices'][mol_data['used_count']]
                    current_batch.append(ms2_idx)
                    used_molecules_in_batch.add(mol_id)
                    mol_data['used_count'] += 1
                    used_ms2_count += 1
            
            if len(current_batch) < self.batch_size and len(current_batch) > 0:
                available_molecules = [mol_id for mol_id in molecule_ms2_usage.keys() 
                                     if mol_id not in used_molecules_in_batch]
                
                while len(current_batch) < self.batch_size and available_molecules:
                    available_molecules.sort(key=lambda x: len(molecule_ms2_usage[x]['indices']), 
                                           reverse=True)
                    
                    mol_id = available_molecules[0]
                    mol_data = molecule_ms2_usage[mol_id]
                    
                    ms2_idx = random.choice(mol_data['indices'])
                    current_batch.append(ms2_idx)
                    used_molecules_in_batch.add(mol_id)
                    available_molecules.remove(mol_id)
            
            if len(current_batch) == 0:
                break
                
            if len(current_batch) < self.batch_size:
                if self.drop_last:
                    print(f"Dropping incomplete batch with {len(current_batch)} samples")
                    break
                else:
                    while len(current_batch) < self.batch_size:
                        available_molecules = [mol_id for mol_id in molecule_ms2_usage.keys() 
                                             if mol_id not in used_molecules_in_batch]
                        
                        if not available_molecules:
                            print(f"Cannot fill batch further: only {len(self.sorted_molecules)} unique molecules available")
                            break
                        
                        mol_id = random.choice(available_molecules)
                        mol_data = molecule_ms2_usage[mol_id]
                        
                        ms2_idx = random.choice(mol_data['indices'])
                        current_batch.append(ms2_idx)
                        used_molecules_in_batch.add(mol_id)
            
            batch_indices.append(current_batch)
        
        print(f"Created {len(batch_indices)} batches")
        print(f"Used {used_ms2_count} MS2 spectra out of {total_ms2_count}")
        
        return batch_indices
    
    def _adjust_for_distributed(self):
        """è°ƒæ•´batchæ•°é‡ä»¥ç¡®ä¿èƒ½è¢«GPUæ•°æ•´é™¤"""
        total_batches = len(self.batch_indices)
        remainder = total_batches % self.num_replicas
        
        if remainder != 0:
            if self.drop_last:
                batches_to_remove = remainder
                self.batch_indices = self.batch_indices[:-batches_to_remove]
                print(f"Dropped {batches_to_remove} batches to ensure divisibility by {self.num_replicas} GPUs")
            else:
                batches_to_add = self.num_replicas - remainder
                for i in range(batches_to_add):
                    batch_to_copy = self.batch_indices[i % len(self.batch_indices)]
                    self.batch_indices.append(batch_to_copy.copy())
                print(f"Added {batches_to_add} batches to ensure divisibility by {self.num_replicas} GPUs")
        
        final_batches = len(self.batch_indices)
        print(f"Final batch count: {final_batches} (divisible by {self.num_replicas} GPUs)")
        print(f"Each GPU will process {final_batches // self.num_replicas} batches")
    
    def __iter__(self) -> Iterator[_T_co]:
        # è·å–å½“å‰è¿›ç¨‹åº”è¯¥å¤„ç†çš„batch
        total_batches = len(self.batch_indices)
        batches_per_replica = total_batches // self.num_replicas
        
        start_batch = self.rank * batches_per_replica
        end_batch = start_batch + batches_per_replica
        
        my_batches = self.batch_indices[start_batch:end_batch]
        
        if self.shuffle:
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            batch_order = torch.randperm(len(my_batches), generator=g).tolist()
            my_batches = [my_batches[i] for i in batch_order]
            
            for batch in my_batches:
                random.Random(self.seed + self.epoch).shuffle(batch)
        
        # *** å…³é”®ï¼šä¸ºæ¯ä¸ªbatchåˆ†é…text ***
        self.dataset.text_assignment.clear()
        
        for batch_indices in my_batches:
            # è·å–batchä¸­æ‰€æœ‰molecule ID
            batch_molecule_ids = []
            ms2_id_to_mol_id = {}
            
            for idx in batch_indices:
                ms2_id = self.dataset.ms2_ids[idx]
                mol_id = self.dataset.preprocessed_ms2_tensors[ms2_id]['molecule_id']
                batch_molecule_ids.append(mol_id)
                ms2_id_to_mol_id[ms2_id] = mol_id
            
            # ä¸ºè¿™ä¸ªbatchåˆ†é…text indices
            mol_to_text_idx = self._assign_texts_for_batch(batch_molecule_ids)
            
            # å°†åˆ†é…ç»“æœå†™å…¥dataset.text_assignment
            for idx in batch_indices:
                ms2_id = self.dataset.ms2_ids[idx]
                mol_id = ms2_id_to_mol_id[ms2_id]
                self.dataset.text_assignment[ms2_id] = mol_to_text_idx.get(mol_id, 0)
        
        # å±•å¹³æ‰€æœ‰batchçš„indices
        all_indices = []
        for batch in my_batches:
            all_indices.extend(batch)
        
        return iter(all_indices)
    
    def __len__(self) -> int:
        return self.num_samples
    
    def set_epoch(self, epoch: int) -> None:
        """è®¾ç½®å½“å‰epochï¼Œç”¨äºç¡®ä¿æ¯ä¸ªepochçš„shuffleç»“æœä¸åŒ"""
        self.epoch = epoch


# function for dataset augmentation
import torch
import numpy as np
import random
import torch
import numpy as np
import random


def sample_truncated_normal(mean, std, low, high):
    """
    ä»æˆªæ–­æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·
    
    Args:
        mean: å‡å€¼
        std: æ ‡å‡†å·®
        low: ä¸‹ç•Œ
        high: ä¸Šç•Œ
    
    Returns:
        é‡‡æ ·å€¼
    """
    max_attempts = 1000
    for _ in range(max_attempts):
        sample = np.random.normal(mean, std)
        if low <= sample <= high:
            return sample
    # å¦‚æœ1000æ¬¡éƒ½æ²¡é‡‡æ ·åˆ°ï¼Œè¿”å›æˆªæ–­åçš„å€¼
    return np.clip(np.random.normal(mean, std), low, high)


def augment_tokenized_ms2_optimized(mz_tokens, intensity, word2idx, args):
    """
    åŸºäºExternalæ•°æ®ç‰¹å¾ä¼˜åŒ–çš„åŠ¨æ€augmentationï¼ˆæ”¯æŒéšæœºnoise ratioï¼‰
    """
    # ===== 0. è¯»å–å‚æ•°å¹¶å†³å®šæ˜¯å¦augment =====
    augment_prob = getattr(args, 'augment_prob', 0.5)
    if random.random() > augment_prob:
        return mz_tokens, intensity
    
    # ç¡®ä¿intensityæ˜¯1D
    if intensity.dim() == 2:
        intensity = intensity.squeeze(0)
    
    device = mz_tokens.device
    
    # ===== 1. æ„å»ºtoken_idåˆ°m/zçš„æ˜ å°„ =====
    idx2word = {v: k for k, v in word2idx.items()}
    
    def token_to_mz(token_id):
        """å°†token idè½¬æ¢ä¸ºå®é™…m/zå€¼"""
        word = idx2word.get(token_id.item(), None)
        if word and word not in ['[PAD]', '[MASK]']:
            try:
                return float(word)
            except ValueError:
                return None
        return None
    
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œè®¡ç®—
    mz_tokens_np = mz_tokens.cpu().numpy()
    intensity_np = intensity.cpu().numpy()
    
    # è·å–å®é™…çš„m/zå€¼ï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼‰
    original_mz = []
    original_intensity = []
    for i, token_id in enumerate(mz_tokens_np):
        mz_val = token_to_mz(torch.tensor(token_id))
        if mz_val is not None:
            original_mz.append(mz_val)
            original_intensity.append(intensity_np[i])
    
    if len(original_mz) == 0:
        return mz_tokens, intensity
    
    original_mz = np.array(original_mz)
    original_intensity = np.array(original_intensity)
    max_intensity = original_intensity.max()
    
    # ===== 2. è¯†åˆ«signal peaksï¼ˆå¼ºåº¦>5%çš„peaksï¼‰ =====
    signal_threshold = 0.05
    signal_mask = original_intensity >= signal_threshold * max_intensity
    signal_mz = original_mz[signal_mask]
    n_signal = len(signal_mz)
    
    if n_signal == 0:
        return mz_tokens, intensity
    
    # ===== 3. ğŸ”¥ éšæœºåŒ–å‚æ•° =====
    align_to_external = getattr(args, 'align_to_external', False)
    randomize_noise_ratio = getattr(args, 'randomize_noise_ratio', True)  # ğŸ”¥ æ–°å¢å¼€å…³
    noise_sampling_strategy = getattr(args, 'noise_sampling_strategy', 'uniform')  # ğŸ”¥ é€‰æ‹©ç­–ç•¥
    
    if align_to_external:
        # ğŸ”¥ éšæœºåŒ–noise ratio
        if randomize_noise_ratio:
            if noise_sampling_strategy == 'uniform':
                # ç­–ç•¥1ï¼šå‡åŒ€åˆ†å¸ƒ
                noise_ratio_range = getattr(args, 'noise_ratio_range', [0.60, 0.90])
                TARGET_NOISE_RATIO = np.random.uniform(noise_ratio_range[0], noise_ratio_range[1])
                
            elif noise_sampling_strategy == 'normal':
                # ç­–ç•¥2ï¼šæ­£æ€åˆ†å¸ƒ
                target_noise_ratio = getattr(args, 'target_noise_ratio', 0.80)
                noise_ratio_std = getattr(args, 'noise_ratio_std', 0.10)
                TARGET_NOISE_RATIO = np.random.normal(target_noise_ratio, noise_ratio_std)
                TARGET_NOISE_RATIO = np.clip(TARGET_NOISE_RATIO, 0.40, 0.95)
                
            elif noise_sampling_strategy == 'bimodal':
                # ç­–ç•¥3ï¼šåŒæ¨¡æ€åˆ†å¸ƒ
                bimodal_dirty_prob = getattr(args, 'bimodal_dirty_prob', 0.7)
                if np.random.random() < bimodal_dirty_prob:
                    # è„æ•°æ®æ¨¡å¼
                    TARGET_NOISE_RATIO = np.random.uniform(0.70, 0.90)
                else:
                    # å¹²å‡€æ•°æ®æ¨¡å¼
                    TARGET_NOISE_RATIO = np.random.uniform(0.30, 0.60)
            else:
                # é»˜è®¤ä½¿ç”¨å›ºå®šå€¼
                TARGET_NOISE_RATIO = getattr(args, 'target_noise_ratio', 0.80)
        else:
            # ä¸éšæœºåŒ–ï¼Œä½¿ç”¨å›ºå®šå€¼
            TARGET_NOISE_RATIO = getattr(args, 'target_noise_ratio', 0.80)
        
        # ğŸ”¥ å¯é€‰ï¼šéšæœºåŒ–proximal ratio
        randomize_proximal_ratio = getattr(args, 'randomize_proximal_ratio', False)
        if randomize_proximal_ratio:
            proximal_ratio_range = getattr(args, 'proximal_ratio_range', [0.15, 0.22])
            PROXIMAL_RATIO_OF_NOISE = np.random.uniform(proximal_ratio_range[0], proximal_ratio_range[1])
        else:
            PROXIMAL_RATIO_OF_NOISE = getattr(args, 'proximal_ratio_of_noise', 0.18)
        
        # Intensityå‚æ•°
        PROXIMAL_MEAN = getattr(args, 'proximal_intensity_mean', 0.0115)
        PROXIMAL_STD = getattr(args, 'proximal_intensity_std', 0.0138)
        ISOLATED_MEAN = getattr(args, 'isolated_intensity_mean', 0.0079)
        ISOLATED_STD = getattr(args, 'isolated_intensity_std', 0.0114)
        
        # åŒºåŸŸæƒé‡
        use_regional_weighting = getattr(args, 'use_regional_weighting', True)
        if use_regional_weighting:
            REGION_WEIGHTS = [
                (0, 100, 0.24),
                (100, 200, 0.53),
                (200, 300, 0.18),
                (300, 500, 0.05)
            ]
        else:
            REGION_WEIGHTS = None
    else:
        # ä¸å¯¹é½Externalæ—¶çš„å‚æ•°
        if randomize_noise_ratio:
            noise_ratio_range = getattr(args, 'noise_ratio_range', [0.30, 0.70])
            TARGET_NOISE_RATIO = np.random.uniform(noise_ratio_range[0], noise_ratio_range[1])
        else:
            TARGET_NOISE_RATIO = getattr(args, 'target_noise_ratio', 0.50)
        
        PROXIMAL_RATIO_OF_NOISE = 0.25
        PROXIMAL_MEAN = 0.0141
        PROXIMAL_STD = 0.0209
        ISOLATED_MEAN = 0.0091
        ISOLATED_STD = 0.0144
        REGION_WEIGHTS = None
    
    # ç©ºé—´åˆ†å¸ƒå‚æ•°
    proximal_distance_range = getattr(args, 'proximal_distance_range', [-1.5, 1.5])
    isolated_min_distance = getattr(args, 'isolated_min_distance', 5.0)
    
    # ===== 4. è®¡ç®—éœ€è¦æ·»åŠ çš„noiseæ€»æ•° =====
    n_noise_total = int(n_signal * TARGET_NOISE_RATIO / (1 - TARGET_NOISE_RATIO))
    n_proximal = int(n_noise_total * PROXIMAL_RATIO_OF_NOISE)
    n_isolated = n_noise_total - n_proximal
    
    # ... åé¢çš„ä»£ç ä¿æŒä¸å˜ ...
    
    mz_min, mz_max = original_mz.min(), original_mz.max()
    
    # ===== 5. ç”ŸæˆProximal Noise =====
    proximal_mz = []
    proximal_intensity = []
    
    for _ in range(n_proximal):
        # é€‰æ‹©ä¸€ä¸ªsignal peakä½œä¸ºbase
        base_mz = np.random.choice(signal_mz)
        
        # Proximalè·ç¦»èŒƒå›´
        offset = np.random.uniform(proximal_distance_range[0], proximal_distance_range[1])
        noise_mz = base_mz + offset
        noise_mz = np.clip(noise_mz, mz_min, mz_max)
        
        # é‡‡æ ·intensityï¼ˆæˆªæ–­æ­£æ€åˆ†å¸ƒï¼‰
        noise_intensity = sample_truncated_normal(
            PROXIMAL_MEAN, PROXIMAL_STD, 0.001, 0.10
        ) * max_intensity
        
        proximal_mz.append(noise_mz)
        proximal_intensity.append(noise_intensity)
    
    # ===== 6. ç”ŸæˆIsolated Noiseï¼ˆåŒºåŸŸåŠ æƒï¼‰ =====
    isolated_mz = []
    isolated_intensity = []
    
    if REGION_WEIGHTS:
        # ä½¿ç”¨åŒºåŸŸåŠ æƒç­–ç•¥
        for low, high, weight in REGION_WEIGHTS:
            # åªåœ¨å…‰è°±m/zèŒƒå›´å†…ç”Ÿæˆ
            region_low = max(low, mz_min)
            region_high = min(high, mz_max)
            
            if region_low >= region_high:
                continue
            
            n_in_region = int(n_isolated * weight)
            attempts = 0
            max_attempts = n_in_region * 10
            generated = 0
            
            while generated < n_in_region and attempts < max_attempts:
                candidate_mz = np.random.uniform(region_low, region_high)
                
                # æ£€æŸ¥æ˜¯å¦è¿œç¦»æ‰€æœ‰signal peaks
                min_dist = np.min(np.abs(signal_mz - candidate_mz))
                
                if min_dist >= isolated_min_distance:
                    # 100-200 DaåŒºåŸŸintensityç¨é«˜
                    if 100 <= candidate_mz <= 200:
                        mean_adj = ISOLATED_MEAN * 1.1
                    else:
                        mean_adj = ISOLATED_MEAN
                    
                    noise_intensity = sample_truncated_normal(
                        mean_adj, ISOLATED_STD, 0.0001, 0.05
                    ) * max_intensity
                    
                    isolated_mz.append(candidate_mz)
                    isolated_intensity.append(noise_intensity)
                    generated += 1
                
                attempts += 1
    else:
        # ä¸ä½¿ç”¨åŒºåŸŸåŠ æƒï¼ˆåŸå§‹éšæœºç­–ç•¥ï¼‰
        attempts = 0
        max_attempts = n_isolated * 10
        generated = 0
        
        while generated < n_isolated and attempts < max_attempts:
            candidate_mz = np.random.uniform(mz_min, mz_max)
            min_dist = np.min(np.abs(signal_mz - candidate_mz))
            
            if min_dist >= isolated_min_distance:
                noise_intensity = sample_truncated_normal(
                    ISOLATED_MEAN, ISOLATED_STD, 0.0001, 0.05
                ) * max_intensity
                
                isolated_mz.append(candidate_mz)
                isolated_intensity.append(noise_intensity)
                generated += 1
            
            attempts += 1
    
    # ===== 7. åˆå¹¶æ‰€æœ‰peaks =====
    all_mz = np.concatenate([original_mz, proximal_mz, isolated_mz])
    all_intensity = np.concatenate([original_intensity, proximal_intensity, isolated_intensity])
    
    # ===== 8. è½¬æ¢å›token ids =====
    def mz_to_token(mz_val):
        """å°†m/zå€¼è½¬æ¢ä¸ºtoken idï¼ˆå››èˆäº”å…¥åˆ°0.01ç²¾åº¦ï¼‰"""
        mz_rounded = round(mz_val, 2)
        mz_str = f"{mz_rounded:.2f}"
        return word2idx.get(mz_str, word2idx.get('[MASK]', 1))
    
    all_tokens = [mz_to_token(mz) for mz in all_mz]
    
    # æŒ‰m/zæ’åº
    sorted_idx = np.argsort(all_mz)
    all_tokens = np.array(all_tokens)[sorted_idx]
    all_intensity = all_intensity[sorted_idx]
    
    # ===== 9. å¯é€‰ï¼šè¿‡æ»¤å’Œæˆªæ–­ =====
    filter_threshold = getattr(args, 'filter_threshold', None)
    if filter_threshold and filter_threshold > 0:
        max_int = all_intensity.max()
        if max_int > 0:
            threshold = max_int * filter_threshold
            mask = all_intensity >= threshold
            all_tokens = all_tokens[mask]
            all_intensity = all_intensity[mask]
    
    # æˆªæ–­åˆ°maxlen
    maxlen = getattr(args, 'maxlen', 100)
    if len(all_tokens) > maxlen:
        # ä¿ç•™æœ€å¼ºçš„peaks
        top_indices = np.argsort(all_intensity)[-maxlen:]
        top_indices = np.sort(top_indices)  # æ¢å¤m/zé¡ºåº
        all_tokens = all_tokens[top_indices]
        all_intensity = all_intensity[top_indices]
    
    # ===== 10. è½¬æ¢å›tensor =====
    augmented_mz = torch.tensor(all_tokens, dtype=mz_tokens.dtype, device=device)
    augmented_intensity = torch.tensor(all_intensity, dtype=intensity.dtype, device=device).unsqueeze(0)
    
    return augmented_mz, augmented_intensity




class MS2BioTextDataset(Dataset):
    def __init__(self, ms2_data, meta_data, biotext_data, tokenizer, max_length=512, 
                 use_paraphrase=False, use_mlm=False, use_ms2_prediction=False, 
                 prediction_label_columns=None, word2idx=None, args=None, split='test'):
        """
        hard_neg_path: path to hard_negatives_v2.json
        num_hard_neg_per_sample: number of hard negatives to use per sample
        """
        self.ms2_data = ms2_data
        self.meta_data = meta_data
        self.biotext_data = biotext_data
        self.preprocessed_ms2_tensors = {}
        
        for ms2_id, ms2_entry in self.ms2_data.items():
            self.preprocessed_ms2_tensors[ms2_id] = {
                'mz': torch.tensor(ms2_entry['mz'], dtype=torch.float32),
                'intensity': torch.tensor(ms2_entry['intensity'], dtype=torch.float32),
                'molecule_id': ms2_entry['molecule_id']
            }
        self.text_assignment = {}
        self.ms2_ids = list(ms2_data.keys())
        self.word2idx = word2idx
        self.args = args or argparse.Namespace()  # ç¡®ä¿éNone
        self.split = split
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_mlm = use_mlm
        self.use_ms2_prediction = use_ms2_prediction
        self.prediction_label_columns = prediction_label_columns
        self.use_paraphrase = use_paraphrase
        
        # === NEW: Load hard negatives ===
        self.hard_negatives = {}
        self.num_hard_neg = getattr(self.args, "num_hard_neg_per_sample", 0)
        hard_neg_path = getattr(self.args, "hard_neg_path", None)

        if hard_neg_path and os.path.exists(hard_neg_path) and split == 'train':
            import json
            with open(hard_neg_path, 'r', encoding='utf-8') as f:
                self.hard_negatives = json.load(f) 
            print(f"Loaded hard negatives for {len(self.hard_negatives)} molecules")
            print(f"Using {self.num_hard_neg} hard negatives per sample")
        else:
            if split == 'train':
                print(f"No valid hard_neg_path found ({hard_neg_path}), skipping hard negatives.")

        # === MS2 prediction checks ===
        if self.use_ms2_prediction:
            if not self.prediction_label_columns or not isinstance(self.prediction_label_columns, list):
                raise ValueError("When MS2 prediction task is enabled, a list of column names 'prediction_label_columns' must be provided.")
            
            for col in self.prediction_label_columns:
                if col not in self.meta_data.columns:
                    raise ValueError(f"Column '{col}' is not found in meta_data.")
            
            self.num_ms2_classes = len(self.prediction_label_columns)
            print(f"Found {self.num_ms2_classes} label columns for multilabel prediction task: {self.prediction_label_columns}")


    def __len__(self):
        return len(self.ms2_ids)
    
    def _create_mlm_inputs(self, input_ids):
        """ä¸ºMLMä»»åŠ¡åˆ›å»ºæ©ç è¾“å…¥å’Œæ ‡ç­¾"""
        labels = input_ids.clone()
        probability_matrix = torch.full(labels.shape, 0.15) # 15%çš„æ¦‚ç‡è¿›è¡Œmask
        
        # é¿å…maskç‰¹æ®Štokens (e.g., [CLS], [SEP], [PAD])
        special_tokens_mask = self.tokenizer.get_special_tokens_mask(labels.tolist(), already_has_special_tokens=True)
        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)
        
        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
        masked_indices = torch.bernoulli(probability_matrix).bool()
        
        # å°†æœªè¢«maskçš„tokençš„labelè®¾ç½®ä¸º-100ï¼Œè¿™æ ·åœ¨è®¡ç®—lossæ—¶ä¼šè¢«å¿½ç•¥
        labels[~masked_indices] = -100
        
        # 80% çš„æ¦‚ç‡ç”¨ [MASK] token æ›¿æ¢
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)
        
        # 10% çš„æ¦‚ç‡ç”¨éšæœºtokenæ›¿æ¢
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer.vocab), labels.shape, dtype=torch.long)
        input_ids[indices_random] = random_words[indices_random]
        
        return input_ids, labels

    def __getitem__(self, idx):
        ms2_id = self.ms2_ids[idx]
        tensor_data = self.preprocessed_ms2_tensors[ms2_id]
        mz = tensor_data['mz']
        intensity = tensor_data['intensity']
        
        # Dynamic augmentation
        if self.split == 'train' and hasattr(self, 'word2idx'):
            mz, intensity = augment_tokenized_ms2_optimized(
                mz, intensity, self.word2idx, self.args
            )
        
        batch = {
            'mz': tensor_data['mz'],
            'intensity': tensor_data['intensity'].unsqueeze(0)
        }
        
        # === ç»Ÿä¸€çš„BioTextå¤„ç†é€»è¾‘ ===
        molecule_id = tensor_data['molecule_id']
        biotext = ""
        paraphrase_text = None
        all_candidate_texts = []

        if molecule_id in self.biotext_data:
            entry = self.biotext_data[molecule_id]
            
            if isinstance(entry, list):
                all_candidate_texts = [record['text'] for record in entry]
                
                # *** å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨sampleråˆ†é…çš„text index ***
                if ms2_id in self.text_assignment:
                    text_idx = self.text_assignment[ms2_id]
                    biotext = all_candidate_texts[text_idx]
                else:
                    # fallback: éšæœºé€‰æ‹©ï¼ˆshouldn't happen in trainingï¼‰
                    biotext = random.choice(entry)['text']
                
                # Paraphrase: å¦‚æœéœ€è¦ï¼Œä»å‰©ä½™çš„å€™é€‰ä¸­é€‰ä¸€ä¸ªä¸åŒçš„
                if self.use_paraphrase and len(entry) >= 2:
                    remaining_indices = [i for i in range(len(all_candidate_texts)) 
                                    if i != text_idx]
                    if remaining_indices:
                        para_idx = random.choice(remaining_indices)
                        paraphrase_text = all_candidate_texts[para_idx]
                        
            elif isinstance(entry, dict):
                original = entry.get("original", "")
                paraphrases = entry.get("paraphrases", [])
                all_candidates = [original] + paraphrases
                all_candidate_texts = all_candidates
                
                # *** åŒæ ·çš„é€»è¾‘ ***
                if ms2_id in self.text_assignment:
                    text_idx = self.text_assignment[ms2_id]
                    biotext = all_candidates[text_idx]
                else:
                    biotext = original
                
                if self.use_paraphrase and len(all_candidates) >= 2:
                    remaining = [i for i in range(len(all_candidates)) if i != text_idx]
                    if remaining:
                        para_idx = random.choice(remaining)
                        paraphrase_text = all_candidates[para_idx]
                        
            elif isinstance(entry, str):
                biotext = entry
                all_candidate_texts = [entry]
        else:
            print(f"âš ï¸ Warning: Molecule ID '{molecule_id}' missing BioText")

        # === åç»­tokenizationç­‰ä¿æŒä¸å˜ ===
        encoded_text = self.tokenizer(
            biotext,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )
        
        input_ids = encoded_text['input_ids'].squeeze(0)
        attention_mask = encoded_text['attention_mask'].squeeze(0)
        
        batch['text_input_ids'] = input_ids
        batch['text_attention_mask'] = attention_mask
        batch['all_candidate_texts'] = all_candidate_texts
        
        # MLM task
        if self.use_mlm:
            masked_input_ids, mlm_labels = self._create_mlm_inputs(input_ids.clone())
            batch['masked_text_input_ids'] = masked_input_ids
            batch['mlm_labels'] = mlm_labels
        
        # Paraphrase
        if paraphrase_text is not None:
            encoded_para = self.tokenizer(
                paraphrase_text,
                padding="max_length",
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            )
            batch['paraphrase_input_ids'] = encoded_para['input_ids'].squeeze(0)
            batch['paraphrase_attention_mask'] = encoded_para['attention_mask'].squeeze(0)
            batch['has_paraphrase'] = True
        else:
            batch['has_paraphrase'] = False
        
        batch['ms2_id'] = ms2_id
        batch['molecule_id'] = molecule_id
        batch['original_text'] = biotext
        
        return batch
    
    @staticmethod
    def custom_collate_fn(batch_list):
        """
        è‡ªå®šä¹‰collate_fnï¼Œç”¨äºå¤„ç†å­—å…¸å½¢å¼çš„æ‰¹æ¬¡æ•°æ®ã€‚
        æ”¯æŒå¯é€‰çš„keysï¼Œå¹¶è¿‡æ»¤ä¸batchæ­£æ ·æœ¬å†²çªçš„hard negativesã€‚
        """
        if not batch_list:
            return {}
        
        # æ”¶é›†batchä¸­æ‰€æœ‰æ­£æ ·æœ¬çš„molecule_id
        batch_molecule_ids = set()
        for sample in batch_list:
            if 'molecule_id' in sample:
                batch_molecule_ids.add(sample['molecule_id'])
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„keys
        all_keys = set()
        for d in batch_list:
            all_keys.update(d.keys())
        
        # æŒ‰keyåˆ†ç»„æ”¶é›†æ•°æ®
        collated_batch = {}
        for key in all_keys:
            values = [d[key] for d in batch_list if key in d]
            collated_batch[key] = values
        
        # === è¿‡æ»¤å†²çªçš„hard negatives ===
        if 'hard_neg_input_ids' in collated_batch and len(batch_molecule_ids) > 0:
            filtered_hard_neg_ids = []
            filtered_hard_neg_masks = []
            filtered_has_hard_neg = []
            
            for i, sample in enumerate(batch_list):
                if sample.get('has_hard_neg', False):
                    hard_neg_ids = sample['hard_neg_input_ids']  # [num_hard_neg, seq_len]
                    hard_neg_mask = sample['hard_neg_attention_mask']
                    hard_neg_mol_ids = sample.get('hard_neg_molecule_ids', [])
                    
                    # æ‰¾å‡ºä¸åœ¨batchä¸­çš„hard negativesçš„ç´¢å¼•
                    valid_indices = []
                    for j, neg_mol_id in enumerate(hard_neg_mol_ids):
                        if neg_mol_id not in batch_molecule_ids:
                            valid_indices.append(j)
                    
                    if valid_indices:
                        # åªä¿ç•™ä¸å†²çªçš„hard negatives
                        filtered_hard_neg_ids.append(hard_neg_ids[valid_indices])
                        filtered_hard_neg_masks.append(hard_neg_mask[valid_indices])
                        filtered_has_hard_neg.append(True)
                    else:
                        # æ‰€æœ‰hard negativeséƒ½å†²çªï¼Œè®¾ä¸ºç©ºtensor
                        max_length = sample['text_input_ids'].shape[0]
                        filtered_hard_neg_ids.append(
                            torch.zeros((0, max_length), dtype=torch.long)
                        )
                        filtered_hard_neg_masks.append(
                            torch.zeros((0, max_length), dtype=torch.long)
                        )
                        filtered_has_hard_neg.append(False)
                else:
                    # åŸæœ¬å°±æ²¡æœ‰hard negatives
                    max_length = batch_list[0]['text_input_ids'].shape[0]
                    filtered_hard_neg_ids.append(
                        torch.zeros((0, max_length), dtype=torch.long)
                    )
                    filtered_hard_neg_masks.append(
                        torch.zeros((0, max_length), dtype=torch.long)
                    )
                    filtered_has_hard_neg.append(False)
            
            collated_batch['hard_neg_input_ids'] = filtered_hard_neg_ids
            collated_batch['hard_neg_attention_mask'] = filtered_hard_neg_masks
            collated_batch['has_hard_neg'] = filtered_has_hard_neg
        
        # é€ä¸ªå¤„ç†å­—å…¸ä¸­çš„é”®å€¼
        final_batch = {}
        for key, values in collated_batch.items():
            # å¯é€‰çš„boolean flags
            if key in ['has_paraphrase', 'has_hard_neg']:
                final_batch[key] = [d.get(key, False) for d in batch_list]
            
            # ä¿æŒä¸ºlistçš„keysï¼ˆåŒ…æ‹¬æ–°å¢çš„hard_neg_molecule_idsï¼‰
            elif key in ['hard_neg_input_ids', 'hard_neg_attention_mask',
                        'paraphrase_input_ids', 'paraphrase_attention_mask',
                        'hard_neg_molecule_ids']:
                final_batch[key] = values
            
            # å¯¹å¼ é‡è¿›è¡Œå †å 
            elif isinstance(values[0], torch.Tensor):
                if len(values) == len(batch_list):
                    final_batch[key] = torch.stack(values)
                else:
                    final_batch[key] = values
            
            # å…¶ä»–ç±»å‹ç›´æ¥ä¿ç•™
            else:
                final_batch[key] = values
        
        return final_batch
        
    @staticmethod
    def custom_collate_fn(batch_list):
        """
        è‡ªå®šä¹‰collate_fnï¼Œè¯†åˆ«batchä¸­æœ‰text overlapçš„æ ·æœ¬å¯¹
        """
        if not batch_list:
            return {}
        
        batch_size = len(batch_list)
        
        # æ”¶é›†batchä¸­æ‰€æœ‰æ­£æ ·æœ¬çš„molecule_id
        batch_molecule_ids = set()
        for sample in batch_list:
            if 'molecule_id' in sample:
                batch_molecule_ids.add(sample['molecule_id'])
        
        # === NEW: æ„å»ºtext overlapçŸ©é˜µ ===
        # text_overlap[i][j] = 1 è¡¨ç¤ºæ ·æœ¬iå’Œæ ·æœ¬jæœ‰å…±äº«çš„å€™é€‰text
        text_overlap = torch.zeros(batch_size, batch_size, dtype=torch.float32)
        
        for i in range(batch_size):
            for j in range(batch_size):
                if i == j:
                    text_overlap[i, j] = 1.0  # è‡ªå·±å’Œè‡ªå·±è‚¯å®šoverlap
                else:
                    # æ£€æŸ¥å€™é€‰texté›†åˆæ˜¯å¦æœ‰äº¤é›†
                    texts_i = set(batch_list[i].get('all_candidate_texts', []))
                    texts_j = set(batch_list[j].get('all_candidate_texts', []))
                    
                    if texts_i & texts_j:  # æœ‰äº¤é›†
                        text_overlap[i, j] = 1.0
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„keys
        all_keys = set()
        for d in batch_list:
            all_keys.update(d.keys())
        
        # æŒ‰keyåˆ†ç»„æ”¶é›†æ•°æ®
        collated_batch = {}
        for key in all_keys:
            values = [d[key] for d in batch_list if key in d]
            collated_batch[key] = values
        
        # === è¿‡æ»¤å†²çªçš„hard negatives ===
        if 'hard_neg_input_ids' in collated_batch and len(batch_molecule_ids) > 0:
            filtered_hard_neg_ids = []
            filtered_hard_neg_masks = []
            filtered_has_hard_neg = []
            
            for i, sample in enumerate(batch_list):
                if sample.get('has_hard_neg', False):
                    hard_neg_ids = sample['hard_neg_input_ids']
                    hard_neg_mask = sample['hard_neg_attention_mask']
                    hard_neg_mol_ids = sample.get('hard_neg_molecule_ids', [])
                    
                    valid_indices = []
                    for j, neg_mol_id in enumerate(hard_neg_mol_ids):
                        if neg_mol_id not in batch_molecule_ids:
                            valid_indices.append(j)
                    
                    if valid_indices:
                        filtered_hard_neg_ids.append(hard_neg_ids[valid_indices])
                        filtered_hard_neg_masks.append(hard_neg_mask[valid_indices])
                        filtered_has_hard_neg.append(True)
                    else:
                        max_length = sample['text_input_ids'].shape[0]
                        filtered_hard_neg_ids.append(
                            torch.zeros((0, max_length), dtype=torch.long)
                        )
                        filtered_hard_neg_masks.append(
                            torch.zeros((0, max_length), dtype=torch.long)
                        )
                        filtered_has_hard_neg.append(False)
                else:
                    max_length = batch_list[0]['text_input_ids'].shape[0]
                    filtered_hard_neg_ids.append(
                        torch.zeros((0, max_length), dtype=torch.long)
                    )
                    filtered_hard_neg_masks.append(
                        torch.zeros((0, max_length), dtype=torch.long)
                    )
                    filtered_has_hard_neg.append(False)
            
            collated_batch['hard_neg_input_ids'] = filtered_hard_neg_ids
            collated_batch['hard_neg_attention_mask'] = filtered_hard_neg_masks
            collated_batch['has_hard_neg'] = filtered_has_hard_neg
        
        # é€ä¸ªå¤„ç†å­—å…¸ä¸­çš„é”®å€¼
        final_batch = {}
        for key, values in collated_batch.items():
            if key in ['has_paraphrase', 'has_hard_neg']:
                final_batch[key] = [d.get(key, False) for d in batch_list]
            elif key in ['hard_neg_input_ids', 'hard_neg_attention_mask',
                        'paraphrase_input_ids', 'paraphrase_attention_mask',
                        'hard_neg_molecule_ids', 'all_candidate_texts']:  # all_candidate_textsä¿æŒlist
                final_batch[key] = values
            elif isinstance(values[0], torch.Tensor):
                if len(values) == len(batch_list):
                    final_batch[key] = torch.stack(values)
                else:
                    final_batch[key] = values
            else:
                final_batch[key] = values
        
        # === æ·»åŠ text_overlapä¿¡æ¯ ===
        final_batch['text_overlap_matrix'] = text_overlap  # [batch_size, batch_size]
        
        return final_batch

            
    @staticmethod
    def load_hmdb_data_subsections(first_path, second_path, jsonl_path, max_text_sharing=5):
        """
        ä½¿ç”¨subsections JSONLæ ¼å¼è¯»å–æ•°æ®ï¼Œå¹¶è¿‡æ»¤é«˜é¢‘å…±äº«çš„text
        
        å‚æ•°:
        first_path (str): MS2æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆh5ã€pklç­‰ï¼‰
        second_path (str): Metaæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆparquetã€csvç­‰ï¼‰
        jsonl_path (str): BioTextçš„jsonlæ–‡ä»¶è·¯å¾„
        max_text_sharing (int): textæœ€å¤šå¯ä»¥è¢«å¤šå°‘ä¸ªmoleculeå…±äº«ï¼Œè¶…è¿‡åˆ™åˆ é™¤
        
        è¿”å›:
        tuple: (ms2_data, meta_data, biotext_data)
            biotext_dataæ ¼å¼: {molecule_id: [{'type': 'xxx', 'text': 'xxx'}, ...]}
        """
        from collections import defaultdict
        
        # è¯»å–MS2æ•°æ®
        ms2_data = {}
        try:
            _, ext1 = os.path.splitext(first_path)
            if ext1 == '.h5':
                with h5py.File(first_path, 'r') as f:
                    spectra_group = f['spectra']
                    for spectrum_id in spectra_group.keys():
                        group = spectra_group[spectrum_id]
                        parts = spectrum_id.split('_')
                        molecule_id = parts[0]
                        ms2_data[spectrum_id] = {
                            'mz': group['mz'][...].tolist(),
                            'intensity': group['intensity'][...].tolist(),
                            'molecule_id': molecule_id
                        }
            elif ext1 == '.pkl':
                with open(first_path, 'rb') as f:
                    ms2_data = pickle.load(f)
            else:
                print(f"Unsupported file format for first path: {ext1}")
                return None, None, None
        except Exception as e:
            print(f"Error: Failed to read first file: {first_path}. Error message: {str(e)}")
            return None, None, None

        # è¯»å–Metaæ•°æ®
        meta_data = None
        try:
            _, ext2 = os.path.splitext(second_path)
            if ext2 == '.parquet':
                meta_data = pd.read_parquet(second_path)
            elif ext2 == '.csv':
                meta_data = pd.read_csv(second_path)
            else:
                print(f"Unsupported file format for second path: {ext2}")
                return ms2_data, None, None
        except Exception as e:
            print(f"Error: Failed to read second file: {second_path}. Error message: {str(e)}")
            return ms2_data, None, None

        # è¯»å– BioText JSONL æ–‡ä»¶
        biotext_data = {}
        try:
            import json
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line.strip())
                    accession = item['accession']
                    
                    if accession not in biotext_data:
                        biotext_data[accession] = []
                    
                    biotext_data[accession].append({
                        'type': item['type'],
                        'text': item['text']
                    })
            
            print(f"âœ“ Loaded BioText subsections from {os.path.basename(jsonl_path)}")
            print(f"  Total molecules: {len(biotext_data)}")
            total_records_before = sum(len(v) for v in biotext_data.values())
            print(f"  Total records (before filtering): {total_records_before}, Avg per molecule: {total_records_before / len(biotext_data):.1f}")
            
        except Exception as e:
            print(f"Error reading BioText JSONL: {e}")
            return ms2_data, meta_data, None

        # ===== è¿‡æ»¤é«˜é¢‘å…±äº«çš„text =====
        print(f"\n=== Filtering texts shared by >{max_text_sharing} molecules ===")
        
        # 1. æ„å»ºtext -> moleculesçš„å€’æ’ç´¢å¼•
        text_to_molecules = defaultdict(set)
        for mol_id, records in biotext_data.items():
            for record in records:
                text = record['text']
                if text:  # é¿å…ç©ºå­—ç¬¦ä¸²
                    text_to_molecules[text].add(mol_id)
        
        # 2. æ‰¾å‡ºéœ€è¦åˆ é™¤çš„é«˜é¢‘text
        texts_to_remove = set()
        sharing_distribution = defaultdict(int)  # ç»Ÿè®¡åˆ†å¸ƒ
        
        for text, molecules in text_to_molecules.items():
            sharing_count = len(molecules)
            sharing_distribution[sharing_count] += 1
            
            if sharing_count > max_text_sharing:
                texts_to_remove.add(text)
        
        print(f"Text sharing distribution (top 10):")
        for count in sorted(sharing_distribution.keys(), reverse=True)[:10]:
            print(f"  {count} molecules share: {sharing_distribution[count]} texts")
        
        print(f"\nFound {len(texts_to_remove)} texts to remove (shared by >{max_text_sharing} molecules)")
        
        # 3. ä»æ¯ä¸ªmoleculeçš„å€™é€‰textä¸­åˆ é™¤è¿™äº›é«˜é¢‘text
        filtered_biotext_data = {}
        total_removed = 0
        molecules_with_no_text = []
        
        for mol_id, records in biotext_data.items():
            filtered_records = [record for record in records 
                            if record['text'] not in texts_to_remove]
            
            if filtered_records:
                filtered_biotext_data[mol_id] = filtered_records
                total_removed += len(records) - len(filtered_records)
            else:
                molecules_with_no_text.append(mol_id)
                total_removed += len(records)
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        print(f"\nFiltering results:")
        print(f"  Text entries removed: {total_removed}")
        print(f"  Molecules before: {len(biotext_data)}")
        print(f"  Molecules after: {len(filtered_biotext_data)}")
        print(f"  Molecules with no text left: {len(molecules_with_no_text)}")
        
        if molecules_with_no_text:
            print(f"  âš ï¸ Warning: {len(molecules_with_no_text)} molecules lost all texts")
            if len(molecules_with_no_text) <= 5:
                print(f"    Lost: {molecules_with_no_text}")
            else:
                print(f"    First 5: {molecules_with_no_text[:5]}")
        
        # 5. éªŒè¯è¿‡æ»¤æ•ˆæœ
        text_to_molecules_after = defaultdict(set)
        for mol_id, records in filtered_biotext_data.items():
            for record in records:
                text = record['text']
                if text:
                    text_to_molecules_after[text].add(mol_id)
        
        max_sharing_after = max(len(mols) for mols in text_to_molecules_after.values()) if text_to_molecules_after else 0
        shared_texts_after = sum(1 for mols in text_to_molecules_after.values() if len(mols) > 1)
        
        print(f"  Max sharing after filtering: {max_sharing_after} molecules")
        print(f"  Texts still shared by multiple molecules: {shared_texts_after}")
        
        total_records_after = sum(len(v) for v in filtered_biotext_data.values())
        print(f"  Total records (after filtering): {total_records_after}, Avg per molecule: {total_records_after / len(filtered_biotext_data):.1f}")
        
        # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®
        biotext_data = filtered_biotext_data

        # æ‰“å°ç»Ÿè®¡
        unique_molecule_ids = set(item['molecule_id'] for item in ms2_data.values())
        print(f"\nFinal data summary:")
        print(f"  Unique molecule IDs in MS2 data: {len(unique_molecule_ids)}")
        print(f"  Molecule IDs in BioText data: {len(biotext_data)}")

        return ms2_data, meta_data, biotext_data

    @staticmethod
    def missing_biotext_handling(ms2_data, biotext_data, method="drop"):
        """
        ms2_data: dict, {ms2_id: {'mz': list, 'intensity': list, 'molecule_id': str}}
        biotext_data: dict, {molecule_id: BioText}
        """
        # If a molecule in ms2_data is missing in biotext_data, remove it from ms2_data
        # Handle missing biotext entries
        if method == "drop":
            ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if info['molecule_id'] in biotext_data}
            unique_molecule_ids = set(item['molecule_id'] for item in ms2_data.values())
            print(f"Post-processing statistics ('drop' method) - Unique molecule IDs in MS2 data: {len(unique_molecule_ids)}")
            print(f"Post-processing statistics ('drop' method) - Molecule IDs in BioText data: {len(biotext_data)}")
            return ms2_data, biotext_data

        if method == "fill":
            # Fill missing entries with empty values; will be handled during dataset initialization
            for info in ms2_data.values():
                molecule_id = info['molecule_id']
                if molecule_id not in biotext_data:
                    biotext_data[molecule_id] = ""
            unique_molecule_ids = set(item['molecule_id'] for item in ms2_data.values())
            print(f"Post-processing statistics ('fill' method) - Unique molecule IDs in MS2 data: {len(unique_molecule_ids)}")
            print(f"Post-processing statistics ('fill' method) - Molecule IDs in BioText data: {len(biotext_data)}")
            return ms2_data, biotext_data

        raise ValueError(f"Unknown method: {method}. Method must be 'drop' or 'fill'.")


    @staticmethod
    def add_noise_peaks(peaks, intensities, noise_ratio=0.5, noise_intensity_range=(0.001, 0.05), seed=None):
        """
        æ·»åŠ éšæœºnoise peaksæ¥æ¨¡æ‹Ÿå¤–éƒ¨æ•°æ®
        
        Args:
            peaks: list of float, åŸå§‹m/zå€¼
            intensities: list of float, åŸå§‹å¼ºåº¦å€¼
            noise_ratio: float, æ·»åŠ çš„noise peaksæ•°é‡ = åŸpeaksæ•° Ã— noise_ratio
            noise_intensity_range: tuple, noiseçš„ç›¸å¯¹å¼ºåº¦èŒƒå›´ï¼ˆç›¸å¯¹äºmax intensityï¼‰
            seed: int, éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            aug_peaks: list of float, æ·»åŠ noiseåçš„m/z
            aug_intensities: list of float, æ·»åŠ noiseåçš„å¼ºåº¦
        """
        import numpy as np
        import random
        
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)
        
        if len(peaks) == 0:
            return peaks, intensities
        
        max_int = max(intensities)
        if max_int == 0:
            return peaks, intensities
        
        # è®¡ç®—è¦æ·»åŠ çš„noiseæ•°é‡
        n_noise = int(len(peaks) * noise_ratio)
        if n_noise == 0:
            return peaks, intensities
        
        # åœ¨å…‰è°±èŒƒå›´å†…éšæœºç”Ÿæˆnoise peaksçš„m/z
        mz_min, mz_max = min(peaks), max(peaks)
        noise_mz = np.random.uniform(mz_min, mz_max, n_noise).tolist()
        
        # ç”Ÿæˆä½å¼ºåº¦noiseï¼ˆç›¸å¯¹äºmax intensityï¼‰
        noise_int = np.random.uniform(
            noise_intensity_range[0] * max_int,
            noise_intensity_range[1] * max_int,
            n_noise
        ).tolist()
        
        # åˆå¹¶åŸå§‹peakså’Œnoise
        aug_peaks = peaks + noise_mz
        aug_intensities = intensities + noise_int
        
        # æŒ‰m/zæ’åº
        sorted_indices = sorted(range(len(aug_peaks)), key=lambda i: aug_peaks[i])
        aug_peaks = [aug_peaks[i] for i in sorted_indices]
        aug_intensities = [aug_intensities[i] for i in sorted_indices]
        
        return aug_peaks, aug_intensities


    @staticmethod
    def filter_low_intensity_peaks(peaks, intensities, threshold=0.01):
        """
        è¿‡æ»¤ä½å¼ºåº¦peaks
        
        Args:
            peaks: list of float, m/zå€¼
            intensities: list of float, å¼ºåº¦å€¼
            threshold: float, ç›¸å¯¹å¼ºåº¦é˜ˆå€¼ï¼ˆ0.01 = 1%ï¼‰
        
        Returns:
            filtered_peaks: list of float
            filtered_intensities: list of float
        """
        if len(peaks) == 0 or len(intensities) == 0:
            return peaks, intensities
        
        max_int = max(intensities)
        if max_int == 0:
            return peaks, intensities
        
        # å½’ä¸€åŒ–å¹¶è¿‡æ»¤
        norm_intensities = [i / max_int for i in intensities]
        filtered_peaks = []
        filtered_intensities = []
        
        for mz, intensity, norm_int in zip(peaks, intensities, norm_intensities):
            if norm_int >= threshold:
                filtered_peaks.append(mz)
                filtered_intensities.append(intensity)
        
        return filtered_peaks, filtered_intensities


    @staticmethod
    def augment_ms2_data(ms2_data, args):
        """
        å¯¹MS2æ•°æ®è¿›è¡Œå¢å¼ºï¼ˆå¿…é¡»åœ¨preprocessä¹‹å‰è°ƒç”¨ï¼‰
        
        Args:
            ms2_data: dict, {ms2_id: {'mz': list, 'intensity': list, 'molecule_id': str}}
                    æ³¨æ„ï¼šmzå’Œintensityå¿…é¡»æ˜¯åŸå§‹çš„floatå€¼ï¼Œä¸èƒ½æ˜¯token_ids
            args: argparse.Namespace, åŒ…å«å¢å¼ºå‚æ•°:
                - augment_noise: bool, æ˜¯å¦æ·»åŠ noiseå¢å¼º (default: False)
                - augment_multiplier: int, æ¯ä¸ªå…‰è°±ç”Ÿæˆå‡ ä¸ªç‰ˆæœ¬ (1=ä¸å¢å¼º, 2=ç”Ÿæˆ2å€æ•°æ®)
                - noise_ratio: float, æ·»åŠ çš„noiseæ•°é‡ = åŸpeaksæ•° Ã— noise_ratio
                - noise_intensity_range: tuple, noiseå¼ºåº¦èŒƒå›´ (ç›¸å¯¹äºmax intensity)
                - filter_threshold: float or None, è¿‡æ»¤ä½å¼ºåº¦peaksçš„é˜ˆå€¼
        
        Returns:
            augmented_ms2_data: dict, åŒ…å«åŸå§‹+å¢å¼ºç‰ˆæœ¬çš„æ•°æ®
                            å¦‚æœaugment_multiplier=1ï¼Œè¿”å›åŸå§‹æ•°æ®
                            å¦‚æœaugment_multiplier=2ï¼Œè¿”å›2å€æ•°æ®ï¼ˆåŸå§‹+1ä¸ªå¢å¼ºç‰ˆæœ¬ï¼‰
        
        Example:
            >>> augmented_data = MS2BioTextDataset.augment_ms2_data(ms2_data, args)
            >>> processed_data, word2idx = MS2BioTextDataset.preprocess_ms2_data_positive_only(
            ...     augmented_data, meta_data
            ... )
        """
        import numpy as np
        
        # è·å–å‚æ•°ï¼ˆå…¼å®¹æ²¡æœ‰è¿™äº›å‚æ•°çš„æƒ…å†µï¼‰
        augment_noise = getattr(args, 'augment_noise', False)
        augment_multiplier = getattr(args, 'augment_multiplier', 1)
        noise_ratio = getattr(args, 'noise_ratio', 0.5)
        noise_intensity_range = getattr(args, 'noise_intensity_range', (0.001, 0.05))
        filter_threshold = getattr(args, 'filter_threshold', None)
        
        # å¦‚æœä¸éœ€è¦å¢å¼ºï¼Œç›´æ¥è¿”å›åŸæ•°æ®
        if not augment_noise or augment_multiplier <= 1:
            print("â„¹ï¸  æœªå¯ç”¨æ•°æ®å¢å¼º (augment_noise=False or augment_multiplier<=1)")
            return ms2_data
        
        print(f"\n{'='*60}")
        print(f"ğŸ”„ MS2æ•°æ®å¢å¼º")
        print(f"{'='*60}")
        print(f"  å¢å¼ºå€æ•°: {augment_multiplier}x")
        print(f"  Noiseæ¯”ä¾‹: {noise_ratio}")
        print(f"  Noiseå¼ºåº¦èŒƒå›´: {noise_intensity_range}")
        if filter_threshold:
            print(f"  è¿‡æ»¤é˜ˆå€¼: {filter_threshold} (ç›¸å¯¹å¼ºåº¦)")
        print(f"  åŸå§‹å…‰è°±æ•°: {len(ms2_data)}")
        
        augmented_ms2_data = {}
        
        for ms2_id, info in ms2_data.items():
            molecule_id = info.get('molecule_id')
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            if not isinstance(info['mz'], list) or not isinstance(info['intensity'], list):
                print(f"âš ï¸  è·³è¿‡ {ms2_id}: mzæˆ–intensityä¸æ˜¯listæ ¼å¼")
                continue
            
            # ç‰ˆæœ¬0: åŸå§‹æ•°æ®ï¼ˆå¯é€‰è¿‡æ»¤ï¼‰
            peaks_original = info['mz'].copy() if isinstance(info['mz'], list) else list(info['mz'])
            intensities_original = info['intensity'].copy() if isinstance(info['intensity'], list) else list(info['intensity'])
            
            # å¯é€‰ï¼šè¿‡æ»¤ä½å¼ºåº¦peaks
            if filter_threshold is not None and filter_threshold > 0:
                peaks_original, intensities_original = MS2BioTextDataset.filter_low_intensity_peaks(
                    peaks_original, intensities_original, threshold=filter_threshold
                )
            
            # ä¿å­˜åŸå§‹ç‰ˆæœ¬
            augmented_ms2_data[ms2_id] = {
                'mz': peaks_original,
                'intensity': intensities_original,
                'molecule_id': molecule_id
            }
            
            # ç”Ÿæˆå¢å¼ºç‰ˆæœ¬ï¼ˆç‰ˆæœ¬1åˆ°N-1ï¼‰
            for aug_idx in range(1, augment_multiplier):
                peaks_aug, intensities_aug = MS2BioTextDataset.add_noise_peaks(
                    peaks_original.copy(),
                    intensities_original.copy(),
                    noise_ratio=noise_ratio,
                    noise_intensity_range=noise_intensity_range,
                    seed=None  # æ¯æ¬¡éšæœºç”Ÿæˆä¸åŒçš„noise
                )
                
                # æ–°çš„IDï¼šåŸID + åç¼€
                aug_ms2_id = f"{ms2_id}_aug{aug_idx}"
                augmented_ms2_data[aug_ms2_id] = {
                    'mz': peaks_aug,
                    'intensity': intensities_aug,
                    'molecule_id': molecule_id  # ä¿æŒç›¸åŒçš„molecule_idï¼
                }
        
        print(f"  âœ“ å¢å¼ºåå…‰è°±æ•°: {len(augmented_ms2_data)}")
        print(f"  å¢å¼ºç‰ˆæœ¬æ•°: {len(augmented_ms2_data) - len(ms2_data)}")
        print(f"{'='*60}\n")
        
        return augmented_ms2_data

    @staticmethod
    def preprocess_ms2_data_positive_only(ms2_data, meta_data, maxlen=100, min_peaks=0):
        """
        Preprocess ms2_data for model input.
        
        Parameters:
        - ms2_data: dict, {ms2_id: {'mz': list, 'intensity': list, 'molecule_id': str}}
        - meta_data: pd.DataFrame, must contain precursor information (column: 'precursor_mass')
        - maxlen: int, maximum sequence length
        - min_peaks: int, minimum number of peaks required (default: 0, no filtering)
        
        Returns:
        - ms_data: dict, same structure as ms2_data but with processed 'mz' and 'intensity' sequences
        - word2idx: dict, maps string-formatted m/z values to token indices
        """
        
        # ===== æ–°å¢ï¼šå®‰å…¨è½¬æ¢precursor_massçš„å‡½æ•° =====
        def safe_convert_precursor(value):
            """å®‰å…¨è½¬æ¢precursor_masså€¼ï¼Œå¤„ç†å¼‚å¸¸æ ¼å¼"""
            if pd.isna(value):
                return None
            
            # å¦‚æœå·²ç»æ˜¯æ•°å­—
            if isinstance(value, (int, float)):
                return float(value)
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            value_str = str(value).strip()
            
            # å¤„ç†ç©ºå­—ç¬¦ä¸²
            if value_str == '' or value_str.lower() == 'nan':
                return None
            
            # å¤„ç† "209/192" è¿™ç§æ ¼å¼ï¼ˆå–ç¬¬ä¸€ä¸ªå€¼ï¼‰
            if '/' in value_str:
                try:
                    return float(value_str.split('/')[0])
                except:
                    return None
            
            # å°è¯•ç›´æ¥è½¬æ¢
            try:
                return float(value_str)
            except:
                return None
        # ============================================
        
        # 1) Create word list: ["0.00", "0.01", ..., "999.99"]
        word_list = list(np.round(np.linspace(0, 1000, 100*1000, endpoint=False), 2))
        word_list = ["%.2f" % i for i in word_list]
        
        # 2) Build word2idx dictionary with special tokens
        word2idx = {'[PAD]': 0, '[MASK]': 1}
        for i, w in enumerate(word_list):
            word2idx[w] = i + 2  # Start from 2 to avoid collision with special tokens
        
        # 3) Initialize output dictionary
        ms_data = {}
        
        # ===== æ–°å¢ï¼šç»Ÿè®¡ä¿¡æ¯ =====
        filter_stats = {
            'total': 0,
            'empty_mz': 0,
            'not_positive': 0,
            'no_meta': 0,
            'no_precursor': 0,
            'precursor_gt_1000': 0,
            'no_peaks_after_filter': 0,
            'too_few_peaks': 0,  # æ–°å¢
            'kept': 0
        }
        # ===========================
        
        # 4) Iterate through each ms2_id
        for ms2_id, info in ms2_data.items():
            filter_stats['total'] += 1
            
            mz_data = info.get('mz')
            if mz_data is None or len(mz_data) == 0:
                filter_stats['empty_mz'] += 1
                continue
            peaks = info['mz']
            intensities = info['intensity']
            molecule_id = info.get('molecule_id', None)
            
            specific_row = meta_data[meta_data["file_name"] == ms2_id]
            if specific_row.empty:
                filter_stats['no_meta'] += 1
                continue
            elif specific_row["Polarity"].values[0] not in ["Positive", "positive"]:
                filter_stats['not_positive'] += 1
                continue
            
            # 4.1 Find precursor mass from meta_data
            if 'HMDB.ID' in meta_data.columns:
                row = meta_data.loc[meta_data['HMDB.ID'] == molecule_id]
            else:
                row = meta_data.loc[meta_data.index == molecule_id]
            if row.empty:
                filter_stats['no_meta'] += 1
                continue
            
            # ===== ä¿®æ”¹ï¼šä½¿ç”¨å®‰å…¨è½¬æ¢å‡½æ•° =====
            precursor_val = safe_convert_precursor(row['precursor_mass'].values[0])
            if precursor_val is None:
                filter_stats['no_precursor'] += 1
                continue
            # ===================================
            
            if precursor_val > 1000:
                filter_stats['precursor_gt_1000'] += 1
                continue
            precursor_str = "%.2f" % precursor_val
            
            # 4.2 Convert m/z values to string and map to indices
            peaks_str = []
            for mz_val in peaks:
                if mz_val <= 1000:
                    peaks_str.append("%.2f" % mz_val)
            
            # ===== æ–°å¢ï¼šæ£€æŸ¥peaksæ•°é‡ =====
            if len(peaks_str) == 0:
                filter_stats['no_peaks_after_filter'] += 1
                continue
            
            if len(peaks_str) < min_peaks:
                filter_stats['too_few_peaks'] += 1
                continue
            # ===============================
            
            token_ids = [word2idx[precursor_str]] + [word2idx[p] for p in peaks_str]
            
            # 4.3 Normalize intensity and prepend a fixed value (2)
            intensities = np.hstack((2, intensities))
            max_intensity = np.max(intensities)
            if max_intensity != 0:
                intensities = intensities / max_intensity
            
            # 4.4 Pad or truncate to maxlen
            n_pad = maxlen - len(token_ids)
            if n_pad < 0:
                token_ids = token_ids[:maxlen]
                intensities = intensities[:maxlen]
                n_pad = 0
            token_ids += [word2idx['[PAD]']] * n_pad
            if len(intensities) < maxlen:
                intensities = np.hstack([intensities, np.zeros(maxlen - len(intensities))])
            else:
                intensities = intensities[:maxlen]
            
            # 4.5 Save processed result
            ms_data[ms2_id] = {
                'mz': token_ids,
                'intensity': intensities.tolist(),
                'molecule_id': molecule_id
            }
            filter_stats['kept'] += 1
        
        # ===== æ–°å¢ï¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯ =====
        print(f"\né¢„å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»å…‰è°±: {filter_stats['total']}")
        print(f"  è¿‡æ»¤:")
        print(f"    ç©ºM/Z: {filter_stats['empty_mz']}")
        print(f"    éPositive: {filter_stats['not_positive']}")
        print(f"    æ— meta: {filter_stats['no_meta']}")
        print(f"    æ— /å¼‚å¸¸precursor: {filter_stats['no_precursor']}")
        print(f"    precursor>1000: {filter_stats['precursor_gt_1000']}")
        print(f"    æ‰€æœ‰peaks>1000: {filter_stats['no_peaks_after_filter']}")
        if min_peaks > 0:
            print(f"    peaks<{min_peaks}: {filter_stats['too_few_peaks']}")
        print(f"  âœ“ ä¿ç•™: {filter_stats['kept']} ({filter_stats['kept']/filter_stats['total']*100:.2f}%)")
        # ================================
        
        # 5) Return processed data and dictionary
        return ms_data, word2idx


    @staticmethod
    def augment_ms2_data_parallel(ms2_data, args, n_workers=None):
        """å¤šè¿›ç¨‹ç‰ˆæœ¬çš„augment_ms2_data"""
        from multiprocessing import Pool, cpu_count
        
        if n_workers is None:
            n_workers = min(cpu_count() - 1, 8)
        
        augment_noise = getattr(args, 'augment_noise', False)
        augment_multiplier = getattr(args, 'augment_multiplier', 1)
        
        if not augment_noise or augment_multiplier <= 1:
            print("â„¹ï¸  æœªå¯ç”¨æ•°æ®å¢å¼º")
            return ms2_data
        
        print(f"\nğŸš€ å¤šè¿›ç¨‹æ•°æ®å¢å¼º (workers={n_workers})...")
        
        # å‡†å¤‡å‚æ•°
        items = list(ms2_data.items())
        chunk_size = max(1, len(items) // (n_workers * 4))
        
        # æå–å‚æ•°
        filter_threshold = getattr(args, 'filter_threshold', None)
        noise_ratio = getattr(args, 'noise_ratio', 0.5)
        noise_intensity_range = getattr(args, 'noise_intensity_range', (0.001, 0.05))
        
        # åˆ†æ‰¹å¹¶æ‰“åŒ…å‚æ•°
        batches = [items[i:i+chunk_size] for i in range(0, len(items), chunk_size)]
        batch_data = [(batch, filter_threshold, noise_ratio, noise_intensity_range, augment_multiplier) 
                      for batch in batches]
        
        with Pool(n_workers) as pool:
            results = pool.map(_augment_worker, batch_data)
        
        # åˆå¹¶ç»“æœ
        augmented_data = {}
        for r in results:
            augmented_data.update(r)
        
        print(f"  âœ“ å®Œæˆ: {len(augmented_data)} å…‰è°±")
        return augmented_data
    

    @staticmethod
    def preprocess_ms2_data_positive_only_parallel(ms2_data, meta_data, maxlen=100, min_peaks=0, n_workers=None,
                                                    precursor_mode='normalize_add', precursor_value=2.0):
        """
        å¤šè¿›ç¨‹ç‰ˆæœ¬çš„preprocess
        
        Args:
            precursor_mode: 
                - 'scale_fixed': ç¼©æ”¾fragmentsåˆ°precursor_valueï¼ˆå¦‚20000ï¼‰ï¼Œprecursorå›ºå®šä¸º2
                - 'normalize_add': å½’ä¸€åŒ–fragmentsåˆ°1ï¼Œprecursorç”¨precursor_valueï¼ˆå¦‚2.0ï¼‰ï¼Œå†æ•´ä½“å½’ä¸€åŒ–
                - 'original': åŸå§‹MSBERTæ–¹å¼
            precursor_value: 
                - mode='scale_fixed'æ—¶: fragmentsç¼©æ”¾çš„ç›®æ ‡å€¼ï¼ˆé»˜è®¤20000ï¼‰
                - mode='normalize_add'æ—¶: precursorçš„å¼ºåº¦å€¼ï¼ˆé»˜è®¤2.0ï¼‰
        """
        from multiprocessing import Pool, cpu_count
        import numpy as np
        import pandas as pd
        
        if n_workers is None:
            n_workers = min(cpu_count() - 1, 8)
        
        print(f"\nğŸš€ å¤šè¿›ç¨‹é¢„å¤„ç† (workers={n_workers}, mode={precursor_mode}, value={precursor_value})...")
        
        # æ„å»ºword2idx
        word_list = list(np.round(np.linspace(0, 1000, 100*1000, endpoint=False), 2))
        word_list = ["%.2f" % i for i in word_list]
        word2idx = {'[PAD]': 0, '[MASK]': 1}
        for i, w in enumerate(word_list):
            word2idx[w] = i + 2
        
        # é¢„å¤„ç† meta_data
        meta_data_processed = meta_data.copy()
        if "Polarity" in meta_data_processed.columns:
            meta_data_processed["Polarity"] = meta_data_processed["Polarity"].astype(str).str.lower().str.strip()
        
        # è®¡ç®—æœ€å¤§ç¢ç‰‡æ•°
        max_frag = max(0, min(100, maxlen - 1))
        
        # å‡†å¤‡æ•°æ®
        items = list(ms2_data.items())
        chunk_size = max(1, len(items) // (n_workers * 4))
        
        # åˆ†æ‰¹å¹¶æ‰“åŒ…å‚æ•°
        batches = [items[i:i+chunk_size] for i in range(0, len(items), chunk_size)]
        batch_data = [(batch, word2idx, meta_data_processed, maxlen, max_frag, min_peaks, 
                    precursor_mode, precursor_value)
                    for batch in batches]
        
        with Pool(n_workers) as pool:
            results = pool.map(_preprocess_worker, batch_data)
        
        # åˆå¹¶
        ms_data = {}
        total_kept = 0
        total_filtered = 0
        for r, stats in results:
            ms_data.update(r)
            total_kept += stats['kept']
            total_filtered += stats['filtered']
        
        print(f"  âœ“ å®Œæˆ: {total_kept}/{len(items)} å…‰è°± (è¿‡æ»¤: {total_filtered})")
        return ms_data, word2idx

    @staticmethod
    def preprocess_ms2_data(ms2_data, meta_data, maxlen=100):
        """
        Preprocess ms2_data for model input.

        Parameters:
        - ms2_data: dict, {ms2_id: {'mz': list, 'intensity': list, 'molecule_id': str}}
        - meta_data: pd.DataFrame, must contain precursor information (column: 'precursor_mass')
        - maxlen: int, maximum sequence length

        Returns:
        - ms_data: dict, same structure as ms2_data but with processed 'mz' and 'intensity' sequences
        - word2idx: dict, maps string-formatted m/z values to token indices
        """
        # 1) Create word list: ["0.00", "0.01", ..., "999.99"]
        word_list = list(np.round(np.linspace(0, 1000, 100*1000, endpoint=False), 2))
        word_list = ["%.2f" % i for i in word_list]

        # 2) Build word2idx dictionary with special tokens
        word2idx = {'[PAD]': 0, '[MASK]': 1}
        for i, w in enumerate(word_list):
            word2idx[w] = i + 2  # Start from 2 to avoid collision with special tokens

        # 3) Initialize output dictionary
        ms_data = {}

        # 4) Iterate through each ms2_id
        for ms2_id, info in ms2_data.items():
            if not info['mz']:
                continue
            peaks = info['mz']
            intensities = info['intensity']
            molecule_id = info.get('molecule_id', None)

            # 4.1 Find precursor mass from meta_data
            if 'HMDB.ID' in meta_data.columns:
                row = meta_data.loc[meta_data['HMDB.ID'] == molecule_id]
            else:
                row = meta_data.loc[meta_data.index == molecule_id]
            if row.empty:
                continue
            precursor_val = float(row['precursor_mass'].values[0])
            if pd.isna(precursor_val):
                continue
            if precursor_val > 1000:
                continue
            precursor_str = "%.2f" % precursor_val

            # 4.2 Convert m/z values to string and map to indices
            peaks_str = []
            for mz_val in peaks:
                if mz_val <= 1000:
                    peaks_str.append("%.2f" % mz_val)
                else:
                    continue
            token_ids = [word2idx[precursor_str]] + [word2idx[p] for p in peaks_str]

            # 4.3 Normalize intensity and prepend a fixed value (2)
            intensities = np.hstack((2, intensities))
            max_intensity = np.max(intensities)
            if max_intensity != 0:
                intensities = intensities / max_intensity

            # 4.4 Pad or truncate to maxlen
            n_pad = maxlen - len(token_ids)
            if n_pad < 0:
                token_ids = token_ids[:maxlen]
                intensities = intensities[:maxlen]
                n_pad = 0
            token_ids += [word2idx['[PAD]']] * n_pad
            if len(intensities) < maxlen:
                intensities = np.hstack([intensities, np.zeros(maxlen - len(intensities))])
            else:
                intensities = intensities[:maxlen]

            # 4.5 Save processed result
            ms_data[ms2_id] = {
                'mz': token_ids,
                'intensity': intensities.tolist(),
                'molecule_id': molecule_id
            }

        # 5) Return processed data and dictionary
        return ms_data, word2idx

    @staticmethod
    def fill_precursor_data(meta_data, ms_data):
        """
        Fill in missing precursor ion mass in mass spectrometry metadata.

        Parameters:
        meta_data (DataFrame): DataFrame containing metadata for mass spectrometry samples.
        ms_data (dict): Dictionary containing peak data, with keys as spectrum IDs and values as dicts with 'mz' and 'intensity'.

        Returns:
        DataFrame: Updated meta_data with filled precursor ion mass.
        """
        # Copy the DataFrame to avoid modifying the original
        meta_data = meta_data.copy()

        # Find column name containing 'precursor'
        precursor_cols = [col for col in meta_data.columns if 'precursor' in col.lower()]
        if not precursor_cols:
            raise ValueError("No column containing 'precursor' found in meta_data")
        precursor_col = precursor_cols[0]
        print(f"Using '{precursor_col}' as the precursor mass column")

        init_nan = meta_data[precursor_col].isna().sum()

        # Find column named 'mz'
        mz_cols = [col for col in meta_data.columns if col.lower() == 'mz']
        has_mz_column = len(mz_cols) > 0
        mz_col = mz_cols[0] if has_mz_column else None

        # Proton mass (H+) is approximately 1.007276 Da
        proton_mass = 1.007276

        # Tolerance threshold for isotopic effect (Da)
        isotope_threshold = 2.0

        # Dictionary of adduct ion modes with their corresponding mass calculations
        adduct_modes = {
            'positive': {
                '[M+H]+': lambda m: m + proton_mass,
                '[M+H-H2O]+': lambda m: m + proton_mass - 18.010565,
                '[M+Na]+': lambda m: m + 22.989218,
                '[M+K]+': lambda m: m + 39.098301,
                '[M+NH4]+': lambda m: m + 18.033823,
                '[2M+H]+': lambda m: 2*m + proton_mass,
                '[2M+Na]+': lambda m: 2*m + 22.989218,
                '[2M+K]+': lambda m: 2*m + 39.098301,
                '[2M+NH4]+': lambda m: 2*m + 18.033823,
                '[2M+H-H2O]+': lambda m: 2*m + proton_mass - 18.010565
            },
            'negative': {
                '[M-H]-': lambda m: m - proton_mass,
                '[M-H2O-H]-': lambda m: m - proton_mass - 18.010565,
                '[M+Cl]-': lambda m: m + 34.969402,
                '[M+HAc-H]-': lambda m: m + 59.013851,
                '[2M-H]-': lambda m: 2*m - proton_mass,
                '[2M+Cl]-': lambda m: 2*m + 34.969402,
                '[2M+HAc-H]-': lambda m: 2*m + 59.013851
            }
        }
        meta_data[precursor_col] = pd.to_numeric(meta_data[precursor_col], errors='coerce')
        # Replace negative precursor values with NaN
        meta_data.loc[meta_data[precursor_col] < 0, precursor_col] = np.nan

        # Fill missing precursor masses
        for idx, row in meta_data.iterrows():
            if pd.isna(row[precursor_col]):
                spectrum_id = idx

                # Determine polarity
                polarity = str(row['Polarity']).lower()
                if 'positive' in polarity:
                    polarity_type = 'positive'
                elif 'negative' in polarity:
                    polarity_type = 'negative'
                else:
                    polarity_type = 'positive'

                # Try to get base mz from meta_data
                base_mz = None
                if has_mz_column and not pd.isna(row[mz_col]):
                    base_mz = row[mz_col]
                else:
                    if row["file_name"] not in ms_data:
                        print(f"Warning: spectrum ID {spectrum_id} not found in ms_data")
                        continue

                    spectrum = ms_data.get(row["file_name"], {})
                    if 'mz' not in spectrum or len(spectrum['mz']) == 0:
                        print(f"Warning: spectrum ID {spectrum_id} has no mz data")
                        continue

                    base_mz = max(spectrum['mz'])

                candidate_precursors = {}
                for mode_name, mode_func in adduct_modes[polarity_type].items():
                    candidate_mass = mode_func(base_mz)
                    candidate_precursors[mode_name] = candidate_mass

                valid_candidates = {}
                spectrum = ms_data.get(row["file_name"], {})
                if 'mz' in spectrum and len(spectrum['mz']) > 0:
                    max_fragment_mz = max(spectrum['mz'])

                    for mode_name, precursor_mass in candidate_precursors.items():
                        if max_fragment_mz <= precursor_mass + isotope_threshold:
                            valid_candidates[mode_name] = precursor_mass

                if valid_candidates:
                    default_mode = '[M+H]+' if polarity_type == 'positive' else '[M-H]-'
                    if default_mode in valid_candidates:
                        selected_mode = default_mode
                    else:
                        selected_mode = list(valid_candidates.keys())[0]
                    precursor_mass = valid_candidates[selected_mode]
                    meta_data.at[idx, precursor_col] = precursor_mass
                else:
                    if 'mz' in spectrum and len(spectrum['mz']) > 0:
                        max_mz = max(spectrum['mz'])
                        if polarity_type == 'positive':
                            adjusted_precursor = max_mz + proton_mass + 1.0
                        else:
                            adjusted_precursor = max_mz - proton_mass + 1.0
                        meta_data.at[idx, precursor_col] = adjusted_precursor
                    else:
                        print(f"Warning: unable to determine precursor mass for spectrum ID {spectrum_id}")

        left_nan = meta_data[precursor_col].isna().sum()
        print(f"Precursor mass missing: initially {init_nan}; filled {init_nan - left_nan}; remaining {left_nan}.")

        return meta_data

    
    @staticmethod
    def preprocess_ms2_data_positive_only(ms2_data, meta_data, maxlen=100):
        """
        Preprocess ms2_data for model input.

        Parameters:
        - ms2_data: dict, {ms2_id: {'mz': list, 'intensity': list, 'molecule_id': str}}
        - meta_data: pd.DataFrame, must contain precursor information (column: 'precursor_mass')
        - maxlen: int, maximum sequence length

        Returns:
        - ms_data: dict, same structure as ms2_data but with processed 'mz' and 'intensity' sequences
        - word2idx: dict, maps string-formatted m/z values to token indices
        """
        import numpy as np
        import pandas as pd

        # 1) Create word list: ["0.00", "0.01", ..., "999.99"]
        word_list = list(np.round(np.linspace(0, 1000, 100 * 1000, endpoint=False), 2))
        word_list = ["%.2f" % i for i in word_list]

        # 2) Build word2idx dictionary with special tokens
        word2idx = {'[PAD]': 0, '[MASK]': 1}
        for i, w in enumerate(word_list):
            word2idx[w] = i + 2  # Start from 2 to avoid collision with special tokens

        # 3) Initialize output dictionary
        ms_data = {}

        # é¢„è®¡ç®—ï¼šæ­£ç¦»å­æ¨¡å¼çš„åˆ¤å®šæ›´ç¨³å¥ï¼ˆlower+stripï¼‰
        if "Polarity" in meta_data.columns:
            meta_data = meta_data.copy()
            meta_data["Polarity"] = meta_data["Polarity"].astype(str).str.lower().str.strip()

        # å…è®¸çš„æœ€å¤§ç¢ç‰‡æ•°ï¼ˆä¿è¯å‰ä½“+ç¢ç‰‡ <= maxlenï¼‰
        max_frag = max(0, min(100, maxlen - 1))

        # 4) Iterate through each ms2_id
        for ms2_id, info in ms2_data.items():
            # åŸºç¡€æ£€æŸ¥
            if not info.get('mz'):
                continue

            peaks = np.asarray(info['mz'], dtype=float)
            intensities = np.asarray(info['intensity'], dtype=float)
            molecule_id = info.get('molecule_id', None)

            # 4.0 æ–‡ä»¶åå¯¹åº”è¡Œç”¨äºææ€§åˆ¤æ–­
            specific_row = meta_data.loc[meta_data["file_name"] == ms2_id] if "file_name" in meta_data.columns else pd.DataFrame()
            if specific_row.empty:
                # è‹¥æ‰¾ä¸åˆ°ï¼Œå°±å°½é‡ç”¨ molecule_id å®šä½ä¸€è¡Œï¼ˆä¸å¼ºåˆ¶ï¼‰
                if molecule_id is not None:
                    if 'HMDB.ID' in meta_data.columns:
                        specific_row = meta_data.loc[meta_data['HMDB.ID'] == molecule_id]
                    else:
                        specific_row = meta_data.loc[meta_data.index == molecule_id]
            if specific_row.empty:
                continue

            # åªä¿ç•™æ­£ç¦»å­
            pol = str(specific_row["Polarity"].values[0]).lower().strip() if "Polarity" in specific_row.columns else ""
            if pol != "positive":
                continue

            # 4.1 Find precursor mass from meta_data
            if 'HMDB.ID' in meta_data.columns and (molecule_id is not None):
                row = meta_data.loc[meta_data['HMDB.ID'] == molecule_id]
            else:
                row = meta_data.loc[meta_data.index == molecule_id]

            if row.empty or ('precursor_mass' not in row.columns):
                continue

            try:
                precursor_val = float(row['precursor_mass'].values[0])
            except Exception:
                continue

            # å‰ä½“èŒƒå›´ [10, 1000)ï¼›å¹¶é¿å… 1000.00 è¢«æ ¼å¼åŒ–åè¶Šç•Œ
            if pd.isna(precursor_val) or (precursor_val < 10.0) or (precursor_val >= 1000.0):
                continue
            precursor_val = min(precursor_val, 999.99)
            precursor_str = "%.2f" % precursor_val

            # 4.2 è¿‡æ»¤å³°åˆ° [10, 1000)
            if peaks.shape[0] != intensities.shape[0]:
                # é•¿åº¦ä¸ä¸€è‡´ç›´æ¥è·³è¿‡ï¼ˆä¹Ÿå¯é€‰æ‹©æˆªæ–­åˆ°å¯¹é½æœ€çŸ­ï¼‰
                n = min(len(peaks), len(intensities))
                peaks = peaks[:n]
                intensities = intensities[:n]

            mask = (peaks >= 10.0) & (peaks < 1000.0) & np.isfinite(peaks) & np.isfinite(intensities)
            peaks = peaks[mask]
            intensities = intensities[mask]

            if peaks.size == 0:
                continue

            # 4.3 æŒ‰å¼ºåº¦é€‰ Top-K ç¢ç‰‡ï¼ˆæœ€å¤š 100ï¼Œä¸”ä¿è¯å‰ä½“+ç¢ç‰‡ <= maxlenï¼‰
            if peaks.size > max_frag:
                idx = np.argpartition(intensities, -max_frag)[-max_frag:]
                # é€‰å®ŒåæŒ‰ m/z å‡åºæ’åºï¼ˆä¹Ÿå¯æŒ‰å¼ºåº¦é™åºï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                order = np.argsort(peaks[idx])
                idx = idx[order]
                peaks_sel = peaks[idx]
                intens_sel = intensities[idx]
            else:
                # ç›´æ¥æŒ‰ m/z å‡åº
                order = np.argsort(peaks)
                peaks_sel = peaks[order]
                intens_sel = intensities[order]

            # 4.4 æ„å»º token åºåˆ—ï¼ˆå‰ä½“åœ¨æœ€å‰ï¼‰
            peaks_str = ["%.2f" % p for p in peaks_sel]
            try:
                token_ids = [word2idx[precursor_str]] + [word2idx[p] for p in peaks_str]
            except KeyError:
                # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼ˆæˆ‘ä»¬å·²é™åˆ¶åˆ° [10, 999.99]ï¼‰ï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                continue

            # 4.5 å¼ºåº¦ï¼šåœ¨æœ€å‰ prepend 2ï¼Œå¹¶æŒ‰ä½ åŸæœ‰é€»è¾‘æ•´ä½“å½’ä¸€åŒ–
            intens_seq = np.hstack((2.0, intens_sel))
            max_intensity = float(np.max(intens_seq)) if intens_seq.size else 1.0
            if max_intensity != 0.0:
                intens_seq = intens_seq / max_intensity

            # 4.6 Pad æˆ–æˆªæ–­åˆ° maxlenï¼ˆåŒåºåˆ—ä¸¥æ ¼å¯¹é½ï¼‰
            if len(token_ids) > maxlen:
                token_ids = token_ids[:maxlen]
                intens_seq = intens_seq[:maxlen]

            n_pad = maxlen - len(token_ids)
            if n_pad > 0:
                token_ids += [word2idx['[PAD]']] * n_pad
                intens_seq = np.hstack([intens_seq, np.zeros(n_pad, dtype=float)])

            # 4.7 Save processed result
            ms_data[ms2_id] = {
                'mz': token_ids,
                'intensity': intens_seq.tolist(),
                'molecule_id': molecule_id
            }

        # 5) Return processed data and dictionary
        return ms_data, word2idx



    @staticmethod
    def load_external_test_dataset(
        external_data_dir,
        biotext_dir,
        paraphrase_dir,
        tokenizer,
        args,
        dataset_configs=None,
        **kwargs
    ):
        """
        åŠ è½½å¹¶å¤„ç†å¤–éƒ¨æµ‹è¯•æ•°æ®é›†ï¼ˆå¦‚HILICå’ŒRPLCï¼‰
        
        å‚æ•°:
        external_data_dir (str): å¤–éƒ¨æ•°æ®ç›®å½•è·¯å¾„
        biotext_dir (str): BioTextæ–‡æœ¬æ–‡ä»¶ç›®å½•
        paraphrase_dir (str): Paraphraseæ–‡æœ¬æ–‡ä»¶ç›®å½•
        tokenizer: æ–‡æœ¬tokenizer
        args: åŒ…å«é¢„å¤„ç†å‚æ•°çš„argså¯¹è±¡ï¼ˆprecursor_mode, precursor_value, n_workersç­‰ï¼‰
        dataset_configs (list): æ•°æ®é›†é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªé…ç½®åŒ…å«name, ms2_file, meta_file
        **kwargs: ä¼ é€’ç»™MS2BioTextDatasetæ„é€ å‡½æ•°çš„å…¶ä»–å‚æ•°
        
        è¿”å›:
        tuple: (external_test_dataset, data_statistics)
            - external_test_dataset: MS2BioTextDatasetå®ä¾‹
            - data_statistics: åŒ…å«æ•°æ®ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        import pickle
        import pandas as pd
        import os
        from pathlib import Path
        
        # é»˜è®¤é…ç½®ï¼ˆHILICå’ŒRPLCï¼‰
        if dataset_configs is None:
            dataset_configs = [
                {
                    'name': 'HILIC',
                    'ms2_file': os.path.join(external_data_dir, 'hilic_ms_data.pkl'),
                    'meta_file': os.path.join(external_data_dir, 'hilic_meta_data.csv'),
                },
                {
                    'name': 'RPLC', 
                    'ms2_file': os.path.join(external_data_dir, 'rplc_ms_data.pkl'),
                    'meta_file': os.path.join(external_data_dir, 'rplc_meta_data.csv'),
                }
            ]
        
        print("\n" + "="*60)
        print("åŠ è½½å¤–éƒ¨æµ‹è¯•æ•°æ®é›†...")
        print("="*60)
        
        # 1. åŠ è½½æ‰€æœ‰æ•°æ®é›†
        all_ms2_data = {}
        all_meta_data = []
        
        for config in dataset_configs:
            print(f"\nğŸ“ åŠ è½½ {config['name']} æ•°æ®é›†...")
            
            # åŠ è½½MS2æ•°æ®
            with open(config['ms2_file'], 'rb') as f:
                ms2_data = pickle.load(f)
            
            # åŠ è½½Metaæ•°æ®
            meta_data = pd.read_csv(config['meta_file'])
            
            print(f"  âœ“ {config['name']}: {len(ms2_data)} å…‰è°±, {len(meta_data)} meta")
            
            # åˆå¹¶MS2
            all_ms2_data.update(ms2_data)
            all_meta_data.append(meta_data)
        
        # 2. åˆå¹¶Metaæ•°æ®ï¼ˆç¡®ä¿åˆ—å¯¹é½ï¼‰
        if len(all_meta_data) > 1:
            all_cols = set()
            for df in all_meta_data:
                all_cols.update(df.columns)
            all_cols = sorted(all_cols)
            
            aligned_meta_data = []
            for df in all_meta_data:
                df = df.reindex(columns=all_cols, fill_value=None)
                aligned_meta_data.append(df)
            
            external_meta_data = pd.concat(aligned_meta_data, ignore_index=True)
        else:
            external_meta_data = all_meta_data[0]
        
        external_ms2_data = all_ms2_data
        
        print(f"\nâœ“ åˆå¹¶åå¤–éƒ¨æ•°æ®é›†: {len(external_ms2_data)} å…‰è°±, {len(external_meta_data)} meta")
        
        # 3. è®¾ç½®HMDB.IDä¸ºç´¢å¼•
        if 'HMDB.ID' in external_meta_data.columns:
            external_meta_data = external_meta_data.set_index('HMDB.ID')
            print(f"  å·²è®¾ç½®HMDB.IDä¸ºç´¢å¼•")
        
        # 4. ç¡®ä¿MS2æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆæ·»åŠ molecule_idå­—æ®µï¼‰
        print("\nğŸ”§ ä¿®æ­£MS2æ•°æ®æ ¼å¼...")
        for spectrum_id, spectrum_data in external_ms2_data.items():
            if 'molecule_id' not in spectrum_data:
                molecule_id = spectrum_id.split('_')[0]
                spectrum_data['molecule_id'] = molecule_id
        
        # 5. è·å–æ‰€æœ‰uniqueçš„HMDB IDs
        unique_hmdb_ids = set()
        for spec_id in external_ms2_data.keys():
            hmdb_id = spec_id.split('_')[0]
            unique_hmdb_ids.add(hmdb_id)
        
        print(f"  å¤–éƒ¨æ•°æ®é›†åŒ…å« {len(unique_hmdb_ids)} ä¸ªunique HMDB IDs")
        
        # 6. åŠ è½½å¯¹åº”çš„BioTextæ•°æ®
        print("\nğŸ“š åŠ è½½BioTextæ•°æ®...")
        external_biotext_data = {}
        missing_biotext = []
        
        biotext_dir = Path(biotext_dir)
        paraphrase_dir = Path(paraphrase_dir) if paraphrase_dir else None
        
        for hmdb_id in unique_hmdb_ids:
            # å¤„ç†å¼‚å¸¸çš„HMDB IDï¼ˆå¦‚åŒ…å«{}çš„ï¼‰
            if '{}' in hmdb_id:
                missing_biotext.append(hmdb_id)
                continue
                
            biotext_file = biotext_dir / f"{hmdb_id}.txt"
            if biotext_file.exists():
                with open(biotext_file, 'r', encoding='utf-8') as f:
                    original_text = f.read().strip()
                
                # åŠ è½½paraphraseï¼ˆå¦‚æœæœ‰ï¼‰
                paraphrases = []
                if paraphrase_dir:
                    paraphrase_file = paraphrase_dir / f"{hmdb_id}_paraphrase.txt"
                    if paraphrase_file.exists():
                        with open(paraphrase_file, 'r', encoding='utf-8') as pf:
                            content = pf.read()
                            versions = content.split("=== version")
                            for version in versions[1:]:
                                _, text = version.split("===", 1)
                                text = text.strip()
                                if text:
                                    paraphrases.append(text)
                
                external_biotext_data[hmdb_id] = {
                    'original': original_text,
                    'paraphrases': paraphrases
                }
            else:
                missing_biotext.append(hmdb_id)
        
        print(f"  âœ“ æˆåŠŸåŠ è½½ {len(external_biotext_data)} ä¸ªBioText")
        print(f"  âœ— ç¼ºå¤±BioText: {len(missing_biotext)} ä¸ª")
        
        # 7. å¤„ç†ç¼ºå¤±çš„biotextï¼ˆä½¿ç”¨dropæ–¹æ³•ï¼‰
        initial_ms2_count = len(external_ms2_data)
        external_ms2_data, _ = MS2BioTextDataset.missing_biotext_handling(
            external_ms2_data, 
            external_biotext_data, 
            method="drop"
        )
        print(f"  åˆ é™¤ {initial_ms2_count - len(external_ms2_data)} æ¡ç¼ºå¤±biotextçš„å…‰è°±")
        
        # 8. æ›´æ–°meta_dataï¼Œåªä¿ç•™æœ‰MS2æ•°æ®çš„æ¡ç›®
        remaining_hmdb_ids = set()
        for spectrum_id in external_ms2_data.keys():
            hmdb_id = spectrum_id.split('_')[0]
            remaining_hmdb_ids.add(hmdb_id)
        
        external_meta_data = external_meta_data[external_meta_data.index.isin(remaining_hmdb_ids)]
        
        # 9. å¡«å……precursoræ•°æ®
        print("\nâš™ï¸ å¡«å……precursoræ•°æ®...")
        external_meta_data = MS2BioTextDataset.fill_precursor_data(
            external_meta_data,
            external_ms2_data
        )
        
        # 10. é¢„å¤„ç†MS2æ•°æ®ï¼ˆæµ‹è¯•é›†ä¸åšæ•°æ®å¢å¼ºï¼‰
        print("\nğŸš€ é¢„å¤„ç†MS2æ•°æ®ï¼ˆä¸è¿›è¡Œæ•°æ®å¢å¼ºï¼‰...")
        external_processed_ms2, external_word2idx = MS2BioTextDataset.preprocess_ms2_data_positive_only_parallel(
            external_ms2_data,
            external_meta_data,
            n_workers=getattr(args, 'n_workers', 4),
            precursor_mode=getattr(args, 'precursor_mode', 'auto'),
            precursor_value=getattr(args, 'precursor_value', 2.0)
        )
        
        # 11. ç»Ÿè®¡ä¿¡æ¯
        data_statistics = {
            'original_ms2_count': initial_ms2_count,
            'processed_ms2_count': len(external_processed_ms2),
            'meta_count': len(external_meta_data),
            'biotext_count': len(external_biotext_data),
            'unique_molecules': len(remaining_hmdb_ids),
            'vocab_size': len(external_word2idx),
            'datasets': [config['name'] for config in dataset_configs]
        }
        
        print("\n" + "="*60)
        print("ğŸ“Š å¤–éƒ¨æµ‹è¯•æ•°æ®é›†æœ€ç»ˆç»Ÿè®¡")
        print("="*60)
        print(f"  åŸå§‹MS2å…‰è°±æ•°: {data_statistics['original_ms2_count']}")
        print(f"  å¤„ç†åMS2å…‰è°±æ•°: {data_statistics['processed_ms2_count']}")
        print(f"  Metaè®°å½•æ•°: {data_statistics['meta_count']}")
        print(f"  BioTextæ•°: {data_statistics['biotext_count']}")
        print(f"  Uniqueåˆ†å­æ•°: {data_statistics['unique_molecules']}")
        print(f"  è¯æ±‡è¡¨å¤§å°: {data_statistics['vocab_size']}")
        print(f"  æ•°æ®é›†æ¥æº: {', '.join(data_statistics['datasets'])}")
        
        # 12. åˆ›å»ºDatasetå®ä¾‹
        print("\nğŸ¯ åˆ›å»ºå¤–éƒ¨æµ‹è¯•Datasetå®ä¾‹...")
        external_test_dataset = MS2BioTextDataset(
            ms2_data=external_processed_ms2,
            meta_data=external_meta_data,
            biotext_data=external_biotext_data,
            tokenizer=tokenizer,
            use_paraphrase=False,
            **kwargs
        )
        
        print(f"âœ… å¤–éƒ¨æµ‹è¯•Datasetåˆ›å»ºæˆåŠŸï¼å¤§å°: {len(external_test_dataset)}")
        
        return external_test_dataset, data_statistics

    
    # @staticmethod
    # def create_train_test_datasets_from_file(
    #     data_dir, 
    #     ms2_data, 
    #     meta_data, 
    #     biotext_data, 
    #     tokenizer, 
    #     test_size=0.2, 
    #     random_state=42, 
    #     use_paraphrase = False,
    #     **kwargs
    # ):
    #     """
    #     Split data into training and test sets based on molecule IDs.
    #     This version checks whether a local file with pre-defined splits exists.
    #     If it exists, it loads the split; otherwise, it creates the split and saves it to file.
    #     (Enhanced to handle empty or corrupted JSON files gracefully)

    #     Parameters:
    #     data_dir (str or Path): Path to the directory containing the split file (molecule_split.json).
    #     ms2_data (dict): Full MS2 data dictionary.
    #     meta_data (pd.DataFrame): Metadata dataframe indexed by molecule ID.
    #     biotext_data (dict): Full BioText data dictionary.
    #     tokenizer: Tokenizer used for initializing the Dataset.
    #     test_size (float): Proportion of test set (used only when creating the split).
    #     random_state (int): Random seed (used only when creating the split).
    #     **kwargs: Other arguments passed to the MS2BioTextDataset constructor.

    #     Returns:
    #     tuple: (train_dataset, test_dataset), two MS2BioTextDataset instances.
    #     """
    #     print("Preparing training and test datasets (with file persistence)...")

    #     # --- 1. Define split file path ---
    #     data_dir = Path(data_dir)
    #     split_file_path = data_dir / 'molecule_split.json'

    #     # --- 2. Load existing split file if valid; otherwise create new split ---
    #     train_mol_ids, test_mol_ids = None, None

    #     if split_file_path.exists() and split_file_path.stat().st_size > 0:
    #         print(f"âœ… Found existing split file: {split_file_path}")
    #         print("    Loading molecule IDs from file...")
    #         try:
    #             with open(split_file_path, 'r', encoding='utf-8') as f:
    #                 split_ids = json.load(f)
    #                 train_mol_ids = split_ids['train_ids']
    #                 test_mol_ids = split_ids['test_ids']
    #         except json.JSONDecodeError:
    #             print(f"    âš ï¸ Warning: File '{split_file_path}' exists but could not be parsed (may be empty or corrupted). Will recreate.")
    #         except KeyError:
    #             print(f"    âš ï¸ Warning: File '{split_file_path}' has invalid format (missing 'train_ids' or 'test_ids'). Will recreate.")

    #     if train_mol_ids is None or test_mol_ids is None:
    #         print(f"âš ï¸ No valid split file found or could not be loaded. Creating a new split...")
    #         valid_molecule_ids = set(item['molecule_id'] for item in ms2_data.values())
    #         all_molecule_ids = [mol_id for mol_id in meta_data.index.unique() if mol_id in valid_molecule_ids]
    #         print(f"    Found {len(all_molecule_ids)} unique molecules for splitting.")

    #         train_mol_ids, test_mol_ids = train_test_split(
    #             all_molecule_ids,
    #             test_size=test_size,
    #             random_state=random_state
    #         )

    #         print(f"    Saving new split to: {split_file_path}")
    #         split_data_to_save = {'train_ids': train_mol_ids, 'test_ids': test_mol_ids}
    #         split_file_path.parent.mkdir(parents=True, exist_ok=True)
    #         with open(split_file_path, 'w', encoding='utf-8') as f:
    #             json.dump(split_data_to_save, f, indent=4)
    #         print("    Split file saved successfully.")


    #     # --- 3. Filter data sources by ID lists ---
    #     train_mol_ids_set = set(train_mol_ids)
    #     test_mol_ids_set = set(test_mol_ids)

    #     train_meta_data = meta_data[meta_data.index.isin(train_mol_ids_set)]
    #     test_meta_data = meta_data[meta_data.index.isin(test_mol_ids_set)]

    #     train_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in train_mol_ids_set}
    #     test_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in test_mol_ids_set}
    #     train_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if info['molecule_id'] in train_mol_ids_set}
    #     test_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if info['molecule_id'] in test_mol_ids_set}

    #     print(f"Data filtering completed:")

    #     # --- 4. Create Dataset instances ---
    #     print("Creating training Dataset instance...")
    #     train_dataset = MS2BioTextDataset(
    #         ms2_data=train_ms2_data, 
    #         meta_data=train_meta_data, 
    #         biotext_data=train_biotext_data, 
    #         tokenizer=tokenizer, 
    #         use_paraphrase = use_paraphrase,
    #         **kwargs
    #     )

    #     print("Creating test Dataset instance...")
    #     test_dataset = MS2BioTextDataset(
    #         ms2_data=test_ms2_data, 
    #         meta_data=test_meta_data, 
    #         biotext_data=test_biotext_data, 
    #         tokenizer=tokenizer, 
    #         use_paraphrase=False,
    #         **kwargs
    #     )

    #     return train_dataset, test_dataset




    # @staticmethod
    # def create_train_test_datasets_from_file(
    #     data_dir, 
    #     ms2_data, 
    #     meta_data, 
    #     biotext_data, 
    #     tokenizer, 
    #     test_size=0.2,  
    #     use_paraphrase = False,
    #     **kwargs
    # ):
    #     """
    #     Split data into training and test sets based on MS2 spectra within each molecule.
    #     For each molecule with multiple MS2 spectra, one MS2 is reserved for testing,
    #     and the rest are used for training. Molecules with only one MS2 spectrum are
    #     only included in the training set.
        
    #     Parameters:
    #     data_dir (str or Path): Path to the directory containing the split file (ms2_split.json).
    #     ms2_data (dict): Full MS2 data dictionary {ms2_id: {molecule_id: ..., ...}}.
    #     meta_data (pd.DataFrame): Metadata dataframe indexed by molecule ID.
    #     biotext_data (dict): Full BioText data dictionary {molecule_id: text}.
    #     tokenizer: Tokenizer used for initializing the Dataset.
    #     test_size (float): Deprecated in this version (kept for compatibility).
    #     random_state (int): Random seed for reproducible splits.
    #     **kwargs: Other arguments passed to the MS2BioTextDataset constructor.

    #     Returns:
    #     tuple: (train_dataset, test_dataset), two MS2BioTextDataset instances.
    #     """
    #     print("Preparing training and test datasets with per-molecule MS2 splitting...")
        
    #     # --- 1. Define split file path ---
    #     data_dir = Path(data_dir)
    #     split_file_path = data_dir / 'ms2_split.json'  # æ”¹åä»¥åŒºåˆ†æ–°çš„åˆ’åˆ†æ–¹å¼
        
    #     # --- 2. Load existing split file if valid; otherwise create new split ---
    #     train_ms2_ids, test_ms2_ids = None, None
        
    #     if split_file_path.exists() and split_file_path.stat().st_size > 0:
    #         print(f"âœ… Found existing split file: {split_file_path}")
    #         print("    Loading MS2 IDs from file...")
    #         try:
    #             with open(split_file_path, 'r', encoding='utf-8') as f:
    #                 split_ids = json.load(f)
    #                 train_ms2_ids = split_ids['train_ms2_ids']
    #                 test_ms2_ids = split_ids['test_ms2_ids']
    #         except json.JSONDecodeError:
    #             print(f"    âš ï¸ Warning: File '{split_file_path}' exists but could not be parsed. Will recreate.")
    #         except KeyError:
    #             print(f"    âš ï¸ Warning: File '{split_file_path}' has invalid format. Will recreate.")
        
    #     if train_ms2_ids is None or test_ms2_ids is None:
    #         print(f"âš ï¸ No valid split file found. Creating a new MS2-level split...")
            
    #         # æŒ‰åˆ†å­IDåˆ†ç»„MS2è°±å›¾
    #         molecule_to_ms2 = {}
    #         for ms2_id, ms2_info in ms2_data.items():
    #             mol_id = ms2_info['molecule_id']
    #             if mol_id not in molecule_to_ms2:
    #                 molecule_to_ms2[mol_id] = []
    #             molecule_to_ms2[mol_id].append(ms2_id)
            
    #         # ç»Ÿè®¡ä¿¡æ¯
    #         single_ms2_molecules = []
    #         multi_ms2_molecules = []
    #         for mol_id, ms2_list in molecule_to_ms2.items():
    #             if len(ms2_list) == 1:
    #                 single_ms2_molecules.append(mol_id)
    #             else:
    #                 multi_ms2_molecules.append(mol_id)
            
    #         print(f"    Found {len(single_ms2_molecules)} molecules with single MS2 spectrum")
    #         print(f"    Found {len(multi_ms2_molecules)} molecules with multiple MS2 spectra")
            
            
    #         train_ms2_ids = []
    #         test_ms2_ids = []
            
    #         # å¤„ç†åªæœ‰ä¸€ä¸ªMS2çš„åˆ†å­ï¼šå…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
    #         for mol_id in single_ms2_molecules:
    #             train_ms2_ids.extend(molecule_to_ms2[mol_id])
            
    #         # å¤„ç†æœ‰å¤šä¸ªMS2çš„åˆ†å­ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
    #         for mol_id in multi_ms2_molecules:
    #             ms2_list = molecule_to_ms2[mol_id]
    #             # éšæœºé€‰æ‹©ä¸€ä¸ªMS2ä½œä¸ºæµ‹è¯•é›†
    #             test_ms2_id = np.random.choice(ms2_list)
    #             test_ms2_ids.append(test_ms2_id)
    #             # å…¶ä½™çš„ä½œä¸ºè®­ç»ƒé›†
    #             train_ms2_ids.extend([ms2_id for ms2_id in ms2_list if ms2_id != test_ms2_id])
            
    #         print(f"    Split results: {len(train_ms2_ids)} training MS2, {len(test_ms2_ids)} test MS2")
            
    #         # ä¿å­˜åˆ’åˆ†ç»“æœ
    #         print(f"    Saving new split to: {split_file_path}")
    #         split_data_to_save = {
    #             'train_ms2_ids': train_ms2_ids, 
    #             'test_ms2_ids': test_ms2_ids
    #         }
    #         split_file_path.parent.mkdir(parents=True, exist_ok=True)
    #         with open(split_file_path, 'w', encoding='utf-8') as f:
    #             json.dump(split_data_to_save, f, indent=4)
    #         print("    Split file saved successfully.")
        
    #     # --- 3. Filter data sources by MS2 ID lists ---
    #     train_ms2_ids_set = set(train_ms2_ids)
    #     test_ms2_ids_set = set(test_ms2_ids)
        
    #     # è¿‡æ»¤MS2æ•°æ®
    #     train_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if ms2_id in train_ms2_ids_set}
    #     test_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if ms2_id in test_ms2_ids_set}
        
    #     # è·å–æ¶‰åŠçš„åˆ†å­ID
    #     train_molecule_ids = set(info['molecule_id'] for info in train_ms2_data.values())
    #     test_molecule_ids = set(info['molecule_id'] for info in test_ms2_data.values())
        
    #     # è¿‡æ»¤å…ƒæ•°æ®å’Œbiotextæ•°æ®
    #     # æ³¨æ„ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†å¯èƒ½åŒ…å«ç›¸åŒçš„åˆ†å­IDï¼ˆå› ä¸ºåŒä¸€åˆ†å­çš„ä¸åŒMS2å¯èƒ½åˆ†å¸ƒåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ï¼‰
    #     train_meta_data = meta_data[meta_data.index.isin(train_molecule_ids)]
    #     test_meta_data = meta_data[meta_data.index.isin(test_molecule_ids)]
        
    #     train_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in train_molecule_ids}
    #     test_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in test_molecule_ids}
        
    #     print(f"Data filtering completed:")
    #     print(f"    Training: {len(train_ms2_data)} MS2 spectra from {len(train_molecule_ids)} molecules")
    #     print(f"    Test: {len(test_ms2_data)} MS2 spectra from {len(test_molecule_ids)} molecules")
        
    #     # --- 4. Create Dataset instances ---
    #     print("Creating training Dataset instance...")
    #     train_dataset = MS2BioTextDataset(
    #         ms2_data=train_ms2_data, 
    #         meta_data=train_meta_data, 
    #         biotext_data=train_biotext_data, 
    #         tokenizer=tokenizer, 
    #         use_paraphrase=use_paraphrase,
    #         **kwargs
    #     )
        
    #     print("Creating test Dataset instance...")
    #     test_dataset = MS2BioTextDataset(
    #         ms2_data=test_ms2_data, 
    #         meta_data=test_meta_data, 
    #         biotext_data=test_biotext_data, 
    #         tokenizer=tokenizer, 
    #         use_paraphrase=False,
    #         **kwargs
    #     )
        
    #     return train_dataset, test_dataset

    @staticmethod
    def filter_shared_texts(biotext_data, max_sharing_molecules=5):
        """
        åˆ é™¤è¢«è¿‡å¤šmoleculeå…±äº«çš„text
        
        Args:
            biotext_data: {molecule_id: [{'type': ..., 'text': ...}, ...]}
            max_sharing_molecules: textæœ€å¤šå¯ä»¥è¢«å¤šå°‘ä¸ªmoleculeå…±äº«
        
        Returns:
            filtered_biotext_data: æ¸…æ´—åçš„æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        from collections import defaultdict
        
        print(f"\n=== Filtering shared texts (max_sharing={max_sharing_molecules}) ===")
        
        # 1. æ„å»ºtext -> moleculesçš„å€’æ’ç´¢å¼•
        text_to_molecules = defaultdict(set)
        
        for mol_id, entry in biotext_data.items():
            texts = []
            if isinstance(entry, list):
                texts = [record['text'] for record in entry]
            elif isinstance(entry, dict):
                texts = [entry.get("original", "")] + entry.get("paraphrases", [])
            elif isinstance(entry, str):
                texts = [entry]
            
            for text in texts:
                if text:  # é¿å…ç©ºå­—ç¬¦ä¸²
                    text_to_molecules[text].add(mol_id)
        
        # 2. æ‰¾å‡ºéœ€è¦åˆ é™¤çš„é«˜é¢‘text
        texts_to_remove = set()
        for text, molecules in text_to_molecules.items():
            if len(molecules) > max_sharing_molecules:
                texts_to_remove.add(text)
        
        print(f"Found {len(texts_to_remove)} texts shared by >{max_sharing_molecules} molecules")
        
        # 3. ä»æ¯ä¸ªmoleculeçš„å€™é€‰textä¸­åˆ é™¤è¿™äº›é«˜é¢‘text
        filtered_biotext_data = {}
        total_removed = 0
        molecules_with_no_text = []
        
        for mol_id, entry in biotext_data.items():
            if isinstance(entry, list):
                # æ–°æ ¼å¼ï¼šåˆ—è¡¨
                filtered_entry = [record for record in entry 
                                if record['text'] not in texts_to_remove]
                
                if filtered_entry:
                    filtered_biotext_data[mol_id] = filtered_entry
                else:
                    molecules_with_no_text.append(mol_id)
                
                total_removed += len(entry) - len(filtered_entry)
                
            elif isinstance(entry, dict):
                # æ—§æ ¼å¼ï¼šå­—å…¸
                original = entry.get("original", "")
                paraphrases = entry.get("paraphrases", [])
                
                filtered_paraphrases = [p for p in paraphrases if p not in texts_to_remove]
                
                # å¦‚æœoriginalä¹Ÿè¢«åˆ é™¤äº†ï¼Œç”¨ç¬¬ä¸€ä¸ªparaphraseä½œä¸ºoriginal
                if original in texts_to_remove:
                    if filtered_paraphrases:
                        original = filtered_paraphrases[0]
                        filtered_paraphrases = filtered_paraphrases[1:]
                    else:
                        molecules_with_no_text.append(mol_id)
                        continue
                
                filtered_biotext_data[mol_id] = {
                    'original': original,
                    'paraphrases': filtered_paraphrases
                }
                
                original_count = 1 if entry.get("original", "") not in texts_to_remove else 0
                total_removed += (len(paraphrases) - len(filtered_paraphrases) + 
                                (1 - original_count))
                
            elif isinstance(entry, str):
                # å­—ç¬¦ä¸²æ ¼å¼
                if entry not in texts_to_remove:
                    filtered_biotext_data[mol_id] = entry
                else:
                    molecules_with_no_text.append(mol_id)
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        print(f"Statistics:")
        print(f"  Total text entries removed: {total_removed}")
        print(f"  Molecules before filtering: {len(biotext_data)}")
        print(f"  Molecules after filtering: {len(filtered_biotext_data)}")
        print(f"  Molecules with no text left: {len(molecules_with_no_text)}")
        
        if molecules_with_no_text:
            print(f"  Warning: {len(molecules_with_no_text)} molecules lost all texts!")
            print(f"  First 5: {molecules_with_no_text[:5]}")
        
        # 5. éªŒè¯è¿‡æ»¤æ•ˆæœ
        text_to_molecules_after = defaultdict(set)
        for mol_id, entry in filtered_biotext_data.items():
            texts = []
            if isinstance(entry, list):
                texts = [record['text'] for record in entry]
            elif isinstance(entry, dict):
                texts = [entry.get("original", "")] + entry.get("paraphrases", [])
            elif isinstance(entry, str):
                texts = [entry]
            
            for text in texts:
                if text:
                    text_to_molecules_after[text].add(mol_id)
        
        max_sharing_after = max(len(mols) for mols in text_to_molecules_after.values()) if text_to_molecules_after else 0
        print(f"  Max molecules sharing one text after filtering: {max_sharing_after}")
        
        return filtered_biotext_data, {
            'removed_texts': len(texts_to_remove),
            'removed_entries': total_removed,
            'molecules_no_text': len(molecules_with_no_text),
            'max_sharing_after': max_sharing_after
        }



    @staticmethod
    def create_train_test_datasets_from_file(
        data_dir, 
        ms2_data, 
        meta_data, 
        biotext_data, 
        tokenizer, 
        word2idx,      
        args,         
        test_size=0.2,  
        use_paraphrase = False,
        **kwargs
    ):
        """
        Split data into training and test sets.
        If a split file exists, use its test_ms2_ids as the test set,
        and use ALL OTHER MS2 spectra (including any new data) as the training set.
        If no split file exists, create a new split following the original logic.
        
        Parameters:
        data_dir (str or Path): Path to the directory containing the split file (ms2_split.json).
        ms2_data (dict): Full MS2 data dictionary {ms2_id: {molecule_id: ..., ...}}.
        meta_data (pd.DataFrame): Metadata dataframe indexed by molecule ID.
        biotext_data (dict): Full BioText data dictionary {molecule_id: text}.
        tokenizer: Tokenizer used for initializing the Dataset.
        test_size (float): Deprecated in this version (kept for compatibility).
        **kwargs: Other arguments passed to the MS2BioTextDataset constructor.

        Returns:
        tuple: (train_dataset, test_dataset), two MS2BioTextDataset instances.
        """
        print("Preparing training and test datasets with per-molecule MS2 splitting...")
        
        # --- 1. Define split file path ---
        data_dir = Path(data_dir)
        split_file_path = data_dir / 'ms2_split.json'
        
        # --- 2. Load existing split file if valid; otherwise create new split ---
        test_ms2_ids = None
        
        if split_file_path.exists() and split_file_path.stat().st_size > 0:
            print(f"âœ… Found existing split file: {split_file_path}")
            print("    Loading test MS2 IDs from file...")
            try:
                with open(split_file_path, 'r', encoding='utf-8') as f:
                    split_ids = json.load(f)
                    test_ms2_ids = split_ids['test_ms2_ids']
                    print(f"    Loaded {len(test_ms2_ids)} test MS2 IDs from existing split.")
            except json.JSONDecodeError:
                print(f"    âš ï¸ Warning: File '{split_file_path}' exists but could not be parsed. Will recreate.")
            except KeyError:
                print(f"    âš ï¸ Warning: File '{split_file_path}' has invalid format. Will recreate.")
        
        if test_ms2_ids is None:
            print(f"âš ï¸ No valid split file found. Creating a new MS2-level split...")
            
            # æŒ‰åˆ†å­IDåˆ†ç»„MS2è°±å›¾
            molecule_to_ms2 = {}
            for ms2_id, ms2_info in ms2_data.items():
                mol_id = ms2_info['molecule_id']
                if mol_id not in molecule_to_ms2:
                    molecule_to_ms2[mol_id] = []
                molecule_to_ms2[mol_id].append(ms2_id)
            
            # ç»Ÿè®¡ä¿¡æ¯
            single_ms2_molecules = []
            multi_ms2_molecules = []
            for mol_id, ms2_list in molecule_to_ms2.items():
                if len(ms2_list) == 1:
                    single_ms2_molecules.append(mol_id)
                else:
                    multi_ms2_molecules.append(mol_id)
            
            print(f"    Found {len(single_ms2_molecules)} molecules with single MS2 spectrum")
            print(f"    Found {len(multi_ms2_molecules)} molecules with multiple MS2 spectra")
            
            test_ms2_ids = []
            
            # å¤„ç†æœ‰å¤šä¸ªMS2çš„åˆ†å­ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºæµ‹è¯•é›†
            for mol_id in multi_ms2_molecules:
                ms2_list = molecule_to_ms2[mol_id]
                # éšæœºé€‰æ‹©ä¸€ä¸ªMS2ä½œä¸ºæµ‹è¯•é›†
                test_ms2_id = np.random.choice(ms2_list)
                test_ms2_ids.append(test_ms2_id)
            
            print(f"    Created new test set with {len(test_ms2_ids)} MS2 spectra")
            
            # ä¿å­˜åˆ’åˆ†ç»“æœï¼ˆåªä¿å­˜test_ms2_idsï¼Œtrainä¼šåŠ¨æ€è®¡ç®—ï¼‰
            print(f"    Saving new split to: {split_file_path}")
            split_data_to_save = {
                'test_ms2_ids': test_ms2_ids,
                'note': 'Training set uses all MS2 IDs not in test_ms2_ids'
            }
            split_file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(split_file_path, 'w', encoding='utf-8') as f:
                json.dump(split_data_to_save, f, indent=4)
            print("    Split file saved successfully.")
        
        # --- 3. Create train set: ALL MS2 IDs except those in test set ---
        test_ms2_ids_set = set(test_ms2_ids)
        all_ms2_ids = set(ms2_data.keys())
        train_ms2_ids_set = all_ms2_ids - test_ms2_ids_set
        
        print(f"\nğŸ“Š Dataset Statistics:")
        print(f"    Total MS2 spectra: {len(all_ms2_ids)}")
        print(f"    Test MS2 spectra: {len(test_ms2_ids_set)}")
        print(f"    Training MS2 spectra: {len(train_ms2_ids_set)}")
        
        # è¿‡æ»¤MS2æ•°æ®
        train_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if ms2_id in train_ms2_ids_set}
        test_ms2_data = {ms2_id: info for ms2_id, info in ms2_data.items() if ms2_id in test_ms2_ids_set}
        
        # è·å–æ¶‰åŠçš„åˆ†å­ID
        train_molecule_ids = set(info['molecule_id'] for info in train_ms2_data.values())
        test_molecule_ids = set(info['molecule_id'] for info in test_ms2_data.values())
        
        # è¿‡æ»¤å…ƒæ•°æ®å’Œbiotextæ•°æ®
        train_meta_data = meta_data[meta_data.index.isin(train_molecule_ids)]
        test_meta_data = meta_data[meta_data.index.isin(test_molecule_ids)]
        
        train_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in train_molecule_ids}
        test_biotext_data = {mol_id: text for mol_id, text in biotext_data.items() if mol_id in test_molecule_ids}
        
        print(f"\nData filtering completed:")
        print(f"    Training: {len(train_ms2_data)} MS2 spectra from {len(train_molecule_ids)} molecules")
        print(f"    Test: {len(test_ms2_data)} MS2 spectra from {len(test_molecule_ids)} molecules")
        
        # --- 4. Create Dataset instances ---
        print("\nCreating training Dataset instance...")
        train_dataset = MS2BioTextDataset(
            ms2_data=train_ms2_data, 
            meta_data=train_meta_data, 
            biotext_data=train_biotext_data, 
            tokenizer=tokenizer, 
            use_paraphrase=use_paraphrase,
            word2idx=word2idx,     
            args=args,             
            split='train',         
            **kwargs
        )
        
        print("Creating test Dataset instance...")
        test_dataset = MS2BioTextDataset(
            ms2_data=test_ms2_data, 
            meta_data=test_meta_data, 
            biotext_data=test_biotext_data, 
            tokenizer=tokenizer, 
            use_paraphrase=False,
            word2idx=word2idx,  
            args=args,            
            split='test',         
            **kwargs
        )
        
        return train_dataset, test_dataset
    



# åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼Œimport è¯­å¥ä¹‹åï¼Œç±»å®šä¹‰ä¹‹å‰

def _augment_worker(batch_data):
    """
    å…¨å±€ worker å‡½æ•°ï¼Œç”¨äºæ•°æ®å¢å¼º
    batch_data: (batch, filter_threshold, noise_ratio, noise_intensity_range, augment_multiplier)
    """
    import numpy as np
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šä¸è¦å¯¼å…¥ MS2BioTextDatasetï¼Œç›´æ¥åœ¨è¿™é‡Œå®šä¹‰éœ€è¦çš„å‡½æ•°
    
    batch, filter_threshold, noise_ratio, noise_intensity_range, augment_multiplier = batch_data
    
    # å°† filter_low_intensity_peaks å’Œ add_noise_peaks çš„é€»è¾‘ç›´æ¥å¤åˆ¶åˆ°è¿™é‡Œ
    def filter_low_intensity_peaks(peaks, intensities, threshold):
        """è¿‡æ»¤ä½å¼ºåº¦å³°"""
        if not peaks or not intensities:
            return peaks, intensities
        
        max_intensity = max(intensities)
        if max_intensity == 0:
            return peaks, intensities
        
        filtered_peaks = []
        filtered_intensities = []
        for mz, intensity in zip(peaks, intensities):
            if intensity / max_intensity >= threshold:
                filtered_peaks.append(mz)
                filtered_intensities.append(intensity)
        
        return filtered_peaks, filtered_intensities
    
    def add_noise_peaks(peaks, intensities, noise_ratio, noise_intensity_range):
        """æ·»åŠ å™ªå£°å³°"""
        import random
        
        if not peaks:
            return peaks, intensities
        
        n_noise = int(len(peaks) * noise_ratio)
        if n_noise == 0:
            return peaks, intensities
        
        # è·å–m/zèŒƒå›´
        min_mz = min(peaks)
        max_mz = max(peaks)
        max_intensity = max(intensities) if intensities else 1.0
        
        # ç”Ÿæˆå™ªå£°å³°
        for _ in range(n_noise):
            # éšæœºm/zï¼ˆé¿å…ä¸ç°æœ‰å³°é‡å¤ï¼‰
            noise_mz = random.uniform(min_mz, max_mz)
            # éšæœºä½å¼ºåº¦
            noise_intensity = random.uniform(
                noise_intensity_range[0] * max_intensity,
                noise_intensity_range[1] * max_intensity
            )
            
            peaks.append(noise_mz)
            intensities.append(noise_intensity)
        
        # æŒ‰m/zæ’åº
        sorted_pairs = sorted(zip(peaks, intensities), key=lambda x: x[0])
        peaks = [p for p, _ in sorted_pairs]
        intensities = [i for _, i in sorted_pairs]
        
        return peaks, intensities
    
    # å¤„ç†é€»è¾‘
    result = {}
    for ms2_id, info in batch:
        molecule_id = info.get('molecule_id')
        peaks_original = info['mz'] if isinstance(info['mz'], list) else list(info['mz'])
        intensities_original = info['intensity'] if isinstance(info['intensity'], list) else list(info['intensity'])
        
        # è¿‡æ»¤
        if filter_threshold:
            peaks_original, intensities_original = filter_low_intensity_peaks(
                peaks_original, intensities_original, filter_threshold
            )
        
        # åŸå§‹ç‰ˆæœ¬
        result[ms2_id] = {
            'mz': peaks_original,
            'intensity': intensities_original,
            'molecule_id': molecule_id
        }
        
        # å¢å¼ºç‰ˆæœ¬
        for aug_idx in range(1, augment_multiplier):
            peaks_aug, intensities_aug = add_noise_peaks(
                peaks_original.copy(), intensities_original.copy(),
                noise_ratio,
                noise_intensity_range
            )
            result[f"{ms2_id}_aug{aug_idx}"] = {
                'mz': peaks_aug,
                'intensity': intensities_aug,
                'molecule_id': molecule_id
            }
    return result

def _preprocess_worker(batch_data):
    """
    å…¨å±€ worker å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹å¤„ç†
    batch_data: (batch, word2idx, meta_data_processed, maxlen, max_frag, min_peaks, precursor_mode, precursor_value)
    """
    import numpy as np
    import pandas as pd
    
    batch, word2idx, meta_data_processed, maxlen, max_frag, min_peaks, precursor_mode, precursor_value = batch_data
    
    result = {}
    stats = {'kept': 0, 'filtered': 0}
    
    for ms2_id, info in batch:
        # åŸºç¡€æ£€æŸ¥
        if not info.get('mz'):
            stats['filtered'] += 1
            continue
        
        # è½¬ä¸º numpy æ•°ç»„
        peaks = np.asarray(info['mz'], dtype=float)
        intensities = np.asarray(info['intensity'], dtype=float)
        molecule_id = info.get('molecule_id', None)
        
        # é•¿åº¦å¯¹é½æ£€æŸ¥
        if peaks.shape[0] != intensities.shape[0]:
            n = min(len(peaks), len(intensities))
            peaks = peaks[:n]
            intensities = intensities[:n]
        
        # æ–‡ä»¶åå¯¹åº”è¡Œç”¨äºææ€§åˆ¤æ–­
        specific_row = meta_data_processed.loc[meta_data_processed["file_name"] == ms2_id] if "file_name" in meta_data_processed.columns else pd.DataFrame()
        if specific_row.empty:
            if molecule_id is not None:
                if 'HMDB.ID' in meta_data_processed.columns:
                    specific_row = meta_data_processed.loc[meta_data_processed['HMDB.ID'] == molecule_id]
                else:
                    specific_row = meta_data_processed.loc[meta_data_processed.index == molecule_id]
        
        if specific_row.empty:
            stats['filtered'] += 1
            continue
        
        # åªä¿ç•™æ­£ç¦»å­
        pol = str(specific_row["Polarity"].values[0]).lower().strip() if "Polarity" in specific_row.columns else ""
        if pol != "positive":
            stats['filtered'] += 1
            continue
        
        # è·å–precursor
        if 'HMDB.ID' in meta_data_processed.columns and (molecule_id is not None):
            row = meta_data_processed.loc[meta_data_processed['HMDB.ID'] == molecule_id]
        else:
            row = meta_data_processed.loc[meta_data_processed.index == molecule_id]
        
        if row.empty or ('precursor_mass' not in row.columns):
            stats['filtered'] += 1
            continue
        
        try:
            precursor_val = float(row['precursor_mass'].values[0])
        except Exception:
            stats['filtered'] += 1
            continue
        
        # å‰ä½“èŒƒå›´ [10, 1000)
        if pd.isna(precursor_val) or (precursor_val < 10.0) or (precursor_val >= 1000.0):
            stats['filtered'] += 1
            continue
        
        precursor_val = min(precursor_val, 999.99)
        precursor_str = "%.2f" % precursor_val
        
        # è¿‡æ»¤å³°åˆ° [10, 1000)
        mask = (peaks >= 10.0) & (peaks < 1000.0) & np.isfinite(peaks) & np.isfinite(intensities)
        peaks = peaks[mask]
        intensities = intensities[mask]
        
        if peaks.size == 0:
            stats['filtered'] += 1
            continue
        
        # æŒ‰å¼ºåº¦é€‰ Top-K ç¢ç‰‡
        if peaks.size > max_frag:
            idx = np.argpartition(intensities, -max_frag)[-max_frag:]
            order = np.argsort(peaks[idx])
            idx = idx[order]
            peaks_sel = peaks[idx]
            intens_sel = intensities[idx]
        else:
            order = np.argsort(peaks)
            peaks_sel = peaks[order]
            intens_sel = intensities[order]
        
        # æ£€æŸ¥ min_peaks
        if peaks_sel.size < min_peaks:
            stats['filtered'] += 1
            continue
        
        # æ„å»º token åºåˆ—
        peaks_str = ["%.2f" % p for p in peaks_sel]
        try:
            token_ids = [word2idx[precursor_str]] + [word2idx[p] for p in peaks_str]
        except KeyError:
            stats['filtered'] += 1
            continue
        
        # â­ æ ¹æ® precursor_mode é€‰æ‹©å¤„ç†æ–¹å¼
        if precursor_mode == 'scale_fixed':
            # æ–¹æ¡ˆä¸€ï¼šç¼©æ”¾fragmentsåˆ°å›ºå®šå€¼precursor_valueï¼ˆå¦‚20000ï¼‰ï¼Œç„¶åprecursoræ·»åŠ 2
            if np.max(intens_sel) > 0:
                intens_sel = intens_sel / np.max(intens_sel) * precursor_value
            intens_seq = np.hstack((2.0, intens_sel))
            # æ•´ä½“å½’ä¸€åŒ–
            max_intensity = float(np.max(intens_seq))
            if max_intensity > 0:
                intens_seq = intens_seq / max_intensity
                
        elif precursor_mode == 'normalize_add':
            # æ–¹æ¡ˆäºŒï¼šå½’ä¸€åŒ–fragmentsåˆ°1ï¼Œæ·»åŠ precursor_valueï¼Œå†æ•´ä½“å½’ä¸€åŒ–
            if np.max(intens_sel) > 0:
                intens_sel = intens_sel / np.max(intens_sel)
            intens_seq = np.hstack((precursor_value, intens_sel))
            # æ•´ä½“å½’ä¸€åŒ–
            max_intensity = float(np.max(intens_seq))
            if max_intensity > 0:
                intens_seq = intens_seq / max_intensity
        
        else:
            # é»˜è®¤ï¼šåŸå§‹MSBERTæ–¹å¼
            intens_seq = np.hstack((2.0, intens_sel))
            max_intensity = float(np.max(intens_seq))
            if max_intensity > 0:
                intens_seq = intens_seq / max_intensity
        
        # Pad æˆ–æˆªæ–­åˆ° maxlen
        if len(token_ids) > maxlen:
            token_ids = token_ids[:maxlen]
            intens_seq = intens_seq[:maxlen]
        
        n_pad = maxlen - len(token_ids)
        if n_pad > 0:
            token_ids += [word2idx['[PAD]']] * n_pad
            intens_seq = np.hstack([intens_seq, np.zeros(n_pad, dtype=float)])
        
        result[ms2_id] = {
            'mz': token_ids,
            'intensity': intens_seq.tolist(),
            'molecule_id': molecule_id
        }
        stats['kept'] += 1
    
    return result, stats




