{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d330104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vibe_widget as vw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701d6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a4e5f450cf43cb933044264f1877c3",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "VibeWidget(description=' Create a Doodle jump game environment, allow for the user to play using their arrow k..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd60324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: score=8025 epsilon=0.99\n",
      "Ep 51: score=8025 epsilon=0.77\n",
      "Ep 101: score=8025 epsilon=0.60\n",
      "Ep 151: score=8025 epsilon=0.47\n",
      "Ep 201: score=8025 epsilon=0.37\n",
      "Ep 251: score=8025 epsilon=0.28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRLAgent:\n",
    "    \"\"\"\n",
    "    A simple Q-learning agent for Doodle Jump-like environments.\n",
    "    The agent can use 'left' or 'right' arrow keys, rewarded by highest score (y position).\n",
    "\n",
    "    State: (vertical velocity, horizontal position binned, vertical position binned)\n",
    "    Actions: 0 = do nothing, 1 = left, 2 = right\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=10, lr=0.05, gamma=0.98, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.1):\n",
    "        self.n_bins = n_bins\n",
    "        self.q_table = np.zeros((n_bins, n_bins, n_bins, 3))  # (vel_y, x, y, action)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def discretize(self, obs, obs_space):\n",
    "        # obs: [vel_y, x, y], obs_space: [(min, max), ...]\n",
    "        bins = []\n",
    "        for i in range(3):\n",
    "            low, high = obs_space[i]\n",
    "            v = obs[i]\n",
    "            binned = int((float(v) - low) / (high - low) * (self.n_bins - 1))\n",
    "            binned = max(0, min(self.n_bins - 1, binned))\n",
    "            bins.append(binned)\n",
    "        return tuple(bins)\n",
    "\n",
    "    def select_action(self, state_bins):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(3)\n",
    "        return np.argmax(self.q_table[state_bins])\n",
    "\n",
    "    def update(self, prev_state, prev_action, reward, next_state, done):\n",
    "        # Q-learning update\n",
    "        q_sa = self.q_table[prev_state][prev_action]\n",
    "        best_next = np.max(self.q_table[next_state])\n",
    "        new_q = q_sa + self.lr * (reward + self.gamma * best_next * (not done) - q_sa)\n",
    "        self.q_table[prev_state][prev_action] = new_q\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Minimal stub for a Doodle Jump-like environment\n",
    "class DoodleJumpEnvStub:\n",
    "    \"\"\"\n",
    "    Emulates a Doodle Jump-like API for RL training.\n",
    "    Provide your own environment with step(action) and reset(), or adapt this stub.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Assume y pos [0,600], x pos [0,400], vel_y [-20, 20]\n",
    "        self.obs_space = [(-20, 20), (0, 400), (0, 600)]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_y = 0\n",
    "        self.player_x = 200\n",
    "        self.player_vely = 0\n",
    "        self.score = 0\n",
    "        return [self.player_vely, self.player_x, self.player_y]\n",
    "\n",
    "    def step(self, action):\n",
    "        # action: 0=none, 1=left, 2=right\n",
    "        if action == 1:\n",
    "            self.player_x -= 20\n",
    "        elif action == 2:\n",
    "            self.player_x += 20\n",
    "        self.player_x = np.clip(self.player_x, 0, 400)\n",
    "        # Simulate upward jump if landed on a platform (randomly every 40px for demo)\n",
    "        if self.player_y % 40 == 0:\n",
    "            self.player_vely = 15\n",
    "        else:\n",
    "            self.player_vely -= 1  # gravity\n",
    "        self.player_y += self.player_vely\n",
    "        self.player_y = max(self.player_y, 0)\n",
    "        done = self.player_y == 0 and self.player_vely <= 0\n",
    "        self.score = max(self.score, self.player_y)\n",
    "        obs = [self.player_vely, self.player_x, self.player_y]\n",
    "        reward = self.score  # reward is max y (score)\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# Training loop\n",
    "def train(agent, env, episodes=300):\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        state = agent.discretize(obs, env.obs_space)\n",
    "        total_reward = 0\n",
    "        for t in range(1000):\n",
    "            action = agent.select_action(state)\n",
    "            obs2, reward, done, _ = env.step(action)\n",
    "            state2 = agent.discretize(obs2, env.obs_space)\n",
    "            agent.update(state, action, reward, state2, done)\n",
    "            state = state2\n",
    "            total_reward = reward\n",
    "            if done:\n",
    "                break\n",
    "        agent.decay_epsilon()\n",
    "        if (episode) % 50 == 0:\n",
    "            print(f\"Ep {episode+1}: score={total_reward} epsilon={agent.epsilon:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "agent = SimpleRLAgent()\n",
    "env = DoodleJumpEnvStub()\n",
    "train(agent, env, episodes=300)\n",
    "\n",
    "doodle = vw.create(\" Create a Doodle jump game environment, allow for the user to play using their arrow keys.\",inputs=vw.inputs(agent, env,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doodle = vw.create(\" Create a Doodle jump game environment, allow for the user to play using their arrow keys.\",inputs=vw.inputs(agent, env,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
