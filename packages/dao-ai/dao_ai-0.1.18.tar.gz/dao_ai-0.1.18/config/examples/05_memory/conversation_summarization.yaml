# yaml-language-server: $schema=../../../schemas/model_config_schema.json

# =============================================================================
# Conversation Summarization Example
# =============================================================================
# This configuration demonstrates conversation summarization using LangChain's
# SummarizationMiddleware with Lakehouse (PostgreSQL) persistence.
#
# Conversation summarization automatically:
# - Summarizes older messages when thresholds are exceeded
# - Replaces them with a summary message in state (permanently)
# - Keeps recent messages intact for context
#
# This is useful for:
# - Long-running conversations that exceed token limits
# - Reducing context window usage while preserving key information
# - Maintaining conversation continuity across sessions

# =============================================================================
# SCHEMAS
# =============================================================================
schemas:
  demo_schema: &demo_schema
    catalog_name: retail_consumer_goods                             # Unity Catalog name
    schema_name: hardware_store                           # Schema within the catalog

# =============================================================================
# RESOURCES
# =============================================================================
resources:
  # ---------------------------------------------------------------------------
  # LANGUAGE MODELS
  # ---------------------------------------------------------------------------
  llms:
    # Primary LLM for conversation
    default_llm: &default_llm
      name: databricks-claude-sonnet-4
      temperature: 0.7                             # Moderate temperature for natural conversation
      max_tokens: 4096

    # Fast LLM for summarization (use smaller/faster model for efficiency)
    summarization_llm: &summarization_llm
      name: databricks-meta-llama-3-1-8b-instruct
      temperature: 0.1                             # Low temperature for consistent summaries
      max_tokens: 512

    # Embedding model for semantic memory (optional, for store)
    embedding_model: &embedding_model
      name: databricks-gte-large-en
      on_behalf_of_user: False

  # ---------------------------------------------------------------------------
  # DATABASES (Lakehouse Integration)
  # ---------------------------------------------------------------------------
  # PostgreSQL database for conversation persistence
  # This enables conversation state to be saved and restored across sessions
  databases:
    lakehouse_database: &lakehouse_database
      name: "Conversation Summarization Database"
      instance_name: "retail-consumer-goods"
      description: "PostgreSQL database for conversation checkpoints and memory"
      # Authentication is automatically provided via instance_name (Databricks Lakebase)

# =============================================================================
# MEMORY CONFIGURATION
# =============================================================================
# Configure persistent storage for conversations with Lakehouse integration
memory: &memory
  # Conversation checkpointing for state persistence
  checkpointer: 
    name: conversation_checkpointer
    database: *lakehouse_database                  # Lakehouse/PostgreSQL persistence (type inferred from presence)

  # Long-term memory store (optional, for semantic memory)
  store: 
    name: conversation_store
    embedding_model: *embedding_model
    dims: 1024
    database: *lakehouse_database                  # Type inferred from presence
    namespace: "{user_id}"                         # Namespace per user

# =============================================================================
# AGENTS
# =============================================================================
agents:
  # Simple conversational agent with summarization
  conversational_agent: &conversational_agent
    name: conversational_agent
    description: "A friendly conversational AI assistant with long-term memory"
    model: *default_llm
    # No tools - this is a simple conversation example
    tools: []
    # No guardrails - keeping it simple
    guardrails: []
    prompt: |
      You are a friendly and helpful AI assistant engaged in natural conversation.
      
      Your goal is to:
      - Have engaging, natural conversations with users
      - Remember context from earlier in the conversation
      - Provide helpful and thoughtful responses
      - Ask follow-up questions to better understand user needs
      
      Be conversational, warm, and personable. Feel free to share relevant 
      information and ask clarifying questions when appropriate.
      
      If you notice a conversation summary at the beginning of messages, 
      use that context to maintain continuity with earlier discussion topics.

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================
app:
  name: conversation_summarization_dao
  description: "Demo of conversation summarization with Lakehouse persistence"
  log_level: DEBUG
  
  registered_model:
    schema: *demo_schema
    name: conversation_summarization_demo
  endpoint_name: conversation_summarization_demo
  
  tags:
    feature: conversation_summarization
    persistence: lakehouse
  
  permissions:
    - principals: [users]
      entitlements:
        - CAN_QUERY

  agents:
    - *conversational_agent
  
  orchestration:
    memory: *memory
    swarm:
      default_agent: *main_agent

  # ---------------------------------------------------------------------------
  # CHAT HISTORY / SUMMARIZATION CONFIGURATION
  # ---------------------------------------------------------------------------
  # This is where conversation summarization is configured
  chat_history:
    model: *summarization_llm                      # Model used for generating summaries
    max_tokens: 2048                               # Max tokens to keep after summarization
    max_tokens_before_summary: 6000                # Trigger summarization at this threshold
    # Alternative: max_messages_before_summary: 20 # Or trigger after N messages

  input_example:
    messages:
      - role: user
        content: Tell me about yourself and what you can help me with.
    custom_inputs:
      configurable:
        # conversation_id is auto-generated as UUID if not provided
        user_id: demo_user
      session: {}
